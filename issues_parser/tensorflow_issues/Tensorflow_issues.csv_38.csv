Issue Number,Issue Title,Issue Body
31116,Cannot open include file: 'absl/strings/string_view.h,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31115,Convert to TFLite Unexpected value for attribute 'data_format'. Expected 'NHWC',"I'm trying to convert a frozen Tensorflow graph. See model [here](https://drive.google.com/open?id=15g1aNF3WdJ7Yw8GRPVEWzBbkaZPIvCGC). I found a lot related bug reports on that issue but non of them was actually solved: [#30411](https://github.com/tensorflow/tensorflow/issues/30411), [#7967](https://github.com/tensorflow/tensorflow/issues/7967),[#24491](https://github.com/tensorflow/tensorflow/issues/24491). 

**System information**
- OS Platform and Distribution: Linux Ubuntu 19.04 64bit
- TensorFlow installed from (source or binary): binary  (CPU)
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.7.3

**Describe the current behavior**
Running in Jupyter I get following error:
```
ConverterError: TOCO failed. See console for info.
2019-07-28 21:08:26.912035: F tensorflow/lite/toco/import_tensorflow.cc:2619] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'
Fatal Python error: Aborted

Current thread 0x00007fc8ee681740 (most recent call first):
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/paul/.local/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/paul/.local/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/paul/anaconda3/envs/openvino/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```

Running over shell:
```
Traceback (most recent call last):
  File ""tensorflow_issue_tflite.py"", line 10, in <module>
    tflite_model = converter.convert()
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 898, in convert
    **converter_kwargs)
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-07-28 21:35:22.220584: F tensorflow/lite/toco/import_tensorflow.cc:2619] Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'
Fatal Python error: Aborted

Current thread 0x00007fae1bce0740 (most recent call first):
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/paul/.local/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/paul/.local/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/paul/anaconda3/envs/openvino/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```

**Code to reproduce the issue**

```python
import tensorflow as tf

graph_def_file = ""model_lcnn_29v2_cpu.pb""

input_arrays = [""0""]
output_arrays = [""MatMul""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""model_lcnn_29v2.tflite"", ""wb"").write(tflite_model)

```
[](url)"
31114,TFLite No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int),"**System information**
- OS Platform and Distribution: Android 5.1.1, API 22
- Mobile device: Xiaomi Redmi 3
- TensorFlow installed from: official binary
- TensorFlow version : tensorflow-lite:1.14.0

**Describe the current behavior**
Tensorflow-lite 1.13.1 works fine on all devices I tested. Whereas tensorflow-lite 1.14.0 is broken for Xiaomi Redmi 3 (Android 5.1.1, API 22), other devices are ok.
I get a runtime error when `Interpreter` is created.

**Describe the expected behavior**
No error.

**Code to reproduce the issue**
```
interpreter = new Interpreter(tfliteModel, null);
```

**Other info / logs**
```
W/linker: /data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so: unused DT entry: type 0x6ffffffe arg 0x2020
    /data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so: unused DT entry: type 0x6fffffff arg 0x3
E/art: dlopen(""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so"", RTLD_LAZY) failed: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so""...
W/System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so""...
W/linker: /data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so: unused DT entry: type 0x6ffffffe arg 0x2020
    /data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so: unused DT entry: type 0x6fffffff arg 0x3
E/art: dlopen(""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so"", RTLD_LAZY) failed: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so""...
W/System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""/data/app/eu.yesse.readerdemo.debug-2/lib/arm64/libtensorflowlite_jni.so""...
E/art: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
D/AndroidRuntime: Shutting down VM
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: eu.yesse.readerdemo.debug, PID: 12710
    java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
        at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at eu.yesse.reader.commons.internal.detector.BlockingCornersClassSingleDetector.<init>(BlockingCornersClassSingleDetector.java:76)
        at eu.yesse.reader.commons.internal.detector.BlockingCornersClassMultiDetector.<init>(BlockingCornersClassMultiDetector.java:25)
        at eu.yesse.reader.commons.shared.detector.AsyncCornersClassMultiDetectorImpl.<init>(AsyncCornersClassMultiDetectorImpl.java:36)
        at eu.yesse.reader.tempregdoc.internal.TempRegDocReaderManager.createDetector(TempRegDocReaderManager.java:79)
        at eu.yesse.reader.tempregdoc.internal.TempRegDocReaderManager.<init>(TempRegDocReaderManager.java:48)
        at eu.yesse.reader.TempRegDocReader.getReader(TempRegDocReader.java:20)
        at eu.yesse.readerdemo.activities.TempRegDocActivity.onCreate(TempRegDocActivity.java:24)
        at android.app.Activity.performCreate(Activity.java:6093)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1106)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2295)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2404)
        at android.app.ActivityThread.access$900(ActivityThread.java:154)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1315)
        at android.os.Handler.dispatchMessage(Handler.java:102)
        at android.os.Looper.loop(Looper.java:135)
        at android.app.ActivityThread.main(ActivityThread.java:5296)
        at java.lang.reflect.Method.invoke(Native Method)
        at java.lang.reflect.Method.invoke(Method.java:372)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:912)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:707)
```
"
31113,MAE and MAPE and ACC makes no sense,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Debian 9 VM on GCP with TPU v3-8
Tensorflow 1.13.1 Keras 2.2.4 

**Describe the current behavior**

training a model using dataset scaled to range 0..1 I am getting improbable metrics consistently

```
loss: 0.3452 - mean_absolute_error: 76.0366 - mean_absolute_percentage_error: 76.0319 - acc: 76.0429 
val_loss: 0.0146 - val_mean_absolute_error: 7.6565 - val_mean_absolute_percentage_error: 7.6635 - val_acc: 7.6594
```

I don't understand these metrics.

1. If accuracy is about the same as MAE then what does it mean? its not accurate at all?

2. If MAPE about the same as MAE - how percentage can be equal to absolute value of error?

3. The data is scaled to 0..1 range. How absolute error can be 76?

**Describe the expected behavior**

meaningful metrics

**Code to reproduce the issue**

```
model = tf.keras.Sequential()
model.add(layers.Conv1D(filters=128, kernel_size=2, activation=activation, input_shape=(window_size // subseq_size, subseq_size)))
model.add(layers.Conv1D(filters=64, kernel_size=2, activation=activation))
model.add(layers.Conv1D(filters=32, kernel_size=2, activation=activation))
model.add(layers.MaxPooling1D(pool_size=2))
model.add(layers.TimeDistributed(Flatten()))
model.add(layers.LSTM(500))
model.add(layers.Dense(100))
model.add(layers.Dense(1))

opt = tf.train.AdamOptimizer(learning_rate)

tpu_model = tf.contrib.tpu.keras_to_tpu_model(model, 
        strategy=tf.contrib.tpu.TPUDistributionStrategy(
            tf.contrib.cluster_resolver.TPUClusterResolver(tpu = [TPU_ADDRESS1])))

tpu_model.compile(optimizer=opt, loss='mse', metrics=['mae', 'mape', 'acc'])

H = tpu_model.fit(X, y, validation_split=0.15, epochs=epochs_n, batch_size = window_size // subseq_size)
```
"
31112,NotFoundError: No registered 'ReadFile' OpKernel for GPU devices compatible with node {{node ReadFile}},"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): installed via pip
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- CUDA/cuDNN version: 
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory: NVIDIA GeForce GTX 1060 3GB

**Describe the current behavior**

I am running the following code (it runs well on Google Colab GPU):
```python
def build_dataset(boxes_img_dataframe, data_directory, augment=True, shuffle=False):
    filenames_ds = tf.data.Dataset.from_tensor_slices(boxes_img_dataframe['image_name'].apply(lambda path: os.path.join(data_directory, path)))
    images_ds    = filenames_ds.map(
        lambda path: (tf.image.decode_jpeg(tf.io.read_file(path)) / 255) * 2 - 1
    )
    x1_ds        = tf.data.Dataset.from_tensor_slices(boxes_img_dataframe['x_1'])
    x2_ds        = tf.data.Dataset.from_tensor_slices(boxes_img_dataframe['x_2'])
    y1_ds        = tf.data.Dataset.from_tensor_slices(boxes_img_dataframe['y_1'])
    y2_ds        = tf.data.Dataset.from_tensor_slices(boxes_img_dataframe['y_2'])
    tmp_ds       = tf.data.Dataset.zip((images_ds, x1_ds, x2_ds, y1_ds, y2_ds))
    #""""""
    images_ds    = tmp_ds.map(
        lambda image, x1, x2, y1, y2: tf.image.resize_images(
            tf.image.crop_to_bounding_box(
                image,
                tf.cast(x1, tf.int32),
                tf.cast(y1, tf.int32),
                tf.cast(x2 - x1, tf.int32),
                tf.cast(y2 - y1, tf.int32)
            ),
            (96, 96)
        )
    )
    if augment:
        images_color_augmented_hue = images_ds.map(
            lambda image: tf.image.random_hue(image, 0.08)
        )
        images_color_augmented_sat = images_ds.map(
            lambda image: tf.image.random_saturation(image, 0.6, 1.6)
        )
        images_color_augmented_bri = images_ds.map(
            lambda image: tf.image.random_brightness(image, 0.05)
        )
        images_color_augmented_con = images_ds.map(
            lambda image: tf.image.random_contrast(image, 0.7, 1.3)
        )
        images_augmented_flipped   = images_ds.map(
            lambda image: tf.image.flip_left_right(image)
        )
        images_augmented_rotation  = images_ds.map(
            lambda image: tf.image.rot90(image, tf.random_uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
        )
        images_ds = images_ds.concatenate(images_color_augmented_hue)
        images_ds = images_ds.concatenate(images_color_augmented_sat)
        images_ds = images_ds.concatenate(images_color_augmented_bri)
        images_ds = images_ds.concatenate(images_color_augmented_con)
        images_ds = images_ds.concatenate(images_augmented_flipped)
        images_ds = images_ds.concatenate(images_augmented_rotation)
        images_ds = images_ds.map(lambda image: tf.clip_by_value(image, -1, 1))
    ds = tf.data.Dataset.zip((images_ds, images_ds))
    ds = ds.apply(tf.data.experimental.ignore_errors())
    if shuffle:
        ds = ds.shuffle(1000)
    return ds
```

This basically builds my dataset, and I call it in the model like that:

```python
with tf.device('/gpu:0'):
        autoencoder.fit(
            dataset.batch(32),
            epochs=epochs,
            steps_per_epoch=steps,
            shuffle=True,
        )
```

So actually pretty straighforward I guess.

And when it runs, I get the following error:

```
WARNING: Logging before flag parsing goes to stderr.
W0728 11:28:51.089698 10264 deprecation_wrapper.py:119] From main.py:80: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

W0728 11:28:51.129585 10264 deprecation.py:506] From C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-07-28 11:28:52.388036: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-28 11:28:52.403659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-28 11:28:52.546637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
2019-07-28 11:28:52.551575: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-28 11:28:52.554530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-28 11:28:53.229349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 11:28:53.233344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-28 11:28:53.235723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-28 11:28:53.237899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2112 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-07-28 11:29:01.339772: E tensorflow/core/common_runtime/executor.cc:641] Executor failed to create kernel. Not found: No registered 'ReadFile' OpKernel for GPU devices compatible with node {{node ReadFile}}
        .  Registered:  device='CPU'

         [[ReadFile]]
2019-07-28 11:29:01.376815: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at iterator_ops.cc:601 : Not found: No registered 'ReadFile' OpKernel for GPU devices compatible with node {{node ReadFile}}
        .  Registered:  device='CPU'

         [[ReadFile]]
Traceback (most recent call last):
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1356, in _do_call
    return fn(*args)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ReadFile' OpKernel for GPU devices compatible with node {{node ReadFile}}
        .  Registered:  device='CPU'

         [[ReadFile]]
         [[MakeIterator]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 144, in <module>
    main()
  File ""main.py"", line 140, in main
    model.train_model(latent_model, dataset, epochs, steps_per_epoch)
  File ""C:\Users\Julien\Documents\Travail\NarcisseTechnologies\narcisse.latent\trainer\model.py"", line 201, in train_model
    shuffle=True,
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 780, in fit
    steps_name='steps_per_epoch')
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 142, in model_iteration
    input_iterator = _get_iterator(inputs, model._distribution_strategy)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 517, in _get_iterator
    return training_utils.get_iterator(inputs)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 1315, in get_iterator
    initialize_iterator(iterator)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 1322, in initialize_iterator
    K.get_session((init_op,)).run(init_op)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 950, in run
    run_metadata_ptr)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1350, in _do_run
    run_metadata)
  File ""C:\Users\Julien\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ReadFile' OpKernel for GPU devices compatible with node {{node ReadFile}}
        .  Registered:  device='CPU'

         [[ReadFile]]
         [[MakeIterator]]
```

It seems that the tf.io.read_file operation is running on the GPU, and my GPU does not support this kind of operation.

This is what I understand from the error.

What can I do to fix this please? Is it a bug in the driver, a compatibility problem... ?"
31111,absl,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31110,keras model fit() crash with large batch_size and 0 validation_split,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-gpu 1.13.1
- Python version: 3.6.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.4.1
- GPU model and memory: GTX 1080Ti, 10G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Raise an error when fitting tf.keras model when the training dataset size less then batch_size and validation_split is 0.0.
If using a batch_size less then dataset size or setting validation_split, the fitting is good.
Using original keras, fitting is good in any case.
Error is :
```
Traceback (most recent call last):
  File ""crf_tf.py"", line 123, in <module>
    model.fit(x, y, batch_size=16, epochs=50, validation_split=0.0)
  File ""/opt/userhome/ichongxiang/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 880, in fit
    validation_steps=validation_steps)
  File ""/opt/userhome/ichongxiang/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 329, in model_iteration
    batch_outs = f(ins_batch)
  File ""/opt/userhome/ichongxiang/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3076, in __call__
    run_metadata=self.run_metadata)
  File ""/opt/userhome/ichongxiang/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/opt/userhome/ichongxiang/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got 2
         [[{{node metrics/crf_accuracy/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_bool_Squeeze}}]]
         [[{{node crf/cond/Maximum}}]]
```
**Describe the expected behavior**
Fit successfully.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
# import keras
# from keras import backend as K
import numpy as np


class CRF(keras.layers.Layer):

    def __init__(self, num_tags, **kwargs):
        super(CRF, self).__init__(**kwargs)
        self.num_tags = num_tags
        self.input_spec = keras.layers.InputSpec(min_ndim=3)
        self.supports_masking = True

    def get_config(self):
        config = {
            'num_tags': self.num_tags,
        }
        base_config = super(CRF, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def build(self, input_shape):
        assert len(input_shape) == 3
        if input_shape[-1] is None:
            raise ValueError('The last dimension of the inputs to `CRF` '
                             'should be defined. Found `None`.')
        if input_shape[-1] != self.num_tags:
            raise ValueError('The last dimension of the input shape must be equal to output'
                             ' shape. Use a linear layer if needed.')
        self.transitions = self.add_weight(name='transitions',
                                           shape=[self.num_tags,
                                                  self.num_tags],
                                           initializer=""glorot_uniform"",
                                           trainable=True)
        self.built = True

    def call(self, inputs, mask=None):
        seq_lens = get_seq_lens(inputs, mask)
        viterbi_sequence, _ = tf.contrib.crf.crf_decode(inputs,
                                                        self.transitions,
                                                        seq_lens)
        outputs = K.one_hot(viterbi_sequence, self.num_tags)
        return K.in_train_phase(inputs, outputs)

    def compute_output_shape(self, input_shape):
        return input_shape[:2] + (self.num_tags,)

    def compute_mask(self, inputs, mask=None):
        if mask is not None:
            return K.any(mask, axis=1)
        return mask


def get_seq_lens(inputs, mask=None):
    if mask is not None:
        return K.sum(K.cast(mask, dtype='int32'), axis=-1)
    else:
        shape = K.int_shape(inputs)
        return K.ones(shape[:-1], dtype='int32') * shape[-1]


def crf_loss(y_true, y_pred):
    crf, idx = y_pred._keras_history[:2]
    inputs = crf.get_input_at(idx)
    mask = crf.get_input_mask_at(idx)
    seq_lens = get_seq_lens(inputs, mask)
    y_true = K.cast(K.argmax(y_true, axis=-1), dtype='int32')
    log_likelihood, crf.transitions = \
        tf.contrib.crf.crf_log_likelihood(y_pred,
                                          y_true,
                                          seq_lens,
                                          transition_params=crf.transitions)
    return K.mean(-log_likelihood)


def crf_accuracy(y_true, y_pred):
    crf, idx = y_pred._keras_history[:2]
    inputs = crf.get_input_at(idx)
    mask = crf.get_input_mask_at(idx)
    seq_lens = get_seq_lens(inputs, mask)
    viterbi_sequence, _ = tf.contrib.crf.crf_decode(inputs,
                                                    crf.transitions,
                                                    seq_lens)
    y_true = K.cast(K.argmax(y_true, -1), dtype='int32')
    judge = K.cast(K.equal(viterbi_sequence, y_true), K.floatx())
    if mask is None:
        return K.mean(judge)
    else:
        mask = K.cast(mask, K.floatx())
        return K.sum(judge * mask) / K.sum(mask)


num_words = 20
num_features = 100
num_tags = 5

inputs = keras.layers.Input(shape=(None,))
embedding = keras.layers.Embedding(10, num_features, mask_zero=True)(inputs)
scores = keras.layers.TimeDistributed(keras.layers.Dense(num_tags))(embedding)
crf = CRF(num_tags)
outputs = crf(scores)
model = keras.models.Model(inputs, outputs)

model.summary()

x = np.array([[1, 2, 3, 4, 0, 0], [4, 5, 6, 0, 0, 0]])
y = np.array([[1, 3, 4, 2, 0, 0], [2, 1, 3, 0, 0, 0]])
y = np.eye(num_tags)[y]

print(x)
print(x.shape)
print(y)
print(y.shape)

model.compile(optimizer=""adam"",
              loss=crf_loss,
              metrics=[crf_accuracy])

model.fit(x, y, batch_size=16, epochs=50, validation_split=0.0)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31109,dilated tf.keras.layers.Conv2D can't estimate output shape,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

when creating tf.keras.Model with functional api, dilated convolution of tf.keras.layers can't estimate the output shape.

**Describe the expected behavior**

the dilated convolution can estimate the output shape as convolution op does.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf

input= tf.keras.Input([100,100,3])
results = tf.keras.layers.Conv2D(filters = 10, kernel_size = (3,3), padding = 'same', dilation_rate = 2)(input)
print(results.shape)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31108,Bulld tensorflow from source for Raspberry PI (Python 3.7),"I successfully have built tensorflow 2.0.0b1 for raspberry PI using the guideline
https://www.tensorflow.org/install/source_rpi and command

sudo CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7"" tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh

However, generated wheel file is called tensorflow-2.0.0b1-cp34-none-linux_armv7l.whl so I suppose it is for python 3.4

Is there a way to tell installer to use python 3.7 during wheel generation?


"
31107,Model failed to serialize as JSON. Ignoring 'Not JSON Serializable:',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Ubuntu 18.04:
- TensorFlow installed from binary:
- TensorFlow version (1.14):
- Python version: (3.6)
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080Ti


**Describe the current behavior**
```
W0728 12:29:27.513279 140257532127040 summary_ops_v2.py:1110] Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', b'\nNreplica_1/Fast_SCNN/tf_op_layer_Relu/replica_0/Fast_SCNN/tf_op_layer_Relu/Relu\x12\x04Relu\x1a\x1ebatch_normalization/cond/Merge*\x07\n\x01T\x12\x020\x01')
```
**Describe the expected behavior**
Running without issue


**Code to reproduce the issue**
```
import tensorflow as tf

def conv_block(inputs, conv_type, filter_count, kernel_size, strides, padding='same', relu=True):

  if(conv_type == 'ds'):
    x = tf.keras.layers.SeparableConv2D(filter_count, kernel_size, padding=padding, strides = strides)(inputs)
  else:
    x = tf.keras.layers.Conv2D(filter_count, kernel_size, padding=padding, strides = strides)(inputs)

  x = tf.keras.layers.BatchNormalization()(x)

  if (relu):
    x = tf.keras.activations.relu(x)

  return x


def _res_bottleneck(inputs, filters, kernel, t, s, r=False):


    tchannel = tf.keras.backend.int_shape(inputs)[-1] * t

    x = conv_block(inputs, 'conv', tchannel, (1, 1), strides=(1, 1))

    x = tf.keras.layers.DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.activations.relu(x)

    x = conv_block(x, 'conv', filters, (1, 1), strides=(1, 1), padding='same', relu=False)

    if r:
        x = tf.keras.layers.add([x, inputs])
    return x


def bottleneck_block(inputs, filters, kernel, t, strides, n):
    x = _res_bottleneck(inputs, filters, kernel, t, strides)

    for i in range(1, n):
        x = _res_bottleneck(x, filters, kernel, t, 1, True)

    return x

def pyramid_pooling_block(input_tensor, bin_sizes,input_height, input_width):
    # concat_list = []
    width = input_width//32
    height = input_height//32
    # x = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, (height, width)))(input_tensor)
    concat_list=[input_tensor]
    for bin_size in bin_sizes:
        x = tf.keras.layers.AveragePooling2D(pool_size=(height // bin_size, width // bin_size),
                                             strides=(height // bin_size, width // bin_size))(input_tensor)
        x = tf.keras.layers.Conv2D(128, 3, 2, padding='same')(x)
        x = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, (height, width)))(x)

        concat_list.append(x)
    
    return tf.keras.layers.concatenate(concat_list)



def buildFastScnn(input_height, input_width, input_channel, n_classes, weights_path=None):

    input_layer = tf.keras.layers.Input(shape=(input_height, input_width, input_channel), name ='input_layer')

    lds_layer = conv_block(input_layer, 'conv', 32, (3, 3), strides = (2, 2))
    lds_layer = conv_block(lds_layer, 'ds', 48, (3, 3), strides = (2, 2))
    lds_layer = conv_block(lds_layer, 'ds', 64, (3, 3), strides = (2, 2))

 
    gfe_layer = bottleneck_block(lds_layer, 64, (3, 3), t=6, strides=2, n=3)
    gfe_layer = bottleneck_block(gfe_layer, 96, (3, 3), t=6, strides=2, n=3)
    gfe_layer = bottleneck_block(gfe_layer, 128, (3, 3), t=6, strides=1, n=3)
    gfe_layer = pyramid_pooling_block(gfe_layer, [2,4,6,8],input_height, input_width)


    ff_layer1 = conv_block(lds_layer, 'conv', 128, (1,1), padding='same', strides= (1,1), relu=False)

    ff_layer2 = tf.keras.layers.UpSampling2D((4, 4))(gfe_layer)
    ff_layer2 = tf.keras.layers.DepthwiseConv2D((3,3), strides=(1, 1), depth_multiplier=1, padding='same')(ff_layer2)
    ff_layer2 = tf.keras.layers.BatchNormalization()(ff_layer2)
    ff_layer2 = tf.keras.activations.relu(ff_layer2)
    ff_layer2 = tf.keras.layers.Conv2D(128, 1, 1, padding='same', activation=None)(ff_layer2)

    ff_final = tf.keras.layers.add([ff_layer1, ff_layer2])
    ff_final = tf.keras.layers.BatchNormalization()(ff_final)
    ff_final = tf.keras.activations.relu(ff_final)


    classifier = tf.keras.layers.SeparableConv2D(128, (3, 3), padding='same', strides = (1, 1), name = 'DSConv1_classifier')(ff_final)
    classifier = tf.keras.layers.BatchNormalization()(classifier)
    classifier = tf.keras.activations.relu(classifier)

    classifier = tf.keras.layers.SeparableConv2D(128, (3, 3), padding='same', strides = (1, 1), name = 'DSConv2_classifier')(classifier)
    classifier = tf.keras.layers.BatchNormalization()(classifier)
    classifier = tf.keras.activations.relu(classifier)


    classifier = tf.keras.layers.Conv2D(n_classes, (1, 1), padding='same', strides=(1, 1),name = 'Conv2_classifier')(classifier)
    classifier = tf.keras.layers.BatchNormalization()(classifier)
    classifier = tf.keras.activations.relu(classifier)
    
    classifier = tf.keras.layers.Dropout(0.3)(classifier)
    classifier = tf.keras.layers.UpSampling2D((8, 8))(classifier)
    classifier = tf.keras.activations.softmax(classifier)
    

    fast_scnn = tf.keras.Model(inputs = input_layer , outputs = classifier, name = 'Fast_SCNN')
    if weights_path is not None:
        fast_scnn.load_weights(weights_path, by_name=True)
    return fast_scnn
    
    
net=buildFastScnn(800, 1600, 3, 20, weights_path=None)
checkpoint = ModelCheckpoint('output_il/weights.{epoch:03d}-{categorical_accuracy:.3f}.h5',
                             monitor='categorical_accuracy',
                             mode='max',
                             verbose=1,save_weights_only=True)
tensorboard = TensorBoard(batch_size=opt.batch_size)
optimizer = tf.keras.optimizers.SGD(momentum=0.9, lr=0.045)
net.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])
net.fit_generator(train_generator, steps_per_epoch=None, epochs=opt.n_epochs, callbacks=[checkpoint, tensorboard],
                    validation_data=val_generator, validation_steps=None, workers=12,
                    use_multiprocessing=True, shuffle=True, max_queue_size=12, initial_epoch=opt.epoch)
```
**Other info / logs**
checked these
[1](https://stackoverflow.com/questions/56804384/tf-keras-model-save-throws-not-json-serializable-when-dtype-of-input-is-uint8)
[2](https://github.com/keras-team/keras/issues/9342)
[3](https://github.com/keras-team/keras/issues/11110)
[4](https://github.com/tensorflow/tensorflow/issues/27112)
"
31104,"W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Invalid argument: Input shape axis 0 must equal 4, got shape [3] ","Epoch: [93/10] step: [186/2] time: 0.2719242572784424s, mse: 0.03679807484149933
2019-07-28 12:23:43.405599: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Invalid argument: Input shape axis 0 must equal 4, got shape [3]
[[{{node crop_to_bounding_box_1/unstack}}]]
Traceback (most recent call last):
File ""train.py"", line 380, in
train()
File ""train.py"", line 178, in train
for step, (lr_patchs, hr_patchs) in enumerate(train_ds):
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 556, in next
return self.next()
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 585, in next
return self._next_internal()
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 577, in _next_internal
output_shapes=self._flat_output_shapes)
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1954, in iterator_get_next_sync
_six.raise_from(_core._status_to_exception(e.code, message), None)
File """", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input shape axis 0 must equal 4, got shape [3]
[[{{node crop_to_bounding_box_1/unstack}}]] [Op:IteratorGetNextSync]
2019-07-28 12:23:43.507127: W tensorflow/core/kernels/data/generator_dataset_op.cc:79] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
[[{{node PyFunc}}]]
"
31103,pip install tensorflow-lite PLEASE!,"I'm finding very, very difficult-to-understand information online for how to install TF-lite.  Most of it involves cross-compilation and 10+ hours of waiting.  Tensorflow installation is easy.  Could you please make it so we can install TF-lite by just typing ""pip install tensorflow-lite?""

Thanks!"
31102,DepthwiseConv2D initializer version problem,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N
- TensorFlow installed from (source or binary):Pip
- TensorFlow version (use command below):1.14
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0.130
- GPU model and memory:7.5.0

**Describe the current behavior**
VarianceScaling throws an warning
**Describe the expected behavior**
No warning
**Code to reproduce the issue**
```
import tensorflow as tf

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

from tensorflow.python.keras import backend as K
from tensorflow.python.keras import datasets as ds
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import *
from tensorflow.python.keras.initializers import GlorotUniformV2

BATCH_SIZE = 128
EPOCHS = 5
INPUT_SHAPE = (28, 28, 1)
NUM_TRAIN = 60000
NUM_TEST = 10000
KERNEL_INIT = GlorotUniformV2()

(train_x, train_y), (val_x, val_y) = ds.mnist.load_data()
train_x, val_x = train_x / 255.0, val_x / 255.0
train_x = train_x.reshape((NUM_TRAIN, *INPUT_SHAPE))
val_x = val_x.reshape((NUM_TEST, *INPUT_SHAPE))


x_in = Input(shape=INPUT_SHAPE)
x = x_in
x = DepthwiseConv2D(3, 2, depthwise_initializer=KERNEL_INIT)(x)
x = Flatten()(x)
x = Dense(128, activation='relu', kernel_initializer=KERNEL_INIT)(x)
x = Dropout(0.5)(x)
x = Dense(10, activation='softmax', kernel_initializer=KERNEL_INIT)(x)
model = Model(x_in, x)

model.compile(
  optimizer='adam',
  loss='sparse_categorical_crossentropy',
  metrics=['accuracy']
)

model.fit(
  train_x,
  train_y,
  batch_size=BATCH_SIZE,
  epochs=EPOCHS,
)

model.evaluate(
  val_x,
  val_y
)
```




**logs**
First Log:
```
WARNING: Logging before flag parsing goes to stderr.
W0728 12:30:00.214958 16060 deprecation_wrapper.py:119] From x:\Suger\hat\unpush\dwconv.py:4: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0728 12:30:00.216953 16060 deprecation_wrapper.py:119] From x:\Suger\hat\unpush\dwconv.py:6: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-28 12:30:00.238369: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-28 12:30:00.257704: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-28 12:30:01.156168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-07-28 12:30:01.172584: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-28 12:30:01.190335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-28 12:30:01.966452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 12:30:01.975391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-28 12:30:01.982825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-28 12:30:01.987863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
W0728 12:30:02.522783 16060 deprecation.py:506] From X:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-07-28 12:30:03.177402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-07-28 12:30:03.197705: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.        
2019-07-28 12:30:03.217384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-28 12:30:03.222728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 12:30:03.232990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-28 12:30:03.237410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-28 12:30:03.249019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Epoch 1/5
60000/60000 [==============================] - 3s 54us/sample - loss: 0.6449 - acc: 0.8029
Epoch 2/5
60000/60000 [==============================] - 2s 31us/sample - loss: 0.2886 - acc: 0.9142
Epoch 3/5
60000/60000 [==============================] - 2s 31us/sample - loss: 0.2315 - acc: 0.9330
Epoch 4/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.2015 - acc: 0.9410
Epoch 5/5
60000/60000 [==============================] - 2s 34us/sample - loss: 0.1849 - acc: 0.9455
10000/10000 [==============================] - 1s 78us/sample - loss: 0.1072 - acc: 0.9671
```
Notice:
```
W0728 12:30:02.522783 16060 deprecation.py:506] From X:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
```
And then I read the its source code and found that DepthwiseConv2D inherits Conv2D.
```
class DepthwiseConv2D(Conv2D):
  def __init__(self,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               depth_multiplier=1,
               data_format=None,
               activation=None,
               use_bias=True,
               depthwise_initializer='glorot_uniform',
               bias_initializer='zeros',
               depthwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               bias_constraint=None,
               **kwargs):
    super(DepthwiseConv2D, self).__init__(
        filters=None,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        activation=activation,
        use_bias=use_bias,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        bias_constraint=bias_constraint,
        **kwargs)
    self.depth_multiplier = depth_multiplier
    self.depthwise_initializer = initializers.get(depthwise_initializer)
    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)
    self.depthwise_constraint = constraints.get(depthwise_constraint)
    self.bias_initializer = initializers.get(bias_initializer)
########
class Conv2D(Conv):
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               **kwargs):
    super(Conv2D, self).__init__(
        rank=2,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activations.get(activation),
        use_bias=use_bias,
        kernel_initializer=initializers.get(kernel_initializer),
        bias_initializer=initializers.get(bias_initializer),
        kernel_regularizer=regularizers.get(kernel_regularizer),
        bias_regularizer=regularizers.get(bias_regularizer),
        activity_regularizer=regularizers.get(activity_regularizer),
        kernel_constraint=constraints.get(kernel_constraint),
        bias_constraint=constraints.get(bias_constraint),
        **kwargs)
```
Obviously, when DepthwiseConv2D inherits Conv2D, there is no `kernel_initializer` parameter incoming, so Conv2D uses the default parameter `'glorot_uniform'`, resulting in a warning.

Then, I changed my code:
```
x = DepthwiseConv2D(3, 2, depthwise_initializer=KERNEL_INIT, kernel_initializer=KERNEL_INIT)(x)
```
**Log**
Second log:
```
WARNING: Logging before flag parsing goes to stderr.
W0728 12:39:05.557960  7540 deprecation_wrapper.py:119] From x:\Suger\hat\unpush\dwconv.py:4: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W0728 12:39:05.568932  7540 deprecation_wrapper.py:119] From x:\Suger\hat\unpush\dwconv.py:6: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-28 12:39:05.586014: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-28 12:39:05.601795: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-28 12:39:06.511195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-07-28 12:39:06.537446: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-28 12:39:06.565361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-28 12:39:07.398542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 12:39:07.412458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-28 12:39:07.420821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-28 12:39:07.435578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-07-28 12:39:08.630786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-07-28 12:39:08.651080: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.        
2019-07-28 12:39:08.666536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-28 12:39:08.680031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-28 12:39:08.687696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-28 12:39:08.702410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-28 12:39:08.718322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Epoch 1/5
60000/60000 [==============================] - 3s 52us/sample - loss: 0.6964 - acc: 0.7821
Epoch 2/5
60000/60000 [==============================] - 2s 32us/sample - loss: 0.2977 - acc: 0.9111
Epoch 3/5
60000/60000 [==============================] - 2s 31us/sample - loss: 0.2382 - acc: 0.9298
Epoch 4/5
60000/60000 [==============================] - 2s 33us/sample - loss: 0.2097 - acc: 0.9378
Epoch 5/5
60000/60000 [==============================] - 2s 33us/sample - loss: 0.1888 - acc: 0.9438
10000/10000 [==============================] - 1s 79us/sample - loss: 0.1104 - acc: 0.9666
```
There's no warning.
"
31101,when will the weights be updated for the sparse tensor in the optimizer,"I notice there are both [SparseApplyAdadeltaOp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L798) and [ApplyAdadeltaOp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L642) in the [/tensorflow/core/kernels/training_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc). 

Apparently, when the weights for the dense feature is optimized using Adadelta, the class of `ApplyAdadeltaOp` will be called, while when the weights for the categorical feature is optimized, `SparseApplyAdadeltaOp` will be called.

My question is when the `SparseApplyAdadeltaOp`  is called, for which part of the weights will be updated seems to not be in the code annotation, on earth the wights that are not zero will be updated or the weights whose gradients that are not zero will be updated?"
31100,"When I used tf.keras.backend.clip in my code, I got the error: TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed","When I implemented the code below with TensorFlow 1.12.0, I got some errors. However, the code can be implemented well with TensorFlow 1.11.0. I am confused by the problem. Actually, I am not sure whether it is the problem of the TensorFlow version. Hope someone can give me some advice.

The code:

```
def lightconstraint(temp):
    # add constraint for the output W
    # 1-norm of W less than min{I_DC-I_L, I_U-I_DC}
    # for simplicity, only clip [0, I_DC]
    W, I_DC= temp
    norms = tf.norm(W, ord=1, axis=2, keepdims=True)
    desired = K.clip(norms, 0, I_DC[0])
    return W * (desired / (K.epsilon() + norms))


# function test code
oo1 = tf.convert_to_tensor(np.array([[[1, 0], [1, 1]],
                                    [[0, 0], [1, 2]]]).astype('float32'))
oo2 = tf.convert_to_tensor(np.array([1]).astype('float32'))
with tf.Session() as sess:
    print(sess.run(lightconstraint([oo1, oo2])))
```
The error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-47-3851ab5448f3> in <module>()
     14 oo2 = tf.convert_to_tensor(np.array([1]).astype('float32'))
     15 with tf.Session() as sess:
---> 16     print(sess.run(lightconstraint([oo1, oo2])))

<ipython-input-47-3851ab5448f3> in lightconstraint(temp)
      5     W, I_DC= temp
      6     norms = tf.norm(W, ord=1, axis=2, keepdims=True)
----> 7     desired = K.clip(norms, 0, I_DC[0])
      8     return W * (desired / (K.epsilon() + norms))
      9 

C:\YYY\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py in clip(x, min_value, max_value)
   1941       A tensor.
   1942   """"""
-> 1943   if max_value is not None and max_value < min_value:
   1944     max_value = min_value
   1945   if max_value is None:

C:\YYY\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __bool__(self)
    669       `TypeError`.
    670     """"""
--> 671     raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
    672                     ""Use `if t is not None:` instead of `if t:` to test if a ""
    673                     ""tensor is defined, and use TensorFlow ops such as ""

TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
```"
31099,Improve fps Resnet model,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.14.1
- Python version:3.6.8
- CUDA/cuDNN version:9.0/7.4
- GPU model and memory: Nvidia GeForce 840m

I have trained a Resnet Model from scratch to detect eye region in real-time.
The trained model gives me some results but the values fps <10.
I have opened the webcam and load Tensorflow model using a python script and OpenCV library.
I tried many solutions ( decrease the width and height of the input image, set the batch size to 1 ...) but any of those methods working.
I want to know if there is a solution to increase the value of fps?


"
31098,conda install tensorflow-mkl version raise error: could not create a dilated convolution forward descriptor,"keras vesion: 2.2.4, tenssoflow version: 1.13.1
CPU: i3-330

exception stack:



Epoch 1/5

---------------------------------------------------------------------------
AbortedError                              Traceback (most recent call last)
<ipython-input-7-ddf42c6880a1> in <module>
     13                               validation_steps=math.ceil(val_flow.samples/val_flow.batch_size),
     14                               steps_per_epoch=math.ceil(train_flow.samples/train_flow.batch_size),
---> 15                               callbacks=[checkpoint, early, tb, csv_logger])

D:\soft\Anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name + '` call to the ' +
     90                               'Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

D:\soft\Anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1416             use_multiprocessing=use_multiprocessing,
   1417             shuffle=shuffle,
-> 1418             initial_epoch=initial_epoch)
   1419 
   1420     @interfaces.legacy_generator_methods_support

D:\soft\Anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
    215                 outs = model.train_on_batch(x, y,
    216                                             sample_weight=sample_weight,
--> 217                                             class_weight=class_weight)
    218 
    219                 outs = to_list(outs)

D:\soft\Anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1215             ins = x + y + sample_weights
   1216         self._make_train_function()
-> 1217         outputs = self.train_function(ins)
   1218         return unpack_singleton(outputs)
   1219 

D:\soft\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2713                 return self._legacy_call(inputs)
   2714 
-> 2715             return self._call(inputs)
   2716         else:
   2717             if py_any(is_tensor(x) for x in inputs):

D:\soft\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)
   2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
   2674         else:
-> 2675             fetched = self._callable_fn(*array_vals)
   2676         return fetched[:len(self.outputs)]
   2677 

D:\soft\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

D:\soft\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1111
	 [[{{node conv1_1/convolution}}]]

--------------------------------------------------------------------------------------
the code work well with version without MKL."
31096,Op request tensorflow lite ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2, TRANSPOSE, UNPACK, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31095,YOLO V3 Tensorflow Lite request,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXP, FILL, GATHER, GREATER_EQUAL, LEAKY_RELU, LOGISTIC, MUL, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, SQUEEZE, STRIDED_SLICE, SUB, WHERE. Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3.

```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31094,Compilation error with protoc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Installed from source
- TensorFlow version: 1.8
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.12
- GCC/Compiler version (if compiling from source): 7.4.0


**Describe the problem**
I'm following the instructions I've found in:
https://medium.com/@fanzongshaoxing/use-tensorflow-c-api-with-opencv3-bacb83ca5683
to build the shared library of libtensorflow_cc.so and trying to compile the code which follows the tutorial in 
https://github.com/lysukhin/tensorflow-object-detection-cpp?source=post_page---------------------------
However, when I run cmake && make I get lots of compilation errors that tell me that the tensorflow library was generated using an old version of protoc and that I need to regenerate the file.
However I have already built protobuf 3.6.1 from source and running the following command lines I am assured of this:

protoc --version
libprotoc 3.6.1

pip show protobuf
Name: protobuf
Version: 3.6.1
Summary: Protocol Buffers
Home-page: https://developers.google.com/protocol-buffers/
Author: None
Author-email: None
License: 3-Clause BSD License
Location: /home/user/.virtualenvs/cv/lib/python3.6/site-packages
Requires: setuptools, six
Required-by: tensorflow, tensorboard

But I do note that in /usr/bin, there is a different version of protoc
/usr/bin/protoc --version
libprotoc 3.0.0

How should I proceed? Thanks for your consideration.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After performing the steps in the tutorial to build tensorflow_cc.so with bazel and move them to usr/local
git clone https://github.com/lysukhin/tensorflow-object-detection-cpp.git
cd tensorflow-object-detection-cpp
mkdir build
cd build
cmake ..
make


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

In file included from /usr/local/include/tf/tensorflow/core/framework/variant.h:26:0,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
 #error This file was generated by an older version of protoc which is
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
 #error incompatible with your Protocol Buffer headers.  Please
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
 #error regenerate this file with a newer version of protoc.

In file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:32:0,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
 #error This file was generated by an older version of protoc which is
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
 #error incompatible with your Protocol Buffer headers.  Please
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
 #error regenerate this file with a newer version of protoc.
  ^~~~~
In file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:33:0,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
 #error This file was generated by an older version of protoc which is
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
 #error incompatible with your Protocol Buffer headers.  Please
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
 #error regenerate this file with a newer version of protoc.
  ^~~~~
In file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:34:0,
                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,
                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,
                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,
                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
 #error This file was generated by an older version of protoc which is
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
 #error incompatible with your Protocol Buffer headers.  Please
  ^~~~~
/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
 #error regenerate this file with a newer version of protoc.

...
"
31093,Loading embeddings into the graph fails with: libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc 164 Cannot allocate buffer larger than kint32max for StringOutputStream,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS 10.14
- TensorFlow installed from (source or binary):
Binary 
- TensorFlow version (use command below):
1.13.1
- Python version:
Python 3.6.7 |Anaconda

**Describe the current behavior**
Loading 1.4mil 100dim embeddings into the graph 
words:1457657; dim:100

**Describe the expected behavior**
Want to be able to use TFRecord data set with words and graph to lookup indexes via TF table
and then parallel lookup imbedding vectors; compute using 3 dim tensor.
This used to work with smaller set of embeddings. 
Anyway to overcome this problem without rewriting data feed?

**Code to reproduce the issue**
w_embedding_vocab = tf.constant(embDic.vocab, dtype=tf.string, shape=[embDic.vocab_size], name=""w_embedding_vocab"")

            w_embedding_vocab_table = lookup_ops.index_table_from_tensor(w_embedding_vocab, default_value=0, name=""word_embidx_tbl"")

            w_embeddings = tf.get_variable(name=""word_embeddings"", shape=[embDic.vocab_size, embDic.dim],
                                           initializer=tf.constant_initializer(np.asmatrix(embDic.embeddings)),
                                           dtype=tf.float32, trainable=False)


**Other info / logs**
No other logs, just one ERROR message

[libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc:164] Cannot allocate buffer larger than kint32max for StringOutputStream.
"
31092,Tensorflow-lite gpu ios memory leak,"I use TensorFlowLiteGpuExperimental  for inference ,I have 60 models, if i only use one model to inference ,it works well, but if i switch different models in running , than memory leak occurs, I just use code :
            interpreter = nullptr;
            DeleteGpuDelegate(delegate);
to free resources
so if i loss other free function ?
if TensorFlowLiteGpuExperimental a bug?
TensorFlowLiteGpuExperimental the newer version number is what? "
31091,[TF 2.0] tf.image.central_crop doesn't work,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-7164-gf2b5825 2.0.0-dev20190726
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
tf.image.central_crop throws an error
**Describe the expected behavior**
tf.image.central_crop crops image
**Code to reproduce the issue**
I've tried two scenarios.
First:
```python
from tensorflow.keras import Input

image = Input(shape=(512,512,3))

image = tf.image.central_crop(image, 0.8)
```

Second:
```python
from tensorflow.keras import Input

image = Input(shape=(512,512,3))

@tf.function
def central_crop_fn(image, fraction):
    return tf.image.central_crop(image, fraction)

central_crop_fn(image, 0.8)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

First log:

```---------------------------------------------------------------------------
OperatorNotAllowedInGraphError            Traceback (most recent call last)
<ipython-input-4-e5a8997e400a> in <module>
      3 image = Input(shape=(512,512,3))
      4 
----> 5 image = tf.image.central_crop(image, 0.8)

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in central_crop(image, central_fraction)
    653       return image
    654 
--> 655     _AssertAtLeast3DImage(image)
    656     rank = image.get_shape().ndims
    657     if rank != 3 and rank != 4:

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in _AssertAtLeast3DImage(image)
    192   """"""
    193   return control_flow_ops.with_dependencies(
--> 194       _CheckAtLeast3DImage(image, require_static=False), image)
    195 
    196 

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in _CheckAtLeast3DImage(image, require_static)
    226         check_ops.assert_positive(
    227             array_ops.shape(image),
--> 228             [""all dims of 'image.shape' ""
    229              'must be > 0.']),
    230         check_ops.assert_greater_equal(

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py in assert_positive(x, data, summarize, message, name)
    266           'x (%s) = ' % name, x]
    267     zero = ops.convert_to_tensor(0, dtype=x.dtype)
--> 268     return assert_less(zero, x, data=data, summarize=summarize)
    269 
    270 

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py in assert_less(x, y, data, summarize, message, name)
    861       ]
    862     condition = math_ops.reduce_all(math_ops.less(x, y))
--> 863     return control_flow_ops.Assert(condition, data, summarize=summarize)
    864 
    865 

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py in wrapped(*args, **kwargs)
    196   """"""
    197   def wrapped(*args, **kwargs):
--> 198     return _add_should_use_warning(fn(*args, **kwargs))
    199   return tf_decorator.make_decorator(
    200       fn, wrapped, 'should_use_result',

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py in Assert(condition, data, summarize, name)
    147   """"""
    148   if context.executing_eagerly():
--> 149     if not condition:
    150       xs = ops.convert_n_to_tensor(data)
    151       data_str = [_summarize_eager(x, summarize) for x in xs]

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __bool__(self)
    749       `TypeError`.
    750     """"""
--> 751     self._disallow_bool_casting()
    752 
    753   def __nonzero__(self):

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _disallow_bool_casting(self)
    530     else:
    531       # Default: V1-style Graph execution.
--> 532       self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
    533 
    534   def _disallow_iteration(self):

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _disallow_in_graph_mode(self, task)
    519     raise errors.OperatorNotAllowedInGraphError(
    520         ""{} is not allowed in Graph execution. Use Eager execution or decorate""
--> 521         "" this function with @tf.function."".format(task))
    522 
    523   def _disallow_bool_casting(self):

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```
Second log:

```---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     60                                                op_name, inputs, attrs,
---> 61                                                num_outputs)
     62   except core._NotOkStatusException as e:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: input_3:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
<ipython-input-6-71417bdb3502> in <module>
      7     return tf.image.central_crop(image, fraction)
      8 
----> 9 central_crop_fn(image, 0.8)

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    449               *args, **kwds)
    450       # If we did not create any variables the trace we have is good enough.
--> 451       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    452 
    453     def fn_with_cond(*inner_args, **inner_kwds):

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
    663          if isinstance(t, (ops.Tensor,
    664                            resource_variable_ops.BaseResourceVariable))),
--> 665         self.captured_inputs)
    666 
    667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
    776     if executing_eagerly or not self.outputs:
    777       outputs = self._inference_function.call(
--> 778           ctx, args, cancellation_manager=cancellation_manager)
    779     else:
    780       self._register_gradient()

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    469               inputs=args,
    470               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 471               ctx=ctx)
    472         else:
    473           outputs = execute.execute_with_cancellation(

~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     73       raise core._SymbolicException(
     74           ""Inputs to eager execution function cannot be Keras symbolic ""
---> 75           ""tensors, but found {}"".format(keras_symbolic_tensors))
     76     raise e
     77   # pylint: enable=protected-access

_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_3:0' shape=(None, 512, 512, 3) dtype=float32>]
```"
31089,Can't set tf.keras.backend.variable(trainable=False),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): maybe, but I don't have time for tests/docs



**Describe the feature and the current behavior/state.**
Currently, I have to do this.
```python
import tensorflow.keras.backend as K

variable = K.variable(5)
variable._trainable = False
```

But changing private attributes always feels dodgy. Ideally, we should just be able to do this.
```python
variable = K.variable(5, trainable=False)
```

**Will this change the current api? How?**

It will add one parameter `trainable=True` to `tensorflow.keras.backend.variable`

**Who will benefit with this feature?**

People who don't like the feeling of having to modify private attributes.

**Any Other info.**
Looking here: https://github.com/tensorflow/tensorflow/blob/ec71eb1ac4986e145845372b44648a03ea0e7545/tensorflow/python/keras/backend.py#L739

```python
def variable(value, dtype=None, name=None, constraint=None, trainable=True):
    ...
    v = variables_module.Variable(
      value,
      dtype=dtypes_module.as_dtype(dtype),
      name=name,
      trainable=trainable, # +++
      constraint=constraint)
    ...
```

Not sure what's going on with the sparse tensor, but you could always raise an exception if 
`hasattr(value, 'tocoo') and trainable == False` if that's a problem."
31088,import error for python3: _pywrap_tensorflow_internal.so: undefined symbol: _Py_ZeroStruct,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 6.9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: build from source
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): gcc 5.5.0
- CUDA/cuDNN version: 10.1/7.6.2
- GPU model and memory: GTX 1080



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I used a customized tool chain in $LINUXBREWHOME (which is ~/.linuxbrew) to build tensorflow from source using these commands (see below) and the build was successful. And then I built the whl for python3 and installed it using pip3 and they all worked. But when I tried importing tensorflow in python3 using ```python3 -c 'import tensorflow'``` I got an error:
```
ImportError: /home/aznb/.linuxbrew/Cellar/python/3.7.4/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_ZeroStruct
``` 
Tensorflow was built using these commands:
```
export GCC_HOST_COMPILER_PREFIX=$LINUXBREWHOME/bin

export PYTHON_BIN_PATH=$(which ${python3})
export PYTHON_LIB_PATH=$LINUXBREWHOME/Cellar/python/3.7.4/lib/python3.7/site-packages
export PYTHONPATH=$LINUXBREWHOME/opt/python3/lib/python3.7/site-packages
export PYTHON_ARG=$LINUXBREWHOME/opt/python3/lib/python3.7/site-packages
export CUDA_TOOLKIT_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/
export CUDNN_INSTALL_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/

export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=""$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)""
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_AWS=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_ROCM=0
export TF_NEED_OPENCL_SYCL=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$LINUXBREWHOME/Cellar/gcc/5.5.0_4/bin/gcc
export CC_OPT_FLAGS=""-march=native""
#export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_KAFKA=0
export TF_NEED_TENSORRT=0

# when using NCCL you need to install it own your own
#export TF_NCCL_VERSION=1.3

export CC_OPT_FLAGS=""-march=native""

#bazel clean --async
#./configure

#bazel build --jobs 16 --crosstool_top=@local_config_cuda//crosstool:toolchain --config=noaws --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package $TMPDIR/tensorflow-pkg
pip3 install $TMPDIR/tensorflow-pkg/tensorflow-1.14.0-cp37-cp37m-linux_x86_64.whl
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31086,TensorFlow 2.0 using pretrained word embeddings as input to sequence model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.0 Beta
- Are you willing to contribute it (Yes/No):
**Describe the feature and the current behavior/state.**
I've just started exploring TF2.0 and it appears that the word embeddings must be generated as part of the seq2seq model.  

**Will this change the current api? How?**
embedding layer would need to be modified to not train and permit inclusion of pretrained word embeddings

**Who will benefit with this feature?**
anyone who would like to use pretrained word embeddings**Any

 Other info.**
inclusion of pretrained word embeddings would further expedite training so a huge win. 
"
31085,Building Tensorflow with VS 2019,"_My apologies for starting this conversation here. I didn't think the TensorFlow discussion forum or StackOverflow would be appropriate._

Hello,

I work on the Visual Studio C++ Compiler team. We are interested in helping TensorFlow upgrade to Visual Studio 2019 while also making sure the best compiler switches are being used. We have seen significant throughput and code generation improvements between the Visual Studio 2017 and the 2019 compilers across various projects.

I have several questions:

1. From the [TensorFlow documents](https://www.tensorflow.org/install/source_windows), it looks like the only supported compiler is Visual Studio 2017. **Is VS 2017 being used in the TensorFlow continuous integration and release builds?** I couldn't verify from the logs [here](https://source.cloud.google.com/results/invocations/45f76066-3000-43cf-b55d-9317b4e6b083/targets/tensorflow%2Fgithub%2Fwindows%2Fbazel%2Fcontinuous/log).

2. Are there any plans to move to another compiler toolchain for TensorFlow on Windows?

3. Is there a better channel to ask any other questions that we might have?

Thank you for your time."
31084,Bug Issue: tf.case incompatible with list comprehension,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **custom**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
$ uname -a
Linux archlinux 5.1.5-arch1-2-ARCH #1 SMP PREEMPT Mon May 27 03:37:39 UTC 2019 x86_64 GNU/Linux
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **None**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below):
 **v1.14.0-rc1-22-gaf24dc91b5**
**1.14.0**
- Python version: **3.6**
- Bazel version (if compiling from source): **None**
- GCC/Compiler version (if compiling from source): **None**
- CUDA/cuDNN version: **None**
- GPU model and memory: **None**

**Describe the current behavior**

Bug 1: the function `random_choice` below has the correct behaviour only when using the decorator `@tf.function`. If the decorator is not used, `tf.case` no longer consider the predicate and always outputs the same value.

Bug 2: even if the decorator `@tf.function` is used, the function `random_choice` does not have the correct behaviour if I use list comprehension to create the list of pairs predicate / functions. Although, it has the correct behaviour when the list is populated iteratively with append (in a for loop). 

**Describe the expected behavior**

1. Decorating the function `random_choice` with `@tf.function` should not affect `tf.case`, unless I am not aware of the intricacies of `tf.function` and `tf.case`.

2. Choosing for loops or list comprehension to create the list of predicate / function pairs should not affect `tf.case`, unless (again) I am not aware of the intricacies of `tf.function` and `tf.case`.

**Code to reproduce the issue**

```python
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow_probability as tfp

K.clear_session()

def fct1(x):
  return x + 5

def fct2(x):
  return x - 5

# Bug 1: if you remove `@tf.function`, the function `random_choice` 
# no longer works, because `tf.case` no longer considers `idx`.  
@tf.function
def random_choice(x, choices):
  idx = tfp.distributions.Bernoulli(probs=0.5).sample(1)[0]
  outputs = [c(x) for c in choices]
 
# Bug 2: if you use `@tf.function`, but you replace this code:
  fns = []
  for o in outputs:
    fns.append(lambda: o)
# by this one:
#   fns = [lambda:o for o in outputs]
# then, `tf.case` no longer considers `idx`.

  y = tf.case([(tf.equal(idx, i), fn) for (i, fn) in enumerate(fns)])
  return outputs, idx, y

x = tf.constant([4.0])
choices = [fct1, fct2]
outputs, idx, y = random_choice(x, choices)
sess = tf.Session()
for i in range(10):
  outputs_np, idx_np, y_np = sess.run([outputs, idx, y])
  print(y_np, outputs_np[idx_np])
```

Example outputs.

For the code above (this is the expected behaviour):
```python
[9.] [9.]
[-1.] [-1.]
[-1.] [-1.]
[-1.] [-1.]
[9.] [9.]
[9.] [9.]
[-1.] [-1.]
[9.] [9.]
[9.] [9.]
[-1.] [-1.]
```

When removing `@tf.function`, or when using `@tf.function` but using list comprehension to create `fns` (this is the unexpected behaviour):
```python
[-1.] [9.]
[-1.] [9.]
[-1.] [-1.]
[-1.] [9.]
[-1.] [9.]
[-1.] [9.]
[-1.] [-1.]
[-1.] [-1.]
[-1.] [-1.]
[-1.] [9.]
```

**Other info / logs**

I could reproduce this bug in tensorflow:
**2.0.0-beta1**
**v2.0.0-beta0-16-g1d91213fe7**
 
"
31083,Build for centos with toolchain in nonstandard location,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 6.9
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: build from source
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): gcc 5.5.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1080 




**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Because the OS's toolchain is too old to build tensorflow, I use the toolchain from linuxbrew installed in $LINUXBREWHOME=~/.linuxbrew (e.g., ~/.linuxbrew/bin/gcc) to build tensorflow. But the build failed with 
```
ERROR: /data/scratch/ssd/cache/_bazel_aznb/19c22cee7007ea5c8439dd1833ab51b5/external/nasm/BUILD.bazel:8:1: Linking of rule '@nasm//:nasm' failed (Exit 1)
/usr/bin/ld: unrecognized option '-plugin'
/usr/bin/ld: use the --help option for usage information
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
as it can't find the linker in ~/.linuxbrew/bin/ld (which I confirm functional)

The commands I used to reproduce this:

```
export VERSION=$1                                                                                                                                              
                                                                                                                                                               
export TF_ROOT=$LINUXBREWHOME/Cellar/tensorflow/$VERSION                                                                                                       
                                                                                                                                                               
export PYTHON_BIN_PATH=$(which ${python3})                                                                                                                     
export PYTHON_LIB_PATH=$LINUXBREWHOME/Cellar/python/3.7.4/lib/python3.7/site-packages                                                                          
export PYTHONPATH=${TF_ROOT}/lib                                                                                                                               
export PYTHON_ARG=${TF_ROOT}/lib                                                                                                                               
export CUDA_TOOLKIT_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/                                                                                                      
export CUDNN_INSTALL_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/                                                                                                     
                                                                                                                                                               
export TF_NEED_GCP=0                                                                                                                                           
export TF_NEED_CUDA=1                                                                                                                                          
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""                                                       
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5                                                                                                                    
export TF_NEED_HDFS=0                                                                                                                                          
export TF_NEED_OPENCL=0                                                                                                                                        
export TF_NEED_JEMALLOC=1                                                                                                                                      
export TF_ENABLE_XLA=0                                                                                                                                         
export TF_NEED_VERBS=0                                                                                                                                         
export TF_CUDA_CLANG=0                                                                                                                                         
export TF_CUDNN_VERSION=""$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)""                                               
export TF_NEED_MKL=0                                                                                                                                           
export TF_DOWNLOAD_MKL=0
export TF_DOWNLOAD_MKL=0                                                                                                                                       
export TF_NEED_AWS=0                                                                                                                                           
export TF_NEED_MPI=1                                                                                                                                           
export MPI_HOME=$LINUXBREWHOME
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_ROCM=0
export TF_NEED_OPENCL_SYCL=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$LINUXBREWHOME/Cellar/gcc/5.5.0_4/bin/gcc
export CC_OPT_FLAGS=""-march=native""
#export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_KAFKA=0
export TF_NEED_TENSORRT=0

# when using NCCL you need to install it own your own
#export TF_NCCL_VERSION=1.3

export CC_OPT_FLAGS=""-march=native""
bazel clean --async
./configure

bazel build --crosstool_top=@local_config_cuda//crosstool:toolchain --config=noaws --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31077,Downgrade tensorflow 1.14 to tensorflow 1.13,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.4
- GPU model and memory: Nivida GeForce 840m 3 Go



**Describe the current behaviour**
I training a Resnet Model ( CNN) that detects an eye region with landmarks on real-time using Tensorflow library 
For the first time, I have used Tensorflow library ( 1.13.1), the model gives sometimes results and for others times not for the webcam ( and for video from my dataset ).
I have upgrade Tensorflow to 1.14.1 recently and I have tested my webcam and I don't get any results contrary for a video the same result does not change when I upgrade the version of Tensorflow.
I have a little doubt that the problem may be can related to the version of Tensorflow.
My question is :

+ Can I downgrade the version of Tensorflow 1.14.1 to 1.13.1? ( all packages )




"
31074,fit_generator validation progress bar,"**System information**
- TensorFlow version (you are using): 2.0 beta
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
Currently it is not possbile to see the progressbar of the valitation process when the `keras.fit_generator` is running in `verbose=1` or `verbose=2`.

**Will this change the current api? How?**
In https://github.com/tensorflow/tensorflow/blob/456fbc0e498e3d10604973de9f46ca48d62267cc/tensorflow/python/keras/engine/training_generator.py#L320 
the control over the verbosity has to be controllable via the `fit_generator` setup.

**Who will benefit with this feature?**
People using the keras `fit_generator` on large datasets. Especially there the validation process can take long time and therefore seeing the progress is nice.
"
31073,tf.linalg.sqrtm() returns nan,"I want to perform the function of matrix square root in my CNN, which can be automatically backpropagated. I used the function tf.linalg.sqrtm() to compute the square root of an invertible matrix. After running the session, it returns an array with all elements equal to ""nan"". Does anyone know about the reason for the incorrect result? Or any other methods to compute the matrix square root in the forward process and compute gradients automatically during the backward process?
The settings are:
- Linux Ubuntu 16.04
- TensorFlow version: 1. 14. 0
- Python version: 3.6

"
31072,torch.unfold function is needed..,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.14
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
I think sending the function page in torch is the best. https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html
**Will this change the current api? How?**
I guess it doesn't.
**Who will benefit with this feature?**
I think it will be popular in Vision Models, cause self attention is arising now to find out relationship in input pixels.(Stand-Alone Self-Attention in Vision Models)


**Any Other info.**
"
31071,Error on model fit with stateful LSTM using Dataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS
- **TensorFlow installed from (source or binary)**: conda-forge
- **TensorFlow version (use command below)**: unknown 1.14.0
- **Python version**: Python 3.7.3
- **CUDA/cuDNN version**: NVIDIA-SMI 418.67, Driver Version: 418.67, CUDA Version: 10.1
- **GPU model and memory**: Quadro RTX 6000, 24190MiB
- **Exact command to reproduce**:

### Describe the problem
When feeding a `Dataset` to `Sequential.fit()` when a _stateful_ LSTM layer is included, a `TypeError` occurs:

    TypeError: 'DatasetV1Adapter' object is not subscriptable

This does not happen with a stateless LSTM (Using `stateful=False` does not produce the error). Below is a minimal example to reproduce this issue.

### Source code / logs
```
import numpy as np
import tensorflow as tf
#tf.enable_eager_execution()

train_X = np.arange(1, 1001).reshape((200, 5))
train_Y = np.array(list(map(
    lambda x: np.array([1, 0]) if x == 0 else np.array([0, 1]),
    np.random.randint(2, size=200))))

ds = tf.data.Dataset.from_tensor_slices((train_X, train_Y))

ws = 3
sh = None
st = 1
bs = 10
nu = 86
ne = 5

ds = ds.window(size=ws, shift=sh, stride=st, drop_remainder=True).flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(ws), y.batch(ws)))).batch(bs, drop_remainder=True)

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(ws, len(train_X[0])), batch_size=bs),
    tf.keras.layers.LSTM(nu, return_sequences=True, stateful=True),
    tf.keras.layers.Dense(2), # categorical
    tf.keras.layers.Activation('softmax'), # categorical
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
print(model.summary())

history = {}
for e in range(ne):
    history[e] = model.fit(ds, epochs=1, shuffle=False)
    model.reset_states()

#history = model.fit(ds, epochs=ne, shuffle=False)
```
Output:
```
Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (10, 3, 86)               31648     
_________________________________________________________________
dense_1 (Dense)              (10, 3, 2)                174       
_________________________________________________________________
activation_1 (Activation)    (10, 3, 2)                0         
=================================================================
Total params: 31,822
Trainable params: 31,822
Non-trainable params: 0
_________________________________________________________________
None


TypeErrorTraceback (most recent call last)
<ipython-input-2-0cef961435e5> in <module>
     30 history = {}
     31 for e in range(ne):
---> 32     history[e] = model.fit(ds, epochs=1, shuffle=False)
     33     model.reset_states()
     34 

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    707         steps=steps_per_epoch,
    708         validation_split=validation_split,
--> 709         shuffle=shuffle)
    710 
    711     # Prepare validation data.

/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2707       # Check that for stateful networks, number of samples is a multiple
   2708       # of the static batch size.
-> 2709       if x[0].shape[0] % batch_size != 0:
   2710         raise ValueError('In a stateful network, '
   2711                          'you should only pass inputs with '

TypeError: 'DatasetV1Adapter' object is not subscriptable
```"
31070,Issue using tf.keras ModelCheckpoint when distributing under MultiWorkerMirroredStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-gpu 2.0.0b1
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 / 7.4.1
- GPU model and memory: 2 x GV100 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Raises an error when using tf.keras.callbacks.ModelCheckpoint in the callbacks_list when training using keras under the MultiWorkerMirroredStrategy distribution strategy on a single machine.

Error is:
```
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: container="""", dtype=DT_INT32, shape=[], shared_name=""cd2c89b7-88b7-44c8-ad83-06c2a9158347""
```

**Describe the expected behavior**
should save a checkpoint model file and not crash

**Code to reproduce the issue**

**run script called - run_distributed_training_minimal_example.py**
```
import json
import subprocess

import os


def create_TF_config(name, id):
    return {
        'cluster': {
            'worker': ['localhost:9999']
            ,'chief': ['localhost:9997']
        },
        'task': {'type': name, 'index': id}
    }


def set_TF_CONFIG(id, name='worker'):
    os.environ['TF_CONFIG'] = json.dumps(create_TF_config(name, id))


def start_processes(cluster_def, key, device=None):
    process_list = []
    if key in cluster_def:
        for i, _ in enumerate(cluster_def[key]):
            if device is not None:
                os.environ[""CUDA_VISIBLE_DEVICES""] = str(device)
                device +=1
            else:
                os.environ[""CUDA_VISIBLE_DEVICES""] = """"

            process_list.append(subprocess.Popen(['python', 'distributed_training_minimal_example.py', '--job-name='+key, '--job-id=' + str(i)]))
    return process_list, device


if __name__ == ""__main__"":
    cluster_def = create_TF_config("""","""")['cluster']

    process_list = []
    #this_list, device = start_processes(cluster_def, 'chief')
    device=0
    for key in ['chief','worker', 'ps']:
        this_list, device = start_processes(cluster_def, key, device)
        process_list.extend(this_list)

    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""
    os.environ['TF_CONFIG'] = ""{}""

    for p in process_list:
        p.wait()
```

**distributed worker script called - distributed_training_minimal_example.py**
```
from run_distributed_training_minimal_example import create_TF_config, set_TF_CONFIG

use_custom_check_point = False

def parse_arguments():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--job-name',
                        type=str,
                        default=""worker"",
                        help='type of job this process is running')
    parser.add_argument('--job-id',
                        type=int,
                        default=0,
                        help='id of this job type for this process to run')
    return parser.parse_args()

args = parse_arguments()

tf_config = create_TF_config("""", """")
cluster_def = tf_config['cluster']
set_TF_CONFIG(args.job_id, args.job_name)

is_chief = args.job_name == 'chief'
print('is_chief:'+str(is_chief))
batchSize = len(cluster_def['worker'])

import tensorflow as tf
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with strategy.scope():

    def create_simple_model():
        return tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.04), input_shape = (128, 128, 1)),
            tf.keras.layers.Conv2D(1, 3, activation='relu', padding='same',kernel_regularizer = tf.keras.regularizers.l2(0.04)),
            tf.keras.layers.Dense(1, activation='softmax')
        ])


    def localised_cross_entropy(y_true, y_pred, ratio=1.0):
        positive_error = ratio * y_true * tf.keras.backend.log(0.0000001 + y_pred)
        negative_error = (1 - y_true) * tf.keras.backend.log(1.0000001 - y_pred)
        errors = positive_error + negative_error
        return tf.keras.backend.mean(errors)

    def localised_cross_entropy_loss(y_true, y_pred, ratio=1.0):
        return -localised_cross_entropy(y_true, y_pred, ratio)

    def create_data():
        import numpy as np
        data_set = []
        for i in range(20):
            ip = np.random.random([128, 128, 1]).astype(np.float32)
            op = np.random.randint(0, 2, [128, 128, 1]).astype(np.float32)
            data_set.append((ip, op))
        return data_set

    model = create_simple_model()
    model.summary()

    trainingData = create_data()
    model.compile('adam', loss=localised_cross_entropy_loss, metrics=[localised_cross_entropy])

    split = int(len(trainingData)*0.8)
    trainData, valData = trainingData[:split], trainingData[split:]

    def create_RAM_generator(data):
        while True:
            for i in data:
                yield i

    def tensorflow_generator_training(data_getter, batchSize=None):
        import tensorflow as tf

        def __getter_generator():
            while True:
                item = next(data_getter)
                yield item

        shapes = ((None, None, 1), (None, None, 1))
        dataset = tf.data.Dataset.from_generator(generator=__getter_generator, output_types=(tf.float32, tf.float32),output_shapes=shapes)
        if batchSize is not None:
            dataset = dataset.batch(batchSize)
        return dataset

    def generatorise(data):
        train_gen = create_RAM_generator(data)
        train_gen = tensorflow_generator_training(train_gen, batchSize=batchSize)
        return train_gen

    train_gen = generatorise(trainData)
    val_gen = generatorise(valData)

if not use_custom_check_point:
    callbacks_list = [tf.keras.callbacks.ModelCheckpoint('tmp.hdf5')]
else:
    from tensorflow.keras.callbacks import Callback


    class CustomModelCheckpointCallback(Callback):
        def __init__(self, path, model, is_chief_task):
            super(CustomModelCheckpointCallback, self).__init__()
            self.model = model
            self.path = path
            self.is_chief = is_chief_task

        def on_epoch_end(self, epoch, logs=None):
            if self.is_chief:
                self.model.save(self.path)
    callbacks_list = [CustomModelCheckpointCallback('tmp.hdf5', model, is_chief)]

model.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))
```

running first script will cause the issue.
setting use_custom_check_point to True in the second script will remove the error.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
F:\ffa_dev\deep-learning-dev-dist\env\Scripts\python.exe F:/ffa_dev/deep-learning-dev-dist/run_distributed_training_minimal_example.py
is_chief:False
is_chief:True
2019-07-26 12:00:27.116726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-26 12:00:27.117037: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-26 12:00:27.421095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:9e:00.0
2019-07-26 12:00:27.421633: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:27.425381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:27.426214: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-26 12:00:27.443567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:9e:00.0
2019-07-26 12:00:27.443960: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:27.448508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:27.497949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:27.498407: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:27.502742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:27.503686: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-26 12:00:27.519979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:27.520543: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:27.523778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:28.548291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.548686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.548929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.554809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)
2019-07-26 12:00:28.569853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:9e:00.0
2019-07-26 12:00:28.570305: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:28.573342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:28.573626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.573956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.574136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.576977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)
2019-07-26 12:00:28.581376: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> localhost:9997}
2019-07-26 12:00:28.581735: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:9999}
2019-07-26 12:00:28.599655: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:9999
2019-07-26 12:00:28.602463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:9e:00.0
2019-07-26 12:00:28.603395: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:28.607136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:28.607483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.607841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.608102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.611206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)
WARNING: Logging before flag parsing goes to stderr.
W0726 12:00:28.611904 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0
2019-07-26 12:00:28.613309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.613666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.613873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.618119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)
2019-07-26 12:00:28.634094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:28.634513: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:28.637145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:28.637447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.637741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.637928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.640533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:chief/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)
2019-07-26 12:00:28.646908: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> localhost:9997}
2019-07-26 12:00:28.647352: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:9999}
2019-07-26 12:00:28.666115: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:9997
2019-07-26 12:00:28.668862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:28.669380: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:28.672907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:28.673317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:28.673744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:28.673983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:28.677930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)
WARNING: Logging before flag parsing goes to stderr.
W0726 12:00:28.675424 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 128, 128, 32)      320       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 128, 1)       289       
_________________________________________________________________
conv2d (Conv2D)              (None, 128, 128, 32)      320       
_________________________________________________________________
dense (Dense)                (None, 128, 128, 1)       2         
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 1)       289       
_________________________________________________________________
dense (Dense)                (None, 128, 128, 1)       2         
=================================================================
Total params: 611
Trainable params: 611
Non-trainable params: 0
_________________________________________________________________
Total params: 611
Trainable params: 611
Non-trainable params: 0
_________________________________________________________________
W0726 12:00:30.992845 51176 deprecation.py:323] From F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0726 12:00:30.992845 29652 deprecation.py:323] From F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0726 12:00:31.031976 51176 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0726 12:00:31.031976 51176 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W0726 12:00:31.031976 29652 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0726 12:00:31.031976 29652 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2019-07-26 12:00:31.036410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:92019-e07-:26 12:00.0
00:31.2019-07-26 12:00:310.033668290: I tensor70: I flow/tensocorre/cflow/ommsotren_rauntime/gpm_execuu/gtorpu_/devpice.cc:16latfo4rm/0] defaultF/dlopeound dn_checkeevir_sce t0 wub.cc:25]ith pro perGPU libraries atiere sstati: 
namcale: ly liQuadro nked, skiGV100 p dmajlor:open check.
 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:31.038318: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:31.042495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 2019-07-26 12:00:310
.042772: I tensorflow/core/comm2019-07-26 12:on00:31.0429_runtime/gp9u/g2pu_: I devictene.cc:1763] sAorfdding visiblelow/co gpru deevi/ces: 0commo
n_runtime/gpu/gpu_device.cc:1181] Device inte201rconnect9-0 StreamE7x-26 12ecutor wi:th 00:31.0437stren11: Igth 1  edge matritenx:
sor2flo0w/cor19-07e/c-26 1o2:00:31mmon_ru.n044108time/:gpu /gpIu_device.cc: tensorflow/core/1c181ommon]_runtime/gpu /gpDevicu_devicee.c interconnect StreamExecutor c:1w1ith strength 1 8edge matrix:7
]20 19-07-     0 
2620 112:00:319-07-.20449886 12:00:31.:045 066I tensorflow/core:/co mmon_runtimIe/g tepnsorfu/gpu_devlow/cice.ccore/comm:on_1run187time/g]   p   0 
u/g2019-pu_de0v7-26 1ice.cc2:00:31.0:456921200] 0: I: te   N 
nsorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:31.050217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)
W0726 12:00:31.044387 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0
2019-07-26 12:00:31.051178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)
2019-07-26 12:00:31.054114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:9e:00.0
2019-07-26 12:00:31.054942: I tensorflow/stream_executor/pW0726 12:00:31.054794 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0
latform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:31.057696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627
pciBusID: 0000:5b:00.0
2019-07-26 12:00:31.058142: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-26 12:00:31.058764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:31.059078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:31.059395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:31.059590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:31.062322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-26 12:00:31.062664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-26 12:00:31.063059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-26 12:00:31.063278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-26 12:00:31.063637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)
W0726 12:00:31.062981 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0
2019-07-26 12:00:31.066050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)
W0726 12:00:31.065107 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0
2019-07-26 12:00:31.075080: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:334] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
2019-07-26 12:00:31.076454: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:334] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
2019-2019-07-26 12:00:31.126357: W tensorflow/core/grapp07-26 1ler2/op:00timizers/:d31.126335: W tensorfatal/auow/core/tgo_srappler/hard.cc:optim334i] Cannotzers/data/auto_shard fi.ccn:33d shardable4] C datasaet,n adnot fidnd ing a shashardabler d ndodeataset, add at theing a  sheardn nod ode at the end of the daftas the det insteaatadset. This  insteamay haved. This may have  performance implicapertions.f
ormance implications.
Train on 16 steps, validate on 4 steps
Train on 16 steps, validate on 4 steps
Traceback (most recent call last):
  File ""distributed_training_minimal_example.py"", line 113, in <module>
Traceback (most recent call last):
  File ""distributed_training_minimal_example.py"", line 113, in <module>
    model.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 643, in fit
    model.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 776, in wrapper
    use_multiprocessing=use_multiprocessing)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_coordinator.py"", line 853, in run_distribute_coordinator
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_coordinator.py"", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_coordinator.py"", line 360, in _run_single_worker
    task_id, session_config, rpc_layer)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 771, in _worker_fn
    return worker_fn(strategy)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 771, in _worker_fn
    return fn(instance, model, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 681, in fit
    return fn(instance, model, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 681, in fit
    steps_name='steps_per_epoch')
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 252, in model_iteration
    steps_name='steps_per_epoch')
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 252, in model_iteration
    callbacks._call_begin_hook(mode)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 246, in _call_begin_hook
    callbacks._call_begin_hook(mode)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 246, in _call_begin_hook
    self.on_train_begin()
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 362, in on_train_begin
    self.on_train_begin()
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 362, in on_train_begin
    callback.on_train_begin(logs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 905, in on_train_begin
    callback.on_train_begin(logs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 905, in on_train_begin
    self.model, self.filepath))
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\distribute\multi_worker_training_state.py"", line 60, in __init__
    self.model, self.filepath))
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\keras\distribute\multi_worker_training_state.py"", line 60, in __init__
    initial_value=CKPT_SAVED_EPOCH_UNUSED_VALUE, name='ckpt_saved_epoch')
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 262, in __call__
    initial_value=CKPT_SAVED_EPOCH_UNUSED_VALUE, name='ckpt_saved_epoch')
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 262, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 256, in _variable_v2_call
    return cls._variable_v2_call(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 256, in _variable_v2_call
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 60, in getter
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 60, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1250, in creator_with_resource_vars
    return captured_getter(captured_previous, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1250, in creator_with_resource_vars
    return self._create_variable(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\collective_all_reduce_strategy.py"", line 368, in _create_variable
    return self._create_variable(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\collective_all_reduce_strategy.py"", line 368, in _create_variable
    _real_mirrored_creator, *args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\mirrored_strategy.py"", line 251, in _create_mirrored_variable
    _real_mirrored_creator, *args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\mirrored_strategy.py"", line 251, in _create_mirrored_variable
    value_list = real_mirrored_creator(devices, *args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\collective_all_reduce_strategy.py"", line 355, in _real_mirrored_creator
    value_list = real_mirrored_creator(devices, *args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\distribute\collective_all_reduce_strategy.py"", line 355, in _real_mirrored_creator
    v = next_creator(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 237, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2551, in default_variable_creator_v2
    v = next_creator(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 237, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2551, in default_variable_creator_v2
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 464, in __init__
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\variables.py"", line 264, in __call__
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 618, in _init_from_args
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 464, in __init__
    graph_mode=self._in_graph_mode)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 225, in eager_safe_variable_handle
    shape=shape)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 618, in _init_from_args
    shape, dtype, shared_name, name, graph_mode, initial_value)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 141, in variable_handle_from_shape_and_dtype
    container=container)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py"", line 1612, in var_handle_op
    graph_mode=self._in_graph_mode)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 225, in eager_safe_variable_handle
    shape, dtype, shared_name, name, graph_mode, initial_value)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 141, in variable_handle_from_shape_and_dtype
    container=container)
  File ""F:\ffa_dev\deep-learning-dev-dist\env\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py"", line 1612, in var_handle_op
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: container="""", dtype=DT_INT32, shape=[], shared_name=""cd2c89b7-88b7-44c8-ad83-06c2a9158347""
	.  Registered:  device='CPU'
  device='GPU'; dtype in [DT_HALF]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_VARIANT]
 [Op:VarHandleOp] name: ckpt_saved_epoch/
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: container="""", dtype=DT_INT32, shape=[], shared_name=""cd2c89b7-88b7-44c8-ad83-06c2a9158347""
	.  Registered:  device='CPU'
  device='GPU'; dtype in [DT_HALF]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_VARIANT]
 [Op:VarHandleOp] name: ckpt_saved_epoch/
2019-07-26 12:00:31.325341: W tensorflow/core/common_runtime/eager/context.cc:232] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
2019-07-26 12:00:31.339402: W tensorflow/core/common_runtime/eager/context.cc:232] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

Process finished with exit code 0
```"
31069,Instructions for updating: Use standard file APIs to delete files with this prefix. While training chicken pose estimation algorithm,"
![11](https://user-images.githubusercontent.com/53338377/61946734-c710f300-afbc-11e9-8f08-b2eab9a06445.PNG)

While training chicken pose estimation data on google colab using this pose estimation algorithm, this is the point where it stucks.  This is the command i am using;
""import os
os.chdir(""/content/gdrive/My Drive/pose-tensorflow-master/models/coco/train"")
!ls ../../
!TF_CUDNN_USE_AUTOTUNE=0 CUDA_VISIBLE_DEVICES=0 python3 ../../../train.py""
what should i do for that?"
31068,CUDA_ERROR_ILLEGAL_ADDRESS but never on low power mode,"**System information**
- Windows 10
- TensorFlow version 1.14 from PIP
- python 3.6.8
- cudatoolkit 10.0.130
- cudnn 7.6.0
- vs2015_runtime  14.15.26706
- GPU model and memory: RTX 2070 - 6315 MB memory

**Describe the current behavior**

When I run TF code ildoonet/tf-pose-estimation on my RTX 2070 I have an issue about:

`2019-07-26 11:57:11.701402: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-07-26 11:57:11.710473: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: error destroying CUDA event in context 0x227ca310ac0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-07-26 11:57:11.717344: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: error destroying CUDA event in context 0x227ca310ac0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-07-26 11:57:11.729093: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.`
and... sometimes that crach windows.

But, If I disconnect the power plug in my laptop, It's works. 


**Code to reproduce the issue**
- Install https://www.github.com/ildoonet/tf-pose-estimation
- Execute the example about Web camera: `python .\run_webcam.py --camera 0`



Thx :-)
"
31067,tensorflow polyfill operation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):Current stable build(1.14)
- Are you willing to contribute it (Yes/No):Maybe



**Describe the feature and the current behavior/state.**
As far as I can tell, there is no easy way to create a polygon mask, given a set of points using pure tensorflow operations during run time. For a given image, with a set of landmark points(let's say 8 points), I would like to create a polygon(more precisely,a convex hull) mask with the points as boundaries, which can be used as an attention mask etc. There are opencv/scipy/etc functions that can easily do this with a single line of code. Unfortunately, there are no equivalent functions in tensorflow and these are tricky to code because of the looping involved. Any ideas on if this is possible would be appreciated. Thanks

**Will this change the current api? How?**No. This is just a request to support a new operation

**Who will benefit with this feature?**Tensorflow community/users

**Any Other info.**
"
31065,Failed to clear function handle cache: Invalid argument: Failed to find FunctionLibraryRuntime,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: Nvidia 1080 Ti Founder Edition 11GB

**Describe the current behavior**
After the program runs for a while, I receive the following error.
tensorflow/core/framework/function_handle_cache.cc:30] Failed to clear function handle cache: Invalid argument: Failed to find FunctionLibraryRuntime for device /job:localhost/replica:0/task:0/device:CPU:0 when releasing multi-device function handle 1

**Describe the expected behavior**
Should not return this error

**Code to reproduce the issue**
I'm using tf_agents library

```
tf.compat.v1.enable_v2_behavior()

num_iterations = 2000 # @param

initial_collect_steps = 10 # @param
collect_steps_per_iteration = 1  # @param
replay_buffer_capacity = 100000  # @param

fc_layer_params = (100,)

batch_size = 32  # @param
learning_rate = 1e-3  # @param
log_interval = 100  # @param

num_eval_episodes = 5  # @param
eval_interval = 1000  # @param

environment = SupplyChainEnv2()
environment2 = SupplyChainEnv2()


train_env = tf_py_environment.TFPyEnvironment(environment)
eval_env = tf_py_environment.TFPyEnvironment(environment2)


q_net = q_network.QNetwork(
    train_env.observation_spec(),
    train_env.action_spec(),
    fc_layer_params=fc_layer_params)

optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)

train_step_counter = tf.compat.v2.Variable(0)

tf_agent = DqnAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,
    train_step_counter=train_step_counter)
tf_agent.initialize()

eval_policy = tf_agent.policy
collect_policy = tf_agent.collect_policy

random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),
                                                train_env.action_spec())


def compute_avg_return(environment, policy, num_episodes=5):

  total_return = 0.0
  for _ in range(num_episodes):

    time_step = environment.reset()
    episode_return = 0.0

    while not time_step.is_last():
      action_step = policy.action(time_step)
      time_step = environment.step(action_step.action)
      episode_return += time_step.reward
    total_return += episode_return
    print(episode_return)

  avg_return = total_return / num_episodes
  return avg_return.numpy()[0]


compute_avg_return(eval_env, random_policy, num_eval_episodes)

# Please also see the metrics module for standard implementations of different
# metrics.

replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=tf_agent.collect_data_spec,
    batch_size=train_env.batch_size,
    max_length=replay_buffer_capacity)

def collect_step(environment, policy):
  time_step = environment.current_time_step()
  action_step = policy.action(time_step)
  next_time_step = environment.step(action_step.action)
  traj = trajectory.from_transition(time_step, action_step, next_time_step)

  # Add trajectory to the replay buffer
  replay_buffer.add_batch(traj)


for _ in range(initial_collect_steps):
  collect_step(train_env, random_policy)

# This loop is so common in RL, that we provide standard implementations of
# these. For more details see the drivers module.

# Dataset generates trajectories with shape [Bx2x...]
dataset = replay_buffer.as_dataset(
    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)

iterator = iter(dataset)

# (Optional) Optimize by wrapping some of the code in a graph using TF function.
tf_agent.train = common.function(tf_agent.train)

# Reset the train step
tf_agent.train_step_counter.assign(0)

# Evaluate the agent's policy once before training.
eval_env.reset()
avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)
returns = [avg_return]

for _ in range(num_iterations):

  # Collect a few steps using collect_policy and save to the replay buffer.
  for _ in range(collect_steps_per_iteration):
    collect_step(train_env, tf_agent.collect_policy)

  # Sample a batch of data from the buffer and update the agent's network.
  experience, unused_info = next(iterator)
  train_loss = tf_agent.train(experience)

  step = tf_agent.train_step_counter.numpy()

  if step % log_interval == 0:
    print('step = {0}: loss = {1}'.format(step, train_loss.loss))

  if step % eval_interval == 0:
    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)
    print('step = {0}: Average Return = {1}'.format(step, avg_return))
    returns.append(avg_return)
```

**Other info / logs**
None
"
31064,Support for Latest NDK ( ndk14b above) for  tensorflow/contrib/makefile/Makefile,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?:  No
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): CLANG
- CUDA/cuDNN version:10.1
- GPU model and memory:
8GB


**Describe the problem**
 tensorflow/contrib/makefile/Makefile is desingned to work with Android ndk14b only , None of the latest NDK can be used to compile libtensorflow-core.a for Android using latest ndk , Please modify the Makefile to support latest NDK 

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31062,Why tensorflow r2.0 still load libcudart_static.a in cuda_configure.bzl ,"In tensorflow r2.0, I still can see libcudart_static.a copied to local cuda repository in third_party/gpus/cuda_configure.bzl file, but looks like tensorflow using cuda shared object to access Nvidia GPUs, so why still we need load static library here? "
31061,tf.data.iterator make_initializer multiple creates new ops instead of using old,"System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
    TensorFlow installed from (source or binary): binary
    TensorFlow version (use command below): v1.13.1
    Python version: 3.7.3
    Bazel version (if compiling from source): NA
    GCC/Compiler version (if compiling from source): NA
    CUDA/cuDNN version: 9.2 / 7.6.0
    GPU model and memory: GeForce GTX 1080 Ti

Describe the current behavior:
   I'm using tf.data.dataset.Iterator.from_structure to make an iterator. And I defined a separate function to use iterator.make_initializer(dataset) to make new initializer after a certain time interval (because I'm doing experiment for growing dataset). The dataset is created using tf.data.Dataset.from_tensor_slices((imgs,labels)). But each time this function is called, it creates new ops causing the memory to grow. 
   I know i can use placeholder, but I want to use tf.data.dataset as it's faster. 
   I've tried using with tf.name_scope('dataset'): when creating the tf.data.Dataset (also tried tf.variable_scope). 
   But it simply avoid the name clashes by creating new op so instead of  dataset\something it gives dataset_1\something etc.  


Observed behavior: new ops are created instead of reusing

Describe the expected behavior
Should reuse the variables and not create new ops."
31058,No Matching Distribution Found for Tensorflow,"**System information**
- Linux Ubuntu 18.04:
- Nvidia Jetson Xavier
- Python version: 3.6.8

**Describe the problem**

`pip install tensorflow` and `pip3 install tensorflow`

returns:

`ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)`
`ERROR: No matching distribution found for tensorflow`

And trying to install the .whl file directly yields:

`ERROR: tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform.`

I can't understand why I'm getting these errors. My OS and specifications seem compatible.

**Any other info / logs**
https://imgur.com/XtowrBL"
31057,`tf.keras.Model.save` does not support subclassed model when saving model as SavedModel format,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NA
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190724
- Python version: 3.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`tf.keras.Model.save`  **DOES NOT** support subclassed model when saving model as SavedModel format
**Describe the expected behavior**
`tf.keras.Model.save` **SHOULD** support subclassed model when saving model as SavedModel format
**Code to reproduce the issue**
```
import tensorflow as tf


class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.d = tf.keras.layers.Dense(2)

    @tf.function
    def call(self, x, training=True, mask=None):
        return self.d(x)


model = Model()
model(tf.random.normal((2, 3)))
# next line raises errors
model.save(""save/model"", save_format=""tf"")
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31055,Saved model and serving preprocessing,"Using: tensorflow2.0.0-beta1
OS: macOS 10.14.6
python: 3.7

This might be a docs or feature request - not sure, so please forgive my general posting of the issue.

I used to use `estimator.export_savedmodel('export', serving_input_receiver_fn)` which would allow me to define `serving_input_receiver_fn ` where I could transform my inputs. Such transformations in my case were decode jpeg, convert to float, resize.

With Keras being the recommended high-level API with 2.0, I'm looking for information on how to do this. 
An example model is:
```python
mobilenet_url = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4'

input = l.Input(shape=(224,224,3), name='input_image')

mobilenet = hub.KerasLayer(mobilenet_url)(input)
logits = l.Dense(units=1, activation=tf.nn.sigmoid, name='prediction')(mobilenet)

model = tf.keras.Model(inputs=input, outputs=logits)
```
Train and then:
```python
tf.saved_model.save(model, 'export/mobilenet_finetuned')
```

Currently the read, decode, resize is being done in `tf.data.Dataset` map functions. I can only see a way to provide `TensorSpec` describing the input, but no way to provide a preprocessing function when saving the model here for serving.

- Is this a documentation issue? If so, guidance on how it's done would be nice and I'd be happy to contribute to the docs.
- Is it not possible to do this preprocessing with the saved model with Keras and if I want to do this, stick to Estimators?"
31054,I0725 19:06:30.700653 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore (When Training the tf estimator model ),"```
estimator = tf.contrib.estimator.DNNEstimator(
    head=multi_label_head,
    hidden_units=[64, 10],
    feature_columns=[descriptions_embeddings])

labels = np.array(train_encoded)
features = {
    'descriptions': np.array(train_descriptions)
}
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    features,
    labels,
    shuffle=True,
    batch_size=100,
    num_epochs=20)

estimator.train(input_fn=train_input_fn)
```

> INFO:tensorflow:Calling model_fn.
> I0725 19:06:29.207903 7708 tf_logging.py:115] Calling model_fn.
> INFO:tensorflow:Saver not created because there are no variables in the graph to restore
> I0725 19:06:30.700653 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore
> INFO:tensorflow:Saver not created because there are no variables in the graph to restore
> I0725 19:06:31.963634 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore

**_ValueError: Feature descriptions is not in features dictionary._**

originally defined at:
File ""C:\Users\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\estimator\canned\dnn.py"", line 108, in dnn_logit_fn
name='dnn')
File ""C:\Users\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\estimator\canned\dnn.py"", line 143, in init
create_scope_now=False)
File ""C:\Users\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\feature_column\feature_column.py"", line 323, in init
self._name, internal_input_layer, create_scope_now=create_scope_now)
File ""C:\Users\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\template.py"", line 154, in make_template
**kwargs)"
31053,"TFLite Interpreter, allocate_tensors() failed to prepare, not kTFLiteInt8/Uint8","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The graph only consist of ```tf.split()```, where I pin down to for this issue.
After quantization with ```representative_dataset()```, the interpreter fail to `allocate_tensor()`.
`RuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTFLiteUInt8 || op_context.input->type == kTFLiteInt8 was not true.Node number 0 (DEQUANTIZE) failed to prepare`.
I assume ```tf.split()``` cannot be quantize, and nothing to be quantize actually, since if I add ```tf.lite.OpsSet.TFLITE_BUILTINS_INT8``` to fully quantize into INT8, it tell me SPLIT_V is not supported.
So this simple graph should not have any quantize/dequantize at tf.split ops?
Thus checking type if is Int8 and has dequantize layer here seems not to make sense?

**Describe the expected behavior**
Interpreter sucessfully

**Code to reproduce the issue**
```
inputs_raw = tf.placeholder(tf.float32, shape=[1, 32, 32, 1], name='inputs_raw')
outputs = tf.split(value = inputs_raw, num_or_size_splits = 2, axis = 1)[0]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    converter = lite.TFLiteConverter.from_session(sess, [inputs_raw], [outputs])

    converter.optimizations = [lite.Optimize.DEFAULT]
    def representative_data_gen():
        for i in range(1):
            yield [np.random.random_sample((1, 32, 32, 1)).astype(np.float32)]
            #yield [sess.run(tf.random.normal(inputs_raw.get_shape(), dtype = tf.float32, seed = 1))] # both have issue
    converter.representative_dataset = representative_data_gen
    tflite_model = converter.convert()
    open(""converted_model_quant_test.tflite"", ""wb"").write(tflite_model)

interpreter = lite.Interpreter(model_path = ""converted_model_quant_test.tflite"")
nterpreter.allocate_tensors()
```

**Other info / logs**
Traceback (most recent call last):
  File ""main.py"", line 532, in <module>
    interpreter.allocate_tensors()
  File ""/proj/gpu_xxx/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 95, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/proj/gpu_xxx/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 0 (DEQUANTIZE) failed to prepare.

"
31052,CUDA optimization: using read-only cache,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**

@chsigg 

I wanted to propose adding `__restrict__` or `const __restrict__` tags on `input` and `output` arrays of ALL tensorflow kernels. But before I go on, I wanted to check a couple things:

1. can I assume that input and output arrays given to a CUDA kernel do not overlap?
2. Can I assume that input arrays are not modified, or would I have to check this per-kernel bases?

**Will this change the current api? How?**

**Who will benefit with this feature?**

Since pytorch arrays are stored in NHWC data layout, many kernels waste global memory read bandwidth. By adding `__restrict__` or `const __restrict__` tags, we can let the compiler cache the reads to read-only cache which displays about 50 ~ 100x faster clockspeed on memory reads.

**Any Other info.**
"
31051,caching cuDNN CNN kernel choices,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**

Pytorch as a functionality to cache the chosen cuDNN kernels when the input dimensions are identical to previous calls:

torch.backend.cudnn.benchmark = True
This improves throughput of a network by 30~40% (again, when the input dimensions of the network do not change).

Here is where the caching is implemented in Aten:
https://github.com/pytorch/pytorch/blob/358fb51e773b8ad509ec270caee5ec1c51d82f38/aten/src/ATen/native/cudnn/Conv.cpp#L340

Do you have any plans to add similar functionality? If not, I would love to send a PR.

**Will this change the current api? How?**

Like pytorch, it would require setting a flag.

**Who will benefit with this feature?**

All forward passes that are performed with fixed input sizes and types

**Any Other info.**
"
31050,Can't set sprite in Keras Tensorboard Callback,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): maybe, but not at this moment



**Describe the feature and the current behavior/state.**

There is no way to specify a sprite for embedding visualizations with the Tensorboard Callback in Keras. 


**Will this change the current api? How?**

It would add two more parameters to `Tensorboard.__init__()`
 - `embeddings_image_path=None,`
 - `embeddings_image_size=None,`

**Who will benefit with this feature?**

Anyone who wants to be able to see what their embedding clusters actually look like. NLP people are covered tho.

**Any Other info.**

Here's basically what you need. I just don't have the time to write unittests and docs. 

```python
class Tensorboard(Callback):

    def __init__(self,
                   ...
                   embeddings_freq=0,
                   embeddings_layer_names=None,
                   embeddings_metadata=None,
                   embeddings_data=None,

                   ### add
                   embeddings_image_path=None,
                   embeddings_image_size=None,
                   ...):

        ### add 
        self.embeddings_image_path = embeddings_image_path
        self.embeddings_image_size = embeddings_image_size


    def set_model(self, model):
        ...

        ### add/rework a tad ~L282-L290
        # allow embedding parameters to be passed as a dict(layer_name -> param)
        # or as a single string that applies to all layers
        layer_names = embeddings_vars.keys()

        embeddings_metadata = (
            {name: self.embeddings_metadata for name in layer_names}
            if not isinstance(self.embeddings_metadata, dict) else data)
            
        embeddings_image_path = (
            {name: self.embeddings_image_path for name in layer_names}
            if not isinstance(self.embeddings_image_path, dict) else data)

        embeddings_image_size = (
            {name: self.embeddings_image_size for name in layer_names}
            if not isinstance(self.embeddings_image_size, dict) else data)
        ###
        
        try:
            from tensorboard.plugins import projector
        except ImportError:
            raise ImportError('Failed to import TensorBoard. Please make sure that '
                              'TensorBoard integration is complete.""')

        # TODO(psv): Add integration tests to test embedding visualization
        # with TensorBoard callback. We are unable to write a unit test for this
        # because TensorBoard dependency assumes TensorFlow package is installed.
        config = projector.ProjectorConfig()
        for layer_name, tensor in embeddings_vars.items():
            embedding = config.embeddings.add()
            embedding.tensor_name = tensor.name

            if (embeddings_metadata is not None and
                layer_name in embeddings_metadata):
                embedding.metadata_path = embeddings_metadata[layer_name]

            ### add 
            if (embeddings_image_path is not None and
                embeddings_image_size is not None and
                layer_name in embeddings_image_path and
                layer_name in embeddings_image_size):

                # this is what I need
                embedding.sprite.image_path = embeddings_image_path[layer_name]
                embedding.sprite.single_image_dim.extend(embeddings_image_size)
            ###

        projector.visualize_embeddings(self.writer, config)
"
31046,[TF 2.0] tf.keras.applications.MobileNetV2 can't be converted to TFLite model,"I convert MobileNetV2 to TFLite from `tf.keras.applications.MobileNetV2`:

```
mobile_net = MobileNetV2(
                input_shape=(96, 96, 3), 
                weights='imagenet', 
                alpha=0.5, 
                include_top=False,
                pooling='avg'
)
```

I got the following error when try to convert to TFlite:

```
ConverterError: TOCO failed. See console for info.
2019-07-26 01:35:52.292264: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 393 operators, 739 arrays (0 quantized)
2019-07-26 01:35:52.303848: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 393 operators, 739 arrays (0 quantized)
2019-07-26 01:35:52.467866: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1086 operators, 2217 arrays (0 quantized)
2019-07-26 01:35:52.490639: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1813] Check failed: axis < input_shape.dimensions_count() (1639659256 vs. 3)
Fatal Python error: Aborted

Current thread 0x00007f7f05021700 (most recent call first):
  File ""/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/otatanov/.conda/envs/project/bin/toco_from_protos"", line 10 in <module>
```

Maybe it is linked with [this issue](https://github.com/tensorflow/tensorflow/issues/22109)."
31043,Feature Request: Able to store weights to disk when not in use. For sparse inputs/data/training,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.0 Beta
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Ability to store weights to disk when not in use, for sparse inputs/data/training.

An example of is training a very large number of item embeddings (such as word2vec). Say that the number of items(words) is very large, 7 or 8 figures. If trying to train this in Tensorflow, this would cause memory issues since there's a very large number of parameters loaded into memory at a time. For example, if there are 10 million items, and the embedding size is 256, then there are 

10 million * 256 embedding size = 256 million parameters

Which all loaded into memory at a time. However during each training step, only a small fraction of these parameters go through a forward or backwards pass. If we can somehow have the other parameters stored to disk when not in use, that would save on a lot of memory, and therefore greatly expand the number of parameters researchers can test with. 

**Will this change the current api? How?**

I am not sure what is the best way to implement this. Perhaps a new datatype? Or the current datatypes can have a store option. 

**Who will benefit with this feature?**

Anyone working with sparse inputs, data, or training. 

**Any Other info.**

This may give some inspiration. Here is a repo that uses a sqlite database to store the vectors, so not all of them have to be loaded at once.

https://github.com/plasticityai/magnitude

I am thinking, a hacky way to go about this is to have a dummy embedding layer in Keras, which holds whatever parameters that will be used in the training step, and keep the rest in sqlite database. After each training step, the sqlite database values are exchanged with the ones in keras dummy layer, and the keras dummy layer values are replaced with the next values which will be used in the next forward pass. 

Though things will get a little more messy for optimizers with parameter-specific momentum, like Adagrad/Adam."
31040,TFLite conversion fails when using BatchNorm after Reshape (Check failed: dim_x == dim_y),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 & Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.12.1-6931-g2b5ece29d3 1.15.0-dev20190724**
- Python version: **3.6.8**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
Trying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering. In particular, the toco converter relocates the mul operation from the BN layer to before the reshape layer, at which point the layers do not have compatible dimensions. This results in the `Check failed: dim_x == dim_y` error. 

From looking at the Graphviz video, this change is introduced in frame 38.

**Describe the expected behavior**
The converter should not reorder operations across reshape if it would cause the dimensions to no longer match.

In addition, the check failure message should provide more information about the location of the error, such as the originating layer names (which are visible in the Graphviz outputs).

**Code to reproduce the issue**
```
#!/usr/bin/env python3.6

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as KL
import tensorflow.keras.backend as K

keras.backend.set_learning_phase(0)  # Build in test mode.

# Construct a minimal graph with Reshape followed by BatchNormalization
input_tensor = KL.Input(shape=[10, 15, 512], name=""input_tensor"")
r = KL.Reshape(target_shape=[10, 30, 256], name=""reshape"")(input_tensor)
output_bn = KL.BatchNormalization(axis=3, name=""BN"")(r)
keras_model = keras.models.Model([input_tensor], [output_bn])

# Convert the graph to TFLite.
converter = tf.lite.TFLiteConverter.from_session(K.get_session(), keras_model.inputs, keras_model.outputs)
#converter.dump_graphviz_dir = ""./graphviz""
#converter.dump_graphviz_video = True
tflite_model = converter.convert()
```

**Other info / logs**
This occurs on all version of TF I've tested (1.13.1, 1.14.0, tf-nightly). It's possible that this sequence of operations just isn't supported, but this should be explicitly stated if so. The error message is quite vague and made the problematic sequence of ops extremely difficult to track down in a large graph.

Relevant log messages:
```
2019-07-25 11:06:27.322822: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-07-25 11:06:27.360607: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-07-25 11:06:27.370031: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node BN/gamma/Assign doesn't exist in graph
Traceback (most recent call last):
  File ""./bug_report.py"", line 21, in <module>
    tflite_model = converter.convert()
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 438, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 189, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-07-25 11:06:31.071452: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 12 operators, 24 arrays (0 quantized)
2019-07-25 11:06:31.071874: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 12 operators, 24 arrays (0 quantized)
2019-07-25 11:06:31.072261: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)
2019-07-25 11:06:31.072428: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (512 vs. 256)Dimensions must match
Fatal Python error: Aborted

Current thread 0x00007f026b9a0740 (most recent call first):
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""[venv]/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""[venv]/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""[venv]/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```
"
31039,TensorRT Slowdown Native->FP32 and FP16->INT8; File Size Increase,"## **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, [nvcr.io/nvidia/tensorrt:19.02-py3](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-7024-g24b3e6cf73 1.15.0-dev20190725
- Python version: 3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0.130 / 7.4.2 as per the above linked container
- GPU model and memory: Tesla V100 32GB

Relevant output from `pip freeze`:
```
tf-estimator-nightly==1.14.0.dev2019072201
tf-nightly-gpu==1.15.0.dev20190725
```

## **Describe the current behavior**

**Inference Speed (frames per second)**

| Model | Native | FP32 | FP16 | INT8 | 
| - | - | - | - | - |
| tiny-yolo | 348 | 333 | 402 | 415 |
| big-yolo | 125 | 140 | 243 | 208 |

**1. Why is there a slowdown for tiny native->FP32?** (@pooyadavoodi same as [here](https://github.com/tensorflow/tensorflow/issues/30717#issuecomment-513938562))
**2. Why is this slowdown not consistent for big?**
**3. Why is there a slowdown for big FP16->INT8?**

**Model Size (megabytes)**

| Model | Native | FP32 | FP16 | INT8 | 
| - | - | - | - | - |
| tiny-yolo | 35 | 67 | 44 | 51 |
| big-yolo | 238 | 439 | 288 | 332 |

I understand that there is currently an issue where new graph weights are saved twice to the .pb file (#30717, #30789). Once the weights in the table are adjusted for this double weight saving, the resulting sizes for fp32 and fp16 seem reasonable.

**1. Why is there an increase in size for fp16->int8?**

## **Describe the expected behavior**

I am trying to quantize two different YOLO models (one tiny, one normal) with TensorRT. The goals of this quantization are:

1. speed up inference
2. decrease model size

As quantization and conversion proceeds from native->fp32->fp16->int8, I expect inference time to decrease (FPS to increase), and model size to decrease.

## **Code to reproduce the issue**

I am using [this](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py) script and a few helper functions from [here](https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/graph_utils.py). The two exact scripts that I use are [do.py](https://github.com/tensorflow/tensorflow/files/3432597/do.py.txt) and [utilities.py](https://github.com/tensorflow/tensorflow/files/3432598/utilities.py.txt). Here are the [tiny model](https://drive.google.com/open?id=1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v) and the [big model](https://drive.google.com/open?id=19BzNDCHpDnxj9wU26BsGM4ewS9bKNmpO).

My command for running the experiments:
```bash
python do.py \
--frozen_graph=big-yolov3_frozen.pb \ # or tiny-yolov3_frozen.pb
--native --fp32 --fp16 --int8 \
--batch_size=32 \ # or 128 for tiny
--output_dir=/workspace \
--input_node=inputs --output_node=output_boxes
```


## **Other info / logs**

I ran a couple of experiments just to make sure that the results were consistent.

```
==========================
network: native_tiny-yolov3_frozen.pb,   batchsize 128, steps 100
  fps   median: 350.2,  mean: 348.0,    uncertainty: 1.4,       jitter: 5.1
  latency       median: 0.36551,        mean: 0.36846,  99th_p: 0.42946,        99th_uncertainty: 0.01453

==========================
network: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 340.9,  mean: 333.4,    uncertainty: 1.5,       jitter: 4.4
  latency       median: 0.37546,        mean: 0.38470,  99th_p: 0.47469,        99th_uncertainty: 0.06110

==========================
network: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 403.3,  mean: 402.3,    uncertainty: 0.6,       jitter: 3.6
  latency       median: 0.31740,        mean: 0.31824,  99th_p: 0.34266,        99th_uncertainty: 0.00263

==========================
network: tftrt_int8_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 417.7,  mean: 414.9,    uncertainty: 1.1,       jitter: 4.4
  latency       median: 0.30641,        mean: 0.30873,  99th_p: 0.35451,        99th_uncertainty: 0.01511

==========================
network: native_big-yolov3_frozen.pb,    batchsize 32, steps 100
  fps   median: 125.2,  mean: 124.7,    uncertainty: 0.3,       jitter: 1.4
  latency       median: 0.25553,        mean: 0.25677,  99th_p: 0.28257,        99th_uncertainty: 0.00308

==========================
network: tftrt_fp32_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 140.3,  mean: 140.2,    uncertainty: 0.4,       jitter: 1.9
  latency       median: 0.22802,        mean: 0.22839,  99th_p: 0.25419,        99th_uncertainty: 0.00890

==========================
network: tftrt_fp16_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 237.6,  mean: 242.5,    uncertainty: 1.4,       jitter: 5.4
  latency       median: 0.13469,        mean: 0.13245,  99th_p: 0.17733,        99th_uncertainty: 0.04387

==========================
network: tftrt_int8_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 210.1,  mean: 207.5,    uncertainty: 1.5,       jitter: 2.7
  latency       median: 0.15231,        mean: 0.15657,  99th_p: 0.16928,        99th_uncertainty: 0.16613
```

```
==========================
network: native_tiny-yolov3_frozen.pb,   batchsize 128, steps 100
  fps   median: 357.4,  mean: 354.3,    uncertainty: 1.7,       jitter: 14.5
  latency       median: 0.35814,        mean: 0.36215,  99th_p: 0.44575,        99th_uncertainty: 0.00629

==========================
network: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 324.9,  mean: 319.4,    uncertainty: 1.4,       jitter: 2.5
  latency       median: 0.39401,        mean: 0.40173,  99th_p: 0.49218,        99th_uncertainty: 0.06484

==========================
network: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 376.1,  mean: 372.8,    uncertainty: 1.1,       jitter: 1.7
  latency       median: 0.34036,        mean: 0.34363,  99th_p: 0.38601,        99th_uncertainty: 0.02051

==========================
network: tftrt_int8_tiny-yolov3_frozen.pb,       batchsize 128, steps 100
  fps   median: 392.1,  mean: 391.3,    uncertainty: 0.5,       jitter: 1.8
  latency       median: 0.32645,        mean: 0.32717,  99th_p: 0.33765,        99th_uncertainty: 0.01967

==========================
network: native_big-yolov3_frozen.pb,    batchsize 32, steps 100
  fps   median: 124.3,  mean: 124.0,    uncertainty: 0.4,       jitter: 1.7
  latency       median: 0.25737,        mean: 0.25842,  99th_p: 0.31292,        99th_uncertainty: 0.00355

==========================
network: tftrt_fp32_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 141.0,  mean: 140.7,    uncertainty: 0.3,       jitter: 0.3
  latency       median: 0.22690,        mean: 0.22761,  99th_p: 0.24239,        99th_uncertainty: 0.00581

==========================
network: tftrt_fp16_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 247.4,  mean: 245.9,    uncertainty: 1.0,       jitter: 4.4
  latency       median: 0.12934,        mean: 0.13044,  99th_p: 0.16018,        99th_uncertainty: 0.02011

==========================
network: tftrt_int8_big-yolov3_frozen.pb,        batchsize 32, steps 100
  fps   median: 206.0,  mean: 204.5,    uncertainty: 1.4,       jitter: 1.0
  latency       median: 0.15536,        mean: 0.15885,  99th_p: 0.16454,        99th_uncertainty: 0.17423
```"
31035,microcontroller ops,"HI, i just want to know in all_ops_resolver.cc 
it only has these ops for microcontroller.

> TfLiteRegistration* Register_DEPTHWISE_CONV_2D();
> TfLiteRegistration* Register_FULLY_CONNECTED();
> TfLiteRegistration* Register_SOFTMAX();
> TfLiteRegistration* Register_CONV_2D();
> TfLiteRegistration* Register_AVERAGE_POOL_2D();
> TfLiteRegistration* Register_MAX_POOL_2D();
> TfLiteRegistration* Register_ABS();
> TfLiteRegistration* Register_PRELU();
>

[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc](url)

And in hello world test, the keras frame can be used, so i want to know if it is possible to use the model with tf.keras.layers.Conv2D and tf.keras.layers.MaxPool2D?

And if is possible to use tf.keras.layers.Convolution1D and tf.keras.layers.MaxPooling1D?


Besides, if the model can be converted to .tflite means that the model can be deploy to the sparkfun board?

thank you
"
31034,Quantization Aware Training On Eager Mode,"I am developing my model in the eager mode by using Keras models. 
I read a couple of examples and guidelines in quantization aware training. 
Most of them are not using gradient tape but uses the inbuilt training function
to train and it looks straight forward. But let's say, someone, is calculating the gradient and have written the training model in a custom way. 
How to use quantization aware training in such a case?
I am asking this question because the function call takes input from a created tf.Graph() instance. If someone has tf.GradientTape() instance, how to deal with this?

I am new to Tensorflow concepts, maybe I am not following it correctly.  It will be great if someone can explain to me how to use quantization aware training in general. "
31033,Failed to load the native TensorFlow runtime - Tensorflow 1.14.0 (CPU) - Python 3.7.3,"System information:

Have I written custom code: No
OS Platform and Distribution: Windows Server 2008 R2 Standard
TensorFlow installed from: pip install
TensorFlow version: 1.14.0 CPU 
Python Version: 3.7.3
Anaconda 2019.07 for Windows
CPU Intel Xeon X5650 @ 2.67GHz 

The installation seems to work but when I try to import tensorflow I get the following error:

ImportError                               Traceback (most recent call last)
C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

C:\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-d6579f534729> in <module>
----> 1 import tensorflow

C:\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     26 
     27 # pylint: disable=g-bad-import-order
---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     29 from tensorflow.python.tools import module_util as _module_util
     30 

C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

Any ideas?
Thank you-"
31032,timestamps in logs please,"in TF 1.12 using the TF API i get nice timing info in the logs:

```
INFO:tensorflow:Elapsed 5976.983817, Step #10825: rate 0.000200, accuracy 96.9%, cross entropy 0.098157
```

the `Elapsed ...` number therein is in seconds.

in TF 2.0-beta using the keras API i sadly don't:

```
710/710 - 0s - loss: 0.3377 - val_loss: 0.5081
```

the `0s` there seems to be the time for that step, not the cumulative elapsed time, as it never changes over the course of an hour long training run.

at the very least it would be nice to have more significant digits on that `0s`.  better yet, would be elapsed time, or just a simple time stamp prefixed to each log entry.

i've searched the docs, but haven't found a way to do this.  sorry if i missed it.

thanks!"
31031,Building tensorflow 1.14 (bazel 0.24.1) on Jetson Xavier with the trouble,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution 
Linux Ubuntu 18.04

- TensorFlow installed from (source or binary):
source

- TensorFlow version:
1.14

- Python version:
3.6.8

- Installed using virtualenv? pip? conda?:
pip3

- Bazel version (if compiling from source):
Build label: 0.24.1- (@non-git)
Build target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jul 25 13:20:03 2019 (1564060803)
Build timestamp: 1564060803
Build timestamp as int: 1564060803

- GCC/Compiler version (if compiling from source):
gcc 5.5.0

- CUDA/cuDNN version: 
CUDA 10.0
cuDNN 7.5

- GPU model and memory:
Nvidia Jetson AGX Xavier


**Describe the problem**
I try to install tensorflow 1.14 with next config:
`bazel build --config=monolithic --jobs 8 -c opt --linkopt='-lrt' //tensorflow:libtensorflow_cc.so`

But i get an error
```
Starting local Bazel server and connecting to it...
ERROR: /root/tensorflow-1.14/WORKSPACE:94:1: Traceback (most recent call last):
	File ""/root/tensorflow-1.14/WORKSPACE"", line 94
		tf_workspace()
	File ""/root/tensorflow-1.14/tensorflow/workspace.bzl"", line 708, in tf_workspace
		native.new_http_archive(name = ""double_conversion"", urls =...""], <3 more arguments>)
type 'struct' has no method new_http_archive()
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 5.360s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```

Provide the exact sequence of commands / steps that you executed before running into the problem

"
31030,tf.while loop behaviour in multigpu setting,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary?
- TensorFlow version (use command below): 1.12
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

I would like to verify if tf.while loop in multiple gpu setting is still an issue. I know that using while loop with variables in multigpu will give wrong result due to the while loop mixing up variables from different gpu under different name scope but same variable scope. I am not sure if this is still the case. Also if this concerns variables only or would similar behaviour happens to non-variables."
31029,Will TF2 include implementations of constrained optimizers?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0-beta1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I would like to use an optimizer with optional problem constraints. I am wondering if something like https://www.tensorflow.org/api_docs/python/tf/contrib/constrained_optimization/AdditiveExternalRegretOptimizer will be available in TF2?

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
31028,tensorflow.keras.layers.TimeDistributed seems to reset batch-size,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.14.0
- Python version:3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
TimeDistributed resets the batch_shape given in the Input layer.
Could be that this is intended. If so, could you give me a hint how to run the example below? Would be really appreciated.
**Describe the expected behavior**
When using keras (not tensorflow.keras) the example below works fine
So, I expect that wen given the batchsize, TimeDistributed keeps the batchsize.

**Code to reproduce the issue**
```
import tensorflow.keras as keras
keras.backend.set_image_data_format('channels_first')
resnet = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')

inputShape = (1,None,3,224,224)
input = keras.layers.Input(batch_shape = inputShape)
x = keras.layers.TimeDistributed(resnet)(input)
x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)
x = keras.layers.LSTM(256, stateful=True)(x)
```

**Other info / logs**
```
ValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: 
- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.
- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.
```"
31027,BatchNormalization axis=0 fails on batch dim changes.,"Will close if this ends up to not be a bug. Posted on stackoverflow now and will follow up later.

https://stackoverflow.com/questions/57202668/keras-batchnormalization-only-works-for-constant-batch-dim-when-axis-0"
31026,How to release keras compiled model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.14.0
- Python version:3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
It seems that tensorflow will increase memory usage when I compiled keras model serveral time.
There is some way to release unused compiled model 
**Describe the expected behavior**
Release compiled model when I don't need it anymore
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` python
model.compile()
""""""freeze training""""""

model.compile()
""""""fine-tuning training""""""
```
Replicate code: https://colab.research.google.com/drive/1GafCUt6BXjKY0LA3lc8fgGhDPwZ6Jgs3

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31024,tensorflow.keras.optimizers.Adadelta() Inconsistencies With keras.optimizers.Adadelta(),"**System information**
- OS Platform and Distribution: Linux-4.9.0-8-amd64-x86_64-with-debian-9.9 . 
- TensorFlow version: v1.14.0-9-gc407b045b8 1.14.0 . 
- Python version: 3.6.6 . 

**Describe the current behavior**  
Take a look at the following two Kaggle kernel:  
1. https://www.kaggle.com/ilhamfp31/keras-mnist  
2. https://www.kaggle.com/ilhamfp31/tensorflow-keras-mnist

Both of them run the same code with the same OS, python version and TensorFlow version. The only difference is the first one importing from keras and the second one from tensorflow.keras. Optimizer Adadelta produces a different result because of the default configuration is not the same. The difference lies in the learning rate.

`>> print(keras.optimizers.Adadelta().get_config())`
`{'lr': 1.0, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-07}`

`>> print(tensorflow.keras.optimizers.Adadelta().get_config())`
`{'name': 'Adadelta', 'learning_rate': 0.001, 'decay': 0.0, 'rho': 0.95, 'epsilon': 1e-07}`


**Describe the expected behavior**
`keras.optimizers.Adadelta()` and `tensorflow.keras.optimizers.Adadelta()` should produce the same result as shown by optimizer Adam. Looking at the source code, both Adam optimizer code in [optimizer.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizers.py) and [optimizer_v2/adam.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/adam.py) default parameter is consistent with [keras optimizer code](https://github.com/keras-team/keras/blob/master/keras/optimizers.py). This is not true with Adadelta. While the code in [optimizer.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizers.py) is consistent, the code in [optimizer_v2/adadelta.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/adadelta.py) is not. 

**Code to reproduce the issue**
Code needed to reproduce the issue is available by downloading it from the given Kaggle kernel link. Simply click three grey dot on the upper right corner and click `Download code`.
"
31022,"Invalid result on some GPUs, probably einsum","**System information**
- I have written custom code.
- Linux Ubuntu 19.04, Debian 9.9
- TensorFlow installed from binary
- TensorFlow version 2.0.0-b1, 1.14, 1.15.0
- Python version: 3.7.1, 3.7.3
- CUDA/cuDNN version: 10.0, 10.1
- GPU model and memory: Tesla T4, Tesla P4, P100, does not exist on k80

**Describe the current behavior**

The result on the GPU is significantly different from CPU. It is correct on CPU, but wrong on GPU. The difference on GPU is much bigger than zero.

**Describe the expected behavior**

The print out of the difference should be zero.

**Code to reproduce the issue**

**tf 2.0.0-b1**
```

import tensorflow as tf
import numpy as np



@tf.function
def sample_y_nn_tf(w, X_sample, config):

        layers = config['layers']
        current_w_index = 0
        h = X_sample
        w_shape = tf.shape(input=w)

        l = layers[0]
        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))
        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
        h = tf.einsum('lm,ikjmn->likjn', h, w_current) + b_current
        current_w_index = current_w_index + (l[0] + 1) * l[1]

        for l in layers[1:]:
            h = tf.nn.elu(h)
            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))
            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
            h = tf.einsum('likjm,ikjmn->likjn',h,w_current)+b_current
            current_w_index = current_w_index+(l[0]+1)*l[1]

        h = tf.squeeze(h, axis=4)

        return h

config = {'batch_size': 8, 'layers': [[2,2],[2,1]]}

np.random.seed(111)

w = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)
x_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)

permutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)

print('permutation', permutation)


with tf.device(""/cpu:0""):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    print('difference on cpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))

with tf.device(""/gpu:0""):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    print('difference on gpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))


```

Output:

> difference on cpu 0.0
> difference on gpu 37953784.0

**tf.1.14**

```
from tensorflow.python.framework.versions import VERSION
import tensorflow as tf
import numpy as np
import os

print(tf.__version__)

@tf.function
def sample_y_nn_tf(w, X_sample, config):
    
        layers = config['layers']
        current_w_index = 0
        h = X_sample
        w_shape = tf.shape(input=w)
        
        l = layers[0]
        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))
        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
        h = tf.einsum('lm,ikjmn->likjn', h, w_current) + b_current
        current_w_index = current_w_index + (l[0] + 1) * l[1]
        
        for l in layers[1:]:
            h = tf.nn.elu(h)
            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))
            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
            h = tf.einsum('likjm,ikjmn->likjn',h,w_current)+b_current
            current_w_index = current_w_index+(l[0]+1)*l[1]
    
        h = tf.squeeze(h, axis=4)

        return h

config = {'batch_size': 8, 'layers': [[2,2],[2,1]]}

np.random.seed(111)

w = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)
precision = np.exp(np.random.normal(size=(config['batch_size'],64,512))).astype(dtype=np.float32)
x_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)

permutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)

conf = tf.ConfigProto()
conf.gpu_options.allow_growth = True
sess = tf.Session(config=conf)


print('permutation', sess.run(permutation))


with tf.device(""/cpu:0""):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    net_cpu = tf.gather(means,permutation, axis=1)-means_p

with tf.device(""/device:GPU:0""):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    net_gpu = tf.gather(means,permutation, axis=1)-means_p

net_cpu_result = sess.run(net_cpu)
net_gpu_result = sess.run(net_gpu)
print(""tensorflow version: {}"".format(VERSION))
print('difference on cpu', np.sum(np.abs(net_cpu_result)))
print('difference on gpu', np.sum(np.abs(net_gpu_result)))

```
Output:

> difference on cpu 0.0
> difference on gpu 39042744.0

**Other info / logs**

I did not check the tf2 version on K80, P100.
The bug probably persists when using Pythonic slicing operations."
31021,estimator.predcit load model every time,"estimator.predict() works to slowly. I want to predict some text in my model in every 2 seconds. Everytime I call the estimator.predict() function, it loads the model all over again. I want to load the model just once and after that use estimator.predict() every 2 seconds on this same model to get the faster prediction.
"
31020,Symbolic Links Aren't Created When Needed Using TF_SYSTEM_LIBS,"**System information**
- OS: FreeBSD
- Building from source 
- TensorFlow version: V1.14.0
- Python version: 3.6
- Bazel version (if compiling from source):  0.28
- GCC/Compiler version (if compiling from source): Clang 6.0.1

**Describe the problem**

When using _TF_SYSTEM_LIBS_ to build Tensorflow from source. It tries to include files which it has yet to create. In the file third_party/systemlibs/protobuf.BUILD. It creates symbolic links to the header files when building with _TF_SYSTEM_LIBS_. However it tries to compile code which relies on these headers before these headers have been created. If I continue the build without cleaning the build continues as normal. 

Setting jobs to 1 even causes this problem. As the files either haven't been created or haven't finished creating them. I can't tell if this is an issue with Bazel or Tensorflow, but it's either not waiting for the sub command to complete and create these files, or when using _TF_SYSTEM_LIBS_ it is compiling code which relies on these before they're created.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

**Compile Tensorflow with the following _TF_SYSTEM_LIBS:_**

```
absl_py astor_archive 
boringssl
com_github_googleapis_googleapis
com_github_googlecloudplatform_google_cloud_cpp
com_google_protobuf
com_google_protobuf_cc
com_googlesource_code_re2
curl
cython
double_conversion
enum34_archive
flatbuffers
gast_archive
gif_archive grpc
hwloc
icu
jpeg
jsoncpp_git
keras_applications_archive
lmdb
nasm
nsync org_sqlite
pasta
pcre
png_archive
protobuf_archive
six_archive
snappy
swig
termcolor_archive
wrapt
zlib_archive""
```

**Additional enviroment variables for building:**

```
PYTHON_BIN_PATH=${PYTHON_CMD}""
PYTHON_LIB_PATH=""${PYTHON_SITELIBDIR}""
TF_NEED_JEMALLOC=0
TF_NEED_KAFKA=0
TF_NEED_OPENCL_SYCL=0
TF_NEED_AWS=0
TF_NEED_GCP=0
TF_NEED_HDFS=0
TF_NEED_S3=0
TF_ENABLE_XLA=0
TF_NEED_GDR=0
TF_NEED_VERBS=0
TF_NEED_OPENCL=0
TF_NEED_MPI=0
TF_NEED_TENSORRT=0
TF_NEED_NGRAPH=0
TF_NEED_IGNITE=0
TF_NEED_ROCM=0
TF_NEED_CUDA=0
TF_SET_ANDROID_WORKSPACE=0
TF_DOWNLOAD_CLANG=0
TF_NEED_NCCL=0
TF_NEED_OPENCL=0
TF_IGNORE_MAX_BAZEL_VERSION=1
CC_OPT_FLAGS=""-march=${CPU_TARGET}""
PREFIX=""${LOCALBASE}""
```

**Using the follow bazelrc file:**

```
startup --batch

build --strip=never
build --verbose_failures --noshow_loading_progress
test --verbose_test_summary --verbose_failures --noshow_loading_progress

build --spawn_strategy=local --genrule_strategy=local
test --spawn_strategy=local --genrule_strategy=local

fetch --repository_cache=""%%BAZEL_DIR%%/bazel-cache/"" --distdir=""%%BAZEL_DIST%%/bazel-distdir/""
build --repository_cache=""%%BAZEL_DIR%%/bazel-cache/"" --distdir=""%%BAZEL_DIST%%/bazel-distdir/""

build --define=PREFIX=%%LOCALBASE%%
build --define=LIBDIR=%%LOCALBASE%%/lib

build --config=noaws --config=nohdfs --config=noignite --config=nokafka
```

** Build Command Used**

```
bazel --bazelrc=""${WRKDIR}/bazelrc"" ${BAZEL_BOOT} build ${BAZEL_COPT} --host_copt=""-I${LOCALBASE}/include"" \
		--host_linkopt=""-L${LOCALBASE}/lib"" --linkopt=""-L${LOCALBASE}/lib"" --config=opt \
		--incompatible_no_support_tools_in_action_inputs=false \
		--verbose_failures -s \
		//tensorflow:libtensorflow.so \
		//tensorflow:libtensorflow_cc.so \
		//tensorflow:install_headers \
		//tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

Compilation log:


```
SUBCOMMAND: # //tensorflow/core:conv_autotuning_proto_cc_genproto [action 'ProtoCompile tensorflow/core/protobuf/conv_autotuning
.pb.cc [for host]']
(cd /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/bazel_out/f19d4b0a99d0cb4c4a90e3adec2fef45/execroot/org_te
nsorflow && \
  exec env - \
    PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin:/home/Amzo/bin \
  bazel-out/host/bin/external/protobuf_archive/protoc.bin '--cpp_out=bazel-out/host/bin/' -I. -I. -Iexternal/protobuf_archive -I
bazel-out/host/bin/external/protobuf_archive -Iexternal/protobuf_archive -Ibazel-out/host/bin/external/protobuf_archive tensorfl
ow/core/protobuf/conv_autotuning.proto)
SUBCOMMAND: # //tensorflow/core:autotuning_proto_cc_genproto [action 'ProtoCompile tensorflow/core/protobuf/autotuning.pb.cc [fo
r host]']
(cd /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/bazel_out/f19d4b0a99d0cb4c4a90e3adec2fef45/execroot/org_te
nsorflow && \
  exec env - \
    PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin:/home/Amzo/bin \
  bazel-out/host/bin/external/protobuf_archive/protoc.bin '--cpp_out=bazel-out/host/bin/' -I. -Iexternal/protobuf_archive -Ibaze
l-out/host/bin/external/protobuf_archive tensorflow/core/protobuf/autotuning.proto)
ERROR: /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/tensorflow-1.14.0/tensorflow/core/BUILD:2255:1: ProtoCo
mpile tensorflow/core/protobuf/autotuning.pb.cc failed (Exit 1)
google/protobuf/any.proto: File not found.
google/protobuf/duration.proto: File not found.
tensorflow/core/protobuf/autotuning.proto:10:1: Import ""google/protobuf/any.proto"" was not found or had errors.
tensorflow/core/protobuf/autotuning.proto:11:1: Import ""google/protobuf/duration.proto"" was not found or had errors.
tensorflow/core/protobuf/autotuning.proto:52:3: ""google.protobuf.Duration"" is not defined.
```

While I know FreeBSD isn't a supported platform, I just need help finding the culprit to this issue. As the problem only occasionally occurs. If I create a new jail and build, then sometimes it will succeed and sometimes it will fail due to trying to compile source before the proto headers have been linked. This is just based on luck as to whether or not another job has created them.

"
31019,Are tf.TFRecord(num_parallel_reads) and interleave(num_parallel_calls) duplicated?,"Are tf.TFRecord(num_parallel_reads) and interleave(num_parallel_calls) duplicated? what is the behavior if they are both set > 1?  And the difference of shuffle before repeat and shuffle after repeat? I think the design of Dataset is too complicated, we need a more clear approach."
31018,did 2.0beta1 version support cuda 10.1,"i installed it by
`C:\Python37\Scripts>pip install tensorflow-gpu==2.0.0-beta1`
it is done ok
but when i run `import tensorflow as tf` in python,
it reports
`ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda`
indeed i have installed cuda 10.1
i found the file `cudart64_101.dll` at `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin`, and it is in PATH
"
31016,"when using tf.train.exponential_decay , how can i  stop learning_rate changing during test",is there a simple example?
31015,Failed to convert an SSD mobilenet model to a frozen graph with 4 outputs by using export_tflite_ssd_graph.py,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary; with conda environment
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.6.0
- GPU model and memory: GeForce GTX 970, AMD64

**Describe the current behavior**
I get a frozen model with only two outputs (raw_outputs/box_encodings, raw_outputs/class_predictions) when using export_tflite_ssd_graph.py with the extra attribute `add_postprocessing_op True`. I tried this script on these models from model zoo: ssdlite_mobilenet_v2_coco_2018_05_09, ssd_mobilenet_v2_coco_2018_03_29, ssd_mobilenet_v2_oid_v4_2018_12_12

**Describe the expected behavior**
I should get a frozen model with four outputs (detection_boxes, detection_classes, detection_scores, num_boxes) when using `add_postprocessing_op True`.

**Code to reproduce the issue**
`Cd .\ssdlite_mobilenet_v2_coco_2018_05_0`
`Python .\models\research\object_detection\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`

**Other info / logs**
I know the outputs of the frozen graphs because I visualized them and tflite_convert worked successfully with this code: `tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect_raw.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=raw_outputs/box_encodings,raw_outputs/class_predictions`

My goal in general is to convert an ssd model from model zoo to a tflite float model with the expected four outputs. If there is another way to achieve this, I'm happy to hear about it. I've already tried to convert the frozen_inference_graph.pb (provided by model zoo) with tflite_convert but it didn't work. I got errors like:
 `Check failed: other_op->type == OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported.` (ssdlite_mobilenet_v2_coco_2018_05_09) or 
`Check failed: dim_size >= 1 (0 vs. 1)` (ssd_mobilenet_v2_coco_2018_03_29, ssd_mobilenet_v2_oid_v4_2018_12_12).
Searching for solving this problem, I read that some ops are missing for tflite and it is recommended to freeze the graph with export_tflite_ssd_graph.py instead and then convert it to tflite. This was successful but only if you want to work with the raw outputs.

Thanks for helping!"
31014,ConvLSTM2D hidden states aren't updated with Session.run,"Hey guys,

I have a problem when I use a Keras ConvLSTM2D layer in my TensorFlow graph. When I compute the output of the graph using `session.run`, the hidden states of the ConvLSTM2D won't be updated. Here is an minimal example to reproduce the behavior:

```
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import ConvLSTM2D

# create input data
shape = [1, 1, 3, 3, 2]
input_tensor = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)

# create and build layer
conv_lstm = ConvLSTM2D(filters=1,
                       kernel_size=(2, 2),
                       padding='same',
                       return_sequences=True,
                       stateful=True)
conv_lstm.build(shape)

# deploy layer and get its hidden states
conv_lstm_out = conv_lstm(input_tensor)
hidden_states = conv_lstm.states

# create and initialise session
sess = tf.compat.v1.Session()
sess.run(tf.compat.v1.global_variables_initializer())

# run op and print results multiple times
# Remark: output should change, because hidden states should be updated, but they won't
for _ in range(3):
    output, states = sess.run([conv_lstm_out, hidden_states])
    print(output.shape, output)
    print(states[0].shape, states[0])
    print(states[1].shape, states[1])
    print()
```

OS Platform and Distribution: Windows 10
TensorFlow installed from: pip
TensorFlow version: 1.14.0
Python version: 3.6

Does anyone knows a fix to this? Is this a bug in TensorFlow/Keras itself or in my code?
"
31013,[TF 2.0 Docs] Include @tf.function in site/en/r2/tutorials/generative/cvae.ipynb,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
site/en/r2/tutorials/generative/cvae.ipynb

## Description of issue (what needs changing): implement @tf.function decorators in the computation to improve performance, and highlight one of the tf 2.0 features.

### Clear description
When implementing in colab the performance improves from 30s / epoch average to 3.6s /epoch which is a huge benefit, and well worth highlighting/recommending by adding only 4 lines of code.

### Submit a pull request?
I can do that, yes

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
31011,What is HalfPixelScalerForNN in nearest interpolation?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.1-dev20190526
- Python version: 3.7
- Bazel version (if compiling from source): **tf_env_collect.sh just crashes here**

**Describe the current behavior**
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/kernels/resize_nearest_neighbor_op.cc#L111

I am implementing resize op for ONNX. https://github.com/onnx/onnx/pull/2057. I don't know why there is not ""-0.5f"" like vanilla HalfPixelScaler. The comments don't solve my concern. For example, if `(static_cast<float>(x) + 0.5f) * scale` equals 1.4, minus 0.5f or not will brings completely different result, no matter there is or is not `std::floor`.

**Describe the expected behavior**
I think `-0.5f` is more reasonable
"
31010,Detail Design doc of tf.Dataset not available,"Want the Detail Design doc of tf.Dataset not available. The current design include quite a lot of tuning parameters such as num_threads, and we users have little about its functionality. It is impossible for use to debug the performance bottleneck, it just looks like a black box.  I think it is necessary for the dataset developers to publish the design doc, and give more details about the data pipeline.
"
31009,Union of string sets cannot be converted to dense tensor (raises TypeError),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=2.0.0-dev20190724
GIT_VERSION=v1.12.1-6931-g2b5ece29d3
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I get an exception when computing the union of two string sets and trying to convert the output to a dense tensor (`TypeError: Cannot convert 0 to EagerTensor of dtype string`).

**Describe the expected behavior**
I should get no exception.

**Code to reproduce the issue**

```python
import tensorflow as tf
a = tf.constant([[""a"", ""b"", ""c"", """", """"], [""d"", ""e"", """", """", """"]])
b = tf.constant([[""c"", ""e"", ""g"", ""h"", """"], [""d"", """", """", """", """"]])
tf.sparse.to_dense(tf.sets.union(a, b))
```

**Other info / logs**
Here's the stacktrace:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-e742e5c0ff6f> in <module>
      2 a = tf.constant([[""a"", ""b"", ""c"", """", """"], [""d"", ""e"", """", """", """"]])
      3 b = tf.constant([[""c"", ""e"", ""g"", ""h"", """"], [""d"", """", """", """", """"]])
----> 4 tf.sparse.to_dense(tf.sets.union(a, b))

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py in sparse_tensor_to_dense(sp_input, default_value, validate_indices, name)
   1478       default_value=default_value,
   1479       validate_indices=validate_indices,
-> 1480       name=name)
   1481
   1482

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_sparse_ops.py in sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, validate_indices, name)
   3158         ""SparseToDense"", name, _ctx._post_execution_callbacks, sparse_indices,
   3159         output_shape, sparse_values, default_value, ""validate_indices"",
-> 3160         validate_indices)
   3161       return _result
   3162     except _core._FallbackException:

TypeError: Cannot convert 0 to EagerTensor of dtype string
```"
31008,throw std::out_of_range while convert Lite quantization model with representative_dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When convert to Lite quantized model with representative_dataset, it throws `std::out_of_range`, when model includes `tf.split()`
My model implement Pixel Shuffle with some split and concat operation,
I narrow down to tf.split() which is causing the issue

**Describe the expected behavior**
Convert sucessfully

**Code to reproduce the issue**
```
inputs_raw = tf.placeholder(tf.float32, shape=[1, 32, 32, 1], name='inputs_raw')
outputs = tf.split(value = inputs_raw, num_or_size_splits = 2, axis = 1)[0]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    converter = lite.TFLiteConverter.from_session(sess, [inputs_raw], [outputs])

    converter.optimizations = [lite.Optimize.DEFAULT]
    def representative_data_gen():
        for i in range(1):
            yield [np.random.random_sample((1, 32, 32, 1)).astype(np.float32)]
            #yield [sess.run(tf.random.normal(inputs_raw.get_shape(), dtype = tf.float32, seed = 1))] # both have issue
    converter.representative_dataset = representative_data_gen
    tflite_model = converter.convert()
```
**Other info / logs**
W0725 10:03:24.455737 140039335962368 deprecation_wrapper.py:119] From main.py:419: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Tensor(""split:0"", shape=(1, 16, 32, 1), dtype=float32)
W0725 10:03:24.459982 140039335962368 deprecation_wrapper.py:119] From main.py:431: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-25 10:03:24.464880: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /xxxoss/Python3/3.6.3_gpu_tf1131_cuda10-ubuntu16/x86_64/lib:/xxxtools/LSF/10/10.1/linux2.6-glibc2.3-x86_64/lib:/xxxoss/tcl/8.4.19/x86_64/lib:/xxxoss/tcl/8.4.19/x86_64/lib/tcl8.4:/xxxoss/tcl/8.4.19/x86_64/lib/tclx8.4:/lib64:/usr/lib64:/lib:
2019-07-25 10:03:24.464949: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-07-25 10:03:24.465006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a07ws012123): /proc/driver/nvidia/version does not exist
2019-07-25 10:03:24.465369: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-25 10:03:24.508462: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394230000 Hz
2019-07-25 10:03:24.508764: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4283070 executing computations on platform Host. Devices:
2019-07-25 10:03:24.508788: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
W0725 10:03:24.521992 140039335962368 deprecation_wrapper.py:119] From main.py:432: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-07-25 10:03:24.525090: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-25 10:03:24.526496: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-07-25 10:03:24.526607: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-07-25 10:03:24.528117: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-25 10:03:24.528143: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-07-25 10:03:24.528153: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2019-07-25 10:03:24.530117: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2019-07-25 10:03:24.530193: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-07-25 10:03:24.532220: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-25 10:03:24.532249: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 3 nodes (-2), 2 edges (0), time = 0.465ms.
INFO: Initialized TensorFlow Lite runtime.
terminate called after throwing an instance of 'std::out_of_range'
  what():  _Map_base::at
Abort (core dumped)
"
31007,Remove the max_batch_size in tf-trt create_inference_graph  api,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.13.1
- Are you willing to contribute it (Yes/No):  Sure, if I had the wherewithal . 

**Describe the feature and the current behavior/state.**
The max_batch_size batch size limit  can  wreak havoc on codes that  can  increase the batch size dynamically (e.g. going from 25 to 50). 

**Will this change the current api? How?**
 trt.create_inference_graph would no longer need max_batch_size 

**Who will benefit with this feature?**
All

**Any Other info.**
"
31006,Contradicting bazel versions suggested,"### URL(s) with the issue:

https://www.tensorflow.org/install/source_windows#install_bazel

## Description of issue (what needs changing):
It should be Bazel <0.23.0, not 0.24.1
### Clear description
First and last sentence contradict each other:
Install Bazel 0.24.1,
Ensure you install Bazel 0.23.0 or lower.

### Correct links

### Parameters defined

### Returns defined

### Raises listed and defined

### Usage example

### Request visuals, if applicable

### Submit a pull request?
No"
31005,"""Not found: Resource does not exist"" exception thrown in runtime","I am facing a similar error mentioned above. I will try my best to help resolve this issue as it benefits my work as well. It is the same problme with issue #22631, 

OS Platform and Distribution: Linux Ubuntu x86_64 - 4.15.0-52-generic (kernel)
TensorFlow installed from: conda 4.7.5
TensorFlow version: 1.13.1
Bazel version: N/A
CUDA/cuDNN version: 10.0
GPU model and memory: Tesla V100-SXM2-16GB
Exact command to reproduce:
Mobile device: N/A
```
import tensorflow as tf
import numpy as np

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

def discriminative_loss(y_true, y_pred):
    """"""Computes loss for a batch of images
    Args:
        y_true: (n, h, w) where each elements contains the ground truth instance id
        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch
    Returns:
        loss
    """"""
    # Compute the loss for each image in the batch
    def compute_loss(input):
        prediction = input[1]
        label = input[0]

        # Number of clusters in ground truth
        clusters,_ = tf.unique(tf.reshape(label, [-1]))

        # Compute cluster means and variances for each cluster
        def compute_mean(c):
            mask = tf.equal(label[:,:,0], c)
            masked_pixels = tf.boolean_mask(prediction, mask)
            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

            return cluster_mean

        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
        return tf.reduce_mean(cluster_means)

    # We want to know the loss for each image in the batch
    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))
    return losses

def discriminative_loss_working(y_true, y_pred):
    # Compute the loss for only the first image in the batch

    prediction = y_pred[0]
    label = y_true[0]

    # Number of clusters in ground truth
    clusters,_ = tf.unique(tf.reshape(label, [-1]))

    # Compute cluster means and variances for each cluster
    def compute_mean(c):
        mask = tf.equal(label[:,:,0], c)
        masked_pixels = tf.boolean_mask(prediction, mask)
        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)

        return cluster_mean

    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))
    return tf.reduce_mean(cluster_means)

class MyModel(tf.keras.Model):
    def __init__(self, input_shape):
        super(MyModel, self).__init__()
        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))

    def call(self, input):
        return self.conv(input)

input_shape = (1,128,128,3)
def my_gen():
    while True:
        x = np.random.rand(1,input_shape[1], input_shape[2],3)
        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))
        yield x,y

train_dataset = tf.data.Dataset.from_generator(
                    my_gen,
                    (tf.float32, tf.float32),
                    (tf.TensorShape([1,128,128,3]),
                     tf.TensorShape([128,128,1])))
train_dataset = train_dataset.batch(1)
train_dataset = train_dataset.repeat()

model = MyModel(input_shape=input_shape)

# This is a fix to make loading weights possible
# x = tf.zeros((1,) + input_shape)
x = tf.zeros(input_shape)
y = model(x)

with tf.Session(config=config):
    optimizer = tf.keras.optimizers.SGD(lr=0.0001)
    model.compile(loss=discriminative_loss,optimizer=optimizer)
    model.fit(train_dataset, epochs=5, steps_per_epoch=2)
```
[tf_error.log](https://github.com/tensorflow/tensorflow/files/3415129/tf_error.log)
"
31004,tf.tensordot documentation describes non-existent `a_axes` and `b_axes` parameters,"## URL(s) with the issue:

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot

## Description of issue (what needs changing):

There are no `a_axes` and `b_axes` parameters, but the documentation describes the function as if there are.
"
30999,r2.0 windows build ProtoCompile issue,"
**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: source 
- TensorFlow version: 2.0
- Python version: 3.6.5
- Installed using: Visual studio
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): Visual Studio 2017
- CUDA/cuDNN version: 10, 7.1
- GPU model and memory: NVIDIA 1080

Building under windows (following https://www.tensorflow.org/install/source_windows) I have an issue (or possibly 2) with compiling XXX.pb.h and XXX.pb.cc files. 

1) Get errors about certain ones not being build (see below)
2) Bazel just sits there for hours with lines like :
    ProtoCompile tensorflow/core/protobuf/autotuning.pb.h

It seems like different files each time but always these two issues. Killing, re-booting machine and starting build again sometimes gets you a bit further in terms of bazel actions completed.

c:\Software\tensorflow>bazel build --config=opt --config=cuda --config=monolithic --config=v2 --define=no_tensorflow_py_deps=true //tensorflow:tensorflow.dll
Starting local Bazel server and connecting to it...
... still trying to connect to local Bazel server after 10 seconds ...
... still trying to connect to local Bazel server after 20 seconds ...
... still trying to connect to local Bazel server after 30 seconds ...
WARNING: The following configs were expanded more than once: [cuda, using_cuda, monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe
INFO: Reading rc options for 'build' from c:\software\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'build' from c:\software\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/lib/site-packages --python_path=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:cuda in file c:\software\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\software\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:monolithic in file c:\software\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:opt in file c:\software\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\software\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\software\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:monolithic in file c:\software\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:v2 in file c:\software\tensorflow\.tf_configure.bazelrc: --define=tf_api_version=2
INFO: Analyzed target //tensorflow:tensorflow.dll (122 packages loaded, 9152 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base C:/users/derek/_bazel_derek/fesmau5k/sandbox
Slow read: a 67402971-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/sobol_direction_vectors.h took 13946 ms.
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/output.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/extension.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/float_conversion.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/arg.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/bind.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/parser.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
Slow read: a 67402971-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/sobol_direction_vectors.h took 23140 ms.
Slow read: a 50122658-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/protobuf_archive/libprotoc_lib.a took 8412 ms.
INFO: From Linking external/grpc/libgrpc++_base.a:
server_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
rpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
create_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From ProtoCompile tensorflow/core/framework/tensor.pb.h:
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h:
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/device_properties.pb.h:
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h:
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.
ERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: output 'tensorflow/core/protobuf/queue_runner.pb.h' was not created
ERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: output 'tensorflow/core/protobuf/queue_runner.pb.cc' was not created
ERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: not all outputs were created or valid
[3,342 / 5,505] 3 actions running
    ProtoCompile tensorflow/core/protobuf/autotuning.pb.h; 11649s local
    ProtoCompile tensorflow/core/profiler/op_profile.pb.h; 11648s local
    ProtoCompile tensorflow/core/protobuf/debug.pb.h; 11648s local


"
30998,Can't install tensorflow-text,"
"
30995,LSTM prediction is numerically inconsistent for the last few instances.,"The predictions you get may differ slightly depending on input length and position within it.  E.g., if you have 11 instances of input, you get one answer for the first 8, and a different answer for the last 3.
I write ""may"" as it happens to me with probability around 0.4. ""Slightly"" means in the order of the least significant bits of the float32 mantissa.



**System information**
- Yes, I have written custom code, supplied below as a reprex in R using keras.
- Tried on two platforms, with identical results.
Platform A:
- Linux Ubuntu Ubuntu 16.04.5 LTS
- TensorFlow version:
      VERSION                        ""1.7.0""               
      GIT_VERSION                    ""v1.7.0-3-g024aecf414""
      COMPILER_VERSION               ""4.8.4""               
- Python version: 2.7.12

Platform B:
- Linux Ubuntu 14.04.6 LTS
- TensorFlow version: tried both
      VERSION                        ""1.12.0""
      GIT_VERSION                    ""v1.12.0-0-ga6d8ffae09""
      COMPILER_VERSION               ""4.8.5""                
- Python version: 2.7.6

both:
- TensorFlow installed from binary.
- Not a mobile device.
- CUDA/cuDNN version: Not used.
- GPU model and memory:  Not used

**Describe the current behavior**
If the first dimension of `x` is `n`, ""row"" `i` will get one value if `0 <= i < (n&-4)`, but a possibly different value for `(n&-4) <= i < n`. (These C++/Python style 0-based indices.  For R, 1-based, it's `0 < i <= bitwAnd(n, -4)` versus `bitwAnd(n, -4) < i <= n`.)

**Describe the expected behavior**
Reproducible prediction from same input instance, independent of row number or input length.  I use generalized ""row"" for a slice of a tensor with a given fixed first index, e.g., `x[i,,]` or `pred[i,]`.

**Code to reproduce the issue**
This is a reprex written in R.  I'd be happy to port to other languages if that's preferable.
``` r
options(digits=8)
fake <- function(shape_) {                        # arbitrary but reproducible
   array(seq_len(prod(shape_)) %% 2.71 - 1.04, shape_)
}

library(keras)
shape <- c(30,5)
model <- keras_model_sequential() %>%
   layer_lstm(units=2, input_shape=shape) %>%
   set_weights(list(fake(c(5, 8)), fake(c(2, 8)), fake(8)))

n <- 11                                           # not a multiple of 4
x <- array(rep(fake(shape), each=n), c(n, shape)) # n copies of identical input
p <- model %>% predict(x)                         # all predictions should match
p                                                 # but last n%%4 rows differ
#>             [,1]        [,2]
#>  [1,] 0.46561426 -0.22865930
#>  [2,] 0.46561426 -0.22865930
#>  [3,] 0.46561426 -0.22865930
#>  [4,] 0.46561426 -0.22865930
#>  [5,] 0.46561426 -0.22865930
#>  [6,] 0.46561426 -0.22865930
#>  [7,] 0.46561426 -0.22865930
#>  [8,] 0.46561426 -0.22865930
#>  [9,] 0.46561423 -0.22865926
#> [10,] 0.46561423 -0.22865926
#> [11,] 0.46561423 -0.22865926
(t(p)-p[1,]) * 2**26                              # the difference is low bits
#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
#> [1,]    0    0    0    0    0    0    0    0   -2    -2    -2
#> [2,]    0    0    0    0    0    0    0    0    3     3     3
stopifnot(t(p)==p[1,])                            # all *should* be equal
#> Error in eval(expr, envir, enclos): t(p) == p[1, ] are not all TRUE

##in contrast...
x12 <- array(rep(fake(shape), each=12), c(12, shape))
p12 <- model %>% predict(x12)
stopifnot(t(p12)==p12[1,])                         # ...all is well for n == 12
```

<sup>Created on 2019-07-25 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1.9000)</sup>

**Other info / logs**"
30994,[TF 2.0 API Docs] tf.math.sqrt,"Documentation contributor guide: https://www.tensorflow.org/community/contribute/docs

TensorFlow.v.2.0
Link to doc : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/sqrt

## Doc issue description:

**1.** The generated file in which this symbol is defined .i.e **python/ops/gen_math_ops.py** is just plain text rather than a link to the source file. It would be great if it's rightly linked to the source file to enable change proposals.

**2.** There isn't a clear distinction between the choice of usage of either **tf.sqrt** or **tf.math.sqrt** and the implications (may be performance wise) of choosing one over the other.

**3.** The usage example isn't a complete code sample but largely syntax-like. One of the parameters the function takes is represented by a placeholder symbol rather than a real valued tensor.

**4.** There should also be a mention of the at-least the common errors that may arise as a result of incorrect usage of this function.
"
30993,Missing shuffle argument on validation call during training,"On TensorFlow 1.14 (OS Ubuntu 16.04), when I call fit() of a tf.Keras model using HDF5Matrix on both training and validation data, I set the argument shuffle='batch' and it works for training batches, however it fails when starting validation batches:

TypeError: TypeError while preparing batch. If using HDF5 input data, pass shuffle=""batch"".

  The problem seems to be the shuffle argument missing on the validation call during training:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_arrays.py#L424

  It seems the recursive call of model_iteration() does not set the argument from parent training call into the validation call.  Simply ""shuffle=shuffle"" in argument list should fix the issue.

Best,
  Andre.
"
30992,INT8 TensorRT Quantization Fails to Calibrate,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, [nvcr.io/nvidia/tensorrt:19.02-py3](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-6532-g9aaf74d733 1.15.0-dev20190718
- Python version: 3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0.130 / 7.4.2 as per the above linked container
- GPU model and memory: Tesla V100 32GB

Relevant output from `pip freeze`:
```
tf-estimator-nightly==1.14.0.dev2019072201
tf-nightly-gpu==1.15.0.dev20190718
```

I am trying to quantize a Tensorflow frozen model to FP32, FP16, and INT8 using [this](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py) script and a few helper functions from [here](https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/graph_utils.py). 

**Describe the current behavior**
The current behavior is that the graph first gets converted to a TRT graph with TRTEngineOps, but then when it gets calibrated for the INT8 quantization, an error occurs. Logs follow:

```
2019-07-24 14:47:33.099857: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 9 ops of 6 different types in the graph that are not converted to TensorRT: ResizeNearestNeighbor, SplitV, $onv2D, ConcatV2, Placeholder, NoOp, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).
2019-07-24 14:47:33.104169: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:731] Number of TensorRT candidate segments: 6
2019-07-24 14:47:33.165775: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 82 nodes succeeded.
2019-07-24 14:47:33.166041: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 4 nodes succeeded.
2019-07-24 14:47:33.166100: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 4 nodes succeeded.
2019-07-24 14:47:33.172398: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_3 added for segment 3 consisting of 54 nodes succeeded.
2019-07-24 14:47:33.174729: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_4 added for segment 4 consisting of 25 nodes succeeded.
2019-07-24 14:47:33.174886: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_5 added for segment 5 consisting of 29 nodes succeeded.
2019-07-24 14:47:33.253907: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph
2019-07-24 14:47:33.253947: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 171 nodes (-129), 186 edges (-130), time = 146.881ms.
2019-07-24 14:47:33.253954: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 223 nodes (52), 238 edges (52), time = 53.403ms.
2019-07-24 14:47:33.253960: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 223 nodes (0), 238 edges (0), time = 40.922ms.
2019-07-24 14:47:33.253966: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 31 nodes (-192), 40 edges (-198), time = 171.468ms.
2019-07-24 14:47:33.253986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 23 nodes (-8), 36 edges (-4), time = 58.458ms.
...
2019-07-24 14:47:45.089947: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1924019480
2019-07-24 14:47:45.090050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5
2019-07-24 14:47:45.091103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5
2019-07-24 14:47:45.098144: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.098663: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.098767: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.098842: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.099041: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.099740: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:45.108558: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:49.380842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-07-24 14:47:50.417645: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1970006de0
2019-07-24 14:47:50.426935: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.427977: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.438033: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.438830: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.439010: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.981266: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1970061730
2019-07-24 14:47:50.997300: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:50.997816: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.592716: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f196400b0b0
2019-07-24 14:47:51.593407: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.593434: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.593455: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.593495: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.593507: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.593523: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.694882: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f19c4007080
2019-07-24 14:47:51.695388: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:47:51.747525: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f19c4014cf0
2019-07-24 14:47:51.748014: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-07-24 14:51:46.016234: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource 
 Calibrator = 0
 Builder    = 0
 Engine     = 0
 Logger     = 0x7f19240194f8
 Thread     = 0x7f192400cf80

2019-07-24 14:51:53.253125: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource 
 Calibrator = 0
 Builder    = 0
 Engine     = 0
 Logger     = 0x7f1970006e58
 Thread     = 0x7f197000a000

2019-07-24 14:51:57.064220: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource 
 Calibrator = 0
 Builder    = 0
 Engine     = 0
 Logger     = 0x7f19700617a8
 Thread     = 0x7f1970006f20

2019-07-24 14:52:05.905501: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource 
 Calibrator = 0
 Builder    = 0
 Engine     = 0
 Logger     = 0x7f196400b128
 Thread     = 0x7f1964007350

2019-07-24 14:52:06.831720: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource 
 Calibrator = 0
 Builder    = 0
 Engine     = 0
 Logger     = 0x7f19c4014d68
 Thread     = 0x7f19c4014c60

pure virtual method called
terminate called without an active exception
Aborted (core dumped)
```
**Describe the expected behavior**
The expected behavior is for this error to not occur, and for the INT8 calibrated and quantized graph to be produced correctly.

**Code to reproduce the issue**
Command executed:
```
python do.py \
--frozen_graph=tiny-yolov3_frozen.pb \
--image_file=/path/to/any/image/file \
--int8 \
--output_dir=/workspace \
--input_node=inputs --output_node=output_boxes
```

You can find the `do.py` script and the `utilities.py` script referenced from it attached, and the `tiny-yolov3_frozen.pb` frozen model [here](https://drive.google.com/file/d/1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v/view).

[do.py.txt](https://github.com/tensorflow/tensorflow/files/3427406/do.py.txt)
[utilities.py.txt](https://github.com/tensorflow/tensorflow/files/3427407/utilities.py.txt)
"
30991,Name Scope Automatic Handling According to the Model Architectures blocks,"**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):
Yes
**Describe the feature and the current behavior/state.**
Tensorflow must be able to give name scopes more logically according to the architecture being built and automatically. Also, must be able to update the names automatically according to the 
Architecture without the dev having to explicity name the things (this should be optional).

Today it only automatically adds numbers sequentially to name/var scope when it is identical
to any other in the collections and you dont add '/' to it

**Will this change the current api? How?**
Add a Name/Var Scope handler. 

This class/template must be used to implement logic of names according to the architecture block connection. E.G. if a block is just the same from the other in the sequence them it must be named as if is a deep part of the same architecture blocks. If it is a ramification of the block given in a sequence but the same arch them he must be named with another tag.
 When duplicating some block to architectonic use, E.G. if you need to frame your data or feature, them each frame must have an coeherent name space block, like the today ordimatlicly done for repeated name space block, but this must be automaticly done for identical built blocks of the model, without the need to explicity code it.
Names must be given as unique ID according to te input-output architecture connection, if it has the same IO from previous Block in the sequence, then each Input-ArchBlock-Output that is IDentical must have the same name with the appropriated tag for deepness in the sequence of blocks.

Somethings still to be cleared, like if you have a sequence of CNN of deepness, say 100, then many of them will have same IO arch type (name), but they must not be enclosed in a name scope according to IO, since it obviously has the same IO pattern.


**Who will benefit with this feature?**

Much more self contained and manageable architectonic blocks, idenpendent of its Variable parameters configuration. This will rapid increase how we search for relevant new Neural Networks architecture, since it is a management tools as much as a vizualization tool and a graph construction tool.

**Any Other info.**

Obs: These Features could be something for the future in Autograph module, if it still dosnt do it"
30990,LSTM is not working with ModelCheckpoint callback,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

While training a model with `tf.keras.layers.LSTM` and having `tf.keras.callbacks.ModelCheckpoint` in callbacks, `model.fit` stops with an error message at end of last epoch, and no model weights is saved as `ModelCheckpoint` should do.

**Describe the expected behavior**

`model.fit` should train the model, and model weights should be saved in desired files.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is an example which reproduces this error:
```
import tensorflow as tf

from tensorflow.python.keras import layers
from tensorflow.python.keras.callbacks import ModelCheckpoint

model = tf.keras.Sequential()
model.add(layers.LSTM(units=64, input_shape=(28, 28), return_sequences=False))
model.add(layers.Dense(10, activation='softmax'))

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
sample, sample_label = x_train[0], y_train[0]

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='sgd',
              metrics=[])

callback = ModelCheckpoint(filepath='saved/',
                           monitor='val_loss',
                           save_weights_only=False,
                           mode='min', save_freq='epoch')

model.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          batch_size=64, epochs=2, callbacks=[callback])
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is error message after `model.fit` with the precedent code snippet:
```
W0724 14:56:18.298580 4508739008 deprecation.py:323] From /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 60000 samples, validate on 10000 samples
Epoch 1/2
59904/60000 [============================>.] - ETA: 0s - loss: 2.2443W0724 14:57:10.469849 4508739008 saved_model.py:733] Skipping full serialization of object <tensorflow.python.keras.layers.recurrent.LSTM object at 0xb286c80b8>, because an error occurred while tracing layer functions. Error message: in converted code:
    relative to /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras:
    saving/saved_model.py:1143 call_and_return_conditional_losses  *
        return layer_call(inputs, training=training), layer.get_losses_for(inputs)
    layers/recurrent.py:2533 call
        inputs, mask=mask, training=training, initial_state=initial_state)
    layers/recurrent.py:743 call
        zero_output_for_mask=self.zero_output_for_mask)
    backend.py:3806 rnn
        input_time_zero, tuple(initial_states) + tuple(constants))
    layers/recurrent.py:728 step
        output, new_states = self.cell.call(inputs, states, **kwargs)
    TypeError: wrapped_call() takes 1 positional argument but 2 were given
W0724 14:57:10.520410 4508739008 saved_model.py:733] Skipping full serialization of object <tensorflow.python.keras.engine.sequential.Sequential object at 0x10f940518>, because an error occurred while tracing layer functions. Error message: in converted code:
    relative to /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras:
    saving/saved_model.py:1143 call_and_return_conditional_losses  *
        return layer_call(inputs, training=training), layer.get_losses_for(inputs)
    engine/sequential.py:248 call
        return super(Sequential, self).call(inputs, training=training, mask=mask)
    engine/network.py:753 call
        return self._run_internal_graph(inputs, training=training, mask=mask)
    engine/network.py:895 _run_internal_graph
        output_tensors = layer(computed_tensors, **kwargs)
    layers/recurrent.py:619 __call__
        return super(RNN, self).__call__(inputs, **kwargs)
    engine/base_layer.py:667 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    layers/recurrent.py:2533 call
        inputs, mask=mask, training=training, initial_state=initial_state)
    layers/recurrent.py:743 call
        zero_output_for_mask=self.zero_output_for_mask)
    backend.py:3806 rnn
        input_time_zero, tuple(initial_states) + tuple(constants))
    layers/recurrent.py:728 step
        output, new_states = self.cell.call(inputs, states, **kwargs)
    TypeError: wrapped_call() takes 1 positional argument but 2 were given
2019-07-24 14:57:10.531729: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
60000/60000 [==============================] - 51s 854us/sample - loss: 2.2442 - val_loss: 2.1145
Epoch 2/2
59968/60000 [============================>.] - ETA: 0s - loss: 1.9488Traceback (most recent call last):
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-d29c3be2bf34>"", line 1, in <module>
    runfile('/Users/user/Desktop/tf2-rnn-callback-bugcheck/main.py', wdir='/Users/user/Desktop/tf2-rnn-callback-bugcheck')
  File ""/Users/user/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/Users/user/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/user/Desktop/tf2-rnn-callback-bugcheck/main.py"", line 37, in <module>
    batch_size=64, epochs=2, callbacks=[callback])
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 664, in fit
    steps_name='steps_per_epoch')
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 439, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 295, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 961, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1008, in _save_model
    self.model.save(filepath, overwrite=True)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1213, in save
    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 106, in save_model
    saved_model.save(model, filepath, overwrite, include_optimizer)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 1492, in save
    save_lib.save(model, filepath)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 812, in save
    checkpoint_graph_view)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_serialization.py"", line 65, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py"", line 139, in list_functions
    self._serialization_cache)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2249, in _list_functions_for_serialization
    fns = (saved_model.serialize_all_attributes(self, serialization_cache)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 723, in serialize_all_attributes
    function_dict['_default_save_signature'] = _default_save_signature(layer)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 881, in _default_save_signature
    fn.get_concrete_function()
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 681, in get_concrete_function
    self._initialize(args, kwargs, add_initializers_to=initializer_map)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 359, in _initialize
    *args, **kwds))
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1360, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1648, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1541, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 716, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 309, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py"", line 139, in _wrapped_model
    outputs_list = nest.flatten(model(inputs=inputs))
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 667, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py"", line 248, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 753, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 895, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 619, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 667, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2533, in call
    inputs, mask=mask, training=training, initial_state=initial_state)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 743, in call
    zero_output_for_mask=self.zero_output_for_mask)
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3806, in rnn
    input_time_zero, tuple(initial_states) + tuple(constants))
  File ""/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 728, in step
    output, new_states = self.cell.call(inputs, states, **kwargs)
TypeError: wrapped_call() takes 1 positional argument but 2 were given
```

The same error occurs on a Linux machine with tf-nightly (`2.0.0-dev20190723`).

Thanks for help! "
30989,tf.compat.v2.summary.scalar  has no attribute 'experimental',"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 2.7 
- Bazel version (if compiling from source): Build label: 0.24.1
- GPU model and memory:  CPU 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)


**Describe the current behavior**
```
import tensorflow.compat.v2.summary as b
b.scalar
```
<function scalar at 0x7fbe6ed09050>
```
b.experimental
```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'experimental'

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` 
train_log_dir = ""logs/scalars/""
self._train_summary_writer = tf.compat.v2.summary.create_file_writer(train_log_dir)
 def get_metrics(self,step):
    self._step= step
    loss=self.metrics.get(""loss"", None)
    accuracy = self.metrics.get(""accuracy"", None)
    with self._train_summary_writer.as_default():
      tf.compat.v2.summary.scalar('loss', loss,step=self._step)
      tf.compat.v2.summary.scalar('loss', accuracy,step=self._step )
      self._train_summary_writer.flush()
    return self.metrics
```
**Other info / logs**
Traceback (most recent call last):
  File ""/home/sendate/Desktop/TeamTrung/Bintu/Tensor/Agent.py"", line 152, in <module>
    print('history dict:', agent.get_metrics(episode))
  File ""/home/sendate/Desktop/TeamTrung/Bintu/Tensor/Agent.py"", line 83, in get_metrics
    tf.compat.v2.summary.scalar('loss', loss,step=self._step)
  File ""/home/sendate/Desktop/TeamTrung/Bintu/Tensor/venv/local/lib/python2.7/site-packages/tensorboard/plugins/scalar/summary_v2.py"", line 57, in scalar
    getattr(tf.summary.experimental, 'summary_scope', None) or
  File ""/home/sendate/Desktop/TeamTrung/Bintu/Tensor/venv/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation_wrapper.py"", line 104, in __getattr__
    attr = getattr(self._dw_wrapped_module, name)
AttributeError: 'module' object has no attribute 'experimental'
"
30988,Relations/differences between Container and Scopes in Context Manager are not Clear,"From respective docs
https://www.tensorflow.org/api_docs/python/tf/container
https://www.tensorflow.org/api_docs/python/tf/VariableScope

The relation between these two context manager is not clear (although variable scope and name scope inter-relation are fine). 

But from the source code ones can see that they are clearly two things that inter-relates but not the same thing. 

Should i use Container only in the eager mode ? why not use only variable scope or name scope instead? also, maybe other questions will come as people uses low-level API to specialize in tensorflow dev.

So i think its an Issue.

Thankyou 
"
30984,[tf2.0] tf.function and dataset perf issue,"Hi everyone,

I'm having a look at datasets and `tf.function` and It feels like I've stumble upon a strange perf issue.
Is it a normal behaviour or am I doing something wrong?

**System information**
- Have I written custom code: `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OSX`
- TensorFlow installed from (source or binary): `binary - 2.0.0-beta1`
- TensorFlow version (use command below): `v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1`
- Python version: `3.6`

**Describe the current behavior**
The tf.function decoration is said to improve eprformance over eager codes. Yet when using datasets the opposite happens on 3 different machines

**Describe the expected behavior**
The tf.function decoration optimize my code even when using dataset

**Code to reproduce the issue**
```python
import time
import multiprocessing

import tensorflow as tf
import numpy as np


NB_LOOP = 100
BATCH_SIZE = 256
NB_EPOCHS = 100
NB_SAMPLES = NB_EPOCHS * BATCH_SIZE
DIM_SIZE = 256


np.random.seed(1)
tf.random.set_seed(1)


def to_float32(x, y=None):
    """"""
    Cast TF inputs as float32
    """"""
    if y is None:
        return tf.cast(x, tf.float32)

    return tf.cast(x, tf.float32), tf.cast(y, tf.float32)


def bench(w, b, optim):
    for i in tf.range(NB_EPOCHS):
        a = tf.random.normal([BATCH_SIZE, DIM_SIZE])
        with tf.GradientTape() as t:
            out = tf.matmul(a, w) + b
            out = tf.square(out)

        grads = t.gradient(out, [w, b])
        optim.apply_gradients(zip(grads, [w, b]))


@tf.function
def bench_f(w, b, optim):
    for i in tf.range(NB_EPOCHS):
        a = tf.random.normal([BATCH_SIZE, DIM_SIZE])
        with tf.GradientTape() as t:
            out = tf.matmul(a, w) + b
            out = tf.square(out)

        grads = t.gradient(out, [w, b])
        optim.apply_gradients(zip(grads, [w, b]))


def dataset_bench(p_data, w, b, optim):
    for a in p_data:
        with tf.GradientTape() as t:
            out = tf.matmul(a, w) + b
            out = tf.square(out)

        grads = t.gradient(out, [w, b])
        optim.apply_gradients(zip(grads, [w, b]))


@tf.function
def dataset_bench_f(p_data, w, b, optim):
    for a in p_data:
        with tf.GradientTape() as t:
            out = tf.matmul(a, w) + b
            out = tf.square(out)

        grads = t.gradient(out, [w, b])
        optim.apply_gradients(zip(grads, [w, b]))


data = np.random.normal(size=[NB_SAMPLES, DIM_SIZE])
p_data = tf.data.Dataset.from_tensor_slices(data) \
           .shuffle(buffer_size=BATCH_SIZE * 10) \
           .map(to_float32, num_parallel_calls=max(multiprocessing.cpu_count() // 2, 1)) \
           .batch(BATCH_SIZE)

w = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)
b = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)
optim = tf.keras.optimizers.SGD(1e-3)
t1 = time.time()
for i in range(NB_LOOP):
    bench(w, b, optim)
t2 = time.time()
print(""bench, total time: {}"".format((t2 - t1) / NB_LOOP))

w = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)
b = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)
optim = tf.keras.optimizers.SGD(1e-3)
t1 = time.time()
# Tracing
bench_f(w, b, optim)
for i in range(NB_LOOP):
    bench_f(w, b, optim)
t2 = time.time()
print(""bench_f, total time: {}"".format((t2 - t1) / NB_LOOP))

w = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)
b = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)
optim = tf.keras.optimizers.SGD(1e-3)
t1 = time.time()
for i in range(NB_LOOP):
    dataset_bench(p_data, w, b, optim)
t2 = time.time()
print(""dataset_bench, total time: {}"".format((t2 - t1) / NB_LOOP))

w = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)
b = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)
optim = tf.keras.optimizers.SGD(1e-3)
# Tracing
dataset_bench_f(p_data, w, b, optim)
t1 = time.time()
for i in range(NB_LOOP):
    dataset_bench_f(p_data, w, b, optim)
t2 = time.time()
print(""dataset_bench_f, total time tf.func: {}"".format((t2 - t1) / NB_LOOP))
```

**Results**
OSX:
```
bench, total time: 0.760720341205597
bench_f, total time: 0.1514965009689331  # <-- nice improvement
dataset_bench, total time: 0.904261839389801
dataset_bench_f, total time tf.func: 0.9035982608795166 # <-- no improvement
```
Linux:
```
bench, total time: 0.14520598888397218
bench_f, total time: 0.0331043267250061 # <-- nice improvement
dataset_bench, total time: 0.3545421266555786
dataset_bench_f, total time tf.func: 0.4910250568389893 # <-- worst result
```
"
30983,Sample weights being expanded to match y_true/y_pred before they're sliced using a class_id in keras metrics.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 7.5

**Describe the current behavior**
When passing sample weights and a class id to a metric class `sample_weights` is expanded to match the shape of y_true in [line 306 of this file](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/utils/metrics_utils.py#L306), then the slicing using `class_id` is done in [line 312](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/keras/utils/metrics_utils.py#L312) which means the broadcast weights op in [line 339-340](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/keras/utils/metrics_utils.py#L339) fails as the shapes don't match anymore.

**Describe the expected behavior**
Sample weights should be squeezed or expanded to match the new shape of y_true and y_pred after they're sliced. Or it could be enforced to be the same shape as y_true and y_pred and then sliced as well.

**Code to reproduce the issue**
This code should illustrate the problem

    import tensorflow as tf
    import numpy as np


    class MaskedRecall(tf.keras.metrics.Metric):

        def __init__(self, class_id=None, name=None, dtype=None):
            super().__init__(name, dtype)
            self.inner = tf.keras.metrics.Recall(class_id=class_id)

        def result(self):
            return self.inner.result()

        def update_state(self, y_true, y_pred, sample_weight=None):
            sample_weight = tf.where(tf.equal(y_true[..., self.inner.class_id], -1),
                                    tf.zeros_like(y_true[..., self.inner.class_id]),
                                    tf.ones_like(y_true[..., self.inner.class_id]))
            self.inner.update_state(y_true, y_pred, sample_weight)

        def reset_states(self):
            self.inner.reset_states()


    if __name__ == ""__main__"":
        # metric = tf.keras.metrics.Recall()
        metric = MaskedRecall(class_id=0)  # 0)
        y_true = np.array([[[1], [0], [-1]], [[0], [1], [0]]])
        y_pred = np.array([[[1], [0], [0]], [[0], [1], [0]]])

        tf.config.experimental_run_functions_eagerly(True)
        metric.reset_states()
        metric.update_state(y_true, y_pred)
        print(""Metric result: "", metric.result())


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback from the above:
Traceback (most recent call last):
~                                                                                                                                                             │  File ""/home/tbradley/Downloads/metric.py"", line 34, in <module>
~                                                                                                                                                             │    metric.update_state(y_true, y_pred)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 75, in decorated
~                                                                                                                                                             │    update_op = update_state_fn(*args, **kwargs)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 400, in __call__
~                                                                                                                                                             │    return self._python_function(*args, **kwds)
~                                                                                                                                                             │  File ""/home/tbradley/Downloads/metric.py"", line 20, in update_state
~                                                                                                                                                             │    self.inner.update_state(y_true, y_pred, sample_weight)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 75, in decorated
~                                                                                                                                                             │    update_op = update_state_fn(*args, **kwargs)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py"", line 1318, in update_state
~                                                                                                                                                             │    sample_weight=sample_weight)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 340, in update_confusion_matrix_vari
~                                                                                                                                                             │ables
~                                                                                                                                                             │    math_ops.cast(sample_weight, dtype=dtypes.float32), y_pred)
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 167, in broadcast_weights
~                                                                                                                                                             │    with ops.control_dependencies((assert_broadcastable(weights, values),)):
~                                                                                                                                                             │  File ""/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py"", line 103, in assert_broadcastable
~                                                                                                                                                             │    weights_rank_static, values.shape, weights.shape))
~                                                                                                                                                             │ValueError: weights can not be broadcast to values. values.rank=1. weights.rank=2. values.shape=(2,). weights.shape=(2, 1).
"
30981,does pip wheels support hdfs on default?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary?
- TensorFlow version: 1.12
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:



**Describe the problem**
Uncertain if pip tensorflow_gpu 1.12 for python 3.5 supports hdfs by default

**Any other info / logs**
util.NativeCodeLoader: Unable to load native-hadoop library for your platform
info retry.RetryInvocationHandler org.apache.hadoop.net.ConnectTimeoutException: Call From ... failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=...] while invoking ClientNameNodeProtocolTranslatorPB.getFileinfo over ... after 1 failover attempts. Trying to failover after sleeping for 781ms

"
30980,Running FaceNet and MTCNN model simutaneously on 2 cameras,"Hello everyone!

I am trying to run the MTCNN and FaceNet model on 2 cameras simultaneously. So, I am not getting any error while doing this but the code don't give me any results.

It just loads both the models and doesn't give me any predictions. Can anyone help me with this? I have created 2 graphs using g=tf.Graph for MTCNN and FaceNet. 

I think this error is coming due to multi-processing with tensorflow as it might trying to load MTCNN input to Facenet graph. *this is my assumption. 

Please let me know if you have any idea about this. Thanks.
"
30979,Converting Tensorflow model to Tensorflowlite model ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30978,attribute error,AttributeError: module 'tensorflow' has no attribute 'get_default_graph'
30977,Online convertor,"hello,
i have too many problem to convert  a custom model, install python and ...
actually i have been tired to make a custom model and have asked from a person to make it.
its kind of hard to ask other people to do these simple things and hard to me cuz i dont know cant understand. can you make a online convertor for us?
we upload our images and you give us .tflite and .txt
i think its easy for you and it can save our time too. (i mean install python and other things)
tnx

or if you have it give me the link plz"
30976,AttributeError: 'NoneType' object has no attribute 'keys',"I am trying to make classification using tensorflow 

I am using anaconda version 1.7.2 on Ubuntu 18.04.2 LTS and python 3.7.3

i downloaded retrain.py code from [ https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py](url)

 when i run this file i got error 

`File ""retrain.py"", line 1349, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/deepak/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/deepak/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/deepak/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""retrain.py"", line 1006, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'`

i was running this command to run that file 

`python retrain.py --image_dir ~/home/deepak/new_data_EHL/Augmented --learning_rate=0.0001 --testing_percentage=20 --validation_percentage=20 --training_batch_size=32 --validation_batch_size=-1 --eval_step_interval=100 --how_many_training_steps=10 --random_scale=30 --random_brightness=30 --architecture mobilenet_1.0_224 --output_graph=output_graph.pb --output_labels=output_labels.txt`

i am a beginner in tensorflow and deep learning i searched about the solution but i couldn't get the solution that work.
"
30975,Bug in TF2.0 & V100: Inconsistent 2d convolution results w.r.t batch size,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10 / cuDNN 7
- GPU model and memory: NVIDIA V100 / 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I set inputs are tensors which contain only ones.
Then, the output of `tf.nn.conv2d` is different when batch_sizes are 64 and 128.

**Describe the expected behavior**
The output of `tf.nn.conv2d` should not be different w.r.t batch sizes.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
code:
```python
import tensorflow as tf
print(""TF_VERSION: %s"" % tf.__version__)

x = tf.ones((128, 28, 28, 32))
kernel = tf.ones((3, 3, 32, 64))

ya = tf.nn.conv2d(x[:64], kernel, [1, 1], ""SAME"")
yb = tf.nn.conv2d(x, kernel, [1, 1], ""SAME"")[:64]
print(tf.reduce_sum(tf.math.abs(ya- yb)))
```
printed outputs:
```
TF_VERSION: 2.0.0-beta1
tf.Tensor(144.375, shape=(), dtype=float32)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

From the code I'm given, the values of `ya` and `yb` are as below:
`ya`:
```
<tf.Tensor: id=10, shape=(64, 28, 28, 64), dtype=float32, numpy=
array([[[[128., 128., 128., ..., 128., 128., 128.],
         [192., 192., 192., ..., 192., 192., 192.],
         [192., 192., 192., ..., 192., 192., 192.],
         ...,
         [192., 192., 192., ..., 192., 192., 192.],
         [192., 192., 192., ..., 192., 192., 192.],
         [128., 128., 128., ..., 128., 128., 128.]]]], dtype=float32)>
```
`yb`:
```
<tf.Tensor: id=15, shape=(64, 28, 28, 64), dtype=float32, numpy=
array([[[[128.00009, 128.00009, 128.00009, ..., 128.00009, 128.00009,
          128.00009],
         [192.0001 , 192.0001 , 192.0001 , ..., 192.0001 , 192.0001 ,
          192.0001 ],
         [192.00009, 192.00009, 192.00009, ..., 192.00009, 192.00009,
          192.00009],
         ...,
         [192.00012, 192.00012, 192.00012, ..., 192.00012, 192.00012,
          192.00012],
         [192.00012, 192.00012, 192.00012, ..., 192.00012, 192.00012,
          192.00012],
         [128.0001 , 128.0001 , 128.0001 , ..., 128.0001 , 128.0001 ,
          128.0001 ]]]], dtype=float32)>
```"
30974,AttributeError: module 'tensorflow.python.ops.summary_op_util' has no attribute 'skip_summary',"
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
device:
- TensorFlow installed from (source or binary): i use pip
- TensorFlow version (use command below): 1.14
- Python version: i tried 3.7.3 and 3.6
- Bazel version (if compiling from source):  i dont know
- GCC/Compiler version (if compiling from source): i dont know
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1070ti

i try to use TecoGAN (https://github.com/thunil/TecoGAN)


Traceback (most recent call last):
File ""main.py"", line 284, in 
Net = TecoGAN( rdata.s_inputs, rdata.s_targets, FLAGS )
File ""/home/ksjin/바탕화면/TecoGAN-master/lib/Teco.py"", line 500, in TecoGAN
gif_sum = [ gif_summary('LR', r_inputs, max_outputs=max_outputs, fps=3),
File ""/home/ksjin/바탕화면/TecoGAN-master/lib/ops.py"", line 507, in gif_summary
if summary_op_util.skip_summary():
AttributeError: module 'tensorflow.python.ops.summary_op_util' has no attribute 'skip_summary'



but i got this error  how can i fix it

"
30972,Excessive overhead with dense layer,"Under normal circumstances, propagation through the dense layer should reduce to a matmul operation and possibly an activation operation. However, in tensorflow 1.14, it sometimes does not. This situation arises if the input to the dense layer is at least 3D and its dimensions are not fully determined at the time of graph construction.
If this happens, the framework calls the method tensordot(), which calls the internal method _tensordot_reshape(), which then attempts to determine how to reshape the tensor before calling matmul.
The GPU trace of the whole thing ends up looking like so:

```

[CUDA memcpy HtoD]
void tensorflow::GatherOpKernel<int, int, bool=1>(int const *, int const *, tensorflow::GatherOpKernel<int, int, bool=1>*, __int64, __int64, __int64, __int64)
void tensorflow::GatherOpKernel<int, int, bool=1>(int const *, int const *, tensorflow::GatherOpKernel<int, int, bool=1>*, __int64, __int64, __int64, __int64)
[CUDA memcpy DtoH]
void tensorflow::functor::BlockReduceKernel<int*, int*, int=256, tensorflow::functor::Prod<int>>(int*, int*, int, int, std::iterator_traits<tensorflow::functor::BlockReduceKernel<int*, int*, int=256, tensorflow::functor::Prod<int>>>::value_type)
[CUDA memcpy DtoH]
[CUDA memcpy DtoH]
volta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nn
```

Note that these memcpy's are all for 4-8 bytes, but each one involves a syscall and a pointer validity check, so, they are not exactly free. 

And it is all unnecessary, because the framework does know how big the tensor is, on the CPU side, and something along these lines https://github.com/ekuznetsov139/tensor2tensor/commit/079f51f2c736d547cbd2b4bcfdcedb039827b93a would compute the shape without the use of the GPU.

It might seem trivial, but, in a graph with lots of small dense layers and especially when tensor cores are in play (so that the actual convolution is fairly fast), the overhead may become significant.

The above change to the tensor2tensor project reduces training time per step (NVIDIA Tesla V100, fp16) by at least 10%. Since the issue is fundamental and not specific to tensor2tensor, it would make sense to address it at the framework level."
30970,graph.get_operation() returns operations that are not from the respective name_scope,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
function get_operations() 
from class Graph()
is not returning the operations from a namescope enclosing the tensorflow operations from respective namescope but is returning the operations for all the graph

**Describe the expected behavior**

Returns only the operations from name scope, and preferable shorted by construction just like in the original list.

**Code to reproduce the issue**
```python
tf.reset_default_graph()
graph = tf.Graph()
with graph.as_default():
  with tf.variable_scope('aa'):
    c = tf.constant(10.0,name='const')
    a = tf.Variable(c,name='nomon')
  with tf.variable_scope('ab'):
    c = tf.constant(10.0,name='const')
    a = tf.Variable(c,name='nomon')  

with graph.as_default():    
  with tf.name_scope('aa/'):
    print(graph.get_operations()[-1].name)
``` 
**Other info / logs**

I know this seems to be a fairly trivial problem to be circumvented (a.k.a search for the operations by the name with if and elses). But this bug brings out a possible ugly spagethi code, and seems fairly easy to fiz in the api, without having to go to the backend."
30969,TF-TRT does not return  correct batch size.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.13.1
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:T4


**Describe the current behavior**
TF-TRT  seems to return max_batch_size instead of returning the feed batch size.  This breaks current code badly.

**Describe the expected behavior**
Return the batch size as plain TF.

**Code to reproduce the issue**
Should be simple to reproduce. 

**Other info / logs**
None
"
30967,TensorFlow 1.14 custom estimator model slower than TensorFlow 1.13,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
TensorFlow 1.14
- Python version:
Python 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
CUDA 10, CuDNN 7.4.2
- GPU model and memory:
GeForce GTX 1080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I was trying to upgrade my tensorflow from 1.13 to tensorflow 1.14. I noticed that the training speed for my models dropped a lot.

Then I tried the tensorflow custom estimator and reproduced the problem.
https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py

This is the speed with tf 1.13.
INFO:tensorflow:loss = 2.3727872, step = 0
INFO:tensorflow:global_step/sec: 293.209
INFO:tensorflow:loss = 0.17633303, step = 100 (0.341 sec)
INFO:tensorflow:global_step/sec: 337.629
INFO:tensorflow:loss = 0.09756016, step = 200 (0.296 sec)
INFO:tensorflow:global_step/sec: 342.554
INFO:tensorflow:loss = 0.10544465, step = 300 (0.292 sec)
INFO:tensorflow:global_step/sec: 342.051
INFO:tensorflow:loss = 0.049246334, step = 400 (0.292 sec)
INFO:tensorflow:global_step/sec: 347.739
INFO:tensorflow:loss = 0.09701028, step = 500 (0.288 sec)
INFO:tensorflow:global_step/sec: 347.722
INFO:tensorflow:loss = 0.063723646, step = 600 (0.287 sec)
INFO:tensorflow:global_step/sec: 329.121
INFO:tensorflow:loss = 0.045053717, step = 700 (0.304 sec)
INFO:tensorflow:global_step/sec: 295.816
INFO:tensorflow:loss = 0.061943192, step = 800 (0.338 sec)
INFO:tensorflow:global_step/sec: 295.844
INFO:tensorflow:loss = 0.029347159, step = 900 (0.338 sec)

This is the speed with tf 1.14.
I0722 22:54:59.609863 139637823923968 basic_session_run_hooks.py:262] loss = 1.0876435, step = 0
I0722 22:54:59.962240 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 283.369
I0722 22:54:59.963597 139637823923968 basic_session_run_hooks.py:260] loss = 0.13201241, step = 100 (0.354 sec)
I0722 22:55:00.268446 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 326.584
I0722 22:55:00.270031 139637823923968 basic_session_run_hooks.py:260] loss = 0.09678831, step = 200 (0.306 sec)
I0722 22:55:00.601743 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 299.99
I0722 22:55:00.602967 139637823923968 basic_session_run_hooks.py:260] loss = 0.07743741, step = 300 (0.333 sec)
I0722 22:55:00.933660 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 301.274
I0722 22:55:00.935039 139637823923968 basic_session_run_hooks.py:260] loss = 0.08071006, step = 400 (0.332 sec)
I0722 22:55:01.271045 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 296.425
I0722 22:55:01.272447 139637823923968 basic_session_run_hooks.py:260] loss = 0.10450166, step = 500 (0.337 sec)
I0722 22:55:01.605127 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 299.334
I0722 22:55:01.606523 139637823923968 basic_session_run_hooks.py:260] loss = 0.06578785, step = 600 (0.334 sec)
I0722 22:55:01.946224 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 293.167
I0722 22:55:01.947628 139637823923968 basic_session_run_hooks.py:260] loss = 0.09801171, step = 700 (0.341 sec)
I0722 22:55:02.284446 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 295.649
I0722 22:55:02.285842 139637823923968 basic_session_run_hooks.py:260] loss = 0.05814831, step = 800 (0.338 sec)
I0722 22:55:02.621150 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 297.002
I0722 22:55:02.622456 139637823923968 basic_session_run_hooks.py:260] loss = 0.056445364, step = 900 (0.337 sec)

**Describe the expected behavior**
I would expect tensorflow has similar or better performance after the upgrade.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30966,Restoring tf.py_funcs used inside of a tf.data.Dataset for use at inference time,"**tensorflow gpu version:** 1.13.1

I'm currently trying to use the Datasets API at inference time but I am having trouble restoring the model and the iterator/py_func ops. I'm setting up the dataset while training like this and I'm using two py_funcs in my input pipeline:
```python
padding_const_str = tf.constant(""<PAD>"")
padding_const_int = tf.constant(0, dtype=tf.int64)
dataset_type = tf.placeholder(tf.string, shape=[])
dataset = tf.data.TextLineDataset(dataset_type)
dataset = dataset.map(lambda map_el: tuple(tf.py_func(preprocess, [map_el], [tf.string, tf.string, tf.string, tf.string, tf.int64, tf.int64], name=""preprocess_py_func"")), num_parallel_calls=16)
dataset = dataset.padded_batch(batch_size=FLAGS.batch_size, padded_shapes=([None],[None],[None],[None],[None],[None]), padding_values=(padding_const_str, padding_const_str, padding_const_str, padding_const_str, padding_const_int, padding_const_int))
dataset = dataset.map(lambda labels, tokens, shapes, chars, seq_lens, tok_len : str_to_id((labels, tokens, shapes, chars, seq_lens, tok_len), labels_str_id_lookup, vocab_str_id_lookup, shape_str_id_lookup, char_str_id_lookup),  num_parallel_calls=16)
dataset = dataset.map(lambda labels, tokens, shapes, padded_chars, seq_len_batch, tok_len : tuple(tf.py_func(postprocess, [labels, tokens, shapes, padded_chars, seq_len_batch, tok_len], [labels.dtype, tokens.dtype, shapes.dtype, padded_chars.dtype, seq_len_batch.dtype, tok_len.dtype, tf.float32, tf.int64, tf.int64, tf.int64], name=""postprocess_py_func"")), num_parallel_calls=16)
dataset = dataset.prefetch(3)
iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)
dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')
```

I am trying to load the model and iterator at inference time by doing this, however I'm running into an issue. 
```python
with tf.Graph().as_default():

    saver = tf.train.import_meta_graph(FLAGS.load_dir + '/dilated-cnn.tf.meta')
    ckpt = tf.train.get_checkpoint_state(os.path.dirname(FLAGS.load_dir + '/checkpoint'))
    
    with tf.Session() as sess:
        saver.restore(sess, ckpt.model_checkpoint_path)

        graph = tf.get_default_graph()
        dataset_init_op = graph.get_operation_by_name('dataset_init')
        dataset_type = graph.get_tensor_by_name('Placeholder:0')
        predictions = graph.get_tensor_by_name('predictions/ArgMax:0')

        sess.run(dataset_init_op, feed_dict={dataset_type : ""sample_text.txt""})
        print(sess.run(predictions))
```

I keep getting an error saying: 
```
ValueError: callback pyfunc_0 is not found
	 [[{{node PyFunc}}]]
	 [[{{node IteratorGetNext}}]]
	 [[{{node IteratorGetNext}}]]
```

I understand this is happening because py_funcs are inherently not stored on the computation graph as they are simply pure Python functions. However, even after looking at this [post](https://stackoverflow.com/questions/43644506/setting-up-py-func-op-after-importing-graphdef-in-tensorflow) and creating two py_func skeleton functions, I'm still getting a similar error. 

My question is, does Tensorflow support using the Datasets API at inference time (without using feed dict i.e feeding in the tensors directly into the model without also recreating the model). I'm trying to do it using iterators and a modular dataset but I can't get around this py_func issue. Is there a way to properly restore an iterator that uses py_funcs at inference time or should I focus on converting all my preprocessing to native TF ops (which seems to be taking longer than the py_func version weirdly)?"
30965,LITE: Interpreter wrapper without libtensorflow_framework.so.2 dependency,"One should be able to use tensorflow lite python wrapper without having to import the entire libtensorflow (really costly on ARM boards such as raspberry), and use only the libtensorflow-lite as dependency.

"
30964,"Redefinition of ""__name__"" when importing keras callbacks or optimizers","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15+
- CUDA/cuDNN version: 10.0

**Describe the current behavior**
When importing all keras callbacks or optimizers via `from tensorflow.keras.callbacks import *` or `from tensorflow.keras.optimizers import *`, the variable `__name__` gets redefined to `tensorflow.keras.callbacks` and `tensorflow.keras.optimizers`.

**Describe the expected behavior**
Variable `__main__` should keep its value `""__main__""` so it can be used to determine if the python code is imported as a module/library or the entry point of a script.

**Code to reproduce the issue**
```Python
from tensorflow.keras.callbacks import *
print(__name__)
```

**Other info / logs**
Only occurs with `tensorflow.keras`, not with the standalone Keras."
30961,Inconsistent behavior with and without distributed scope,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 16.7.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed from pypi
- TensorFlow version (use command below):  v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.7.3
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Training on mac os, with CPU only, with and without distributed scope for mnist example. The loss and accuracy is incorrect with distributed scope turned off

With distributed scope on:
Train on 550.0 steps, validate on 50 steps
Epoch 1/5
550/550 [==============================] - 6s 10ms/step - loss: 4.5770 - accuracy: 0.7565 - val_loss: 1.0736 - val_accuracy: 0.7990
Epoch 2/5
550/550 [==============================] - 2s 3ms/step - loss: 0.7952 - accuracy: 0.8072 - val_loss: 0.6584 - val_accuracy: 0.8282
Epoch 3/5
550/550 [==============================] - 2s 3ms/step - loss: 0.5272 - accuracy: 0.8367 - val_loss: 0.5347 - val_accuracy: 0.8438
Epoch 4/5
550/550 [==============================] - 2s 3ms/step - loss: 0.4477 - accuracy: 0.8501 - val_loss: 0.4830 - val_accuracy: 0.8572
Epoch 5/5
550/550 [==============================] - 2s 3ms/step - loss: 0.4037 - accuracy: 0.8591 - val_loss: 0.4780 - val_accuracy: 0.8456

Without distributed scope:
Epoch 1/5
550/550 [==============================] - 3s 6ms/step - loss: 14.5062 - accuracy: 0.1000 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 2/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 3/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 4/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 5/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012

**Describe the expected behavior**
The training is expected to work correctly even with distributed scope turned off. This was working correctly in the tf2.0 alpha release but is an issue in the tf2.0 beta1 release. 

**Code to reproduce the issue**
```python
import tensorflow as tf
import contextlib
import sys

distributed=False
if len(sys.argv) > 1 and sys.argv[1] == ""distributed"":
    distributed=True
    print(""asdf enabled distributed trainer"")

class NoOpScope:
    def scope(self):
        return contextlib.suppress()

distribution_strategy = tf.distribute.MirroredStrategy() if distributed else NoOpScope()

with distribution_strategy.scope():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
    model.add(tf.keras.layers.Dense(300, activation=""relu""))
    model.add(tf.keras.layers.Dense(100, activation=""relu""))
    model.add(tf.keras.layers.Dense(10, activation=""softmax""))
    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])

(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_dev, x_train = x_train_full[:5000], x_train_full[5000:]
y_dev, y_train = y_train_full[:5000], y_train_full[5000:]
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(55000).repeat().batch(100)
dev_data = tf.data.Dataset.from_tensor_slices((x_dev, y_dev)).batch(100)

model.fit(train_data, 
          epochs=5,
          steps_per_epoch=55000/100,
          validation_data=dev_data)
```

Please copy to a script and run as:
Without distributed scope: python3 script.py
With distributed scope : python3 script.py distributed

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
30960,Make Github templates more intuitive,"Hi,
Frequently I stumble upon issues on the Tensorflow githubpage which do not fill out the template properly. I was curious whether it's possible to make the filling in of the template more intuitive. 

For instance I think it would be nice if there were field to fill in the asked questions right after each question.
Furthermore it would be nice to be required to at least fill some part of the form before being able to submit it.

What do you think? Is it possible to change the template on github in such a way?
I believe that this would improve the speed of resolving issues."
30959,What can replace 'initializer = tf.contrib.layers.xavier_initializer_conv2d()' in tensorflow 2.0?,Now that tensorflow has no attribute 'contrib'. Thanks a lot!
30958,fit and save methods fail with a subclassed model using a SequenceFeatures layer with sequence_numeric_column,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): from pip install
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**

`tf.keras.Model.fit` and `tf.keras.Model.save` method fail and return an error when applied to a model created by subclassing the `tf.keras.Model` class which uses a `SequenceFeatures` layer created by calling `SequenceFeatures.__init__` on a feature colmun created with `tf.feature_column.sequence_numeric_column`. Note that the issue does not occur when the `SequenceFeatures` layer is created by calling `SequenceFeatures.__init__` on a feature colmun created with `tf.feature_column.embedding_column`, as my code to reproduce the issue shows below.

Note also that the `call` method of the subclassed model using `sequence_numeric_column` works fine, in eager execution or in graph mode, see also the example code.

The error message I get with `fit` and `export_saved_model` is 
```
ValueError: Cannot convert a partially known TensorShape to a Tensor: (None, 9)
```
Hence it seems to be occuring because I did not explicitely set the batch size. But the last batch usually have a size less than the batch size so I am not interested in setting it to a fixed value before training. As a side note, I did not find how to do it anyway, my experiments with passing a `batch_input_shape` keyword argument to the `__init__` method of `SequenceFeatures` have failed, as if the argument was always ignored, and this combines poorly with nested structures of inputs like the one I have here with a dictionary of tensors.

**Describe the expected behavior**

The case when I use `embedding_column`, which is the case when the functions do not fail, suggests that the case with `sequence_numeric_column` should behave similarly and should not make the functions fail. It does not make sense to me that not knowing the batch size is relevant in one case and not relevant in the other. It may be due to the use, somewhere, of `tf.Tensor.shape` (which is static and returns the `None`) instead of `tf.shape` (which is dynamic). Additionally, it seems a standard practice to let the batch dimension to `None` (for the reason given above) so this should be supported.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.python.feature_column.feature_column_v2 import EmbeddingColumn
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.experimental import SequenceFeatures

print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

class Toy(Model):

    def __init__(self,
                 fc_list,
                 nb_features,
                 name='toy_model',
                 **kwargs):
        super(Toy, self).__init__(name=name, **kwargs)
        self.fc_list = fc_list
        self.dict_layers = {}
        for fc in self.fc_list:
            fc_name = fc.name
            self.dict_layers[fc_name] = SequenceFeatures(fc)
        self.lstm = LSTM(64, return_sequences=False)
        self.output_layer = Dense(nb_features, activation='softmax')
        
    def call(self, inputs, training=None):
        dict_apply_layers = {}
        for fc in self.fc_list:
            fc_name = fc.name
            if type(fc) == EmbeddingColumn:
                dict_apply_layers[fc_name] = self.dict_layers[fc_name](inputs)[0]
            else:
                # we need to convert inputs[fc_name] to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879
                zero = tf.constant(0, dtype=tf.float32)
                dense = inputs[fc_name]
                indices = tf.where(tf.not_equal(dense, zero))
                values = tf.gather_nd(dense, indices)
                sparse = tf.SparseTensor(indices, values, dense.shape)
                dict_apply_layers[fc_name] = self.dict_layers[fc_name]({fc_name: sparse})[0]
        x = tf.concat([v for _, v in dict_apply_layers.items()], axis=-1)
        x = self.lstm(x)
        x = self.output_layer(x)
        return x
    
#Dataset Parameters
nb_batches = 15
batch_size = 24
sequence_length = 9
nb_features = 10

#Dataset construction
input_dense = tf.constant(np.random.normal(0, 1, (nb_batches, batch_size, sequence_length)))
input_dense = tf.cast(input_dense, dtype=tf.float32)
input_cat = tf.constant(np.random.randint(0, nb_features, (nb_batches, batch_size, sequence_length)))
input_dict = {'dense': input_dense, 'categorical': input_cat}
input_dataset = tf.data.Dataset.from_tensor_slices(input_dict)

target_cat = tf.constant(np.random.randint(0, high=nb_features, size=(nb_batches, batch_size)))
target_dataset = tf.data.Dataset.from_tensor_slices(target_cat)

training_dataset = tf.data.Dataset.zip((input_dataset, target_dataset))

#Feature columns definition
fc_dense = tf.feature_column.sequence_numeric_column('dense')
fc_cat = tf.feature_column.sequence_categorical_column_with_identity('categorical', nb_features)
embedding_units = 16
fc_cat = tf.feature_column.embedding_column(fc_cat, embedding_units)
    
#Model Parameters
rnn_units = 64

#Training Parameters
epochs = 2

#Try the model with the sequence_numeric_column feature column
model = Toy([fc_dense,], nb_features)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])
try:
    model.fit(x=training_dataset, epochs=epochs)
    model.evaluate(x=training_dataset)
except ValueError as e:
    print(e)
try:
    path = 'tmp/test'
    model.save(path)
    new_model = tf.keras.models.load_model(path)
    new_model.evaluate(x=training_dataset)
except ValueError as e:
    print(e)
    
#Try the call method with the sequence_numeric_column feature column
model = Toy([fc_dense], nb_features)
try:
    for x, y in training_dataset:
        model(x)
    print('call worked')
except ValueError as e:
    print(e)

#Try the call method in graph mode with the sequence_numeric_column feature column
@tf.function
def call_graph(model, inputs):
    return model(inputs)
model = Toy([fc_dense], nb_features)
try:
    for x, y in training_dataset:
        call_graph(model, x)
    print('call in graph mode worked')
except ValueError as e:
    print(e)
    
#Try the model with the embedding_column feature column
model = Toy([fc_cat,], nb_features)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])
try:
    model.fit(x=training_dataset, epochs=epochs)
    model.evaluate(x=training_dataset)
except ValueError as e:
    print(e)
try:
    path = 'tmp/test'
    model.save(path)
    new_model = tf.keras.models.load_model(path)
    new_model.evaluate(x=training_dataset)
except ValueError as e:
    print(e)
```

Note that trying to build the model first, by inserting the lines
```
for x, y in training_dataset.take(1):
    model(x)
```
just before calling `fit` does not change anything."
30957,Segfault when passing empty Tensor to cholesky_solve,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Linux 93fd36b9ffb5 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When passing an empty Tensor as the second argument to the `tf.cholesky_solve` function a segfault is encountered.

Valgrind reports the following:

```
==8399== Process terminating with default action of signal 11 (SIGSEGV)
==8399==  Access not within mapped region at address 0x0
==8399==    at 0x1896E2B0: tensorflow::MatrixTriangularSolveOp<double>::ComputeMatrix(tensorflow::OpKernelContext*, absl::InlinedVector<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1> const, 0, Eigen::Stride<0, 0> >, 4ul, std::allocator<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1> const, 0, Eigen::Stride<0, 0> > > > const&, absl::InlinedVector<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4ul, std::allocator<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8399==    by 0x18A4A661: tensorflow::LinearAlgebraOp<double>::ComputeTensorSlice(tensorflow::OpKernelContext*, long long, absl::InlinedVector<tensorflow::Tensor const*, 4ul, std::allocator<tensorflow::Tensor const*> > const&, absl::InlinedVector<tensorflow::TensorShape, 4ul, std::allocator<tensorflow::TensorShape> > const&, absl::InlinedVector<tensorflow::Tensor*, 4ul, std::allocator<tensorflow::Tensor*> > const&, absl::InlinedVector<tensorflow::TensorShape, 4ul, std::allocator<tensorflow::TensorShape> > const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8399==    by 0x18A4A996: std::_Function_handler<void (long long, long long), tensorflow::LinearAlgebraOp<double>::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long, long long) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8399==    by 0x214615A3: tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x2146181E: tensorflow::thread::ThreadPool::ParallelFor(long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x212C6E80: tensorflow::Shard(int, tensorflow::thread::ThreadPool*, long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x18A46E44: tensorflow::LinearAlgebraOp<double>::Compute(tensorflow::OpKernelContext*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8399==    by 0x213C1D93: tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x213C20E9: std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x2145F6D3: Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x2145E543: std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8399==    by 0x2212D9DF: ??? (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25)
```

**Describe the expected behavior**

Although we clearly shouldn't be passing an empty Tensor to this method, I would expect an exception to occur (in Python) explaining that the input is invalid, rather than a segfault.

**Code to reproduce the issue**

Run the following code:

```
import numpy as np
import tensorflow as tf


def go():

    with tf.Graph().as_default():

        with tf.Session() as sess:

            X = tf.placeholder(shape=None, dtype=tf.float64)

            rhs = X[1:]
            rhs = tf.reshape(rhs, shape=(-1, 1))

            chol = tf.ones(shape=(1, 1), dtype=tf.float64)

            iP0 = tf.cholesky_solve(chol, rhs)

            feed_dict = {X: np.asarray([1.])}
            print(sess.run(iP0, feed_dict=feed_dict))

            print('SUCCESS!')


go()
```

Here the `rhs` argument to `cholesky_solve` ends up as an empty Tensor (it has a shape of (0, 1)), which appears to cause the segfault. 
"
30956,"Segfault in custom op with TensorFlow 1.14, Python 3.6 and GCC 5.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Linux 93fd36b9ffb5 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): g++-5 (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When compiling the example custom op from https://www.tensorflow.org/guide/extend/op with GCC 5, a segfault is encountered when running with TensorFlow 1.14 and Python 3.6 (on Linux).

The same code works fine when running with either Python 3.7 (instead of Python 3.6) or compiling with GCC 4.8 (instead of GCC 5). The code also works fine on Mac.

Valgrind shows the following:

```
==8208== Process terminating with default action of signal 11 (SIGSEGV)
==8208==  Access not within mapped region at address 0x88
==8208==    at 0x40292BC0: {lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*) (in /tmp/tmp1vwpnrsl)
==8208==    by 0x40292D0F: std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&) (in /tmp/tmp1vwpnrsl)
==8208==    by 0x2119ED1C: tensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)
==8208==    by 0x1BF7871F: tensorflow::ShapeRefiner::RunShapeFn(tensorflow::Node const*, tensorflow::OpRegistrationData const*, tensorflow::ExtendedInferenceContext*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8208==    by 0x1BF7A247: tensorflow::ShapeRefiner::AddNode(tensorflow::Node const*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8208==    by 0x19932F19: TF_FinishOperation (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8208==    by 0x179A3DB5: _wrap_TF_FinishOperation (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==8208==    by 0x4F858C: ??? (in /usr/bin/python3.6)
==8208==    by 0x4F98C6: _PyEval_EvalFrameDefault (in /usr/bin/python3.6)
==8208==    by 0x4F7A27: ??? (in /usr/bin/python3.6)
==8208==    by 0x4F876C: ??? (in /usr/bin/python3.6)
==8208==    by 0x4F98C6: _PyEval_EvalFrameDefault (in /usr/bin/python3.6)
```

**Describe the expected behavior**

My understanding is that both GCC 5 and Python 3.6 are both supported when building and running custom ops, so the code that uses the op should be able to run fine without segfaulting.

**Code to reproduce the issue**

Make a file called `zero_out.cc` with the following contents (as per the example from https://www.tensorflow.org/guide/extend/op):

```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""


using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

Make another file called `test_zero_out.py` with the following contents: 

```
import os
from tempfile import NamedTemporaryFile

import tensorflow as tf
from numpy.testing import assert_array_equal
import numpy as np

BUILD_COMMAND = 'g++-5 -std=c++11 -shared zero_out.cc -o {0} -fPIC {1} {2} -O2'


def test_zero_out():

    with NamedTemporaryFile() as temp_file:

        compile_args = ' '.join(tf.sysconfig.get_compile_flags())
        link_args = ' '.join(tf.sysconfig.get_link_flags()).replace(
            '-l:libtensorflow_framework.1.dylib', '-ltensorflow_framework.1')

        os.system(BUILD_COMMAND.format(temp_file.name, compile_args, link_args))

        zero_out_module = tf.load_op_library(temp_file.name)
        with tf.Graph().as_default():
            with tf.Session().as_default():
                result = zero_out_module.zero_out([5, 4, 3, 2, 1])
                assert_array_equal(result.eval(), np.asarray([5, 0, 0, 0, 0]))
                print('SUCCESS')


test_zero_out()
```

Ensure that GCC 5 is installed, along with Python 3.6 (on Ubuntu 18.04), and execute the following:

```
python3 test_zero_out.py
```

This compiles and runs the custom op. This should cause a segfault.

Ensure that Python 3.7 is installed and try the same thing:

```
python3.7 test_zero_out.py
```

This should run successfully and print ""SUCCESS"".

Now create an equivalent test file for GCC 4.8 called `test_zero_out-4.8.py`

```
import os
from tempfile import NamedTemporaryFile

import tensorflow as tf
from numpy.testing import assert_array_equal
import numpy as np

BUILD_COMMAND = 'g++-4.8 -std=c++11 -shared zero_out.cc -o {0} -fPIC {1} {2} -O2'


def test_zero_out():

    with NamedTemporaryFile() as temp_file:

        compile_args = ' '.join(tf.sysconfig.get_compile_flags())
        link_args = ' '.join(tf.sysconfig.get_link_flags()).replace(
            '-l:libtensorflow_framework.1.dylib', '-ltensorflow_framework.1')

        os.system(BUILD_COMMAND.format(temp_file.name, compile_args, link_args))

        zero_out_module = tf.load_op_library(temp_file.name)
        with tf.Graph().as_default():
            with tf.Session().as_default():
                result = zero_out_module.zero_out([5, 4, 3, 2, 1])
                assert_array_equal(result.eval(), np.asarray([5, 0, 0, 0, 0]))
                print('SUCCESS')


test_zero_out()

```

Ensure GCC 4.8 is installed and run:

```
python3 test_zero_out-4.8.py
```

This will compile and run the same code with GCC 4.8. This should succeed and print ""SUCCESS""."
30955,Subclassing Model prevents the computation of intermediate values in graph mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab
- TensorFlow version (use command below):
Tensorflow 2.0 beta 1

**Describe the current behavior**
Computing intermediate values within a subclassed model doesn't work in graph mode.

**Describe the expected behavior**
It should most likely work, as it was possible in TensorFlow 1.x.

**Code to reproduce the issue**
Try it directly in Google Colab. Setting 'use_eager_mode' to True/False switches between the two cases.
https://gist.github.com/DeepBlender/6ab324ab3b14552109979a97bf4acb8f"
30954,Potential bugs found with static analysis,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Issue 1:**
https://github.com/tensorflow/tensorflow/blob/1ee51a3b868a3ccd5f80724f6b9389fd0a9aed07/tensorflow/compiler/tf2xla/functionalize_cond.cc#L445-L447
`dst_copy != nullptr` is checked immediately after `if (dst_copy == nullptr) continue;`. Is one of comparisons supposed to be different, or can the `TF_RET_CHECK` be removed?

**Issue 2:**
https://github.com/tensorflow/tensorflow/blob/9d67841e6c1f852516abf6ff44490b5d5a8331af/tensorflow/contrib/ignite/kernels/dataset/ignite_dataset_iterator.cc#L126-L128
This code is unreachable, because both branches of `if` above return. Either it should be deleted or some part of `if` modified.

**Issue 3:**
https://github.com/tensorflow/tensorflow/blob/514004a2347058214d1e7b13b9769a2abdd06830/tensorflow/core/profiler/rpc/client/capture_profile.cc#L218-L220
The condition is always true. My guess is that `==` is intended instead of `!=`.

**Issue 4:**
https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/profiler/internal/tfprof_tensor.h#L55
Should this be `void` since nothing gets returned in any branch?

**Issue 5:**
https://github.com/tensorflow/tensorflow/blob/9380a41290e8fb8b9ea85f614472deab56dbc481/tensorflow/stream_executor/stream_executor_pimpl.cc#L112-L125
`ScopedTracer` doesn't obey the rule of 3, but this `SCOPED_TRACE` macro expands to calling its copy constructor. This should be safe in practice because of copy elision, I believe, but undesirable to rely on. Unfortunately, changing to `auto tracer{MakeScopedTracer(this, &LOC##Begin, &LOC##Complete, ##__VA_ARGS__)};` will infer `initializer_list<...>` (is that right?).

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30953,TF-Lite micro low performance,"This issue regards the tflite micro.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746NG
- TensorFlow installed from (source or binary): github
- TensorFlow version (use command below): 1.14.0-718503b075d
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I've ported a model from Keras to TF and trained it. Then I've converted the model to tflite.

One issue is that the tf.lite.Optimize.OPTIMIZE_FOR_SIZE doesn't work as the tflite-micro complains that one of the layers is not compatible with int8 types. Anyway, I've converted the model without quantization and also used CMSIS-NN. The performance is extremely slow. It's 40x times slower on the same CPU compared to X-CUBE-AI API. 

**Describe the expected behavior**
I expected the inference to be much faster especially for the depthwise_conv.cc kernel which supports the cmsis-nn. It seems that using cmsis-nn, makes an insignificant difference.

**Code to reproduce the issue**
I have a repo here that you can use for validation:
https://bitbucket.org/dimtass/stm32f746-tflite-micro-mnist

**Other info / logs**
Also, I've written a blog post with the issue here:
https://www.stupid-projects.com/machine-learning-on-embedded-part-3/

**Edit**: I've found out that I haven't enabled the FPU during the g++ compilation. With the correct architecture flags (-mthumb -mcpu=cortex-m7 -mfpu=fpv5-sp-d16 -mfloat-abi=hard), the performance is now 3x times faster, but it's still ~12x times slower than the x-cube-ai.

This is the execution time for each layer with comparison to x-cube-ai. Have in mind that the x-cube-ai merges some layers, so I'm using '-' and the sum in the next layer:

Layer | tflite-micro /soft-float (msec) | tflite-micro /hard-float (msec) | x-cube-ai (msec)
-|-|-|-
DEPTHWISE_CONV_2D | 235 | 69 | -
MAX_POOL_2D | 23 | 7 | 11.2
CONV_2D | 2346 | 733 | -
MAX_POOL_2D | 7 | 2 | 57.19
CONV_2D | 348 | 108 | 8.69
FULLY_CONNECTED | 5 | 3 | -
FULLY_CONNECTED | 0 | 0 | -
SOFTMAX | 0 | 0 | 2
TOTAL TIME | 2964 | 922 | 78.2

That's almost 40 times slower on the same MCU. I'm not sure if I'm doing something so terribly wrong or this is the real performance of the API.

I really hope for some input. Thanks!"
30952,`tf.keras.layers.Embedding` causes memory leak,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 19.1
- TensorFlow installed from: binary (using pip)
- TensorFlow version: 2.0.0-beta1 (v2.0.0-beta0-16-g1d91213fe7)
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5

**Describe the current behavior**

A GPU (_edit: CPU as well, see addendum below_) memory leak (rapidly) emerges from using (high-dimensional) `tf.keras.layers.Embedding` layers.

To be more precise, I am working on Transformer networks, and found out that when I try to fit one, _e.g._ on the portuguese-to-english translation task presented in [this official tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer), a GPU memory leak emerges after a few iterations. Based on [this StackOverflow post](https://stackoverflow.com/questions/42499592/resourceexhaustederror-oom-when-allocating-tensor-with-shape#42512916), I rapidly came to suspect that the issue comes from the (learnable) embedding layers at the base of both the encoder and decoder parts of the network.

To further assess the issue and its source, I implemented a pseudo-Transformer network (see code linked below) that is stripped of most technical components the actual model embarks (_e.g._ I removed positional encoding, residual connections, masking mechanisms, etc.) - the rationale being to provide a more condense (and faster-run) code to document this issue, but also to confirm that the leak does not come from custom layers or any ""complex"" data processing mechanism.

The provided code includes a data pre-processing pipeline entirely based on the aforementioned [tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer), a model-construction function that makes use of the keras functional API, and a main function to call the former and start the fitting process. On my computer, everything runs fine and I can see the first few fitting iterations pass, until an ugly stack of allocation error messages show up (see full log linked below), whose informative part seems to be `W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor`

**Addendum**: I re-ran the provided code disabling access to the GPU, and it turns out there also is a high memory usage when running on CPU. During the first epoch (and mostly during its first half), memory usage goes up multiple GB (in my case, from 2 to 10 GB, with an increase from 2 to 7 within the first 60 train steps out of 704), and keeps slowly increasing throughout the following epochs (with minor decreases between increases, thus displaying local plateaux which I would guess are related to the loading / discarding of data batches). Although it is a bit less of a problem than with GPU since it is relatively common to have quite some RAM available (plus some swap space, on linux), it still does not feel right that fitting the fake model on a dataset which can be fully loaded in memory (creating a list of Eager Tensors from the `tf.data.Dataset` object containing the batched, padded training set results in a marginal usage of around 100 MB of RAM) would end up using 16GB or RAM. I would also like to note that calling `gc.collect` after training _does not_ empty the used RAM, which is only freed (instantly) when ending the python process.


**Describe the expected behavior**

The fitting process should go one fine, and the memory should not get saturated (I would expect some tensors to be de-allocated as iterations pass).

**Code to reproduce the issue**

The script I wrote to illustrate the issue is publicly accessible as a gist [here](https://gist.github.com/pandrey-fr/c4ba8022c5dd956388e984f49c89ce61).

**Other info / logs**

The full error stack (_with GPU enabled_) is publicly accessible as a gist [here](https://gist.github.com/pandrey-fr/ff004b4cdd6d22b9cd84f82ef4e3a5ac)"
30951,Missing input after saving/loading Keras Model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce FTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am using [keras functional API](https://keras.io/getting-started/functional-api-guide/) to create a model. During training it has 2 inputs - one is actual input and second one is ground truth mask, I am applying using `Lambda` layer (see code example). When saving the model (not the weights) (via `ModelCheckpoint` callback or manually) and loading it via `keras.models.load_model` I get model without second input.

**Describe the expected behavior**
Model has both inputs after loading.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
from tensorflow import keras
import numpy as np


input_size = 30
output_size = 10

inp = keras.layers.Input((input_size,))
mask = keras.layers.Input((output_size,), dtype=tf.bool)

x = keras.layers.Dense(output_size)(inp)
x = keras.layers.Lambda(lambda x: x, mask=mask)(x)

model = keras.models.Model(inputs=[inp, mask], outputs=[x])
model.compile(loss='mean_squared_error', optimizer='sgd')

print('Model inputs: {}'.format(model.inputs)) # Prints: ""Model inputs: [<tf.Tensor 'input_1:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 10) dtype=bool>]""

batch_size = 20
x_train = np.random.rand(batch_size, input_size)
y_train = np.random.rand(batch_size, output_size)
y_train_mask = np.random.rand(batch_size, output_size)
y_train_mask = y_train_mask > .5

model.fit([x_train, y_train_mask], y_train)

checkpoint = './model.h5'
model.save(checkpoint)

model = keras.models.load_model(checkpoint)
print('Model inputs: {}'.format(model.inputs)) # Prints: ""Model inputs: [<tf.Tensor 'input_1_1:0' shape=(?, 30) dtype=float32>]""
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
30950,Transfer Learning tutorial Attribute Error: 'GFile' object has no attribute 'seekable',"Following the tutorial on downloading and using data sets found here: https://www.tensorflow.org/beta/tutorials/images/transfer_learning
I get the error 

> Attribute Error: 'GFile' object has no attribute 'seekable'

It breaks when I run tfds.load

You can reproduce it with the following syntax:

'import tensorflow_datasets as tfds
tfds.disable_progress_bar()
SPLIT_WEIGHTS = (8, 1, 1)
splits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)

(raw_train, raw_validation, raw_test), metadata = tfds.load(
    cats_vs_dogs', split=list(splits),
    with_info=True, `as_supervised=True)`

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Platform: Windows
- Tensorflow Datasets version 1.0.2
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- CUDA/cuDNN version: 10.0/7.6"
30948,The use of List and Tuple is vague in Tensorflow tf.data.Dataset.map.,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0
Python version: 3.7.3
CUDA/cuDNN version: 10.1.168 / 7.6.0 but also failing in CPU-only mode
GPU model and memory: GTX 1080 8G
Exact command to reproduce:
```
import tensorflow as tf
input = tf.constant([10,20,30])
ds = tf.data.Dataset.from_tensor_slices(input)
#Out: <DatasetV1Adapter shapes: (), types: tf.int32>

ds1=ds.map(lambda x: [x+1, x+2, x+3])
#Out: <DatasetV1Adapter shapes: ((), (), ()), types: (tf.int32, tf.int32, tf.int32)>
ds2=ds.map(lambda x: [[x+1, x+2, x+3]])
#Out: <DatasetV1Adapter shapes: ((3,),), types: (tf.int32,)>
ds3=ds.map(lambda x: ([x+1, x+2, x+3]))
#Out: <DatasetV1Adapter shapes: ((), (), ()), types: (tf.int32, tf.int32, tf.int32)>
```
I think tensorflow treats tuple as nested structures of Tensor and list as tensor, as shown in [link](https://github.com/tensorflow/tensorflow/issues/20481). 
However, `tf.data.Dataset.map` doesn't behave like this in above code. I expect `ds1, d2, ds3` to be of shape `[3, ], [1, 3], ([3, ], )` respectively,  rather than `((), (), ()), ((3,),), ((), (), ())` since top-level list [] should be treated as tensor rather than nested structure.
A [solution](https://stackoverflow.com/questions/57142462/something-strange-about-output-tensor-shape-of-tf-data-dataset-map/57146524?noredirect=1#comment100834064_57146524) is that I use `tf.convert_to_tensor` to force convert `[x+1, x+2, x+3]` to a single tensor but it's not elegant.

ps: It's confusing to output shape only with tuple, why not using list to represent shape and using tuple to represent nested structure of tensor jsut like 
the output of Tensor.shape and Tensor.get_shape. e.g. `#Out: <DatasetV1Adapter shapes: ([],[],[]), types: (tf.int32, tf.int32, tf.int32)>`"
30947,"[TF 2.0 nightly] tf.keras.estimator.model_to_estimator with strategy=tf.distribute.MirroredStrategy() -> Method requires being in cross-replica context, use get_replica_context().merge_call()","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190722
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

>>> import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
v1.12.1-6737-gb4e5625437 2.0.0-dev20190722

**Describe the current behavior**
when using `strategy = tf.distribute.MirroredStrategy()`, `tf.keras.estimator.model_to_estimator()`

is crashing during training with 
`
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()`

**Describe the expected behavior**
Should work as before and without any error messages

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import tensorflow_datasets as tfds
from absl import logging

logging.set_verbosity(logging.INFO)
# Define the estimator's input_fn
STEPS_PER_EPOCH = 5
BATCH_SIZE = 64
NUM_EPOCHS = 5


def input_fn():
    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    BUFFER_SIZE = 10000
    BATCH_SIZE = 64

    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
    
        return image, label[..., tf.newaxis]

    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
    return train_data.repeat()

# Define train & eval specs
train_spec = tf.estimator.TrainSpec(input_fn=input_fn,
                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)
eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,
                                  steps=STEPS_PER_EPOCH)

def make_model():
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu',
                               kernel_regularizer=tf.keras.regularizers.l2(0.02),
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

model = make_model()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#####
#strategy=None 
# crashing
strategy = tf.distribute.MirroredStrategy()

# config tf.estimator to use a give strategy
training_config = tf.estimator.RunConfig(train_distribute=strategy)
#####

estimator = tf.keras.estimator.model_to_estimator(
    keras_model = model,
    config=training_config
)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
W0723 11:29:42.079696 4746859968 cross_device_ops.py:1207] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
I0723 11:29:42.087183 4746859968 run_config.py:554] Initializing RunConfig with distribution strategies.
I0723 11:29:42.088423 4746859968 estimator_training.py:167] Not using Distribute Coordinator.
W0723 11:29:42.097567 4746859968 estimator.py:1812] Using temporary folder as model directory: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmp8lhzz88_
I0723 11:29:42.098804 4746859968 keras.py:527] Using the Keras model provided.
I0723 11:29:43.420326 4746859968 estimator.py:209] Using config: {'_model_dir': '/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmp8lhzz88_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0xb3aa517b8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb3aa51a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
I0723 11:29:43.421994 4746859968 estimator_training.py:186] Not using Distribute Coordinator.
I0723 11:29:43.422926 4746859968 training.py:612] Running training and evaluation locally (non-distributed).
I0723 11:29:43.423843 4746859968 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
I0723 11:29:43.429722 4746859968 dataset_builder.py:184] Overwrite dataset info from restored data version.
I0723 11:29:43.434206 4746859968 dataset_info.py:380] Field info.location from disk and from code do not match. Keeping the one from code.
I0723 11:29:43.435199 4746859968 dataset_builder.py:253] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)
I0723 11:29:43.435896 4746859968 dataset_builder.py:399] Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/mnist/1.0.0
W0723 11:29:43.586170 4746859968 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
I0723 11:29:43.751439 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).
I0723 11:29:43.764024 123145625903104 estimator.py:1145] Calling model_fn.
I0723 11:29:43.943940 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).
I0723 11:29:43.948454 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).
I0723 11:29:43.952039 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).
I0723 11:29:43.954842 4746859968 cross_device_ops.py:425] Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).
I0723 11:29:44.504832 123145625903104 coordinator.py:219] Error reported to Coordinator: Method requires being in cross-replica context, use get_replica_context().merge_call()
Traceback (most recent call last):
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 865, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py"", line 326, in model_fn
    saver = saver_lib.Saver(var_list=var_list, sharded=True)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 497, in _build_internal
    per_device = self._GroupByDevices(saveables)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 404, in _GroupByDevices
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py"", line 404, in <genexpr>
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py"", line 52, in tensor
    return self._tensor() if callable(self._tensor) else self._tensor
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py"", line 1136, in tensor
    return strategy.extended.read_var(sync_on_read_variable)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py"", line 722, in read_var
    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py"", line 1222, in _get_cross_replica
    self, axis=None)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 794, in reduce
    _require_cross_replica_or_default_context_extended(self._extended)
  File ""/Users/tarrade/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 206, in _require_cross_replica_or_default_context_extended
    raise RuntimeError(""Method requires being in cross-replica context, use ""
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-3-f33ca3103e62> in <module>
     64 )
     65 
---> 66 tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    471         '(with task id 0).  Given task id {}'.format(config.task_id))
    472 
--> 473   return executor.run()
    474 
    475 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
    611         config.task_type != run_config_lib.TaskType.EVALUATOR):
    612       logging.info('Running training and evaluation locally (non-distributed).')
--> 613       return self.run_local()
    614 
    615     # Distributed case.

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
    712         max_steps=self._train_spec.max_steps,
    713         hooks=train_hooks,
--> 714         saving_listeners=saving_listeners)
    715 
    716     eval_result = listener_for_eval.eval_result or _EvalResult(

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    365 
    366       saving_listeners = _check_listeners_type(saving_listeners)
--> 367       loss = self._train_model(input_fn, hooks, saving_listeners)
    368       logging.info('Loss for final step: %s.', loss)
    369       return self

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1154   def _train_model(self, input_fn, hooks, saving_listeners):
   1155     if self._train_distribution:
-> 1156       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1157     else:
   1158       return self._train_model_default(input_fn, hooks, saving_listeners)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1217       self._config._train_distribute.configure(self._config.session_config)
   1218       return self._actual_train_model_distributed(
-> 1219           self._config._train_distribute, input_fn, hooks, saving_listeners)
   1220     # pylint: enable=protected-access
   1221 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)
   1297                     labels,  # although this will be None it seems
   1298                     ModeKeys.TRAIN,
-> 1299                     self.config))
   1300           loss = strategy.reduce(
   1301               _get_loss_reduce_op_for_reporting(),

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1769       kwargs = {}
   1770     with self._container_strategy().scope():
-> 1771       return self._call_for_each_replica(fn, args, kwargs)
   1772 
   1773   def _call_for_each_replica(self, fn, args, kwargs):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)
    645                           self._container_strategy().__class__.__name__, 5)
    646     return _call_for_each_replica(self._container_strategy(), self._device_map,
--> 647                                   fn, args, kwargs)
    648 
    649   def _configure(self,

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, device_map, fn, args, kwargs)
    194     for t in threads:
    195       t.should_run.set()
--> 196     coord.join(threads)
    197 
    198   return values.regroup(device_map, tuple(t.main_result for t in threads))

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
    387       self._registered_threads = set()
    388       if self._exc_info_to_raise:
--> 389         six.reraise(*self._exc_info_to_raise)
    390       elif stragglers:
    391         if ignore_live_threads:

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py in stop_on_exception(self)
    295     """"""
    296     try:
--> 297       yield
    298     except:  # pylint: disable=bare-except
    299       self.request_stop(ex=sys.exc_info())

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in run(self)
    863               self._var_scope, reuse=self.replica_id > 0), \
    864           variable_scope.variable_creator_scope(self.variable_creator_fn):
--> 865         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    866         self.done = True
    867     finally:

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1144 
   1145     logging.info('Calling model_fn.')
-> 1146     model_fn_results = self._model_fn(features=features, **kwargs)
   1147     logging.info('Done calling model_fn.')
   1148 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)
    324       object_graph = graph_view.ObjectGraphView(model)
    325       var_list = object_graph.frozen_saveable_objects()
--> 326       saver = saver_lib.Saver(var_list=var_list, sharded=True)
    327       saver._object_restore_saver = trackable_util.frozen_saver(model)
    328       scaffold = monitored_session.Scaffold(saver=saver)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in __init__(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)
    826           time.time() + self._keep_checkpoint_every_n_hours * 3600)
    827     elif not defer_build:
--> 828       self.build()
    829     if self.saver_def:
    830       self._check_saver_def()

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in build(self)
    838     if context.executing_eagerly():
    839       raise RuntimeError(""Use save/restore instead of build in eager mode."")
--> 840     self._build(self._filename, build_save=True, build_restore=True)
    841 
    842   def _build_eager(self, checkpoint_path, build_save, build_restore):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _build(self, checkpoint_path, build_save, build_restore)
    876           filename=checkpoint_path,
    877           build_save=build_save,
--> 878           build_restore=build_restore)
    879     elif self.saver_def and self._name:
    880       # Since self._name is used as a name_scope by builder(), we are

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _build_internal(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)
    495       # Add the save ops.
    496       if sharded:
--> 497         per_device = self._GroupByDevices(saveables)
    498         if build_save:
    499           save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in _GroupByDevices(self, saveables)
    402     for saveable in saveables:
    403       canonical_device = set(
--> 404           pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
    405       if len(canonical_device) != 1:
    406         raise ValueError(""All tensors of a saveable object must be ""

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py in <genexpr>(.0)
    402     for saveable in saveables:
    403       canonical_device = set(
--> 404           pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
    405       if len(canonical_device) != 1:
    406         raise ValueError(""All tensors of a saveable object must be ""

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py in tensor(self)
     50   @property
     51   def tensor(self):
---> 52     return self._tensor() if callable(self._tensor) else self._tensor
     53 
     54 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in tensor()
   1134     def tensor():
   1135       strategy = sync_on_read_variable._distribute_strategy  # pylint: disable=protected-access
-> 1136       return strategy.extended.read_var(sync_on_read_variable)
   1137 
   1138     spec = saver.BaseSaverBuilder.SaveSpec(

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py in read_var(self, replica_local_var)
    720     """"""Read the aggregate value of a replica-local variable.""""""
    721     if isinstance(replica_local_var, values.SyncOnReadVariable):
--> 722       return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
    723     assert isinstance(replica_local_var, values.Mirrored)
    724     return array_ops.identity(replica_local_var.get())

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in _get_cross_replica(self)
   1220       return self._distribute_strategy.reduce(
   1221           reduce_util.ReduceOp.from_variable_aggregation(self.aggregation),
-> 1222           self, axis=None)
   1223 
   1224   def _as_graph_element(self):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in reduce(self, reduce_op, value, axis)
    792     """"""
    793     # TODO(josh11b): support `value` being a nest.
--> 794     _require_cross_replica_or_default_context_extended(self._extended)
    795     if isinstance(reduce_op, six.string_types):
    796       reduce_op = reduce_util.ReduceOp(reduce_op.upper())

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _require_cross_replica_or_default_context_extended(extended)
    204     _wrong_strategy_scope(strategy, context)
    205   assert cross_replica is None
--> 206   raise RuntimeError(""Method requires being in cross-replica context, use ""
    207                      ""get_replica_context().merge_call()"")
    208 

RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()

```
"
30946,tf.keras.layers.Dropout does not accept noise_shape with None-dimension,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE 42.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (via conda)
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.6.0
- GPU model and memory: GeForce GTX 1080 Ti (10.92 GB)

**Describe the current behavior**

The Dropout layer accepts a `noise_shape` argument. If set, this shape has to include the batch size, which is usually not known at model construction time. Thus, this argument is actually only useful if `None` can be specified for any axis and is then expanded to the actual size of that axis in the input tensor passed to the layer.

This is currently perfectly possible in standard `keras` but raises an error in `tf.keras` (see below).

**Describe the expected behavior**

It should be possible to include `None`-sized axes in the `noise_shape` argument to `tf.keras.layers.Dropout`, which are expanded at run-time to the actual size of that axis in the input tensor of the layer.

**Code to reproduce the issue**

```python
from tensorflow import keras

inp_ = keras.layers.Input((2,3,4))
dropout = keras.layers.Dropout(0.5, noise_shape=(None,1,1,1))
x = dropout(inp_, training=True)
dropfun = keras.backend.function([inp_], [x])
```

This code works completely fine if the first line is replaced with `import keras`.

**Other info / logs**

Traceback:
```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-1601abc84b49> in <module>
      1 inp_ = keras.layers.Input((2,3,4))
      2 dropout = keras.layers.Dropout(0.5, noise_shape=(None,1,1,1))
----> 3 x = dropout(inp_, training=True)
      4 dropfun = keras.backend.function([inp_], [x])

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    633                 else:
--> 634                   outputs = call_fn(inputs, *args, **kwargs)
    635 
    636             except TypeError as e:

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, training)
    160     output = tf_utils.smart_cond(training,
    161                                  dropped_inputs,
--> 162                                  lambda: array_ops.identity(inputs))
    163     return output
    164 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     56         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     57   return smart_module.smart_cond(
---> 58       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     59 
     60 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in dropped_inputs()
    156           noise_shape=self._get_noise_shape(inputs),
    157           seed=self.seed,
--> 158           rate=self.rate)
    159 
    160     output = tf_utils.smart_cond(training,

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py in dropout(x, keep_prob, noise_shape, seed, name, rate)
   4168     raise ValueError(""You must provide a rate to dropout."")
   4169 
-> 4170   return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)
   4171 
   4172 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py in dropout_v2(x, rate, noise_shape, seed, name)
   4247     # and subtract 1.0.
   4248     random_tensor = random_ops.random_uniform(
-> 4249         noise_shape, seed=seed, dtype=x.dtype)
   4250     keep_prob = 1 - rate
   4251     scale = 1 / keep_prob

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)
    237     maxval = 1
    238   with ops.name_scope(name, ""random_uniform"", [shape, minval, maxval]) as name:
--> 239     shape = _ShapeTensor(shape)
    240     minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")
    241     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in _ShapeTensor(shape)
     42   else:
     43     dtype = None
---> 44   return ops.convert_to_tensor(shape, dtype=dtype, name=""shape"")
     45 
     46 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1085   preferred_dtype = deprecation.deprecated_argument_lookup(
   1086       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1087   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1088 
   1089 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1143       name=name,
   1144       preferred_dtype=dtype_hint,
-> 1145       as_ref=False)
   1146 
   1147 

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
   1222 
   1223     if ret is None:
-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1225 
   1226     if ret is NotImplemented:

~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)
    324   if not s.is_fully_defined():
    325     raise ValueError(
--> 326         ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
    327   s_list = s.as_list()
    328   int64_value = 0

ValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 1, 1, 1)
```

The bug is probably due to the `tf.keras.layers.Dropout._get_noise_shape` function, which explicitly performs the `None`-expansion in [standard keras](https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L110).
"
30945,/home/hdp_teu_dia/guesslike/user/suwenyuan/emotion/bert-master/chinese_L-12_H-768_A-12,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
30944,Keras' performance is worse than pure TF,"Hello, 
I am trying to update my pure Tensorflow code to incorporate Keras. But what I find is that my Keras implementation of the same NN structure does not converge as much as the pure TF code for the same number of iteration. In fact, Keras implementation does very poorly.  I have attached my code and data set [here](https://github.com/ronyeapen/TF-and-Keras). It is a Regression model.  No response here with regards to it. 
https://stackoverflow.com/questions/57137235/tf-low-level-api-vs-tf-keras-performance-difference
"
30943,TensorFlow C API Nightly URLs,"[This README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/README.md) mentions the following URLs for nightly builds of libtensorflow:
- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-linux-x86_64.tar.gz
- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-gpu-linux-x86_64.tar.gz
- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-darwin-x86_64.tar.gz

Are there any versions of these including dates? Maybe something matching the naming convention used by the nightly pip packages (e.g. `libtensorflow-cpu-linux-x86_64-1.15.0.dev20190722.tar.gz`)?

I looked through some issues and RFCs  (including https://github.com/tensorflow/tensorflow/issues/21524 and https://github.com/tensorflow/community/blob/master/rfcs/20181026-tf-nightly.md), but didn't see anything about versioned nightly builds.

Thank you!"
30942,unable to load trained model,"I trained the NMT model on my RTX2080 ti. I saved the model by

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

and in the training step 

if (epoch + 1) % 2 == 0:
   checkpoint.save(file_prefix = checkpoint_prefix)

and after training, I am loading the trained model by

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

But my model's output is random. 

### System information
- ** TensorFlow version == 1.14.0(installed through "" conda install -c anaconda tensorflow-gpu "")
- **OS Platform and Distribution (e.g., Linux Ubuntu 18.04)**:
- **Python version == 3.7
- **CUDA/cuDNN version**:== 10
- **GPU model and memory**: 11GB


and I am getting this at the end

W0723 07:13:25.044782 140413660407616 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta1_power
W0723 07:13:25.044880 140413660407616 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta2_power
W0723 07:13:25.044922 140413660407616 util.py:248] Unused attribute in object (root).decoder.embedding: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.044954 140413660407616 util.py:248] Unused attribute in object (root).decoder.gru: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.044982 140413660407616 util.py:248] Unused attribute in object (root).decoder.fc: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045008 140413660407616 util.py:248] Unused attribute in object (root).decoder.W1: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045034 140413660407616 util.py:248] Unused attribute in object (root).decoder.W2: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045060 140413660407616 util.py:248] Unused attribute in object (root).decoder.V: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045086 140413660407616 util.py:248] Unused attribute in object (root).encoder.embedding: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045122 140413660407616 util.py:248] Unused attribute in object (root).encoder.gru: ['OBJECT_CONFIG_JSON']
W0723 07:13:25.045150 140413660407616 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.
"
30938,shape_a should be an arange not just a list ,"`
#I think it should be 
shape_a = np.arange(a.get_shape[0])
#instead of 
shape_a = a.get_shape().as_list()
`
<em>Please make sure that this is a bug. As per our [GitHub ### ### Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30935,Wrong URL for Cuda download in dll not found error message,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windos 10 64Bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.0.0-beta1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: 1050



**Describe the problem**
When Tensorflow 2.0.0-beta1 is installed with pip, the error message stating that dll for Cuda is not found gives a wrong URL to [Cuda 9.0](https://developer.nvidia.com/cuda-90-download-archive) instead of [10.0, which is required by TF 2.0](https://developer.nvidia.com/cuda-10.0-download-archive).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- `pip install tensorflow-gpu==2.0.0-beta1`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> >>> import tensorflow as tf
> Traceback (most recent call last):
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
>     ctypes.WinDLL(build_info.cudart_dll_name)
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\ctypes\__init__.py"", line 348, in __init__
>     self._handle = _dlopen(self._name, mode)
> OSError: [WinError 126] The specified module could not be found
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
>     from tensorflow.python.tools import module_util as _module_util
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
>     self_check.preload_check()
>   File ""C:\ProgramData\Anaconda3\envs\py368\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
>     % (build_info.cudart_dll_name, build_info.cuda_version_number))
> ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive"
30934,[BUG] [TF 2.0 Keras] Pointwise Conv2D numerically inconsistent in keras model vs manual run,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Docker container**: 2.0.0b1-gpu-py3-jupyter
- **CUDA/cuDNN version**: CUDA 10.2
- **GPU model and memory**: GeForce GTX 1080
- **Exact command to reproduce**: See script below.

### Describe the problem
Wrapping an identical tf.nn.conv2d operation **that has ksize of (1, 1)** in a tf.keras.Model and calling the model, with or without predict, on identical data, produces different results.

The differences are small but accumulate through a deep network if many pointwise convs are used.

### Source code / logs

```
import tensorflow as tf
import numpy as np

np.random.seed(123)
pool = np.ones([32, 64, 64, 64], 'float32')
w1 = np.random.randn(1, 1, 64, 64).astype('float32')

# Manual convolution
conv1 = tf.nn.conv2d(pool, w1, [1, 1], padding='SAME').numpy()

# The same convolution op via keras model
tmp_input = tf.keras.Input(shape=[64, 64, 64], dtype='float32')
tmp_out = tf.nn.conv2d(tmp_input, w1, [1, 1], padding='SAME)

# .predict can also be removed
conv2 = tf.keras.Model(inputs=tmp_input, outputs=tmp_out).predict(pool)

# The individual error is small, but it compounds through a deep network.
print('Disagreement between manual and keras-model wrapped conv:', np.abs(conv1 - conv2).sum())
```

This script can be run in a fresh pull of the docker container."
30932,[BUG] tf.saved_model.load cannot load Keras compiled model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Elementary OS Loki
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190601
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.saved_model.load cannot load compiled keras model, saved as SavedModel in graph mode.
**Describe the expected behavior**
No Error Expected
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python3
import tensorflow as tf
model = tf.keras.models.Sequential()
model.compile(optimizer=""adam"", loss=""mean_squared_error"")
tf.saved_model.save(model, ""/tmp/model"")
```
Now loading the model:
```python3
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
tf.saved_model.load(""/tmp/model"")
```
This raises the following error:
`ValueError: '_RestoredOptimizer' is not a valid scope name`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 407, in load
    export_dir)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 58, in __init__
    self._load_all()
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 157, in _load_all
    node, setter = self._recreate(proto)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 240, in _recreate
    return factory[kind]()
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 227, in <lambda>
    ""user_object"": lambda: self._recreate_user_object(proto.user_object),
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/load.py"", line 244, in _recreate_user_object
    looked_up = revived_types.deserialize(proto)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/revived_types.py"", line 166, in deserialize
    return (type_registration.from_proto(proto), type_registration.setter)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/saved_model/revived_types.py"", line 87, in from_proto
    return self._object_factory(proto)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 1039, in <lambda>
    object_factory=lambda proto: _RestoredOptimizer(),
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 1025, in __init__
    super(_RestoredOptimizer, self).__init__(""_RestoredOptimizer"")
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 263, in __init__
    with backend.name_scope(self._name) as name_scope:
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 6523, in __enter__
    return self._name_scope.__enter__()
  File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/home/rick/tf2.0/env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 4320, in name_scope
    raise ValueError(""'%s' is not a valid scope name"" % name)
ValueError: '_RestoredOptimizer' is not a valid scope name
```
CC:
@vbardiovskyg 
@srjoglekar246 "
30929,"tf.data.experimental.prefetch_to_device(""/gpu:0"") moves tensors back to CPU","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: Python 3.6.8
- CUDA/cuDNN version: CUDA Version: 10.0 / cuDNN 7.6.0
- GPU model and memory: Tesla T4  15079MiB

**Describe the current behavior**
`tf.data.experimental.prefetch_to_device(""/gpu:0"")` moves tensors back to CPU.

**Describe the expected behavior**
`tf.data.experimental.prefetch_to_device(""/gpu:0"")` should leave tensors on GPU.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

data = np.array([[1, 2],[3, 4]])

dataset = tf.data.Dataset.from_tensor_slices(data)
dataset = dataset.apply(tf.data.experimental.prefetch_to_device(""/gpu:0""))

#dataset = dataset.apply(tf.data.experimental.copy_to_device(""/gpu:0""))
#
#Uncommenting the line above line will print:
#
#Tensor [1 2] is on device /job:localhost/replica:0/task:0/device:GPU:0
#Tensor [3 4] is on device /job:localhost/replica:0/task:0/device:GPU:0

for datum in dataset:
  print(f'Tensor {datum} is on device {datum.device}')
  #Prints
  #Tensor [1 2] is on device /job:localhost/replica:0/task:0/device:CPU:0
  #Tensor [3 4] is on device /job:localhost/replica:0/task:0/device:CPU:0
```

It is convinient to reproduce it as [a notebook in Google Colaboratory](https://colab.research.google.com/drive/1LpOX04L6l8SW3krQvzFdQVvXE-LK3ehH).

"
30928,BatchNormalization() does not work with autograph,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0-beta
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When using the `tf.keras.layers.BatchNormalization()` function using autograph the following error occurs: `ValueError: ('Input has undefined rank:', TensorShape(None))`.

**Describe the expected behavior**

Model will compile and train without issue. As it does without the `@tf.function` decorator.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is an example of a simple model where this error would occur. This will train without problem in eager mode, but the addition of the `@tf.function` decorator causes the error.

```
class MyModel(tf.keras.Model):

    def__init__(self):
        super(MyModel, self).__init__()

        self.conv1 = tf.keras.layers.Conv2d(8)
        self.bn1 = tf.keras.layers.BatchNormalization()
        self.conv2 = tf.keras.layers.Conv2d(8)
        self.bn2 = tf.keras.layers.BatchNormalization()

    def call(inputs):

        net = self.conv1(inputs)
        net = self.bn1(net)
        net = self.conv2(inputs)
        net = self.bn2(net)

        return tf.nn.softmax(net)

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 594, in __call__
    self._maybe_build(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1713, in _maybe_build
    self.build(input_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 252, in build
    raise ValueError('Input has undefined rank:', input_shape)
ValueError: ('Input has undefined rank:', TensorShape(None))
```

"
30925,How do you feel about Shape Variables?,"**System information**
- TensorFlow version (you are using): 2b1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Ragged/variable shapes are currently described with (None, None, 10) type syntax. This is not informative for the user or the compiler.

How do you feel about the concept of Shape Variables? Shape syntax with strings or variables like (""batch_size"", ""n_atoms"", 10) could be matched across the graph, potentially this could help with input signatures in tf.function and compiler code. 

this would be nice for reinforcement learning projects, and shape variables may be extremely useful in multi-task models and architecture search projects. Also this could improve explainability and interpretability of code

**Will this change the current api? How?**
Yes, users can specify variable ""ragged"" dimensions with strings or variables instead of **None**
**Who will benefit with this feature?**
Everyone who works on biochem, NLP, RL, multitask learning, and neural architecture search
**Any Other info.**
Just curious because we built a recursive NAS system for tf.keras in 2.0b1 this weekend and want to apply it to a bunch of different tasks, but they all have different shapes ... sometimes the shapes are variable and we need output shapes to match input shapes. ""shape hell"" sucks"
30922,Java tensorflow got negative value when use package class insdead of primitive class to create Tensor.,"**System information**
- custom code : 
```
            Tensor t = Tensor.create(new Integer[][]{{0, 1, 5, 10, 15, 20, 1, 1, 1, 1, 1, 1}});
            int[][] ids = new int[1][12];
            t.copyTo(ids);
            System.out.println(ids[0][0]);//-223646206
            System.out.println(ids[0][1]);//-223646204
            System.out.println(ids[0][2]);//-223646186
            System.out.println(ids[0][3]);//-223646176
            System.out.println(ids[0][4]);//-223646166
```
- OS Platform and Distribution : macOS 10.14.4
- TensorFlow version (use command below):
```
        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>libtensorflow</artifactId>
            <version>1.13.1</version>
        </dependency>
        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>proto</artifactId>
            <version>1.13.1</version>
        </dependency>
        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>libtensorflow_jni</artifactId>
            <version>1.13.1</version>
        </dependency>
        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>tensorflow</artifactId>
            <version>1.13.1</version>
        </dependency>
```
Thanks for help.
When I use package class to create Tensor, then I will got negative value and the output result have a pattern as you can see. While there is no problem when I use primitive class.
And thanks for help agin.


"
30919,tf.keras.layers.Embedding set trainable to False does not work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5.2


The output of the 2 print statements should be the same. Same issue for other layer like Dense, please confirm trainable parameter behavior.
```
embedding = np.random.rand(2,3).astype(np.float32)
emb_layer = tf.keras.layers.Embedding(2, 3, weights=[embedding], trainable=False)
emb = emb_layer(tf.constant([0],tf.int32))
opt = tf.train.AdamOptimizer()
loss = tf.nn.l2_loss(tf.reduce_mean(emb)-0)
train_op = opt.minimize(loss)
sess.run(tf.initialize_all_variables())
out = np.mean(sess.run(emb_layer.variables[0])[0])
sess.run(train_op)
print(out)
print(np.mean(sess.run(emb_layer.variables[0])[0]))
```
"
30918,Keras fails to initiate training with custom BERT layer.,"**System information**
- Have I manipulated custom code in attempt to build a Keras layer for BERT. Following this example: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b 
- Windows 10:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.14
- Python version: 3.7
- CUDA/cuDNN version: CUDA 10
- GPU model and memory:  NVIDIA GTX1080ti

**Describe the current behavior**
The training data is pre-processed and loaded into memory. The model is compiled and the correct model output is produced with model.summary(). See logs bellow...

On model.fit(), nothing happens... The GPU is at 3% utilization and one CPU core is at 100%.

**Describe the expected behavior**
I was expecting the keras training logging to be printed post model.fit(). It doesn't appear to be training at all.

**Code to reproduce the issue**

**Custom Code:**
```
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import backend as K
import sys

from BertLayer import BertLayer
from preprocessing import MyDocs

sess = tf.Session()

def build_model(bert_path, max_seq_length):
    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_ids"")
    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=""input_masks"")
    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=""segment_ids"")
    bert_inputs = [in_id, in_mask, in_segment]

    bert_output = BertLayer(n_fine_tune_layers=3, bert_path=bert_path, pooling=""first"")(bert_inputs)
    dense = tf.keras.layers.Dense(256, activation=""relu"")(bert_output)
    pred = tf.keras.layers.Dense(5, activation=""sigmoid"")(dense)  # change this to build classifier

    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
    model.compile(loss=""binary_crossentropy"", optimizer=""adam"",  metrics=['binary_accuracy', 'categorical_accuracy'])
    model.summary()

    return model


def initialize_vars(sess):
    sess.run(tf.compat.v1.local_variables_initializer())
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.tables_initializer())
    K.set_session(sess)

def main():
    # Params for bert model and tokenization
    bert_path = ""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1""
    max_seq_length = 256

    corpus = MyDocs(""datasets/bbc/raw"", bert_path, max_seq_length)

    ids = []
    masks = []
    segment_ids = []
    for id, mask, segment, label in corpus:
        ids.append(id)
        masks.append(masks)
        segment_ids.append(segment)
    X = [ids, masks, segment_ids]

    labels = corpus.labels
    label_encoder = OneHotEncoder()
    y = label_encoder.fit_transform(np.array(labels).reshape(-1, 1)).todense()
    print('Dimension of labels input is {}.'.format(y.shape))

    print('Building model...')
    model = build_model(bert_path, max_seq_length)

    print('Training model...')
    history = model.fit(X, y,
                        validation_split=0.2,
                        epochs=1,
                        batch_size=1,
                        verbose=2,
                        use_multiprocessing=True)

if __name__ == ""__main__"":
    main()
```

**BertLayer**
```
import tensorflow as tf
from tensorflow.keras import backend as K
import tensorflow_hub as hub

class BertLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        n_fine_tune_layers=10,
        pooling=""mean"",
        bert_path=""https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1"",
        **kwargs,
    ):
        self.n_fine_tune_layers = n_fine_tune_layers
        self.trainable = True
        self.output_size = 768
        self.pooling = pooling
        self.bert_path = bert_path

        if self.pooling not in [""first"", ""mean""]:
            raise NameError(
                f""Undefined pooling type (must be either first or mean, but is {self.pooling}""
                )

        super(BertLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path, trainable=self.trainable, name=f""{self.name}_module""
        )

        # Remove unused layers
        trainable_vars = self.bert.variables
        if self.pooling == ""first"":
            trainable_vars = [var for var in trainable_vars if not ""/cls/"" in var.name]
            trainable_layers = [""pooler/dense""]

        elif self.pooling == ""mean"":
            trainable_vars = [
                var
                for var in trainable_vars
                if not ""/cls/"" in var.name and not ""/pooler/"" in var.name
            ]
            trainable_layers = []
        else:
            raise NameError(
                f""Undefined pooling type (must be either first or mean, but is {self.pooling}""
            )

        # Select how many layers to fine tune
        for i in range(self.n_fine_tune_layers):
            trainable_layers.append(f""encoder/layer_{str(11 - i)}"")

        # Update trainable vars to contain only the specified layers
        trainable_vars = [
            var
            for var in trainable_vars
            if any([l in var.name for l in trainable_layers])
        ]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BertLayer, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype=""int32"") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        if self.pooling == ""first"":
            # pooled output of the entire sequence [batch, hidden_size]
            pooled = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[""pooled_output""]
        elif self.pooling == ""mean"":
            # representation of every token in the sequence [batch, max_seq_length, hidden_size]
            result = self.bert(inputs=bert_inputs, signature=""tokens"", as_dict=True)[""sequence_output""]

            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)
            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (
                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)
            input_mask = tf.cast(input_mask, tf.float32)
            pooled = masked_reduce_mean(result, input_mask)
        else:
            raise NameError(f""Undefined pooling type (must be either first or mean, but is {self.pooling}"")

        return pooled

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_size)
```

**Other info / logs**

model.summary()
```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 256)]        0                                            
__________________________________________________________________________________________________
input_masks (InputLayer)        [(None, 256)]        0                                            
__________________________________________________________________________________________________
segment_ids (InputLayer)        [(None, 256)]        0                                            
__________________________________________________________________________________________________
bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  
                                                                 input_masks[0][0]                
                                                                 segment_ids[0][0]                
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            1285        dense[0][0]                      
==================================================================================================
Total params: 110,303,039
Trainable params: 22,052,357
Non-trainable params: 88,250,682
__________________________________________________________________________________________________
```

Other logs:
```
WARNING: Logging before flag parsing goes to stderr.
W0722 16:38:28.879759 14352 deprecation_wrapper.py:119] From C:\Users\jorda\OneDrive\Documents\Programs\aria_projects\document_classifier\BERT\venv\preprocessing.py:7: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-22 16:38:28.917117: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-07-22 16:38:29.034609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:08:00.0
2019-07-22 16:38:29.034810: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-22 16:38:29.035881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-22 16:38:29.041192: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-07-22 16:38:29.044644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:08:00.0
2019-07-22 16:38:29.044900: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-22 16:38:29.045354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-22 16:38:30.462499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-22 16:38:30.462613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-22 16:38:30.462745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-22 16:38:30.464681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8788 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
2019-07-22 16:38:30.470977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:08:00.0
2019-07-22 16:38:30.471127: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
Building docs...
2019-07-22 16:38:30.471801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-22 16:38:30.471966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-22 16:38:30.472067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-22 16:38:30.472126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-22 16:38:30.472605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8788 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
W0722 16:38:34.138283 14352 deprecation_wrapper.py:119] From C:\Users\jorda\OneDrive\Documents\Programs\aria_projects\document_classifier\BERT\venv\bert\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

Dimension of labels input is (2225, 5).
Building model...
W0722 16:39:05.050155 14352 deprecation.py:506] From C:\Program Files\Python37\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0722 16:39:05.109156 14352 deprecation.py:323] From C:\Program Files\Python37\lib\site-packages\tensorflow\python\ops\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: ""model""
```"
30916,Problems with running keras models in colab TPU,"```
filepath = ""./xlnet_models/batch-128/saved-model-128-{epoch:02d}-{acc:.2f}--{loss:.5f}.hdf5""
save_callback = tf.keras.callbacks.ModelCheckpoint(filepath)
model.fit_generator(train_generator_128,epochs=20,callbacks=[save_callback])

```
`fit_generator` is not supported for models compiled with tf.distribute.Strategy.

Is there any way to solve this?"
30915,Dataset Iterator is not an iterator when using fit_generator,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_generator.py#L363


below is ok.
```
    if isinstance(generator, (iterator_ops.IteratorV2, iterator_ops.Iterator)):
363        generator_output = generator.get_next()
364     else:
365        generator_output = next(generator)
```

"
30914,CPU.ModifyGraphWithDelegate is disallowed when graph is immutable. Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.ed to invoke.,"<em>
-We are trying to enable GPU using GPU delegate in TensorFlow[ object detection sample project](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), by taking reference from GPU enabled sample project of [Object  Classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).-- we are also using Float Model for detection. </em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android Studio installed on Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Realme One
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
dependencies added:
`implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'`
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: OpenGL ES 3.2 available on the phone

The project has been compiled in Android Studio using these dependencies:
`dependencies { implementation fileTree(dir: 'libs', include: ['*.jar','*.aar']) implementation 'com.android.support:appcompat-v7:28.0.0' implementation 'com.android.support:design:28.0.0' implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly' } 
`
**Describe the current behavior**
We are facing this error unable to understand the exact issue:
`CPU.ModifyGraphWithDelegate is disallowed when graph is immutable.
       at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)

   java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:
   CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
   First 63 operations will run on the GPU, and the remaining 1 on the CPU.TfLiteGpuDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 64 (TfLiteGpuDelegate) failed to invoke.
   
       at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)
       at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
       at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:228)
       at com.demo.webview.bt.webviewbtdemo.basic.TFLiteWrapper$5.run(TFLiteWrapper.java:360)
       at android.os.Handler.handleCallback(Handler.java:873)`
**Describe the expected behavior**

**Code to reproduce the issue**
`public class TFLiteObjectDetectionAPIModel implements Classifier {
  private static final Logger LOGGER = new Logger();

  // Only return this many results.
  private static final int NUM_DETECTIONS = 10;
  // Float model
  private static final float IMAGE_MEAN = 128.0f;
  private static final float IMAGE_STD = 128.0f;
  // Number of threads in the java app
  private static final int NUM_THREADS = 4;
  private boolean isModelQuantized;
  // Config values.
  private int inputSize;
  // Pre-allocated buffers.
  private Vector<String> labels = new Vector<String>();
  private int[] intValues;
  // outputLocations: array of shape [Batchsize, NUM_DETECTIONS,4]
  // contains the location of detected boxes
  private float[][][] outputLocations;
  // outputClasses: array of shape [Batchsize, NUM_DETECTIONS]
  // contains the classes of detected boxes
  private float[][] outputClasses;
  // outputScores: array of shape [Batchsize, NUM_DETECTIONS]
  // contains the scores of detected boxes
  private float[][] outputScores;
  // numDetections: array of shape [Batchsize]
  // contains the number of detected boxes
  private float[] numDetections;

  private ByteBuffer imgData;

  private Interpreter tflite;

  /** Options for configuring the Interpreter. */
  private final Interpreter.Options tfliteOptions = new Interpreter.Options();

  /** The loaded TensorFlow Lite model. */
  private MappedByteBuffer tfliteModel;


  private TFLiteObjectDetectionAPIModel() {}

  /** holds a gpu delegate */
  GpuDelegate gpuDelegate = null;

  /** Memory-map the model file in Assets. */
  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)
      throws IOException {
    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();

    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
  }


  private void recreateInterpreter() {
    if (tflite != null) {
      tflite.close();
      tflite = new Interpreter(tfliteModel, tfliteOptions);
    }
  }

  public void useGpu() {
    if (gpuDelegate == null) {
      gpuDelegate = new GpuDelegate();
      tfliteOptions.addDelegate(gpuDelegate);
      recreateInterpreter();
    }
  }

  /**
   * Initializes a native TensorFlow session for classifying images.
   *
   * @param assetManager The asset manager to be used to load assets.
   * @param modelFilename The filepath of the model GraphDef protocol buffer.
   * @param labelFilename The filepath of label file for classes.
   * @param inputSize The size of image input
   * @param isQuantized Boolean representing model is quantized or not
   */
  public static Classifier create(
      final AssetManager assetManager,
      final String modelFilename,
      final String labelFilename,
      final int inputSize,
      final boolean isQuantized)
      throws IOException {
    final TFLiteObjectDetectionAPIModel d = new TFLiteObjectDetectionAPIModel();
    //d.setUseNNAPI(true);
    d.useGpu();
    InputStream labelsInput = null;
    String actualFilename = labelFilename.split(""file:///android_asset/"")[1];
    labelsInput = assetManager.open(actualFilename);
    BufferedReader br = null;
    br = new BufferedReader(new InputStreamReader(labelsInput));
    String line;
    while ((line = br.readLine()) != null) {
      LOGGER.w(line);
      d.labels.add(line);
    }
    br.close();

    d.inputSize = inputSize;

    try {
      d.tflite = new Interpreter(loadModelFile(assetManager, modelFilename));
    } catch (Exception e) {
      throw new RuntimeException(e);
    }

    d.isModelQuantized = isQuantized;
    // Pre-allocate buffers.
    int numBytesPerChannel;
    if (isQuantized) {
      numBytesPerChannel = 1; // Quantized
    } else {
      numBytesPerChannel = 4; // Floating point
    }
    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);
    d.imgData.order(ByteOrder.nativeOrder());
    d.intValues = new int[d.inputSize * d.inputSize];

    d.tflite.setNumThreads(NUM_THREADS);
    d.outputLocations = new float[1][NUM_DETECTIONS][4];
    d.outputClasses = new float[1][NUM_DETECTIONS];
    d.outputScores = new float[1][NUM_DETECTIONS];
    d.numDetections = new float[1];
    return d;
  }

  @Override
  public List<Recognition> recognizeImage(final Bitmap bitmap) {
    // Log this method so that it can be analyzed with systrace.
    Trace.beginSection(""recognizeImage"");

    Trace.beginSection(""preprocessBitmap"");
    // Preprocess the image data from 0-255 int to normalized float based
    // on the provided parameters.
    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
  //  useGpu();
    imgData.rewind();
    for (int i = 0; i < inputSize; ++i) {
      for (int j = 0; j < inputSize; ++j) {
        int pixelValue = intValues[i * inputSize + j];
        if (isModelQuantized) {
          // Quantized model
          imgData.put((byte) ((pixelValue >> 16) & 0xFF));
          imgData.put((byte) ((pixelValue >> 8) & 0xFF));
          imgData.put((byte) (pixelValue & 0xFF));
        } else { // Float model
          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
        }
      }
    }
    Trace.endSection(); // preprocessBitmap

    // Copy the input data into TensorFlow.
    Trace.beginSection(""feed"");
    outputLocations = new float[1][NUM_DETECTIONS][4];
    outputClasses = new float[1][NUM_DETECTIONS];
    outputScores = new float[1][NUM_DETECTIONS];
    numDetections = new float[1];

    Object[] inputArray = {imgData};
    Map<Integer, Object> outputMap = new HashMap<>();
    outputMap.put(0, outputLocations);
    outputMap.put(1, outputClasses);
    outputMap.put(2, outputScores);
    outputMap.put(3, numDetections);
    Trace.endSection();

    // Run the inference call.
    Trace.beginSection(""run"");
    tflite.runForMultipleInputsOutputs(inputArray, outputMap);
    Trace.endSection();

    // Show the best detections.
    // after scaling them back to the input size.
    final ArrayList<Recognition> recognitions = new ArrayList<>(NUM_DETECTIONS);
    for (int i = 0; i < NUM_DETECTIONS; ++i) {
      final RectF detection =
          new RectF(
              outputLocations[0][i][1] * inputSize,
              outputLocations[0][i][0] * inputSize,
              outputLocations[0][i][3] * inputSize,
              outputLocations[0][i][2] * inputSize);
      // SSD Mobilenet V1 Model assumes class 0 is background class
      // in label file and class labels start from 1 to number_of_classes+1,
      // while outputClasses correspond to class index from 0 to number_of_classes
      int labelOffset = 1;
      recognitions.add(
          new Recognition(
              """" + i,
              labels.get((int) outputClasses[0][i] + labelOffset),
              outputScores[0][i],
              detection));
    }
    Trace.endSection(); // ""recognizeImage""
   // runInference();
    return recognitions;
  }`
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Please provide help or suggestion to resolve this issue**

"
30913,Maybe there's a bug in SparseApplyFtrlOp for the sparse solution,"in the class SparseApplyFtrlOp from tensorflow/core/kernel/training_ops.cc

```
#define COMPUTE_FTRL(grad, grad_maybe_with_shrinkage)                          \
  auto new_accum = accum + grad.square();                                      \
  if (lr_power_scalar == static_cast<T>(-0.5)) {                               \
    linear += grad_maybe_with_shrinkage -                                      \
              (new_accum.sqrt() - accum.sqrt()) / lr_scalar * var;             \
  } else {                                                                     \
    linear += grad_maybe_with_shrinkage - (new_accum.pow(-lr_power_scalar) -   \
                                           accum.pow(-lr_power_scalar)) /      \
                                              lr_scalar * var;                 \
  }                                                                            \
  auto l1_reg_adjust = linear.cwiseMin(l1_scalar).cwiseMax(-l1_scalar);        \
  auto x = l1_reg_adjust - linear;                                             \
  if (lr_power_scalar == static_cast<T>(-0.5)) {                               \
    auto y = new_accum.sqrt() / new_accum.constant(lr_scalar) +                \
             linear.constant(static_cast<T>(2) * l2_scalar);                   \
    var = x / y;                                                               \
  } else {                                                                     \
    auto y = new_accum.pow(-lr_power_scalar) / new_accum.constant(lr_scalar) + \
             linear.constant(static_cast<T>(2) * l2_scalar);                   \
    var = x / y;                                                               \
  }                                                                            \
  accum += grad.square();
```

I only find the update of `var = x / y;`  directly, not considering the situation var = 0 when abs(linear) <= l1_scalar according to the paper
![ftrl_paper](https://user-images.githubusercontent.com/12267324/61602302-8a1ac880-ac6b-11e9-80dc-d982f6ebdc50.png)
"
30912,TF-GPU v1.14 CUDA runs out of memory ,"Hello TF Team,

I am hoping you can please pull down my custom TF-GPU, CUDA X, and Anaconda container solution with Jupyter. I am hoping you can assist because whatever the problem is, the fix for the container is same as Ubuntu. Link to repo, [here](https://github.com/joehoeller/Anaconda-CUDA-Accelerated-TensorFlowGPU-Development-Environment).
Here is the error when running Tensorboard and benchmarks.py (see instructions in README.md as to how to run it after spinning up container).

This is the error that I get, scroll right to read entire line:

```
root@e71bda560638:/apps/apps/gpu_benchmarks# python tensorboard.py 
WARNING: Logging before flag parsing goes to stderr.
W0721 23:23:02.790582 139901430245184 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0721 23:23:02.810480 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:191: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

W0721 23:23:02.810840 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:161: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0721 23:23:02.810928 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:163: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0721 23:23:02.811073 139901430245184 deprecation.py:323] From tensorboard.py:20: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
W0721 23:23:02.811128 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
W0721 23:23:02.811218 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please use urllib or similar directly.
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
W0721 23:23:04.074006 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
W0721 23:23:04.547974 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
W0721 23:23:04.550884 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
W0721 23:23:05.421007 139901430245184 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
W0721 23:23:05.558143 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:22: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

2019-07-21 23:23:05.558423: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-07-21 23:23:05.562806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2019-07-21 23:23:05.563094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e55a519380 executing computations on platform Host. Devices:
2019-07-21 23:23:05.563108: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-21 23:23:05.563894: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-21 23:23:05.580061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.580637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:01:00.0
2019-07-21 23:23:05.580748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:23:05.581760: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:23:05.582704: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-07-21 23:23:05.582852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-07-21 23:23:05.583850: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-07-21 23:23:05.584484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-07-21 23:23:05.586678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-21 23:23:05.586776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.587222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.587602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-21 23:23:05.587629: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:23:05.646174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-21 23:23:05.646191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-21 23:23:05.646197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-21 23:23:05.646385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.646756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.647144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:05.647446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 170 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-07-21 23:23:05.649957: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e55ba1ac60 executing computations on platform CUDA. Devices:
2019-07-21 23:23:05.649969: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
W0721 23:23:05.650787 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:27: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0721 23:23:05.652657 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:32: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

W0721 23:23:05.653614 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:37: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

W0721 23:23:05.658960 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0721 23:23:05.666438 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:55: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

W0721 23:23:05.681653 139901430245184 deprecation.py:506] From tensorboard.py:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0721 23:23:05.714577 139901430245184 deprecation.py:323] From tensorboard.py:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W0721 23:23:05.727325 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:106: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0721 23:23:05.796838 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:118: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0721 23:23:05.797843 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:119: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W0721 23:23:05.817948 139901430245184 deprecation_wrapper.py:119] From tensorboard.py:121: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-07-21 23:23:06.136829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:23:06.224853: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-07-21 23:23:06.226553: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-07-21 23:23:06.228049: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.228406: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.228869: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.229269: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.229750: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.230151: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.230537: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.230914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.231278: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.231646: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.232019: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.232382: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.232740: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.233104: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.233457: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.233823: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.234184: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.234533: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.234906: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.235263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.235914: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.236498: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.236888: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:06.237265: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:16.229311: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:16.230144: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:16.230171: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 29.91MiB (rounded to 31360000).  Current allocation summary follows.
2019-07-21 23:23:16.230190: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): 	Total Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 240B client-requested in use in bin.
2019-07-21 23:23:16.230203: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230215: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-07-21 23:23:16.230226: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): 	Total Chunks: 5, Chunks in use: 5. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 9.8KiB client-requested in use in bin.
2019-07-21 23:23:16.230237: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230250: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230275: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): 	Total Chunks: 5, Chunks in use: 4. 97.8KiB allocated for chunks. 79.0KiB in use in bin. 78.1KiB client-requested in use in bin.
2019-07-21 23:23:16.230286: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230299: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): 	Total Chunks: 1, Chunks in use: 1. 78.2KiB allocated for chunks. 78.2KiB in use in bin. 78.1KiB client-requested in use in bin.
2019-07-21 23:23:16.230311: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230325: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): 	Total Chunks: 2, Chunks in use: 1. 833.0KiB allocated for chunks. 390.8KiB in use in bin. 390.6KiB client-requested in use in bin.
2019-07-21 23:23:16.230336: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230348: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): 	Total Chunks: 3, Chunks in use: 3. 4.49MiB allocated for chunks. 4.49MiB in use in bin. 4.49MiB client-requested in use in bin.
2019-07-21 23:23:16.230361: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): 	Total Chunks: 2, Chunks in use: 1. 4.50MiB allocated for chunks. 2.00MiB in use in bin. 1.50MiB client-requested in use in bin.
2019-07-21 23:23:16.230373: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): 	Total Chunks: 1, Chunks in use: 0. 5.01MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230386: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230398: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230409: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230420: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230434: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230444: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:16.230454: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 29.91MiB was 16.00MiB, Chunk State: 
2019-07-21 23:23:16.230463: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 4194304
2019-07-21 23:23:16.230473: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b73000000 next 15 of size 1568000
2019-07-21 23:23:16.230484: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3b7317ed00 next 18446744073709551615 of size 2626304
2019-07-21 23:23:16.230493: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 8388608
2019-07-21 23:23:16.230502: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b73400000 next 19 of size 1568000
2019-07-21 23:23:16.230511: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3b7357ed00 next 27 of size 1568000
2019-07-21 23:23:16.230521: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3b736fda00 next 18446744073709551615 of size 5252608
2019-07-21 23:23:16.230529: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 1048576
2019-07-21 23:23:16.230540: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000000 next 1 of size 256
2019-07-21 23:23:16.230555: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000100 next 2 of size 256
2019-07-21 23:23:16.230570: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000200 next 3 of size 2048
2019-07-21 23:23:16.230583: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000a00 next 4 of size 256
2019-07-21 23:23:16.230597: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000b00 next 5 of size 256
2019-07-21 23:23:16.230611: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000c00 next 6 of size 256
2019-07-21 23:23:16.230625: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf000d00 next 7 of size 2048
2019-07-21 23:23:16.230654: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf001500 next 8 of size 256
2019-07-21 23:23:16.230669: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf001600 next 10 of size 20224
2019-07-21 23:23:16.230684: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006500 next 11 of size 1280
2019-07-21 23:23:16.230697: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006a00 next 12 of size 256
2019-07-21 23:23:16.230709: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006b00 next 13 of size 256
2019-07-21 23:23:16.230723: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf006c00 next 16 of size 2048
2019-07-21 23:23:16.230735: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007400 next 29 of size 256
2019-07-21 23:23:16.230747: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007500 next 30 of size 256
2019-07-21 23:23:16.230760: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007600 next 32 of size 256
2019-07-21 23:23:16.230773: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf007700 next 33 of size 256
2019-07-21 23:23:16.230785: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3bdf007800 next 17 of size 19200
2019-07-21 23:23:16.230796: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf00c300 next 20 of size 256
2019-07-21 23:23:16.230809: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf00c400 next 21 of size 20224
2019-07-21 23:23:16.230823: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf011300 next 22 of size 20224
2019-07-21 23:23:16.230836: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf016200 next 23 of size 2048
2019-07-21 23:23:16.230848: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf016a00 next 24 of size 2048
2019-07-21 23:23:16.230861: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017200 next 25 of size 256
2019-07-21 23:23:16.230874: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017300 next 26 of size 256
2019-07-21 23:23:16.230886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf017400 next 28 of size 20224
2019-07-21 23:23:16.230899: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf01c300 next 31 of size 400128
2019-07-21 23:23:16.230911: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf07de00 next 35 of size 80128
2019-07-21 23:23:16.230924: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f3bdf091700 next 18446744073709551615 of size 452864
2019-07-21 23:23:16.230938: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 2097152
2019-07-21 23:23:16.230953: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f3bdf400000 next 18446744073709551615 of size 2097152
2019-07-21 23:23:16.230967: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: 
2019-07-21 23:23:16.230983: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 15 Chunks of size 256 totalling 3.8KiB
2019-07-21 23:23:16.230995: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB
2019-07-21 23:23:16.231007: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 5 Chunks of size 2048 totalling 10.0KiB
2019-07-21 23:23:16.231023: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 20224 totalling 79.0KiB
2019-07-21 23:23:16.231039: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 80128 totalling 78.2KiB
2019-07-21 23:23:16.231053: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 400128 totalling 390.8KiB
2019-07-21 23:23:16.231068: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 3 Chunks of size 1568000 totalling 4.49MiB
2019-07-21 23:23:16.231085: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2097152 totalling 2.00MiB
2019-07-21 23:23:16.231102: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 7.04MiB
2019-07-21 23:23:16.231111: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 15728640 memory_limit_: 178782208 available bytes: 163053568 curr_region_allocation_bytes_: 33554432
2019-07-21 23:23:16.231125: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: 
Limit:                   178782208
InUse:                     7377664
MaxInUse:                 10028288
NumAllocs:                      61
MaxAllocSize:              2626304

2019-07-21 23:23:16.231150: W tensorflow/core/common_runtime/bfc_allocator.cc:319] **********________________*********************_________________________________****__***********xxx
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
  (1) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
	 [[layer1/Wx_plus_b/add/_25]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorboard.py"", line 191, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""tensorboard.py"", line 164, in main
    train()
  File ""tensorboard.py"", line 139, in train
    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
  (1) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
	 [[layer1/Wx_plus_b/add/_25]]
0 successful operations.
0 derived errors ignored.
root@e71bda560638:/apps/apps/gpu_benchmarks# python tensorboard.py 
WARNING: Logging before flag parsing goes to stderr.
W0721 23:23:48.399826 139765773813568 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0721 23:23:48.418498 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:191: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

W0721 23:23:48.418813 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:161: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0721 23:23:48.418894 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:162: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.

W0721 23:23:48.419052 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:163: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0721 23:23:48.419144 139765773813568 deprecation.py:323] From tensorboard.py:20: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
W0721 23:23:48.419198 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
W0721 23:23:48.419269 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
W0721 23:23:48.555204 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
W0721 23:23:48.555730 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
W0721 23:23:48.580618 139765773813568 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
W0721 23:23:48.672224 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:22: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

2019-07-21 23:23:48.672475: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-07-21 23:23:48.676436: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2019-07-21 23:23:48.676919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e235e7cac0 executing computations on platform Host. Devices:
2019-07-21 23:23:48.676941: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-21 23:23:48.677678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-21 23:23:48.699374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.699807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:01:00.0
2019-07-21 23:23:48.699921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:23:48.701017: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:23:48.702035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-07-21 23:23:48.702201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-07-21 23:23:48.703269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-07-21 23:23:48.704016: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-07-21 23:23:48.706595: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-21 23:23:48.706691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.707180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.707594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-21 23:23:48.707623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:23:48.767926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-21 23:23:48.767946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-21 23:23:48.767953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-21 23:23:48.768148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.768525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.768871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:23:48.769198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 161 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-07-21 23:23:48.770262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e237153f80 executing computations on platform CUDA. Devices:
2019-07-21 23:23:48.770273: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
W0721 23:23:48.770924 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:27: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0721 23:23:48.772640 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:32: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

W0721 23:23:48.773568 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:37: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

W0721 23:23:48.778774 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W0721 23:23:48.786193 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:55: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

W0721 23:23:48.801232 139765773813568 deprecation.py:506] From tensorboard.py:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0721 23:23:48.833851 139765773813568 deprecation.py:323] From tensorboard.py:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

W0721 23:23:48.846037 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:106: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0721 23:23:48.914657 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:118: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

W0721 23:23:48.915626 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:119: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W0721 23:23:48.934774 139765773813568 deprecation_wrapper.py:119] From tensorboard.py:121: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-07-21 23:23:49.254414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:23:49.324353: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-07-21 23:23:49.330331: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-07-21 23:23:49.331837: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.332226: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.332706: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.333106: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.333517: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.333936: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.334331: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.334722: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.335239: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.335604: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.336030: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.336411: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.336789: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.337165: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.337532: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.337899: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.338252: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:49.338602: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:59.333998: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:59.336335: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 32.00M (33554432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:23:59.336404: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 29.91MiB (rounded to 31360000).  Current allocation summary follows.
2019-07-21 23:23:59.336453: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): 	Total Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 240B client-requested in use in bin.
2019-07-21 23:23:59.336491: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): 	Total Chunks: 2, Chunks in use: 0. 1.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336534: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-07-21 23:23:59.336588: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): 	Total Chunks: 5, Chunks in use: 5. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 9.8KiB client-requested in use in bin.
2019-07-21 23:23:59.336636: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336683: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336711: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): 	Total Chunks: 5, Chunks in use: 4. 96.5KiB allocated for chunks. 79.0KiB in use in bin. 78.1KiB client-requested in use in bin.
2019-07-21 23:23:59.336719: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336725: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): 	Total Chunks: 1, Chunks in use: 1. 78.2KiB allocated for chunks. 78.2KiB in use in bin. 78.1KiB client-requested in use in bin.
2019-07-21 23:23:59.336732: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336740: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): 	Total Chunks: 2, Chunks in use: 1. 833.0KiB allocated for chunks. 390.8KiB in use in bin. 390.6KiB client-requested in use in bin.
2019-07-21 23:23:59.336748: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336758: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): 	Total Chunks: 3, Chunks in use: 2. 4.49MiB allocated for chunks. 2.99MiB in use in bin. 2.99MiB client-requested in use in bin.
2019-07-21 23:23:59.336765: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): 	Total Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 2.99MiB client-requested in use in bin.
2019-07-21 23:23:59.336774: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): 	Total Chunks: 1, Chunks in use: 0. 5.01MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336782: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336789: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336796: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336803: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336811: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336818: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:23:59.336827: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 29.91MiB was 16.00MiB, Chunk State: 
2019-07-21 23:23:59.336832: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 4194304
2019-07-21 23:23:59.336840: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf000000 next 16 of size 1568000
2019-07-21 23:23:59.336846: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf17ed00 next 18446744073709551615 of size 2626304
2019-07-21 23:23:59.336850: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 8388608
2019-07-21 23:23:59.336856: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1bdf400000 next 20 of size 1568000
2019-07-21 23:23:59.336862: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1bdf57ed00 next 28 of size 1568000
2019-07-21 23:23:59.336869: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1bdf6fda00 next 18446744073709551615 of size 5252608
2019-07-21 23:23:59.336875: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 1048576
2019-07-21 23:23:59.336880: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000000 next 1 of size 256
2019-07-21 23:23:59.336886: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000100 next 2 of size 256
2019-07-21 23:23:59.336892: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000200 next 3 of size 2048
2019-07-21 23:23:59.336898: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000a00 next 4 of size 256
2019-07-21 23:23:59.336903: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000b00 next 5 of size 256
2019-07-21 23:23:59.336909: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000c00 next 6 of size 256
2019-07-21 23:23:59.336915: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f000d00 next 7 of size 2048
2019-07-21 23:23:59.336920: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f001500 next 8 of size 256
2019-07-21 23:23:59.336926: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f001600 next 10 of size 20224
2019-07-21 23:23:59.336933: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006500 next 11 of size 1280
2019-07-21 23:23:59.336939: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006a00 next 29 of size 256
2019-07-21 23:23:59.336945: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006b00 next 30 of size 256
2019-07-21 23:23:59.336951: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f006c00 next 34 of size 768
2019-07-21 23:23:59.336957: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f006f00 next 35 of size 256
2019-07-21 23:23:59.336963: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f007000 next 38 of size 512
2019-07-21 23:23:59.336968: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f007200 next 39 of size 256
2019-07-21 23:23:59.336974: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f007300 next 12 of size 17920
2019-07-21 23:23:59.336980: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00b900 next 13 of size 256
2019-07-21 23:23:59.336986: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00ba00 next 14 of size 256
2019-07-21 23:23:59.336992: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f00bb00 next 17 of size 20224
2019-07-21 23:23:59.336998: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f010a00 next 18 of size 20224
2019-07-21 23:23:59.337004: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f015900 next 21 of size 256
2019-07-21 23:23:59.337010: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f015a00 next 22 of size 2048
2019-07-21 23:23:59.337016: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016200 next 23 of size 2048
2019-07-21 23:23:59.337022: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016a00 next 24 of size 256
2019-07-21 23:23:59.337028: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f016b00 next 25 of size 2048
2019-07-21 23:23:59.337034: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f017300 next 26 of size 256
2019-07-21 23:23:59.337040: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f017400 next 27 of size 20224
2019-07-21 23:23:59.337046: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f01c300 next 32 of size 400128
2019-07-21 23:23:59.337052: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f07de00 next 36 of size 80128
2019-07-21 23:23:59.337058: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7f1c4f091700 next 18446744073709551615 of size 452864
2019-07-21 23:23:59.337064: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 2097152
2019-07-21 23:23:59.337070: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7f1c4f400000 next 18446744073709551615 of size 2097152
2019-07-21 23:23:59.337076: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: 
2019-07-21 23:23:59.337084: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 15 Chunks of size 256 totalling 3.8KiB
2019-07-21 23:23:59.337091: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB
2019-07-21 23:23:59.337097: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 5 Chunks of size 2048 totalling 10.0KiB
2019-07-21 23:23:59.337104: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 20224 totalling 79.0KiB
2019-07-21 23:23:59.337111: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 80128 totalling 78.2KiB
2019-07-21 23:23:59.337118: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 400128 totalling 390.8KiB
2019-07-21 23:23:59.337124: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1568000 totalling 2.99MiB
2019-07-21 23:23:59.337130: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2097152 totalling 2.00MiB
2019-07-21 23:23:59.337137: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 2626304 totalling 2.50MiB
2019-07-21 23:23:59.337143: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 8.04MiB
2019-07-21 23:23:59.337149: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 15728640 memory_limit_: 168951808 available bytes: 153223168 curr_region_allocation_bytes_: 33554432
2019-07-21 23:23:59.337158: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: 
Limit:                   168951808
InUse:                     8435968
MaxInUse:                 10029568
NumAllocs:                      61
MaxAllocSize:              2626304

2019-07-21 23:23:59.337168: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ********************xxxxxxx_________***********_________________________________****__***********xxx
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
  (1) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
	 [[layer1/Wx_plus_b/add/_25]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorboard.py"", line 191, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""tensorboard.py"", line 164, in main
    train()
  File ""tensorboard.py"", line 139, in train
    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
  (1) Internal: Dst tensor is not initialized.
	 [[{{node _arg_input/x-input_0_1}}]]
	 [[layer1/Wx_plus_b/add/_25]]
0 successful operations.
0 derived errors ignored.
root@e71bda560638:/apps/apps/gpu_benchmarks# ls
__pycache__  benchmark.py  tensorboard.py
root@e71bda560638:/apps/apps/gpu_benchmarks# python benchmark.py gpu 10000
WARNING: Logging before flag parsing goes to stderr.
W0721 23:26:34.769160 140354788767552 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
W0721 23:26:34.788027 140354788767552 deprecation_wrapper.py:119] From benchmark.py:18: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0721 23:26:34.792794 140354788767552 deprecation_wrapper.py:119] From benchmark.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W0721 23:26:34.792872 140354788767552 deprecation_wrapper.py:119] From benchmark.py:24: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2019-07-21 23:26:34.792978: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-07-21 23:26:34.818743: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2019-07-21 23:26:34.819354: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560751921f80 executing computations on platform Host. Devices:
2019-07-21 23:26:34.819370: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-21 23:26:34.819978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-21 23:26:34.834675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.835206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:01:00.0
2019-07-21 23:26:34.835319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:26:34.836303: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:26:34.837230: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10
2019-07-21 23:26:34.837382: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10
2019-07-21 23:26:34.838334: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10
2019-07-21 23:26:34.838907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10
2019-07-21 23:26:34.840924: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-21 23:26:34.841007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.841449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.841823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-21 23:26:34.841845: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
2019-07-21 23:26:34.894491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-21 23:26:34.894509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-21 23:26:34.894513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-21 23:26:34.894703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.895050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.895367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-21 23:26:34.895665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 159 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-07-21 23:26:34.896687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560752b2a3a0 executing computations on platform CUDA. Devices:
2019-07-21 23:26:34.896697: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
2019-07-21 23:26:34.897194: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device

random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897731: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897741: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897746: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897751: I tensorflow/core/common_runtime/placer.cc:54] random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0
transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897757: I tensorflow/core/common_runtime/placer.cc:54] transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897761: I tensorflow/core/common_runtime/placer.cc:54] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0
Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897771: I tensorflow/core/common_runtime/placer.cc:54] Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897781: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897790: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0
random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897802: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0
transpose/perm: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897810: I tensorflow/core/common_runtime/placer.cc:54] transpose/perm: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.897817: I tensorflow/core/common_runtime/placer.cc:54] Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0
2019-07-21 23:26:34.901219: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
2019-07-21 23:26:35.082216: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 159.69M (167444480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.082576: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 143.72M (150700032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.082925: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 129.35M (135630080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.083263: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 116.41M (122067200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.083600: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 104.77M (109860608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.083937: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 94.29M (98874624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.084275: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 84.86M (88987392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:35.084614: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 76.38M (80088832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-07-21 23:26:45.085467: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 381.47MiB (rounded to 400000000).  Current allocation summary follows.
2019-07-21 23:26:45.085552: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085613: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085661: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2019-07-21 23:26:45.085701: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085739: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085779: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085816: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085857: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085897: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085936: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.085975: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086012: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086043: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086078: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086122: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086154: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086187: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086217: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086247: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): 	Total Chunks: 1, Chunks in use: 0. 68.74MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086281: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086317: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-07-21 23:26:45.086361: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 381.47MiB was 256.00MiB, Chunk State: 
2019-07-21 23:26:45.086394: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 72080128
2019-07-21 23:26:45.086432: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0x7fa540000000 next 1 of size 1280
2019-07-21 23:26:45.086468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 0x7fa540000500 next 18446744073709551615 of size 72078848
2019-07-21 23:26:45.086493: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: 
2019-07-21 23:26:45.086521: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.2KiB
2019-07-21 23:26:45.086546: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 1.2KiB
2019-07-21 23:26:45.086575: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 72080128 memory_limit_: 167444480 available bytes: 95364352 curr_region_allocation_bytes_: 334888960
2019-07-21 23:26:45.086610: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: 
Limit:                   167444480
InUse:                        1280
MaxInUse:                     1280
NumAllocs:                       1
MaxAllocSize:                 1280

2019-07-21 23:26:45.086709: W tensorflow/core/common_runtime/bfc_allocator.cc:319] *___________________________________________________________________________________________________
2019-07-21 23:26:45.086813: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at random_op.cc:76 : Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node random_uniform/RandomUniform}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node random_uniform/RandomUniform}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Sum/_1]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""benchmark.py"", line 25, in <module>
    result = session.run(sum_operation)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node random_uniform/RandomUniform (defined at benchmark.py:18) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[10000,10000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node random_uniform/RandomUniform (defined at benchmark.py:18) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Sum/_1]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Original stack trace for 'random_uniform/RandomUniform':
  File ""benchmark.py"", line 18, in <module>
    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py"", line 247, in random_uniform
    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 820, in random_uniform
    name=name)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()


```
"
30910,[TF 2.0 keras] Conv2D looses shapes with dilation rate other than 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When I use Conv2D layer with dilation rate other than 1 output looses shapes
**Describe the expected behavior**
I expect to see calculated shape with None only in first dimension
**Code to reproduce the issue**
```python
from tensorflow.keras import Input
from tensorflow.keras.layers import Conv2D

tensor = Input(shape=(512,512,3))

y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, name='dilation_1')(tensor)
y_6 = Conv2D(filters=256, kernel_size=1, dilation_rate=6, name='dilation_6')(tensor)

print(y_1)
print(y_6)
```

**Other info / logs**
Output is:
> Tensor(""dilation_1/Identity:0"", shape=(None, 512, 512, 256), dtype=float32)
Tensor(""dilation_6/Identity:0"", shape=(None, None, None, 256), dtype=float32)
"
30909,TF 2.0 nightly: tf.keras.estimator.model_to_estimator -> got an unexpected keyword argument 'use_v2_estimator',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YEs
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): No 
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190721
- Python version: Python 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

```
>>> import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
v1.12.1-6727-g97b7aa03b7 2.0.0-dev20190721
```

**Describe the current behavior**
The following code was working with some earlier release but now it is crahsing
```
estimator = tf.keras.estimator.model_to_estimator(
    keras_model = model,
    config=training_config
)
```

**Describe the expected behavior**
Should work out of the box as before

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import tensorflow_datasets as tfds
from absl import logging

logging.set_verbosity(logging.INFO)
# Define the estimator's input_fn
STEPS_PER_EPOCH = 5
BATCH_SIZE = 64
NUM_EPOCHS = 5


def input_fn():
    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    BUFFER_SIZE = 10000
    BATCH_SIZE = 64

    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
    
        return image, label[..., tf.newaxis]

    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
    return train_data.repeat()

# Define train & eval specs
train_spec = tf.estimator.TrainSpec(input_fn=input_fn,
                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)
eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,
                                  steps=STEPS_PER_EPOCH)

def make_model():
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu',
                               kernel_regularizer=tf.keras.regularizers.l2(0.02),
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

model = make_model()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#####
#strategy=None 
strategy = tf.distribute.MirroredStrategy()

# config tf.estimator to use a give strategy
training_config = tf.estimator.RunConfig(train_distribute=strategy)
#####

estimator = tf.keras.estimator.model_to_estimator(
    keras_model = model,
    config=training_config
)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
WARNING: Logging before flag parsing goes to stderr.
W0721 17:02:51.036228 4662060480 cross_device_ops.py:1207] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
I0721 17:02:51.037434 4662060480 run_config.py:558] Initializing RunConfig with distribution strategies.
I0721 17:02:51.038416 4662060480 estimator_training.py:167] Not using Distribute Coordinator.

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-490d351436b4> in <module>
     64 estimator = tf.keras.estimator.model_to_estimator(
     65     keras_model = model,
---> 66     config=training_config
     67 )
     68 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator_v2(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)
    164       config=config,
    165       checkpoint_format=checkpoint_format,
--> 166       use_v2_estimator=True)
    167 # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)

TypeError: model_to_estimator() got an unexpected keyword argument 'use_v2_estimator'


```
"
30908,model.fit output is badly formatted in TF2 on Windows (distributed training example),"**System information**
- Custom code: No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Tensorflow-GPU 2 beta 1
- Python version: 3.7.3 Anaconda
- CUDA/cuDNN version: 10.0 / 7.6.1
- GPU model and memory: Occurs with RTX 2080 Ti and RTX 2080 Max Q on 2 separate machines

**Describe the current behavior**
I downloaded the notebook from https://www.tensorflow.org/beta/tutorials/distribute/keras to test running on my local machines and also some EC2 instances.  The EC2 instances have Ubuntu 18.04 and either 1 or 4 Tesla V100 GPUs.  I installed both local and EC2 instances from scratch, used Anaconda on all of them, and installed the tensorflow 2 beta 1 binary as per instructions on the website.  The notebook runs faultlessly on the EC2 instances.  However, this is the output of model.fit(train_dataset, epochs=12, callbacks=callbacks) on my Windows machines.  As you can see it does successfully traini but the output is not formatted properly and chucks out a whole lot of text:

Train on None steps
Epoch 1/12
    346/Unknown - 6s 34ms/step - loss: 0.5130 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5108 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5089 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5072 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5064 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5057 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5048 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5034 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5018 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5001 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5001 - accuracy: 0.85 - 6s 32ms/step - loss: 0.4988 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4969 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4956 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4944 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4931 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4926 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4913 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4897 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4882 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4867 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4858 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4846 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4844 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4836 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4830 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4820 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4808 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4804 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4800 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4786 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4775 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4771 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4764 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4762 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4753 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4736 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4722 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4706 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4692 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4683 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4671 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4670 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4657 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4646 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4635 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4622 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4611 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4598 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4584 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4570 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4566 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4555 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4549 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4540 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4532 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4523 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4508 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4495 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4481 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4483 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4471 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4457 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4447 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4439 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4428 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4420 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4414 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4404 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4401 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4391 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4378 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4371 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4361 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4347 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4349 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4336 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4330 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4321 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4316 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4308 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4294 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4291 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4280 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4270 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4262 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4251 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4238 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4231 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4220 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4209 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4199 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4187 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4175 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4164 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4156 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4151 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4140 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4139 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4142 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4133 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4125 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4118 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4116 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4110 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4100 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4103 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4101 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4097 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4096 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4089 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4080 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4073 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4063 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4060 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4055 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4049 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4041 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4034 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4031 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4019 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4015 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4006 - accuracy: 0.88 - 7s 22ms/step - loss: 0.4001 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3996 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3993 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3989 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3979 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3972 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3964 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3960 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3956 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3952 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3949 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3941 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3931 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3924 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3920 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3916 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3908 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3898 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3892 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3891 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3885 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3882 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3878 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3871 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3870 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3868 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3862 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3853 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3851 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3849 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3850 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3844 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3837 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3832 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3825 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3821 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3815 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3812 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3806 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3800 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3798 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3792 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3785 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3779 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3773 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3774 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3767 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3764 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3761 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3755 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3747 - accuracy: 0.8925
    520/Unknown - 7s 20ms/step - loss: 0.3742 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3738 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3739 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3737 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3733 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3731 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3727 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3722 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3713 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3708 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3707 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3700 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3696 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3692 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3687 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3679 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3674 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3668 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3665 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3660 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3653 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3655 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3653 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3652 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3647 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3644 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3637 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3634 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3632 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3625 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3617 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3613 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3608 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3603 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3597 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3594 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3590 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3591 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3584 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3579 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3573 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3567 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3562 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3555 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3549 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3543 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3540 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3536 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3530 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3524 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3520 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3516 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3512 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3505 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3498 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3494 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3491 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3485 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3480 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3473 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3473 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3467 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3461 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3460 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3455 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3448 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3444 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3440 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3433 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3432 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3432 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3430 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3425 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3425 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3419 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3419 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3414 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3409 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3406 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3400 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3395 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3394 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3391 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3386 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3383 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3380 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3375 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3371 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3367 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3363 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3358 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3354 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3352 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3347 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3343 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3338 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3333 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3332 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3328 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3322 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3320 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3318 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3313 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3312 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3306 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3301 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3299 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3295 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3293 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3291 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3285 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3283 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3282 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3278 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3271 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3268 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3264 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3262 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3260 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3254 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3251 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3246 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3241 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3238 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3235 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3233 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3230 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3224 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3220 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3215 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3211 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3205 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3202 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3200 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3196 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3192 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3193 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3191 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3185 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3182 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3183 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3182 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3180 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3177 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3173 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3171 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3167 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3163 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3161 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3157 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3154 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3149 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3147 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3143 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3141 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3136 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3133 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3132 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3129 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3125 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3122 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3120 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3118 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3116 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3114 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3110 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3106 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3103 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3099 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3095 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3095 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3093 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3089 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3086 - accuracy: 0.9111
    694/Unknown - 8s 15ms/step - loss: 0.3081 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3082 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3079 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3074 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3072 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3069 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3065 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3063 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3060 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3058 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3054 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3053 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3051 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3049 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3049 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3045 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3040 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3038 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3037 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3032 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3030 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3026 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3025 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3021 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3017 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3015 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3013 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3009 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3005 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3002 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2998 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2997 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2993 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2991 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2988 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2985 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2981 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2978 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2975 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2972 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2969 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2968 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2967 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2965 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2961 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2958 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2956 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2953 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2950 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2947 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2943 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2940 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2936 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2933 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2931 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2927 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2923 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2920 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2916 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2913 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2909 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2905 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2902 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2898 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2895 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2892 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2888 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2887 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2884 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2881 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2879 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2875 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2872 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2868 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2866 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2865 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2862 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2860 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2859 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2858 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2859 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2856 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2853 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2852 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2850 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2846 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2843 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2840 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2838 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2839 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2836 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2832 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2829 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2826 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2823 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2819 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2817 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2814 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2811 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2808 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2805 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2803 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2800 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2797 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2795 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2793 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2791 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2788 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2785 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2781 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2779 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2777 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2775 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2771 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2769 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2767 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2764 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2761 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2758 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2755 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2752 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2751 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2748 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2746 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2748 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2745 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2743 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2740 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2737 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2736 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2732 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2731 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2728 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2727 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2723 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2721 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2717 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2714 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2712 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2708 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2706 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2704 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2702 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2698 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2695 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2694 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2691 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2690 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2688 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2685 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2683 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2682 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2679 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2676 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2673 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2670 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2668 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2665 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2662 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2659 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2657 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2656 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2653 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2652 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2649 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2647 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2644 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2642 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2642 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2640 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2638 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2636 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2633 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2632 - accuracy: 0.9241
    864/Unknown - 9s 13ms/step - loss: 0.2629 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2627 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2625 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2622 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2619 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2615 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2612 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2610 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2607 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2605 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2603 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2601 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2599 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2597 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2594 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2591 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2589 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2587 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2585 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2584 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2583 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2581 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2579 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2580 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2577 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2576 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2573 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2571 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2569 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2567 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2565 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2564 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2562 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2560 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2558 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2556 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2554 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2552 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2551 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2549 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2548 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2546 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2545 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2543 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2541 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2539 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2537 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2537 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2534 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2533 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2530 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2529 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2526 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2525 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2522 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2520 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2517 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2515 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2513 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2512 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2510 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2508 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2506 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2505 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2503 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2500 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2498 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2495 - accuracy: 0.92 - 10s 12ms/step - loss: 0.2492 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2491 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2488 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2486 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2485 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2483 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2481 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2480 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2478 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2477 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2476 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2474 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2471 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2470 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2468 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2466 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2464 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2461 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2460 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2458 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2456 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2454 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2452 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2450 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2449 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2447 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2445 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2443 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2441 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2439 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2438 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2436 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2434 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2432 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2429 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2427 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2425 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2422 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2421 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2418 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2416 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2414 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2412 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2410 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2407 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2406 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2403 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2402 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2400 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2399 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2397 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2395 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2394 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2392 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2390 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2388 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2386 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2385 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2384 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2382 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2380 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2379 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2377 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2378 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2376 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2374 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2372 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2369 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2368 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2367 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2365 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2364 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2363 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2361 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2359 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2357 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2356 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2356 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2355 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2353 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2351 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2349 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2348 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2347 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2345 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2343 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2342 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2341 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2340 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2338 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2336 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2335 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2333 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2331 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2330 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2329 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2328 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2326 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2325 - accuracy: 0.9329
    938/Unknown - 10s 12ms/step - loss: 0.2323 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2322 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2320 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2318 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2316 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2314 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2312 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2310 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2309 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2307 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2306 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2304 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2302 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2301 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2298 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2297 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2295 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2293 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2294 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2293 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2291 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2289 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2288 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2286 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2285 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2283 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2281 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2280 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2278 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2277 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2275 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2273 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2272 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2271 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2269 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2267 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2266 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2266 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2265 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2263 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2261 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2261 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2260 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2258 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2257 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2255 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2253 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2251 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2250 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2248 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2247 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2246 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2245 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2243 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2241 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2239 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2239 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2238 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2237 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2235 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2234 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2233 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2232 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2230 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2228 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2226 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2225 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2223 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2221 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2220 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2219 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2218 - accuracy: 0.936 - 10s 11ms/step - loss: 0.2216 - accuracy: 0.936 - 10s 11ms/step - loss: 0.2215 - accuracy: 0.9360
Learning rate for epoch 1 is 0.0010000000474974513
938/938 [==============================] - 11s 11ms/step - loss: 0.2215 - accuracy: 0.9360
Epoch 2/12
915/938 [============================>.] - ETA: 9:19 - loss: 0.0448 - accuracy: 1.00 - ETA: 40s - loss: 0.1246 - accuracy: 0.9646 - ETA: 22s - loss: 0.1092 - accuracy: 0.966 - ETA: 16s - loss: 0.1017 - accuracy: 0.970 - ETA: 12s - loss: 0.1019 - accuracy: 0.969 - ETA: 10s - loss: 0.1007 - accuracy: 0.969 - ETA: 9s - loss: 0.1063 - accuracy: 0.968 - ETA: 8s - loss: 0.0995 - accuracy: 0.97 - ETA: 7s - loss: 0.0975 - accuracy: 0.97 - ETA: 7s - loss: 0.0970 - accuracy: 0.97 - ETA: 6s - loss: 0.0969 - accuracy: 0.97 - ETA: 6s - loss: 0.0949 - accuracy: 0.97 - ETA: 5s - loss: 0.0936 - accuracy: 0.97 - ETA: 5s - loss: 0.0933 - accuracy: 0.97 - ETA: 5s - loss: 0.0931 - accuracy: 0.97 - ETA: 5s - loss: 0.0908 - accuracy: 0.97 - ETA: 4s - loss: 0.0888 - accuracy: 0.97 - ETA: 4s - loss: 0.0874 - accuracy: 0.97 - ETA: 4s - loss: 0.0878 - accuracy: 0.97 - ETA: 4s - loss: 0.0873 - accuracy: 0.97 - ETA: 4s - loss: 0.0864 - accuracy: 0.97 - ETA: 4s - loss: 0.0860 - accuracy: 0.97 - ETA: 3s - loss: 0.0851 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 3s - loss: 0.0841 - accuracy: 0.97 - ETA: 3s - loss: 0.0829 - accuracy: 0.97 - ETA: 3s - loss: 0.0832 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 3s - loss: 0.0825 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 3s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0817 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0806 - accuracy: 0.97 - ETA: 2s - loss: 0.0798 - accuracy: 0.97 - ETA: 2s - loss: 0.0791 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0794 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0791 - accuracy: 0.97 - ETA: 2s - loss: 0.0789 - accuracy: 0.97 - ETA: 2s - loss: 0.0788 - accuracy: 0.97 - ETA: 1s - loss: 0.0786 - accuracy: 0.97 - ETA: 1s - loss: 0.0779 - accuracy: 0.97 - ETA: 1s - loss: 0.0775 - accuracy: 0.97 - ETA: 1s - loss: 0.0773 - accuracy: 0.97 - ETA: 1s - loss: 0.0771 - accuracy: 0.97 - ETA: 1s - loss: 0.0772 - accuracy: 0.97 - ETA: 1s - loss: 0.0768 - accuracy: 0.97 - ETA: 1s - loss: 0.0768 - accuracy: 0.97 - ETA: 1s - loss: 0.0767 - accuracy: 0.97 - ETA: 1s - loss: 0.0763 - accuracy: 0.97 - ETA: 1s - loss: 0.0761 - accuracy: 0.97 - ETA: 1s - loss: 0.0759 - accuracy: 0.97 - ETA: 1s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0747 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0744 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0734 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.9789
Learning rate for epoch 2 is 0.0010000000474974513
938/938 [==============================] - 4s 4ms/step - loss: 0.0719 - accuracy: 0.9791
Epoch 3/12
926/938 [============================>.] - ETA: 10:02 - loss: 0.0245 - accuracy: 1.000 - ETA: 49s - loss: 0.0628 - accuracy: 0.9868  - ETA: 27s - loss: 0.0623 - accuracy: 0.986 - ETA: 19s - loss: 0.0527 - accuracy: 0.988 - ETA: 15s - loss: 0.0555 - accuracy: 0.987 - ETA: 13s - loss: 0.0552 - accuracy: 0.985 - ETA: 11s - loss: 0.0553 - accuracy: 0.986 - ETA: 10s - loss: 0.0551 - accuracy: 0.985 - ETA: 9s - loss: 0.0535 - accuracy: 0.986 - ETA: 8s - loss: 0.0554 - accuracy: 0.98 - ETA: 7s - loss: 0.0536 - accuracy: 0.98 - ETA: 7s - loss: 0.0537 - accuracy: 0.98 - ETA: 6s - loss: 0.0546 - accuracy: 0.98 - ETA: 6s - loss: 0.0556 - accuracy: 0.98 - ETA: 6s - loss: 0.0553 - accuracy: 0.98 - ETA: 5s - loss: 0.0565 - accuracy: 0.98 - ETA: 5s - loss: 0.0558 - accuracy: 0.98 - ETA: 5s - loss: 0.0575 - accuracy: 0.98 - ETA: 5s - loss: 0.0573 - accuracy: 0.98 - ETA: 5s - loss: 0.0570 - accuracy: 0.98 - ETA: 4s - loss: 0.0558 - accuracy: 0.98 - ETA: 4s - loss: 0.0553 - accuracy: 0.98 - ETA: 4s - loss: 0.0549 - accuracy: 0.98 - ETA: 4s - loss: 0.0552 - accuracy: 0.98 - ETA: 4s - loss: 0.0554 - accuracy: 0.98 - ETA: 4s - loss: 0.0551 - accuracy: 0.98 - ETA: 3s - loss: 0.0545 - accuracy: 0.98 - ETA: 3s - loss: 0.0540 - accuracy: 0.98 - ETA: 3s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0531 - accuracy: 0.98 - ETA: 3s - loss: 0.0527 - accuracy: 0.98 - ETA: 3s - loss: 0.0523 - accuracy: 0.98 - ETA: 3s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0524 - accuracy: 0.98 - ETA: 3s - loss: 0.0522 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0524 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0517 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0518 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0511 - accuracy: 0.98 - ETA: 2s - loss: 0.0510 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.9850
Learning rate for epoch 3 is 0.0010000000474974513
938/938 [==============================] - 4s 5ms/step - loss: 0.0490 - accuracy: 0.9850
Epoch 4/12
930/938 [============================>.] - ETA: 9:29 - loss: 0.0240 - accuracy: 1.00 - ETA: 43s - loss: 0.0553 - accuracy: 0.9821 - ETA: 23s - loss: 0.0465 - accuracy: 0.983 - ETA: 16s - loss: 0.0397 - accuracy: 0.986 - ETA: 13s - loss: 0.0417 - accuracy: 0.986 - ETA: 11s - loss: 0.0395 - accuracy: 0.987 - ETA: 9s - loss: 0.0392 - accuracy: 0.988 - ETA: 8s - loss: 0.0367 - accuracy: 0.98 - ETA: 7s - loss: 0.0364 - accuracy: 0.98 - ETA: 7s - loss: 0.0353 - accuracy: 0.98 - ETA: 6s - loss: 0.0360 - accuracy: 0.98 - ETA: 6s - loss: 0.0358 - accuracy: 0.98 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0350 - accuracy: 0.98 - ETA: 5s - loss: 0.0344 - accuracy: 0.98 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.99 - ETA: 4s - loss: 0.0337 - accuracy: 0.99 - ETA: 4s - loss: 0.0341 - accuracy: 0.99 - ETA: 3s - loss: 0.0342 - accuracy: 0.98 - ETA: 3s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0343 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0337 - accuracy: 0.99 - ETA: 3s - loss: 0.0334 - accuracy: 0.99 - ETA: 3s - loss: 0.0330 - accuracy: 0.99 - ETA: 3s - loss: 0.0327 - accuracy: 0.99 - ETA: 2s - loss: 0.0325 - accuracy: 0.99 - ETA: 2s - loss: 0.0328 - accuracy: 0.99 - ETA: 2s - loss: 0.0324 - accuracy: 0.99 - ETA: 2s - loss: 0.0322 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0320 - accuracy: 0.99 - ETA: 2s - loss: 0.0319 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0309 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0305 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 0s - loss: 0.0296 - accuracy: 0.99 - ETA: 0s - loss: 0.0297 - accuracy: 0.99 - ETA: 0s - loss: 0.0295 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0289 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0287 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.9924
Learning rate for epoch 4 is 9.999999747378752e-05
938/938 [==============================] - 4s 5ms/step - loss: 0.0279 - accuracy: 0.9925
Epoch 5/12
934/938 [============================>.] - ETA: 11:27 - loss: 0.0173 - accuracy: 1.000 - ETA: 1:21 - loss: 0.0281 - accuracy: 0.991 - ETA: 45s - loss: 0.0214 - accuracy: 0.9936 - ETA: 30s - loss: 0.0197 - accuracy: 0.993 - ETA: 22s - loss: 0.0249 - accuracy: 0.992 - ETA: 18s - loss: 0.0270 - accuracy: 0.991 - ETA: 15s - loss: 0.0246 - accuracy: 0.993 - ETA: 13s - loss: 0.0232 - accuracy: 0.993 - ETA: 12s - loss: 0.0234 - accuracy: 0.993 - ETA: 11s - loss: 0.0223 - accuracy: 0.993 - ETA: 10s - loss: 0.0232 - accuracy: 0.993 - ETA: 9s - loss: 0.0230 - accuracy: 0.992 - ETA: 9s - loss: 0.0236 - accuracy: 0.99 - ETA: 8s - loss: 0.0247 - accuracy: 0.99 - ETA: 7s - loss: 0.0248 - accuracy: 0.99 - ETA: 7s - loss: 0.0258 - accuracy: 0.99 - ETA: 7s - loss: 0.0266 - accuracy: 0.99 - ETA: 6s - loss: 0.0272 - accuracy: 0.99 - ETA: 6s - loss: 0.0277 - accuracy: 0.99 - ETA: 6s - loss: 0.0285 - accuracy: 0.99 - ETA: 5s - loss: 0.0279 - accuracy: 0.99 - ETA: 5s - loss: 0.0280 - accuracy: 0.99 - ETA: 5s - loss: 0.0275 - accuracy: 0.99 - ETA: 5s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0276 - accuracy: 0.99 - ETA: 4s - loss: 0.0275 - accuracy: 0.99 - ETA: 4s - loss: 0.0273 - accuracy: 0.99 - ETA: 4s - loss: 0.0270 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.9932
Learning rate for epoch 5 is 9.999999747378752e-05
938/938 [==============================] - 5s 5ms/step - loss: 0.0250 - accuracy: 0.9933
Epoch 6/12
921/938 [============================>.] - ETA: 11:48 - loss: 0.0304 - accuracy: 0.984 - ETA: 57s - loss: 0.0276 - accuracy: 0.9916  - ETA: 34s - loss: 0.0299 - accuracy: 0.990 - ETA: 26s - loss: 0.0301 - accuracy: 0.990 - ETA: 21s - loss: 0.0314 - accuracy: 0.990 - ETA: 17s - loss: 0.0290 - accuracy: 0.991 - ETA: 14s - loss: 0.0269 - accuracy: 0.992 - ETA: 12s - loss: 0.0255 - accuracy: 0.992 - ETA: 11s - loss: 0.0240 - accuracy: 0.992 - ETA: 10s - loss: 0.0235 - accuracy: 0.993 - ETA: 9s - loss: 0.0232 - accuracy: 0.993 - ETA: 8s - loss: 0.0229 - accuracy: 0.99 - ETA: 8s - loss: 0.0236 - accuracy: 0.99 - ETA: 7s - loss: 0.0236 - accuracy: 0.99 - ETA: 7s - loss: 0.0237 - accuracy: 0.99 - ETA: 6s - loss: 0.0237 - accuracy: 0.99 - ETA: 6s - loss: 0.0249 - accuracy: 0.99 - ETA: 6s - loss: 0.0245 - accuracy: 0.99 - ETA: 6s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0245 - accuracy: 0.99 - ETA: 5s - loss: 0.0250 - accuracy: 0.99 - ETA: 5s - loss: 0.0250 - accuracy: 0.99 - ETA: 5s - loss: 0.0249 - accuracy: 0.99 - ETA: 5s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.99 - ETA: 4s - loss: 0.0251 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0236 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0234 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.9937
Learning rate for epoch 6 is 9.999999747378752e-05
938/938 [==============================] - 5s 5ms/step - loss: 0.0231 - accuracy: 0.9937
Epoch 7/12
929/938 [============================>.] - ETA: 11:36 - loss: 0.0087 - accuracy: 1.000 - ETA: 1:13 - loss: 0.0184 - accuracy: 0.996 - ETA: 41s - loss: 0.0189 - accuracy: 0.9967 - ETA: 27s - loss: 0.0227 - accuracy: 0.994 - ETA: 21s - loss: 0.0234 - accuracy: 0.994 - ETA: 17s - loss: 0.0263 - accuracy: 0.993 - ETA: 14s - loss: 0.0245 - accuracy: 0.994 - ETA: 12s - loss: 0.0232 - accuracy: 0.994 - ETA: 11s - loss: 0.0230 - accuracy: 0.994 - ETA: 10s - loss: 0.0220 - accuracy: 0.994 - ETA: 9s - loss: 0.0213 - accuracy: 0.994 - ETA: 8s - loss: 0.0215 - accuracy: 0.99 - ETA: 8s - loss: 0.0219 - accuracy: 0.99 - ETA: 7s - loss: 0.0219 - accuracy: 0.99 - ETA: 7s - loss: 0.0228 - accuracy: 0.99 - ETA: 6s - loss: 0.0239 - accuracy: 0.99 - ETA: 6s - loss: 0.0235 - accuracy: 0.99 - ETA: 6s - loss: 0.0237 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0242 - accuracy: 0.99 - ETA: 5s - loss: 0.0246 - accuracy: 0.99 - ETA: 5s - loss: 0.0244 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0244 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 3s - loss: 0.0241 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0238 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0229 - accuracy: 0.99 - ETA: 3s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0228 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.99 - ETA: 0s - loss: 0.0217 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.9944
Learning rate for epoch 7 is 9.999999747378752e-05
938/938 [==============================] - 5s 5ms/step - loss: 0.0215 - accuracy: 0.9944
Epoch 8/12
922/938 [============================>.] - ETA: 11:03 - loss: 0.0745 - accuracy: 0.953 - ETA: 50s - loss: 0.0263 - accuracy: 0.9922  - ETA: 28s - loss: 0.0237 - accuracy: 0.992 - ETA: 20s - loss: 0.0197 - accuracy: 0.994 - ETA: 16s - loss: 0.0179 - accuracy: 0.994 - ETA: 13s - loss: 0.0199 - accuracy: 0.994 - ETA: 11s - loss: 0.0184 - accuracy: 0.994 - ETA: 10s - loss: 0.0184 - accuracy: 0.995 - ETA: 9s - loss: 0.0198 - accuracy: 0.994 - ETA: 8s - loss: 0.0205 - accuracy: 0.99 - ETA: 8s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0196 - accuracy: 0.99 - ETA: 7s - loss: 0.0198 - accuracy: 0.99 - ETA: 6s - loss: 0.0201 - accuracy: 0.99 - ETA: 6s - loss: 0.0196 - accuracy: 0.99 - ETA: 6s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0206 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0208 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0207 - accuracy: 0.99 - ETA: 4s - loss: 0.0208 - accuracy: 0.99 - ETA: 4s - loss: 0.0210 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0209 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0202 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0192 - accuracy: 0.99 - ETA: 0s - loss: 0.0191 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.9954
Learning rate for epoch 8 is 9.999999747378752e-06
938/938 [==============================] - 4s 5ms/step - loss: 0.0189 - accuracy: 0.9954
Epoch 9/12
919/938 [============================>.] - ETA: 11:06 - loss: 0.0661 - accuracy: 0.984 - ETA: 58s - loss: 0.0357 - accuracy: 0.9922  - ETA: 30s - loss: 0.0301 - accuracy: 0.992 - ETA: 22s - loss: 0.0310 - accuracy: 0.991 - ETA: 17s - loss: 0.0337 - accuracy: 0.991 - ETA: 14s - loss: 0.0304 - accuracy: 0.991 - ETA: 12s - loss: 0.0306 - accuracy: 0.991 - ETA: 11s - loss: 0.0282 - accuracy: 0.992 - ETA: 9s - loss: 0.0276 - accuracy: 0.992 - ETA: 9s - loss: 0.0269 - accuracy: 0.99 - ETA: 8s - loss: 0.0255 - accuracy: 0.99 - ETA: 7s - loss: 0.0245 - accuracy: 0.99 - ETA: 7s - loss: 0.0240 - accuracy: 0.99 - ETA: 6s - loss: 0.0249 - accuracy: 0.99 - ETA: 6s - loss: 0.0247 - accuracy: 0.99 - ETA: 6s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0237 - accuracy: 0.99 - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0230 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0227 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0203 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0200 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0192 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.9956
Learning rate for epoch 9 is 9.999999747378752e-06
938/938 [==============================] - 4s 5ms/step - loss: 0.0186 - accuracy: 0.9956
Epoch 10/12
919/938 [============================>.] - ETA: 9:17 - loss: 0.0493 - accuracy: 0.98 - ETA: 39s - loss: 0.0212 - accuracy: 0.9937 - ETA: 21s - loss: 0.0178 - accuracy: 0.994 - ETA: 15s - loss: 0.0171 - accuracy: 0.994 - ETA: 12s - loss: 0.0164 - accuracy: 0.995 - ETA: 10s - loss: 0.0156 - accuracy: 0.995 - ETA: 9s - loss: 0.0169 - accuracy: 0.995 - ETA: 8s - loss: 0.0171 - accuracy: 0.99 - ETA: 7s - loss: 0.0177 - accuracy: 0.99 - ETA: 6s - loss: 0.0178 - accuracy: 0.99 - ETA: 6s - loss: 0.0186 - accuracy: 0.99 - ETA: 5s - loss: 0.0193 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0186 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0190 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.9957
Learning rate for epoch 10 is 9.999999747378752e-06
938/938 [==============================] - 4s 4ms/step - loss: 0.0184 - accuracy: 0.9957
Epoch 11/12
926/938 [============================>.] - ETA: 10:17 - loss: 0.0177 - accuracy: 1.000 - ETA: 50s - loss: 0.0256 - accuracy: 0.9952  - ETA: 26s - loss: 0.0294 - accuracy: 0.992 - ETA: 18s - loss: 0.0294 - accuracy: 0.992 - ETA: 14s - loss: 0.0263 - accuracy: 0.992 - ETA: 12s - loss: 0.0261 - accuracy: 0.993 - ETA: 10s - loss: 0.0245 - accuracy: 0.993 - ETA: 9s - loss: 0.0231 - accuracy: 0.994 - ETA: 8s - loss: 0.0218 - accuracy: 0.99 - ETA: 8s - loss: 0.0207 - accuracy: 0.99 - ETA: 7s - loss: 0.0215 - accuracy: 0.99 - ETA: 7s - loss: 0.0215 - accuracy: 0.99 - ETA: 6s - loss: 0.0212 - accuracy: 0.99 - ETA: 6s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0207 - accuracy: 0.99 - ETA: 5s - loss: 0.0203 - accuracy: 0.99 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0203 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0184 - accuracy: 0.99 - ETA: 1s - loss: 0.0184 - accuracy: 0.99 - ETA: 1s - loss: 0.0183 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0182 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0182 - accuracy: 0.9957
Learning rate for epoch 11 is 9.999999747378752e-06
938/938 [==============================] - 4s 5ms/step - loss: 0.0182 - accuracy: 0.9957
Epoch 12/12
920/938 [============================>.] - ETA: 11:13 - loss: 0.0046 - accuracy: 1.000 - ETA: 54s - loss: 0.0102 - accuracy: 0.9988  - ETA: 31s - loss: 0.0183 - accuracy: 0.996 - ETA: 21s - loss: 0.0226 - accuracy: 0.995 - ETA: 17s - loss: 0.0211 - accuracy: 0.995 - ETA: 14s - loss: 0.0203 - accuracy: 0.995 - ETA: 12s - loss: 0.0203 - accuracy: 0.995 - ETA: 10s - loss: 0.0209 - accuracy: 0.995 - ETA: 10s - loss: 0.0213 - accuracy: 0.994 - ETA: 9s - loss: 0.0208 - accuracy: 0.994 - ETA: 8s - loss: 0.0199 - accuracy: 0.99 - ETA: 7s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0200 - accuracy: 0.99 - ETA: 6s - loss: 0.0195 - accuracy: 0.99 - ETA: 6s - loss: 0.0196 - accuracy: 0.99 - ETA: 6s - loss: 0.0192 - accuracy: 0.99 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0191 - accuracy: 0.99 - ETA: 2s - loss: 0.0191 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.9958
Learning rate for epoch 12 is 9.999999747378752e-06
938/938 [==============================] - 5s 5ms/step - loss: 0.0180 - accuracy: 0.9958
<tensorflow.python.keras.callbacks.History at 0x1715158c048>

**Describe the expected behavior**
You get a neat concise overview of training on the linux EC2 installations.  Just as it should be.

**Code to reproduce the issue**
Yes, cleanly install Windows 10, update to the latest 19.03 release with Windows updates, install the latest Anaconda, install the Tensorflow 2 beta 1 binary, and run this notebook locally.

**Other info / logs**
Nil"
30907,tf.contrib.factorization.WALSMatrixFactorization( AttributeError: 'module' object has no attribute 'WALSMatrixFactorization'),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I just run walsmodel through gcloud ml-engine jobs
I run it through my mac terminal, I didn't change any code over here https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/10_recommend/walsmodel

But it always has the following error:
experiment_fn tf.contrib.factorization.WALSMatrixFactorization( AttributeError: 'module' object has no attribute 'WALSMatrixFactorization'

`gcloud ml-engine jobs submit training wals_190721_012226 --region=us-east1 --module-name=walsmodel.task --runtime-version 1.14 --python-version 3.5 --package-path=/home/mabodx/anguis/news_recommendation/10_recommend/walsmodel --job-dir=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/ --staging-bucket=gs://buzzbreak --scale-tier=BASIC_GPU --runtime-version= -- --output_dir=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/model_trained_190721_012212 --input_path=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/news/data --num_epochs=500 --nitems=39681 --nusers=38781 --topk=1000`
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/10_recommend/walsmodel

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30906,tf.scatter_nd_update missing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.0.0-beta1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

 Tensorflow 2.0 does not have the function tf.scatter_nd_update. Is this normal ? 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

tf.scatter_nd_update
AttributeError: module 'tensorflow' has no attribute 'scatter_nd_update'

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30905,Failed to install tensorflow-estimator,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10.0.18362.0

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip / binary
- TensorFlow version:
1.14 (?)
- Python version:
Python 3.7.4 (installed from Windows Store)
- Installed using virtualenv? pip? conda?:
pip 19.1.1
- Bazel version (if compiling from source):
n/a
- GCC/Compiler version (if compiling from source):
n/a
- CUDA/cuDNN version:
n/a
- GPU model and memory:
Nvidia GeForce GTX 1080; 16 GB

**Describe the problem**

When trying to install Tensorflow, I get an error while installing the Tensorflow-Estimator dependency. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

* Open `cmd.exe` as administrator

```
pip install --upgrade tensorflow-estimator

ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\movgp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear_optimizer\\python\\utils\\__pycache__\\sharded_mutable_dense_hashtable.cpython-37.pyc'
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
 Directory of C:\Users\movgp\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_estimator\python\estimator\canned\linear_optimizer\python\utils\__pycache__

2019-07-21  09:46    <DIR>          .
2019-07-21  09:46    <DIR>          ..
2019-07-21  09:46            21,504 sdca_ops.cpython-37.pyc
               1 File(s)         21,504 bytes
```
"
30903,Duplication indices in the results of hessian_vector_product(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code.
- OS Platform and Distribution: Ubuntu 18.04.
- TensorFlow installed from pip
- TensorFlow version 1.14 with GPU support
- Python version: 3.7.3
- CUDA/cuDNN version: 10

**Describe the current behavior**

Hi, when I am using the hessian_vector_product() to computer the hessian_vector_product of my model, it returns results organized as IndexedSliceValue. This is OK because the model include data structure like embeddings. 

However, when I do the assert on the indices of the returned IndexedSliceValue. I notice that there are duplicated indices with different corresponding values. May I know how to deal with these conflict <indices, values>? Thanks in advance.
"
30901,Issue with AttentionWrapper and non unique key dictionary in feed_dict,"Hello,

I would like to use Bahdanau Attention in a decoder, using tf.dynamic_rnn

For my decoder, I am feeding the previous state of AttentionWrapperState at each time step.

~~~
    attn_meca = tf.contrib.seq2seq.BahdanauAttention(config.num_lstm_units, multimodal_feature)   
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell_expl, attn_meca, output_attention=False)
    
...

        lstm_outputs, final_state = tf.nn.dynamic_rnn(cell=attn_cell,
                                                    inputs=seq_embedding,
                                                    sequence_length=sequence_length,
                                                    initial_state=initial_state,
                                                    dtype=tf.float32,
                                                    scope=lstm_scope)
~~~

I am feeding the state as follows 
~~~
 feed_dict={model['input_seqs']: current_pred, 
                   model['initial_state']: state, 
                   model['input_mask']: mask, 
                   model['dropout_input']: 1.0,
                   model['keep_prob']: keep_prob}

current_pred, state = sess.run([model['preds'], model['final_state']], feed_dict=feed_dict)
~~~

And I get this error :

`ValueError: Could not flatten dictionary: key Tensor(""lstm/lstm/attention_wrapper/Softmax:0"", shape=(?, ?), dtype=float32) is not unique.`

I printed the state that I'm feeding:
`AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'lstm/lstm/attention_wrapper/lstm_cell/add_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'lstm/lstm/attention_wrapper/lstm_cell/mul_2:0' shape=(?, 512) dtype=float32>), attention=<tf.Tensor 'lstm/lstm/attention_wrapper/concat_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'lstm/lstm/attention_wrapper/add:0' shape=() dtype=int32>, alignments=<tf.Tensor 'lstm/lstm/attention_wrapper/Softmax:0' shape=(?, ?) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'lstm/lstm/attention_wrapper/Softmax:0' shape=(?, ?) dtype=float32>)`

So I see that there are two `'lstm/lstm/attention_wrapper/Softmax:0'`

How can I change that when I define my attention cell?

https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapperState


Thanks in advance.

"
30900,ERROR: saved_model_cli,"working on ubuntu 18.4

tensorflow version 2.0.0b1

I am trying to find `output_node_names` with the help of command `saved_model_cli show --dir DIR_PATH --all`

got this ERROR `OSError: SavedModel file does not exist at: DIR_PATH`

this is my SAVED MODEL

![Screenshot from 2019-07-20 21-06-51](https://user-images.githubusercontent.com/32290207/61580704-5b4d0700-ab32-11e9-8fac-7fc38ab4b00b.png)

"
30899,failed to “sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow.so”,"I want to compile tensorflow‘ c library version
system info：
  ubuntu 16.04
  bazel 0.19.2
  tensorflow 1.13
  gpu RTX 2080 Ti
  cuda10.0
  cudnn 7.4.2

steps：
1.Install bazel by sh file
2.git clone --recursive https://github.com/tensorflow/tensorflow
3.git checkout r1.3
4.sudo ./configure
5.sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow.so

error:
 Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
ERROR: /home/liupeng/tensorflow/tensorflow/core/kernels/BUILD:4841:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/core/kernels:unicode_ops'
ERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761
INFO: Elapsed time: 780.835s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (119 packages loaded, 8680 targets configured)
"
30898,Makefile build does not create contrib ops,"**System information**
- OS Platform and Distribution: MacOS
- TensorFlow installed from (source or binary): Source
- TensorFlow version: c6352706d6280d7ed34964969503b104471a9335
- Bazel version (if compiling from source): not used
- GCC/Compiler version (if compiling from source):
```
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```

**Describe the problem**

When compiling with `tensorflow/contrib/makefile/build_all_linux.sh`, the following inclusion does not work:
```
#include <tensorflow/cc/ops/standard_ops.h>
```
because the header `#includes` something that's not there:
```
<tfdir>/tensorflow/tensorflow/cc/ops/standard_ops.h:19:10: fatal error: 'tensorflow/cc/ops/array_ops.h' file not found
#include ""tensorflow/cc/ops/array_ops.h""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
```

Searching for `array_ops` in the repository e.g. reveals a CMake macro named `GENERATE_CONTRIB_OP_LIBRARY` defined in in `tensorflow/contrib/cmake/tf_core_ops.cmake`.

https://github.com/tensorflow/tensorflow/blob/6c7c3bb0f3563e8a147935597617cf3d052c7852/tensorflow/contrib/cmake/tf_core_ops.cmake#L76

Obviously, the Makefile does not use this. Is it intentional that these ops are not being built with the makefile? Even if I remove the header, a program does not work at runtime due to missing ops.

I can provide a minimal C++ file and build scripts if desired, but would like to know first if it's supposed to be like this.
"
30897,Inconsistency between using keras and tf.keras,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: Tensorflow 2 
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA



There is inconsistency when using keras or tf.keras to fit the model to the same dataset. As everyone can see, everything is similar in both code except calling layers and other objects from tf.keras or pure Keras. The codes are attached. 

I'm absolutely confused! I tried to double-check the code line by line, and everything is the same in both codes, except calling modules from either pure keras or tf.keras.

It is absolutely disappointing to me! Why they would be totally different.
In fact, the codes are the same except that layers call from either keras or tf.keras; why these models should be such completely different behaviors.

`   

# The modules/layers can be called from either:

    from tensorflow.python.keras.layers import (Input, Dense, BatchNormalization, GaussianNoise, 
    GaussianDropout)
    from tensorflow.python.keras.models import Model
    from tensorflow.python.keras.utils import np_utils
    from tensorflow.python.keras.callbacks import CSVLogger, History

Or 

    from keras.layers import Input, Dense, BatchNormalization, GaussianNoise, GaussianDropout
    from keras.models import Model
    from keras.utils import np_utils
    from keras.callbacks import CSVLogger, History

# Creating Models: Everything in the models' description is the same.
    
   
    inputs = Input(shape=(19671,), name=""inputs"")

    inputs_0 = BatchNormalization(name=""inputs_0"")(inputs)

    inputs_1 = Dense(1024, activation=""linear"", name=""inputs_1"")(inputs_0)

    inputs_2 = BatchNormalization(name=""inputs_2"")(inputs_1)

    inputs_3 = Dense(128, activation=""softplus"", name=""inputs_3"")(inputs_2)

    inputs_4 = BatchNormalization(name=""inputs_4"")(inputs_3)

    encoded = Dense(units=12, activation='relu', name='encoded')(inputs_4)

    inputs_5 = Dense(512, activation=""softplus"", name=""inputs_5"")(encoded)

    decoded_tcga = Dense(units=19671, activation='linear', name=""m_rna"")(inputs_5)

    decoded_micro_rna = Dense(units=2588, activation='linear', name=""mi_rna"")(inputs_5)

    cl_0 = Dense(units=27, activation=""softmax"", name=""cl_tissue"")(encoded)

    cl_2 = Dense(units=33, activation=""softmax"", name=""cl_disease"")(encoded)

    scae = Model(inputs=inputs, outputs=[decoded_tcga, decoded_micro_rna, cl_0, cl_2])

    scae.compile(optimizer='nadam',
                 loss=[""mse"", ""mse"", ""cosine_proximity"", ""cosine_proximity""],
                 loss_weights=[0.001, 0.001, 0.5, 0.5],
                 metrics={""m_rna"": [""mse"", ""mae""],
                          ""mi_rna"": [""mse"", ""mae""], ""cl_tissue"": ""acc"", ""cl_disease"": ""acc""})

    scae
`

` 

# Loading Datasets (The datasets are too large, about 5.5 GB).
    local_dataset_folder = 
    ""/home/ermia/MEGA/PycharmProjects_VSC/TCGA_Full_Version/TCGA/Dataset/""
    local_results_folder = 
    ""/home/ermia/MEGA/PycharmProjects_VSC/TCGA_Full_Version/TCGA/Results/""
    
    from sklearn.preprocessing import LabelEncoder, normalize
    import pandas as pd

    seed = 2019
    np.random.seed(seed=seed)
    dataset_folder = local_dataset_folder
    df_m_rna_address = dataset_folder + ""fpkm.csv""
    df_mi_rna_address = dataset_folder + ""miRNA.csv""
    df_tissue_address = dataset_folder + ""tissue.csv""
    df_disease_address = dataset_folder + ""disease.csv""

    df_m_rna = np.loadtxt(df_m_rna_address, delimiter="","")
    df_mi_rna = np.loadtxt(df_mi_rna_address, delimiter="","")
    df_tissue = np.ravel(pd.DataFrame.as_matrix(pd.read_csv(df_tissue_address, delimiter="","", 
    header=None)))
    df_disease = np.ravel(pd.DataFrame.as_matrix(pd.read_csv(df_disease_address, delimiter="","", 
    header=None)))

    df_m_rna = normalize(X=df_m_rna, axis=0, norm=""max"")
    df_mi_rna = normalize(X=df_mi_rna, axis=0, norm=""max"")

    label_encoder_tissue = LabelEncoder()
    label_encoder_tissue.fit(df_tissue)
    encoded_tissue = label_encoder_tissue.transform(df_tissue)

    label_encoder_disease = LabelEncoder()
    label_encoder_disease.fit(df_disease)
    encoded_disease = label_encoder_disease.transform(df_disease)

    categorical_tissue = np_utils.to_categorical(encoded_tissue)
    categorical_disease = np_utils.to_categorical(encoded_disease)
    m_rna = df_m_rna
    mi_rna = df_mi_rna

    indices = np.arange(m_rna.shape[0])
    indices = indices[0:10750]
    np.random.shuffle(indices)

    m_rna = m_rna[indices]
    mi_rna = mi_rna[indices]

    categorical_tissue = categorical_tissue[indices]
    categorical_disease = categorical_disease[indices]

    m_rna_train = m_rna[0:9750, ]
    m_rna_test = m_rna[9750:10750, ]

    mi_rna_train = mi_rna[0:9750, ]
    mi_rna_test = mi_rna[9750:10750, ]

    categorical_tissue_train = categorical_tissue[0:9750, ]
    categorical_tissue_test = categorical_tissue[9750:10750, ]

    categorical_disease_train = categorical_disease[0:9750, ]
    categorical_disease_test = categorical_disease[9750: 10750, ]

    print(""data loading has just been finished"")
    print(m_rna.shape, mi_rna.shape, categorical_tissue.shape, categorical_disease.shape)

     batch_size = 64
     nb_epochs = 200

`

`

# Fitting Models.
    scae.fit(m_rna_train, [m_rna_train, mi_rna_train, categorical_tissue_train, categorical_disease_train], 
    batch_size=batch_size, epochs=nb_epochs,
          callbacks=[csv_logger, history],
          validation_data=(m_rna_test, [m_rna_test, mi_rna_test, categorical_tissue_test, 
    categorical_disease_test]), verbose=2)


`
The first model is not convergent at all! and the accuracy of subproblems are about 0.

@fchollet "
30896,Standard way of sharing architectures?,"**System information**
- TensorFlow version (you are using):
1.14, and sometimes the latest v2 pre-release
- Are you willing to contribute it (Yes/No):
Yes

**Describe the feature and the current behavior/state.**
Found a number of interesting architectures on GitHub. Starting making PRs to them, to support Python 2.7–3.6, and soon for proper setuptools integration.

The [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API, and [tfds in general](https://www.tensorflow.org/datasets/api_docs/python/tfds) are both useful approaches for preparing data, and exposing it for others to consume.

However, AFAIK, there is no standard way of sharing, say: Keras models. Internally have moved to [`gin`](https://github.com/google/gin-config), but not sure if that's the best approach for all.

**Will this change the current api? How?**
Introduce a new module, similar to how tfds was introduced for preparing and packaging datasets.

**Who will benefit with this feature?**
Everyone. Especially those that want to combine multiple architectures from different sources—or in my scenario—to experiment with different architectures.

**Any Other info.**
Concept:
```python
import some_broke_arch
import other_neat_arch
import horrible_v_arch

model   = some_broke_arch.get_arch(   **standard_arch_params  )
metrics = other_neat_arch.get_metrics(**standard_metric_params)
loss    = horrible_v_arch.get_loss(   **standard_loss_params  )

model.compile(loss=loss, optimizer=keras.optimizers.RMSprop, metrics=metrics)
print(model.summary())
# &etc.
````"
30895,[TF 2.0] autograph issue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  
Yes 

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
 Windows 10 Pro 64-bit (10.0, Build 18362) & Ubuntu 18.04.2 LTS 

- TensorFlow installed from (source or binary):  PyPI 

- TensorFlow version (use command below):  2.0.0-dev20190718 

- Python version: 3.6.7 

- CUDA/cuDNN version: cudatoolkit-10.0.130-0/cudnn-7.6.0-cuda10.0_0 

- GPU model and memory: 
(/device:GPU:0 with 1387 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)

**Describe the current behavior**
tf.function raised a warning log (only on Windows Python Shell, even IPython works fine)
and tf.autograph.to_code did not work... (Both Windows and Colab)
```
>>> @tf.function
... def calc(x):
...     return 0 if x<=0.5 else 1
...
>>> calc(12)
WARNING: Logging before flag parsing goes to stderr.
W0720 14:49:32.090106 25088 ag_logging.py:146] Entity <function calc at 0x00000280F0429C80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calc at 0x00000280F0429C80>: ValueError: Unable to locate the source code of <function calc at 0x00000280F0429C80>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
WARNING: Entity <function calc at 0x00000280F0429C80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calc at 0x00000280F0429C80>: ValueError: Unable to locate the source code of <function calc at 0x00000280F0429C80>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
<tf.Tensor: id=16, shape=(), dtype=int32, numpy=1>
```
and
```
>>> print(tf.autograph.to_code(calc))
Traceback (most recent call last):
  File ""C:\tools\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 614, in to_graph
    return conversion.convert(entity, program_ctx)
  File ""C:\tools\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\autograph\impl\conversion.py"", line 310, in convert
    free_nonglobal_var_names = entity.__code__.co_freevars
AttributeError: 'Function' object has no attribute '__code__'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\tools\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 754, in to_code
    experimental_optional_features=experimental_optional_features))
  File ""C:\tools\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 618, in to_graph
    entity, e.__class__.__name__, str(e)))
tensorflow.python.autograph.impl.api.ConversionError: converting <tensorflow.python.eager.def_function.Function object at 0x00000280F0426DD8>: AttributeError: 'Function' object has no attribute '__code__'
```
**Describe the expected behavior**
autograph works

**Code to reproduce the issue**
```
import tensorflow as tf
@tf.function
def calc(x):
    return 0 if x <= 0.5 else 1

print(calc(2.5))
print(tf.autograph.to_code(calc))
```

Please take a look, Thank you.
"
30894,tf.TensorArray with Defun,"when using Defun under tf 1.x, the tf.TensorArray input will degrade to a Tensor.

```
xxxx output_tags : <dtype: 'float32'>
Tensor(""output_tas:0"", dtype=float32, device=/device:CPU:0)
[  FAILED  ] SpeechOpsFeatTest.test_splice
======================================================================
ERROR: test_splice (__main__.SpeechOpsFeatTest)
test_splice (__main__.SpeechOpsFeatTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""speech_ops_test.py"", line 202, in test_splice
    out = tffeat.splice(feat, left_context=2, right_context=2)
  File ""/delta/delta/data/feat/speech_ops.py"", line 376, in splice
    swap_memory=False)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py"", line 535, in __call__
    self.add_to_graph(ops.get_default_graph())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py"", line 516, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py"", line 348, in _create_definition_if_needed                                                                                                                             
    self._create_definition_if_needed_impl()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py"", line 379, in _create_definition_if_needed_impl                                                                                                                        
    capture_resource_var_by_value=self._capture_resource_var_by_value)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py"", line 915, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/delta/delta/data/feat/speech_ops.py"", line 357, in _loop_body
    new_output_tas = output_tas.write(time,  new_feat)
AttributeError: 'Tensor' object has no attribute 'write'
```

without Defun decorator,  everything is ok

```
xxxx output_tags : <dtype: 'float32'>
<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2528260908>
I0720 03:20:01.964524 139798580987712 speech_ops_test.py:203] splice : Tensor(""TensorArrayStack_1/TensorArrayGatherV3:0"", shape=(?, ?, 1, ?), dtype=float32)
[       OK ] SpeechOpsFeatTest.test_splice
```



```
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/fields.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/parsing.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timestamps.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/indexing.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/internals.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /usr/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/matplotlib/ft2font.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/kiwisolver.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/skiplist.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reduction.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/writers.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so
      8583:
      8583:
      8583:     calling fini: /usr/bin/python3 [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libffi.so.6 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.1 [0]
      8583:
      8583:
      8583:
      8583:
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libssl.so.1.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
      8583:
      8583:
      8583:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/conversion.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/c_timestamp.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/nattype.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/np_datetime.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timezones.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/tzconversion.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timedeltas.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/offsets.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/ccalendar.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/strptime.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/fields.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/parsing.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timestamps.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/indexing.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/internals.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /usr/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/matplotlib/ft2font.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/kiwisolver.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/skiplist.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so [0]
      8583:
      8583:
      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reduction.cpython-36m-x86_64-linux-gnu.so [0]




































== check python ===================================================
python version: 3.6.8
python branch:
python build version: ('default', 'Jan 14 2019 11:02:34')
python compiler version: GCC 8.0.1 20180414 (experimental) [trunk revision 259383
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Fri Sep 7 08:20:28 UTC 2018
os release version: 4.9.125-linuxkit
os platform: Linux-4.9.125-linuxkit-x86_64-with-Ubuntu-18.04-bionic
linux distribution: ('Ubuntu', '18.04', 'bionic')
linux os distribution: ('Ubuntu', '18.04', 'bionic')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='39917737c69c', release='4.9.125-linuxkit', version='#1 SMP Fri Sep 7 08:20:28 UTC 2018', machine='x86_64', processor='x86_64')                                                                                       
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 4.8.5-4ubuntu8) 4.8.5
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                         1.16.4   
protobuf                      3.8.0    
tensorflow                    1.14.0   
tensorflow-estimator          1.14.0   

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 1.14.0
tf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5
tf.version.COMPILER_VERSION = 4.8.5

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 1.14.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.6/dist-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(2, 7, 15, 'final', 0)

== bazel version  ===============================================
```
 "
30892,tf.keras.models.load_model fails on Sequential model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): `tf-nightly-gpu-2.0-preview==2.0.0-dev20190718`
- Python version: 3.7

**Describe the current behavior**
Attempt to run `tf.keras.models.load_model` on a `tf.keras.Sequential` with lazily generated inputs produces an error.

**Describe the expected behavior**
No error is thrown when running the script below. `model2` is a clone of `model`.

**Code to reproduce the issue**
```python
import tempfile

import tensorflow as tf


def main():
    batch_size = 3

    image_shape = (32, 32, 3)
    inputs = tf.random.uniform((batch_size, *image_shape))

    model = tf.keras.Sequential((
        tf.keras.layers.Conv2D(
            filters=16,
            kernel_size=3,
            strides=2,
            padding='SAME',
            activation='linear'),
    ))

    _ = model(inputs)

    with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:
        tf.keras.models.save_model(model, fd.name, overwrite=True)
        model2 = tf.keras.models.load_model(fd.name, compile=False)

    print(model2.summary())


if __name__ == '__main__':
    main()
```

**Other info / logs**
Running the script above produces the following error:
```
Traceback (most recent call last):
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/kristian/github/hartikainen/softlearning-2/tests/test_sequential_serialize.py"", line 31, in <module>
    main()
  File ""/home/kristian/github/hartikainen/softlearning-2/tests/test_sequential_serialize.py"", line 25, in main
    model2 = tf.keras.models.load_model(fd.name, compile=False)
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 138, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 165, in load_model_from_hdf5
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 671, in load_weights_from_hdf5_group
    ' layers.')
ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.
Exception ignored in: <function _TensorCacheDeleter.__del__ at 0x7f9fabdde158>
Traceback (most recent call last):
  File ""/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py"", line 316, in __del__
TypeError: argument of type 'NoneType' is not iterable
```"
30891,ffmpeg not found when build in Raspberry Pi 3 B+ Raspbian 10,"**System information**
- OS : Raspbian 10
- Device : Rasberry Pi 3 B+
- TensorFlow installed from source
- TensorFlow version: 1.9
- Python version: 3
- Installed using : I followed the instruction from https://www.tensorflow.org/install/source_rpi 

I followed the instruction from https://www.tensorflow.org/install/source_rpi   The commands are copy from the link as follows:

`git clone https://github.com/tensorflow/tensorflow.git`

`cd tensorflow`

`git checkout r1.9`

`CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4"" `
`    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \`
`    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`

I got the following error.

> Package ffmpeg is not available, but is referred to by another package.
> This may mean that the package is missing, has been obsoleted, or
> is only available from another source
> 
> E: Package 'ffmpeg' has no installation candidate
> The command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 100
> ERROR: docker build failed. Dockerfile is at /home/pi/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python3
"
30887,TF 2.0: tf.distribute example not working (NCCL all reduce issue),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0b1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.4
- GPU model and memory: Titan Xp

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I try to run one of the distributed examples, the model fails using the default NCCL all reduce cross devices ops:

<details>
        <summary> NCCL All Reduce Error </summary>

```
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-15-df42e8590c26> in <module>
----> 1 model.fit(train_dataset, epochs=50, callbacks=callbacks)

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--> 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    679           validation_steps=validation_steps,
    680           validation_freq=validation_freq,
--> 681           steps_name='steps_per_epoch')
    682 
    683   def evaluate(self,

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    292           else:
    293             actual_inputs = ins()
--> 294           batch_outs = f(actual_inputs)
    295         except errors.OutOfRangeError:
    296           if is_dataset:

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in execution_function(input_fn)
    811     def execution_function(input_fn):
    812       # `numpy` translates Tensors to values in Eager mode.
--> 813       return [out.numpy() for out in distributed_function(input_fn)]
    814     return execution_function
    815 

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    426         # Lifting succeeded, so variables are initialized and we can run the
    427         # stateless function.
--> 428         return self._stateless_fn(*args, **kwds)
    429     else:
    430       canon_args, canon_kwds = \

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1333     """"""Calls a graph function specialized to the inputs.""""""
   1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1336 
   1337   @property

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    587     """"""
    588     return self._call_flat(
--> 589         (t for t in nest.flatten((args, kwargs), expand_composites=True)
    590          if isinstance(t, (ops.Tensor,
    591                            resource_variable_ops.ResourceVariable))))

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    669     # Only need to override the gradient in graph mode and when we have outputs.
    670     if context.executing_eagerly() or not self.outputs:
--> 671       outputs = self._inference_function.call(ctx, args)
    672     else:
    673       self._register_gradient()

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    443             attrs=(""executor_type"", executor_type,
    444                    ""config_proto"", config),
--> 445             ctx=ctx)
    446       # Replace empty list with None
    447       outputs = outputs or None

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/.virtualenvs/physics3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InternalError: 2 root error(s) found.
  (0) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at <ipython-input-15-df42e8590c26>:1) ]]
  (1) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at <ipython-input-15-df42e8590c26>:1) ]]
	 [[GroupCrossDeviceControlEdges_4/Adam/Adam/update_9/Const/_679]]
0 successful operations.
9 derived errors ignored. [Op:__inference_distributed_function_9447]

Function call stack:
distributed_function -> distributed_function
```

</details>

Switching to hierarchical all reduce in the scope allows the code to run, but with a performance penalty and more errrors:

<details>
        <summary> Hierarchical All Reduce Error </summary>

```
2019-07-19 15:06:47.632060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-19 15:06:47.947109: W tensorflow/core/framework/model.cc:475] Failed to find a tunable parameter that would decrease the output time. This means that the autotuning optimization got stuck in a local maximum. The optimization attempt will be aborted.
2019-07-19 15:06:48.925479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-19 15:07:02.595311: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-07-19 15:07:02.610888: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
2019-07-19 15:07:07.254401: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 1045 kernel records, 173 memcpy records.
2019-07-19 15:07:12.732921: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_9/Const/_879]]
2019-07-19 15:07:12.732922: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_4/metrics/accuracy/AssignAddVariableOp_1/_375]]
2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[Adam/Adam/group_deps/_951]]
2019-07-19 15:07:12.733034: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_9/metrics/accuracy/AssignAddVariableOp_1/_203]]
2019-07-19 15:07:12.732989: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_631]]
2019-07-19 15:07:12.732981: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_359]]
2019-07-19 15:07:12.733029: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_16/_450]]
2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_9}}]]
2019-07-19 15:07:12.733232: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_9/Const/_727]]
2019-07-19 15:07:12.733380: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_763]]
2019-07-19 15:07:12.735619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
[I 15:08:10.336 LabApp] Saving file at /k
```

</details>

Further investigation has revealed that this only occurs when I add the 10th GPU on the node to the strategy... Using the *first* 9 GPUs is fine, but including the 10th GPU with any other combination of GPUs leads to this behavior. Confirmed on multiple independent systems after reinstalling cuda/drivers and restarting nodes. I still get an out-of-range error, but the model trains. The model also trains when *only* using the 10th gpu.

<details>

<summary> out of range error with nccl all reduce </summary>

```
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:09.708295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:09.710603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:09.712948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:09.715631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:09.717939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:09.720316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:09.720644: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:09.722035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:09.723341: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:09.723662: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:09.725225: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:09.726450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:09.730176: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:09.761821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:09.762583: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-07-20 12:10:11.263993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c306c0 executing computations on platform CUDA. Devices:
2019-07-20 12:10:11.264051: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264071: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264088: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264119: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264135: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264151: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.273304: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300010000 Hz
2019-07-20 12:10:11.275776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45d7fb0 executing computations on platform Host. Devices:
2019-07-20 12:10:11.275796: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-20 12:10:11.308790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:11.310775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:11.312785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:11.314726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:11.316719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:11.318675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:11.320680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:11.320722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.320732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:11.320741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:11.320767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:11.320778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:11.320787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:11.320797: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:11.347344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:11.347438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.362271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-20 12:10:11.362286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6
2019-07-20 12:10:11.362292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y
2019-07-20 12:10:11.362296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y
2019-07-20 12:10:11.362300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y
2019-07-20 12:10:11.362304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y
2019-07-20 12:10:11.362309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y
2019-07-20 12:10:11.362313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y
2019-07-20 12:10:11.362317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N
2019-07-20 12:10:11.378594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11424 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)
2019-07-20 12:10:11.380969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11424 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.385188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11424 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.389861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11424 MB memory) -> physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.393361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 11424 MB memory) -> physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.397087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 11424 MB memory) -> physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)
2019-07-20 12:10:11.400702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 11424 MB memory) -> physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)
2019-07-20 12:10:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:11.581543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:11.583872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:11.585869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:11.587873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:11.589889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:11.591870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:11.591903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.591913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:11.591921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:11.591931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:11.591957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:11.591967: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:11.591978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:11.618747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:11.619236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-20 12:10:11.619246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6
2019-07-20 12:10:11.619255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y
2019-07-20 12:10:11.619261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y
2019-07-20 12:10:11.619292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y
2019-07-20 12:10:11.619297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y
2019-07-20 12:10:11.619303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y
2019-07-20 12:10:11.619320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y
2019-07-20 12:10:11.619326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N
2019-07-20 12:10:11.635672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 11424 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)
2019-07-20 12:10:11.637653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 11424 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.639716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 11424 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.641688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 11424 MB memory) -> physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.643693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 11424 MB memory) -> physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.645687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 11424 MB memory) -> physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)
2019-07-20 12:10:11.647730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 11424 MB memory) -> physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)
2019-07-20 12:10:26.189075: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-20 12:10:26.273130: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:28.489688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:36.757915: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-07-20 12:10:36.759100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
2019-07-20 12:10:40.328991: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 722 kernel records, 110 memcpy records.
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_6/Const/_297]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_187]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_5/Adam/Adam/update_5_1/Const/_365]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[PermConstNCHWToNHWC-LayoutOptimizer/_16]]
2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_6/Const/_421]]
2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_2/Identity_1/_523]]
2019-07-20 12:10:45.501354: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[replica_5/loss/mul/_260]]
2019-07-20 12:10:45.502198: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
```
</details>

**Describe the expected behavior**

**Code to reproduce the issue**

https://www.tensorflow.org/beta/tutorials/distribute/keras
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30885,Possible GPU Memory Leak in Non-Distributed Use of `tf.estimator.train_and_evaluate`,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.10.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: K80 12GB

**Describe the current behavior**

When running `tf.estimator.train_and_evaluate` it seems the graph or session is somehow not properly destroyed when `max_steps` is `None` or less than the number of steps in the `input_fn`.  This can result in OOM errors when running prediction afterward on GPU.

**Describe the expected behavior**

Using `tf.estimator.train_and_evaluate` should destroy the graph/session when either the max steps has been reached, OR the input_fn raises an `OutOfRangeError`

**Code to reproduce the issue**

`python min.py bad`

For expected behavior:

`python min.py good`

NOTE: Your running the bad code may or may not crash depending on your GPU memory.  To force crashing, you can increase the array size: `np.random.rand(xx, yy)`

```python
import sys

import tensorflow as tf
import numpy as np

from hooks import InitHook

def input_fn():
  dataset = tf.data.Dataset.range(100)
  # Make sequence data
  dataset = dataset.map(lambda x: {'x': [x]})
  dataset = dataset.repeat(3)
  return dataset

def model_fn(features, labels, mode, params):
  seq = features['x']
  with tf.device('/gpu:0'):
    arr = np.random.rand(3000000, 400)
    var = tf.get_variable('big', arr.shape, trainable=False)
    emb = tf.nn.embedding_lookup(var, seq)
  logits = tf.layers.dense(emb, 1000)
  predictions = tf.greater(logits, 0.0)
  # Don't care about loss but have to provide something
  loss = tf.reduce_mean(logits)
  trainable_vars = tf.trainable_variables()
  global_step = tf.train.get_or_create_global_step()
  saveable_vars = trainable_vars + [global_step]
  def init_fn(scaffold, sess):
    sess.run(var.initializer, {var.initial_value: arr})
  saver = tf.train.Saver(var_list=saveable_vars)
  if mode == tf.estimator.ModeKeys.TRAIN:
    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver)
    optimizer = tf.train.GradientDescentOptimizer(0.1)
    train_op = optimizer.minimize(loss, global_step=global_step)
    output_spec = tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      scaffold=scaffold,
      train_op=train_op)
  elif mode == tf.estimator.ModeKeys.EVAL:
    hooks = [InitHook(var.initializer, {var.initial_value: arr})]
    ready_for_local_init_op = tf.constant([], dtype=tf.string)
    ready_op = tf.constant([], dtype=tf.string)
    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver, ready_for_local_init_op=ready_for_local_init_op, ready_op=ready_op)
    output_spec = tf.estimator.EstimatorSpec(
      scaffold=scaffold,
      mode=mode,
      evaluation_hooks=hooks,
      loss=loss)
  elif mode == tf.estimator.ModeKeys.PREDICT:
    ready_for_local_init_op = tf.constant([], dtype=tf.string)
    ready_op = tf.constant([], dtype=tf.string)
    scaffold = tf.train.Scaffold(ready_for_local_init_op=ready_for_local_init_op, ready_op=ready_op, saver=saver)
    hooks = [InitHook(var.initializer, {var.initial_value: arr})]
    predictions = {
      'predictions': predictions
    }
    output_spec = tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=predictions,
      scaffold=scaffold,
      prediction_hooks=hooks)
  return output_spec

tf.logging.set_verbosity(tf.logging.INFO)
estimator = tf.estimator.Estimator(model_fn=model_fn, config=tf.estimator.RunConfig())
eval_spec = tf.estimator.EvalSpec(input_fn=input_fn, start_delay_secs=0, throttle_secs=3)
if sys.argv[1] == 'bad':
  train_spec = tf.estimator.TrainSpec(input_fn=input_fn)
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
elif sys.argv[1] == 'good':
  train_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=100)
  estimator.evaluate(input_fn=input_fn)
results = estimator.predict(input_fn=input_fn)
for result in results:
  pass
```

Edit:
In the same directory is `hooks.py` containing the following:
```python
class InitHook(training.SessionRunHook):
  def __init__(self, op, feed_dict):
    self.op = op
    self.feed_dict = feed_dict

  def after_create_session(self, session, coord):  # pylint: disable=unused-argument
    session.run(self.op, feed_dict=self.feed_dict)
```"
30883,How to define a ReLU with TensorFlow custom_gradient?,"I'm practicing using TensorFlow's custom_gradient decorator and I tried to define a simple ReLU. One would think it would be as simple as defining the gradient to be 1 when x > 0 and 0 otherwise. However, the following code does not yield the same gradients as a ReLU:

```
@tf.custom_gradient
def relu(x):
    def grad(dy):
        return tf.cond(tf.reshape(x, []) > 0,
                       lambda: tf.cast(tf.reshape(1, dy.shape), tf.float32),
                       lambda: tf.cast(tf.reshape(0, dy.shape), tf.float32))
    return tf.nn.relu(x), grad
```

Can someone explain to me why this standard definition of ReLU's gradient does not yield the same performance as:

```
@tf.custom_gradient
def relu(x):
    def grad(dy):
        return dy
    return tf.nn.relu(x), grad
```"
30882,Dense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- TensorFlow installed from (source or binary): from pip install
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

**Describe the current behavior**
A note in `Dense` [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) says that 
> Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.

I don't see this happening in real life. Instead, `Dense` behaves on a 3-rank tensor as it would behave if it was wrapped in a `TimeDistributed` layer, making me question the utility of `TimeDistributed` at all.

**Describe the expected behavior**
`Dense` should flatten its input like the documentation says. In the first example bellow, the shape of the kernel weights of `dense` should be `(5 * 3, 2)` = `(15, 2)` instead of `(3, 2)`, which is the shape of `dense2` (as expected in the case of `dense2`).

**Code to reproduce the issue**

First example:
```
import tensorflow as tf
import numpy as np

print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

tf.random.set_seed(12)
np.random.seed(12)

init = tf.keras.initializers.GlorotUniform(seed=12)

inp = tf.constant(np.random.normal(0, 1, (1, 5, 6)))
inp = tf.cast(inp, dtype=tf.float32)

gru = tf.keras.layers.GRU(3, return_sequences=True)(inp)
print(gru.shape)
#(1, 5, 3)

dense = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)
print(dense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)

for w in dense.weights:
    print(w.shape)
#(3, 2) instead of (5 * 3, 2) if Dense indeed flattened its input
#(2,)

tddense = tf.keras.layers.TimeDistributed(dense)
print(tddense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)
# if Dense kernel had shape (15, 2), this should result in the following error:
# InvalidArgumentError: Matrix size-incompatible: In[0]: [5,3], In[1]: [15,2] [Op:MatMul]
# but instead what we get is the same output
# than without TimeDistributed, without error

dense2 = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)
tddense = tf.keras.layers.TimeDistributed(dense2)
print(tddense(gru))
#tf.Tensor(
#[[[ 1.5456871  -0.5280464 ]
#  [ 0.11647969 -0.20553198]
#  [ 0.58126366 -0.16031623]
#  [-0.22882831 -0.22649539]
#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)

for w in dense2.weights:
    print(w.shape)
#(3, 2) as expected
#(2,)
```

Second example, with a rank even larger than 3:
```
import tensorflow as tf

print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))

inp = tf.keras.Input(shape=(10, 25, 25, 3))
dense_layer1 = tf.keras.layers.Dense(78)
x = dense_layer1(inp)
print('Output shape without TimeDistributed:')
print(x.shape)

dense_layer2 = tf.keras.layers.Dense(78)
y=tf.keras.layers.TimeDistributed(dense_layer2)(inp)
print('Output shape with TimeDistributed:')
print(y.shape)

print('Weight shapes without TimeDistributed:')
for weight in dense_layer1.trainable_weights:
    if len(weight.shape) == 2:
        print('    kernel shape:')
    else:
        print('    bias shape:')
    print(weight.shape)
    
print('Weight shapes with TimeDistributed:')
for weight in dense_layer2.trainable_weights:
    if len(weight.shape) == 2:
        print('    kernel shape:')
    else:
        print('    bias shape:')
    print(weight.shape)
```

which outputs is:
```
Using Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7)
Output shape without TimeDistributed:
(None, 10, 25, 25, 78)
Output shape with TimeDistributed:
(None, 10, 25, 25, 78)
Weight shapes without TimeDistributed:
    kernel shape:
(3, 78)
    bias shape:
(78,)
Weight shapes with TimeDistributed:
    kernel shape:
(3, 78)
    bias shape:
(78,)
```
We see, in this example, that `Dense` and `TimeDistributed(Dense)` behave the same in that they only touch to the last dimension of the input.


"
30881,"Inference on TFLite object detection model fails on run : ""(num_classes_with_background - num_classes <= 1) was not true""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, however greatly adapted from scripts
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo PB2-690M Android 6.0.1 API 23
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA toolkit 10.0.130 / cuDNN 7.3.1
- GPU model and memory: Intel i7-6700 3.7GHz 16Go

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The script is in two parts :
First few pictures are taken on phone to re-train a model with overfiting (ssd_resnet_50_fpn_coco) on the server, which produces a .tflite model, then sends it back to the device. 

Problem emerges when I try to run the inference with a newly taken image and the .tflite model as in the associated TFLite object detection example script : 
Caused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/detection_postprocess.cc:688 (num_classes_with_background - num_classes <= 1) was not true.Node number 228 (TFLite_Detection_PostProcess) failed to invoke. 
at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:164)

**Describe the expected behavior**
As you can guess, it would be expected to run without failure. Note that using the .tflite model given by the TFLite object detection example script produces no error with the same picture (although it wouldn't yield any pertinent result). 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Server Python Part, used to convert the exported graph into .tflite file

  ```
    graph_def_file=os.path.join(BOX_PATH, ""tflite_graph.pb"")
    input_arrays=[""normalized_input_image_tensor""]
    output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
    input_tensor={""normalized_input_image_tensor"":[1, 640, 640, 3]}
    converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_tensor)
    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.allow_custom_ops = True;
    tflite_quant_model = converter.convert()
    open(os.path.join(BOX_PATH, ""frozen_inference_graph.tflite"", ""wb"").write(tflite_quant_model)

```
Here is the principal change in script for the inference run (Java code part):
[TFLiteObjectDetectionAPIModel.txt](https://github.com/tensorflow/tensorflow/files/3411835/TFLiteObjectDetectionAPIModel.txt)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Inference script taken from https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android
Model tested with following script prior to .tflite conversion
https://github.com/tiangolo/tensorflow-models/blob/master/research/object_detection/object_detection_tutorial.ipynb

Here it is, thanks for your reply and help !
"
30879,"TFLite conversion: AudioSpectogram, Mfcc, RandomUniform not supported","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.13


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DIV, FLOOR, FULLY_CONNECTED, MAX_POOL_2D, MUL, RESHAPE, SHAPE, SUB. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, Mfcc, RandomUniform.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30877,Feature request: 24-bit audio support in tf.audio.decode_wav,"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I'm working with music data of ""DVD-quality"" i.e. 48 kHz / 24-bit WAV and want to read the files into a `tf.data` pipeline for model training via `tf.io.read_file` and `tf.audio.decode_wav` but unfortunately I got caught up on this exception:

> InvalidArgumentError: Can only read 16-bit WAV files, but received 24 [Op:DecodeWav]

While mastered mixes tend to end up being 44.1 kHz / 16-bit, in my experience 24-bit audio is extremely common among music producers as that format has a good trade-off between disk space and noise floor so the mic gain isn't as finicky etc.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Audio researchers

**Any Other info.**
"
30876,tensorflow issue. I have installed keras and tensorflow but still get issues loading data,"ImportError                               Traceback (most recent call last)
~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\envs\AirtificialIntelligence\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\Anaconda3\envs\AirtificialIntelligence\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-17-725e8432e2c0> in <module>
----> 1 from keras.models import Sequential
      2 from keras.layers import Dense
      3 from keras.layers import Flatten
      4 from keras.layers import Dropout
      5 #from keras.layers.convolutional import Conv1D

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\keras\__init__.py in <module>
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\keras\utils\__init__.py in <module>
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\keras\utils\conv_utils.py in <module>
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\keras\backend\__init__.py in <module>
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\keras\backend\tensorflow_backend.py in <module>
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\PIPES\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\PIPES\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\PIPES\Anaconda3\envs\AirtificialIntelligence\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\PIPES\Anaconda3\envs\AirtificialIntelligence\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\PIPES\Anaconda3\envs\AirtificialIntelligence\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.
"
30875,getting unexpected result from customize pre-Training model form Keras (Xception model) in C++ after freezing the model .pb .,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution  (debian 9.1):
- TensorFlow installed from (source):
- TensorFlow version (13.1):
- Python version: python 3.5
- Bazel version (11):
- CUDA/cuDNN version: 10
- GPU model and memory: 32 GB, GTX 1080 TI

**Describe the current behavior**
I am wondering why I can not get the same result when i run the frozen model in c++. I do not know exactly if the tf.keras does preprocessing step when I load the model.  


**Python code: **
-  creat tfrecord:
def get_example_object(image_string, label): ##label is in integer value( from 1 to number of classes )
  feature = {'image':  _bytes_feature(tf.compat.as_bytes(image_string.tostring())),
           'label':  _int64_feature(int(label))}
  return tf.train.Example(features=tf.train.Features(feature=feature))
image = cv2.imread(img_path)
image= cv2.resize(image, (required_height, required_width))

-  training:
def XceptionCNN(IMG_SIZE,OUTPUT_SIZE):

    INPUT_SHAPE = (IMG_SIZE, IMG_SIZE,3)
    input = tf.keras.layers.Input(
        shape=INPUT_SHAPE, dtype=tf.float32,name='input') # 

    Xception_model = Xception(input_tensor=input, include_top=False,weights=None) 
    print('XceptionCNN Model loaded.')
    print(Xception_model.summary())
    x = Xception_model.output    
    x = GlobalAveragePooling2D()(x)
    return model
def _parse_function(proto):
    image_size=150
    class_number =16
    keys_to_features = {'image': tf.FixedLenFeature([], tf.string),
                        ""label"": tf.FixedLenFeature([], tf.int64)}

    parsed_features = tf.parse_single_example(proto, keys_to_features)
    image = tf.decode_raw(parsed_features['image'], tf.uint8)
    image = tf.cast(image, tf.float32) * (1. / 255)
    image = tf.reshape(image, shape=(image_size, image_size, 3))
 
    label = tf.cast(parsed_features[""label""], tf.int32)
    onehot_labels = tf.one_hot(tf.cast(label, tf.int32), class_number)
    return image, onehot_labels

def get_dataset(filepath,epochs, batch_size ):
    dataset = tf.data.TFRecordDataset(filepath)
    dataset = dataset.map(_parse_function, mt.cpu_count())
    dataset = dataset.repeat(epochs).shuffle(1000).batch(batch_size)
    return dataset

**CPP code: **
-  creat float tensor:
        cv::cvtColor(rgbImage, inputImage, CV_RGB2BGR);
        cv::Mat img(imageTargetSize, CV_32FC1, dataTensor);
         resizedPic.convertTo(img, CV_32FC3, 1.0 / 255, 0);
        dataTensor += imageTargetSize.width * imageTargetSize.height * depth;

**output**
- The Training process:
2019-07-18 13:07:35.544638: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2306/2307 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9935     
Epoch 00001: val_loss improved from inf to 0.00693, saving model to /mnt/4TB_HD/Training/flaps_keras/txt/model/class_net_normal/weights
2307/2307 [==============================] - 1074s 466ms/step - loss: 0.0217 - acc: 0.9935 - val_loss: 0.0069 - val_acc: 0.9979
Epoch 2/4
2306/2307 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981       
Epoch 00002: val_loss did not improve from 0.00693
2307/2307 [==============================] - 1029s 446ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.0162 - val_acc: 0.9968
Epoch 3/4
2306/2307 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988   
Epoch 00003: val_loss improved from 0.00693 to 0.00427, saving model to /mnt/4TB_HD/Training/flaps_keras/txt/model/class_net_normal/weights
2307/2307 [==============================] - 1008s 437ms/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0043 - val_acc: 0.9991
Epoch 4/4
2306/2307 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991       
Epoch 00004: val_loss did not improve from 0.00427
2307/2307 [==============================] - 1012s 439ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0090 - val_acc: 0.9986
tf_input: Tensor(""input:0"", shape=(?, 150, 150, 3), dtype=float32)
model_input: <tensorflow.python.keras.engine.training.Model object at 0x7f8fac3f2f98>
tf_output: Tensor(""dense/Softmax:0"", shape=(?, 16), dtype=float32)
model_output: dense/Softmax

"
30874,"base64 strings contains  "" ""","## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/io/decode_base64

## Description of issue (what needs changing):
Some base64 strings contains  "" "".
### Clear description
It could be useful to add this information after this paragraph:

> Input may or may not have padding at the end. See EncodeBase64 for padding. Web-safe means that input must use - and _ instead of + and  /.


"
30873,TF2: Memory leak during implicit conversion from EagerTensor to numpy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: 3.7
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: GTX 1080Ti


Running the following code on GPU causes a memory leak on my machine during implicit conversion to numpy. Leak does not manifest when doing explicit conversion (`r = np.sum(x.numpy())`).

**Code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

N = 100000

x = tf.zeros((10000)).gpu()
for i in range(N):
    r = np.sum(x)
```

"
30872,cna't use tf.image.resize_images in TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04.2
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):1.13.1


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, FULLY_CONNECTED, MEAN, MUL, RSQRT, SQUARED_DIFFERENCE, SQUEEZE, SUB. Here is a list of operators for which you will need custom implementations: ResizeArea.
```

I think the error happened because of using function of tf.image.resize_images:
```stft = tf.image.resize_images(stft, (128, 128), method=3)```

"
30871,TensorRT INT8 Calibration Table is Missing,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v1.13.0-rc1-0-g54c5616308
- Python version:3.5.2
- CUDA/cuDNN version: CUDA 10
- GPU model and memory: Tesla V100 16GB

**Describe the current behavior**
TFTRT failed to collect calibration info and cannot create the INT8 inference graph. 

**Code to reproduce the issue**
Here is the model [link](https://drive.google.com/drive/folders/1hi23VeNomhHGj6Ai94Z44S6NRJ6uByJ5?usp=sharing).
```
def run_calibration(calib_graph, dataset):
    tf.reset_default_graph()
    tf_config = tf.ConfigProto()
    tf_config.gpu_options.allow_growth = True
    x = np.random.randint(0,10000,(1,40))
    with tf.Graph().as_default() as g:
        input, output = tf.import_graph_def(graph_def=calib_graph, return_elements=[""X"", ""model/dense/BiasAdd""],
                                            name='')
        input = input.outputs[0]
        output = output.outputs[0]
        sess = tf.Session(config=tf_config, graph=g)

        for i in range(10):
            val = sess.run(output, {input: x})
        return calib_graph


def int8quant():
    with tf.Session() as sess:
        saver = tf.train.import_meta_graph(""/workspace/exps/ptb/ptb_rnn_128/ptb_rnn_128-72600.meta"")
        saver.restore(sess, ""/workspace/exps/ptb/ptb_rnn_128/ptb_rnn_128-72600"")
        your_outputs = ['model/dense/BiasAdd']
        frozen_graph = tf.graph_util.convert_variables_to_constants(
            sess,
            tf.get_default_graph().as_graph_def(),
            output_node_names=your_outputs)
        trt_graph = trt.create_inference_graph(
            input_graph_def=frozen_graph,
            outputs=your_outputs,
            max_batch_size=10,
            max_workspace_size_bytes=2 << 30,
            precision_mode='INT8',
            minimum_segment_size=2  # minimum number of nodes in an engine
       )
    int8graph = run_calibration(trt_graph, None)
    int8_graph = trt.calib_graph_to_infer_graph(int8graph)
```

**Other info / logs**
Here is the trace logs.
```
2019-07-19 03:47:34.167937: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Tensor TensorRTOutputPH_0cannot be both input and output
2019-07-19 03:47:34.167960: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Network must have at least one output
2019-07-19 03:47:38.553760: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Tensor (Unnamed ITensor* 3) is uniformly zero; network calibration failed.
2019-07-19 03:47:38.578654: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:220] Calibration table is empty
Traceback (most recent call last):
  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/workspace/code/rnnquant/trt/convert.py"", line 99, in <module>
    main()
  File ""/workspace/code/rnnquant/trt/convert.py"", line 95, in main
    int8quant()
  File ""/workspace/code/rnnquant/trt/convert.py"", line 89, in int8quant
    int8_graph = trt.calib_graph_to_infer_graph(int8graph)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 416, in calib_graph_to_infer_graph
    int(msg[0]))
tensorflow.python.framework.errors_impl.UnknownError: Calibration table is missing. This shouldn't have happened!

```

"
30870,Post-Training Integer Quantization on Models for 'Edge' / Coral TPUs,"**System information**
- Have I written custom code: Yes (below)
- OS Platform and Distribution: Arch
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version: v2.0.0-beta0-17-g8e423e3
- Python version: 3.7.3

**Description**
I'm interested in using TF2.0's ```tf.function``` to build graphs that can be run on an Edge TPU. As that device only supports (8-bit) integer quantized models, I'm trying to use the relatively new [post-training integer quantization](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba). That Medium post states,
> Our new post-training integer quantization enables users to take an already-trained floating-point model and fully quantize it to only use 8-bit signed integers (i.e. `int8`). ... Fixed point hardware accelerators, such as Edge TPUs, will also be able to run these models.

However, even a simple proof-of-concept graph doesn't seem to be accepted by the [Coral web compiler](https://coral.withgoogle.com/web-compiler) (with no useful error messages). Unless this is an issue with the Coral site, I wanted to write in here—whether this is a bug or a not-yet-implemented feature, in hopes that it may be resolved. Am I doing anything unsupported at this time? Or, has anyone been able to compile a model for Coral/Edge that may be able to serve as an example?

**Code to reproduce the issue**
```python
import pathlib
import tensorflow as tf

@tf.function
def simple_layer(x):
    return tf.nn.relu(2 * x + 1)

c_fn = simple_layer.get_concrete_function(tf.TensorSpec(shape=[1], dtype=tf.float32))

converter = tf.lite.TFLiteConverter.from_concrete_functions([c_fn])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
my_dataset = tf.data.Dataset.from_tensor_slices(np.arange(-100, 100, 0.01, dtype=np.float32))
my_dataset = my_dataset.batch(1)

def rep_dataset():
    for val in my_dataset.shuffle(tf.data.experimental.cardinality(my_dataset)).take(1000):
        yield [val]
converter.representative_dataset = rep_dataset

tflite_quant_model = converter.convert()

dir = 'supersimple_coral'
tflite_models_dir = pathlib.Path(dir)
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_file = tflite_models_dir/""model_quant.tflite""
tflite_model_file.write_bytes(tflite_quant_model)
```"
30869,"""'NoneType' object has no attribute '_fetch_cloud_tpu_metadata'"" when using TPU","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: Cloud TPU v2

**Describe the current behavior**
I've been using T2T training script using cloud TPU. After upgrading to TF 1.14.0 from TF 1.13.1, I'm getting the error below:

```
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/app/poodle/asr/t2t/t2t_train_venv.image.binary.runfiles/skelterlabs/poodle/asr/t2t/py-pkg/tensorflow/python/tpu/preempted_hook.py"", line 86, in run
    response = self._cluster._fetch_cloud_tpu_metadata()  # pylint: disable=protected-access
AttributeError: 'NoneType' object has no attribute '_fetch_cloud_tpu_metadata'
```

The training script continues w/o any further issue despite the error.

**Describe the expected behavior**
No error.

**Code to reproduce the issue**
Run https://github.com/tensorflow/tensor2tensor#speech-recognition problems using transformer model on TPUs.

**Other info / logs**
N/A"
30868,"Tensorflow on Jetson TX2's performance suddenly decrease, don't know reason ","A week ago, I run my face-id project, that achieved 7fps, but now, after one week, I dont know why my project just achieve 3-4 fps, I dont modify source code and any dependency library.

**Project**: Face-Id based on Tensorflow (FaceNet)
**OS**: Ubuntu 16.04
**Board**: Jetson TX2
**Cuda/CuDNN**: 9.0/7.1.5.14
**Tensorflow**: 1.9.0"
30866,Tensorflow 2.0 add regularization losses,"<em>In Tensorflow 1.x, I can add regularization losses by using code like this:
`regularization_loss = tf.add_n(tf.losses.get_regularization_losses(), 'regu')`
`total_loss = loss + regularization_loss`
But in tensorflow 2.0.0beta1 api, the 'losses.get_regularization_losses()' was canceled.So how can I add that loss In this case?


**System information**
- TensorFlow version :2.0.0beta1

"
30865,TF2.0 is graph_transform still available in TF 2.0?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
30864,How can I get h5 file to tflite file. I tried to do with the documentation but it is giving me an error,"`from keras.models import load_model
keras_file =""project.h5""
keras.models.save_model(model,keras_file)
from tensorflow import lite
coverter = lite.TFLiteConverter.from_keras_model_file(keras_file)`

This is the error I am getting

```
`ValueError                                Traceback (most recent call last)
<ipython-input-27-d2fc0cb4c75c> in <module>
----> 1 coverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)

/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)
    741 
    742       frozen_func = _convert_to_constants.convert_variables_to_constants_v2(
--> 743           concrete_func)
    744       _set_tensor_shapes(frozen_func.inputs, input_shapes)
    745       return cls(frozen_func.graph.as_graph_def(), frozen_func.inputs,

/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)
    164         input_name = get_name(map_name_to_node[input_name].input[0])
    165       if map_name_to_node[input_name].op != ""Placeholder"":
--> 166         raise ValueError(""Cannot find the Placeholder op that is an input ""
    167                          ""to the ReadVariableOp."")
    168       # Build a map of Placeholder ops that are inputs to ReadVariableOps to the

ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.
```




This is my keras model.
```
model = keras.Sequential()
model.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=Combined.shape[1]))
model.add(keras.layers.SpatialDropout1D(0.2))
model.add(keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(keras.layers.Dense(11, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
epochs = 40
batch_size = 64
history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])


Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 50, 100)           40000     
_________________________________________________________________
dropout (Dropout)            (None, 50, 100)           0         
_________________________________________________________________
lstm (LSTM)                  (None, 100)               80400     
_________________________________________________________________
dense (Dense)                (None, 11)                1111      
=================================================================
```
`"
30863,tf.test.is_gpu_available return False,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2 AMI
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3
- TensorFlow version:tensorflow-gpu-1.14.0
- Python version:3.7.3
- Installed using virtualenv? pip? conda?:pip3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:Nvidia Tesla K80, 11441MiB



**Describe the problem**
tf.test.is_gpu_available returns Flase

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python3
import tensorflow as tf
tf.test.is_gpu_available()

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

<img width=""688"" alt=""Screen Shot 2019-07-18 at 17 21 57"" src=""https://user-images.githubusercontent.com/22419710/61493054-ae7f5800-a980-11e9-9234-175b26c624c3.png"">
"
30861,representative_dataset error for TFlite converter quantization,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Titan Xp, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
My representative_data_gen() iterate through a dataset that i created with some custom images and I set converter.representative_dataset with the function and convert the frozen model to tflite with int8 quantization. It produces the following error:

> Traceback (most recent call last):
>   File ""./src/freeze_graph.py"", line 100, in <module>
>     tflite_quant_model = converter.convert()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 908, in convert
>     inference_output_type)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 200, in _calibrate_quantize_model
>     inference_output_type, allow_float)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py"", line 76, in calibrate_and_quantize
>     self._calibrator.FeedTensor(calibration_sample)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 112, in FeedTensor
>     return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
> ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 98, name: image_input 

I tried to only quantize the weights with tf.lite.Optimized set to OPTIMIZE_FOR_SIZE and without setting representative_dataset option. It can produce tflite model successfully.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
train = []
path = '/home/liuyanqi/caffe/pyramid_cnn/data/adversarial'
for i in range(1, 11):
    filename = os.path.join(path, 'exp{:03d}_B'.format(i), 'scenergb.jpg' )
    im = cv2.imread(filename)
    im = im.astype(np.float32, copy=False)
    input_image = im - mc.BGR_MEANS
    train.append(input_image)

train = tf.convert_to_tensor(np.array(train, dtype='float32'))
my_ds = tf.data.Dataset.from_tensor_slices((train)).batch(1)

#POST TRAINING QUANTIZATION
def representative_dataset_gen():
    for input_value in my_ds.take(10):
        yield [input_value]



graph_def_file = '/tmp/logs/+zynqDet+tmp/train/freeze_graph.pbtxt'
input_arrays = [""image_input""]
# # output_arrays = [""probability/final_class_prob_concat"",""IOU_1/det_boxes_concat""]
output_arrays = [""predictor/bias_add"",""predictor_1/bias_add"", ""predictor_2/bias_add""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays, input_shapes={input_arrays[0]:[1, 480, 640, 3]})
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30858,Nested Layer saving with tf.train.Checkpoint,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.6

**Describe the current behavior**
tf.train.CheckPoint fails to save variables associated with the child layer, but displays the variables associated with it. 

**Describe the expected behavior**
tf.train.CheckPoint should save variables associated with the child layer

I was trying to compare model saving between nested and non-nested `tf.keras` layers, since I need a nested layer for my task.

I put together a simple script and its corresponding output. 

In the first case, the `Linear` Layer is its own parent and in the second case, the `Linear` layer is made the child of an `NestedLinear` layer. A `tf.keras.Model` is setup with the inputs and outputs and the trainable variables are printed out. After initializing, I attempt to save a checkpoint and list the variables in the checkpoint. 

If you see the output, the number of trainable variables are the same in both cases and the child layer variables are joined to the parent layer.

I assumed that the Checkpoint saving would work in a similar fashion with gathering all variable tensors and having a mapping between the Tensor and its position in the graph topology and save them but the checkpoints fail to save the `Linear` layer variables in the second case.

Maybe there is something I missed, and would like some help on this.

**Code**
```
import tensorflow as tf
import numpy as np
from tensorflow.keras import layers
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class Linear(layers.Layer):

  def __init__(self, units=32, input_dim=32):
    super(Linear, self).__init__()
    self.w = self.add_weight(shape=(input_dim, units),
                             initializer='random_normal',
                             trainable=True, name=""W"")
    self.b = self.add_weight(shape=(units,),
                             initializer='zeros',
                             trainable=True, name='b')

  def call(self, inputs):
    return tf.matmul(inputs, self.w) + self.b

class NestedLinear(layers.Layer):

  def __init__(self):
    super(NestedLinear, self).__init__()
    self.linear_1 = Linear(32)

  def call(self, inputs):
    x = self.linear_1(inputs)
    return x

inputs = layers.Input(shape=(32,))

print(""SAVING SINGLE LAYER"")
outputs = Linear(32)(inputs)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
for var in model.trainable_variables:
    print(var)
ckpt = tf.train.Checkpoint(model=model)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
with sess.as_default():
    ckpt.save(""ckpt"")

for var in tf.train.list_variables(""./""):
    print(var)

print(""--------------------------------------"")

nl_layer = NestedLinear()
outputs = nl_layer(inputs)

print(""SAVING NESTED LAYER"")

model = tf.keras.Model(inputs=inputs, outputs=outputs)
for var in model.trainable_variables:
    print(var)
ckpt = tf.train.Checkpoint(model=model)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
with sess.as_default():
    ckpt.save(""ckpt"")

for var in tf.train.list_variables(""./""):
    print(var)
```
**Output**

```
SAVING SINGLE LAYER
<tf.Variable 'W:0' shape=(32, 32) dtype=float32>
<tf.Variable 'b:0' shape=(32,) dtype=float32>
('_CHECKPOINTABLE_OBJECT_GRAPH', [])
('model/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('model/layer-0/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('model/layer_with_weights-0/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('model/layer_with_weights-0/W/.ATTRIBUTES/VARIABLE_VALUE', [32, 32])
('model/layer_with_weights-0/b/.ATTRIBUTES/VARIABLE_VALUE', [32])
('save_counter/.ATTRIBUTES/VARIABLE_VALUE', [])
--------------------------------------
SAVING NESTED LAYER
<tf.Variable 'W_1:0' shape=(32, 32) dtype=float32>
<tf.Variable 'b_1:0' shape=(32,) dtype=float32>
('_CHECKPOINTABLE_OBJECT_GRAPH', [])
('model/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('model/layer-0/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('model/layer_with_weights-0/.ATTRIBUTES/OBJECT_CONFIG_JSON', [])
('save_counter/.ATTRIBUTES/VARIABLE_VALUE', [])
```
"
30857,TF2: Out of Memory using model.fit with class_weight parameter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
```
Yes
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
Container derived from nvidia/cuda:10.0-cudnn7-runtime-ubuntu18.04 running on Ubuntu 18.04 via Kubernetes 1.13
```
- TensorFlow installed from (source or binary):
```
pip3 install tensorflow-gpu==2.0.0-beta1
```
- TensorFlow version (use command below):
```
v1.12.1-6250-g37eafe0e74 2.0.0-dev20190715
```
- Python version:
```
Python 3.6.8
```
- CUDA/cuDNN version:
```
NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0
```
```
        libcudnn.so.7 -> libcudnn.so.7.6.0
libcudnn is installed
```
- GPU model and memory:
```
name, pci.bus_id, vbios_version
Tesla V100-SXM2-32GB, 00000000:1B:00.0, 88.00.43.00.03
```

**Describe the current behavior**
Doing a training with tf.keras results in out of memory after some time when including the `class_weight` parameter. Also there is a long delay between the start of each epoch. If I omit the `class_weight` parameter, training proceeds normally with constant memory.

**Describe the expected behavior**
Training proceed normally when the `class_weight` parameter is included without running out of memory.

**Code to reproduce the issue**
```
import tensorflow as tf

################ Data
def _parse_fn2(fn, label):
    img = tf.random.uniform([224, 224, 3])
    return img, label

train_data2 = tf.data.Dataset.from_tensor_slices(
  (tf.random.uniform([100]), tf.random.uniform([100], maxval=9, dtype=tf.dtypes.int32))
)

val_data2 = tf.data.Dataset.from_tensor_slices(
  (tf.random.uniform([100]), tf.random.uniform([100], maxval=9, dtype=tf.dtypes.int32))
)
train_data2 = (train_data2.map(_parse_fn2)).batch(32)
val_data2 = (val_data2.map(_parse_fn2)).batch(32)

############### Model
IMG_SHAPE = (224, 224, 3)

base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,include_top=False, weights=None)
base_model.trainable = True
maxpool_layer = tf.keras.layers.GlobalMaxPooling2D()
prediction_layer = tf.keras.layers.Dense(9, activation='softmax')

model = tf.keras.Sequential([
    base_model,
    maxpool_layer,
    tf.keras.layers.Dropout(0.4),
    prediction_layer
])

model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
history = model.fit(train_data2.repeat(),
                epochs=100,
                steps_per_epoch = 50,
                validation_data=val_data2.repeat(),
                validation_steps=10,
                class_weight={0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1},
                callbacks = [])
```

**Other info / logs**
Memory Usage at Each Epoch:
```
              total        used        free      shared  buff/cache   available
E1:           502G         11G        461G        140M         30G        487G
E2:           502G         16G        456G        140M         30G        483G
E3:           502G         22G        449G        140M         30G        476G
E4:           502G         30G        441G        141M         30G        468G
E5:           502G         40G        431G        141M         30G        458G
E6:           502G         52G        419G        141M         30G        446G
```
vmstat output:
[vmstat_output.txt](https://github.com/tensorflow/tensorflow/files/3407988/vmstat_output.txt)

"
30856,ImportError: /opt/gnu/gcc/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOs 6.10 Linux: 2.6.32-754.2.1.el6.x86_64
- TensorFlow installed from (source or binary): Conda install
- TensorFlow version: 1.14.0
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: Conda
- GCC/Compiler version (if compiling from source): c++ 4.9.2
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Import error ImportError: /opt/gnu/gcc/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so). Indicated file is there at the path.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Import tensorflow

**Any other info / logs**
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/atanteck/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/atanteck/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /opt/gnu/gcc/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 34, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/atanteck/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/atanteck/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /opt/gnu/gcc/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/atanteck/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
30855,TF2: tf.distribute.MirroredStrategy() running sequentially on GPUs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI release 2018.03
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0 beta
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Tesla K80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have built a custom subclassed Keras model and am using MirroredStrategy for multi-GPU training. I am using custom training loops exactly as described in the official guide here:
https://www.tensorflow.org/beta/tutorials/distribute/training_loops

However, I am running it in `eager-mode` without using `@tf.function`

I am also using `TFRecordDataset` as part of the input pipeline and when I run it separately, it iterates over the dataset very fast, so that is not a bottleneck. 

When I use `nvidia-smi` to monitor the GPU usage during training, I can see that each GPU gets used in a sequential manner. I am not sure why this behavior is arising. 

**Describe the expected behavior**
The model should be running parallel on all GPUs at the same time. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**

When using the basic MirroredStrategy, I keep getting a warning that Efficient Allreduce does not support IndexedSlices. What does that mean? 

I have also changed to `tf.distribute.ReductionToOneDevice()` which removes the warning but the sequential behavior still persists. 
"
30854,Out of Memory Issues with beam search decoder ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13
- Python version: 3.7
- CUDA/cuDNN version: 10/7.66
- GPU model and memory: v100 and 16GB

**Describe the current behaviour**
I am trying to implement a beam search decoder for a project. Currently, I am following the TensorFlow tutorial for neural machine translation from this link 
[tensorflow tutorial](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb
)
I have replaced the default GRU layer with LSTM layer. Without using beam search decoder, it was working fine with LSTM.

**Code to reproduce the issue**

My current decoder looks as follows:
`
class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, start_tokens, end_token):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.lstm = lstm(self.dec_units)
        self.fc = tf.keras.layers.Dense(vocab_size)
        self.cell = tf.contrib.rnn.BasicLSTMCell(self.dec_units)  # tf.contrib.rnn.LSTMCell(self.dec_units)
        self.beam_size = BEAM_SIZE
        self.start_tokens = start_tokens
        self.end_token = end_token

        # used for attention
        self.W1 = tf.keras.layers.Dense(self.dec_units)
        self.W2 = tf.keras.layers.Dense(self.dec_units)
        self.W3 = tf.keras.layers.Dense(self.dec_units)
        self.V = tf.keras.layers.Dense(1)



    def call(self, x, hidden, hidden2, enc_output):
        # enc_output shape == (batch_size, max_length, hidden_size)

        # hidden shape == (batch_size, hidden size)
        # hidden_with_time_axis shape == (batch_size, 1, hidden size)
        # we are doing this to perform addition to calculate the score

        hidden_with_time_axis = tf.expand_dims(hidden, 1)

        hidden_with_time_axis2 = tf.expand_dims(hidden2, 1)

        # score shape == (batch_size, max_length, 1)
        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V
        score = self.V(
            tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis) + self.W3(hidden_with_time_axis2)))

        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(score, axis=1)

        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * enc_output
        context_vector = tf.reduce_sum(context_vector, axis=1)

        # x shape after passing through embedding == (batch_size, 1, embedding_dim)
        x = self.embedding(x)

        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        output, h, c = self.lstm(x)

        logits = self.fc(output)

        beam = tf.contrib.seq2seq.BeamSearchDecoder(self.cell, self.embedding, self.start_tokens, self.end_token,
                                                    tf.contrib.rnn.LSTMStateTuple(
                                                        tf.contrib.seq2seq.tile_batch(h, multiplier=self.beam_size),
                                                        tf.contrib.seq2seq.tile_batch(c,
                                                                                      multiplier=self.beam_size)),
                                                    self.beam_size)
        output, beamOp , _ = tf.contrib.seq2seq.dynamic_decode(
            beam, output_time_major=True, maximum_iterations=MAX_SEQUENCE_LENGTH)

        predicted_ids = tf.transpose(tf.cast(output.predicted_ids[:, :, 0], tf.float32))
        beamOp_h = beamOp[0][0][:, 0]

        beamOp_c = beamOp[0][1][:, 0]

        return predicted_ids, logits, beamOp_h, beamOp_c, attention_weights

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_sz, self.dec_units))
`


I am a newbie to deep learning and TensorFlow and not sure if I am doing it in the correct way.

I am getting Out of memory issue when I start training the model.

I have already asked the question on StackOverflow a few days back since there is no reply, I am raising the issue here. Apologies if it's not an issue from the tensorflow side, as, as of now I am not sure if the issue in my code or not. 

"
30852,Teacher-forcing in the Transformer tutorial,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/beta/tutorials/text/transformer#training_and_checkpointing

## Description of issue (what needs changing):
Teacher-forcing seems to not be implemented?

### Clear description
The documentation here mentions that the training uses teacher-forcing, however, it doesn't seem like, with the code shown, that this is implemented. The variable `tar_real` is the true outputs, but it seems to only be used for loss and accuracy computations?

Please let me know if I'm making a mistake here! Thanks in advance."
30851,Can not Exporting a GraphDef from file,"Hi, i used code provided in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/cmdline_examples.md. 

Convert a TensorFlow GraphDef 

```
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.50_128_frozen.tgz \
  | tar xzv -C /tmp
tflite_convert \
  --output_file=/tmp/foo.tflite \
  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1**
```

### error
 Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-18 17:14:34.558992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494150000 Hz
2019-07-18 17:14:34.559779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560a0f59d790 executing computations on platform Host. Devices:
2019-07-18 17:14:34.559868: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-18 17:14:35.103189: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-07-18 17:14:35.103362: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-07-18 17:14:35.426148: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-18 17:14:35.426184: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 174 nodes (-381), 173 edges (-408), time = 270.209ms.
2019-07-18 17:14:35.426195: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 174 nodes (0), 173 edges (0), time = 14.101ms.
Traceback (most recent call last):
  File ""/home/wangxy/miniconda3/envs/ml/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 437, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 155, in toco_convert_protos
    fp_debug.write(debug_info_str)
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/tempfile.py"", line 485, in func_wrapper
    return func(*args, **kwargs)
TypeError: a bytes-like object is required, not 'str'
Exception ignored in: <bound method _TensorCacheDeleter.__del__ of <tensorflow.python.eager.context._TensorCacheDeleter object at 0x7f484fd2c550>>
Traceback (most recent call last):
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 316, in __del__
TypeError: argument of type 'NoneType' is not iterable


I also try code fromhttps://www.tensorflow.org/lite/convert/cmdline_examples?source=post_page
there still appears mistake and it can not get tflite

```
import tensorflow as tf
graph_def_file = ""/path/to/Downloads/mobilenet_v1_1.0_224/frozen_graph.pb""
input_arrays = [""input""]
output_arrays = [""MobilenetV1/Predictions/Softmax""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

### error:
2019-07-18 17:51:00.100545: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-18 17:51:00.130812: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494150000 Hz
2019-07-18 17:51:00.131427: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564072ecb610 executing computations on platform Host. Devices:
2019-07-18 17:51:00.131468: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/home/wangxy/miniconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 668, in from_frozen_graph
    raise IOError(""File '{0}' does not exist."".format(graph_def_file))
OSError: File '/home/python/frozen_graph.pb' does not exist.
>>> tflite_model = converter.convert()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'converter' is not defined
>>> open(""converted_model.tflite"", ""wb"").write(tflite_model)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'tflite_model' is not defined
>>> 
Exception ignored in: <bound method _TensorCacheDeleter.__del__ of <tensorflow.python.eager.context._TensorCacheDeleter object at 0x7fa9c5d9c278>>
TypeError: argument of type 'NoneType' is not iterable


**System information**
- OS Platform is Linux Ubuntu 18.04
- TensorFlow installed from binary and its version is 1.15.0-dev20190717 
- environment is anaconda  
- Python version:Python 3.6.8


So, if anyone have the solution please tell me, thank you very much


"
30850,Restoring Keras model fails inside a distribution strategy scope,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**
- TensorFlow installed from (source or binary): **binary** (using `pip`)
- TensorFlow version (use command below): both `v1.14.0-rc1-22-gaf24dc9 1.14.0` and `v2.0.0-beta0-17-g8e423e3 2.0.0-beta1`
- Python version: **3.7.3**
- CUDA/cuDNN version: **CUDA 10.1.168-4, cuDNN 7.6.1.34-1**
- GPU model and memory: **NVIDIA Quadro P2000, 4GB**

**Describe the current behavior**
Inside a distribution strategy scope, restoring a Keras model (that has been trained at all) with `tf.keras.models.load_model` raises the exception shown below (while handling the optimizer in particular, it seems).

(Looks a bit similar to #28599 if you squint, but many details differ.)

**Describe the expected behavior**
Restoring the model should succeed.

**Code to reproduce the issue**
```python
import numpy as np, tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
path = ""/tmp/model.hdf5""

with strategy.scope():
    # Construct model.
    model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
    model.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.metrics.mse)
    # Do a fit so the optimizer weights are created. Removing this lets the restore succeed.
    model.fit(np.array([[1]]), np.array([[1]]))
    # Save and attempt to restore.
    tf.keras.models.save_model(model, path)
    tf.keras.models.load_model(path)
```
**Other info / logs**
Traceback for TF 2.0 (TF 1.14 is the same except for line numbers):
```
  File "".../tensorflow/python/keras/saving/save.py"", line 137, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File "".../tensorflow/python/keras/saving/hdf5_format.py"", line 187, in load_model_from_hdf5
    model._make_train_function()
  File "".../tensorflow/python/keras/engine/training.py"", line 1974, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File "".../tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 491, in get_updates
    grads = self.get_gradients(loss, params)
  File "".../tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 391, in get_gradients
    grads = gradients.gradients(loss, params)
  File "".../tensorflow/python/ops/gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File "".../tensorflow/python/ops/gradients_util.py"", line 543, in _GradientsHelper
    for x in xs
  File "".../tensorflow/python/ops/gradients_util.py"", line 543, in <listcomp>
    for x in xs
  File "".../tensorflow/python/distribute/values.py"", line 643, in handle
    raise ValueError(""`handle` is not available outside the replica context""
ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.
```"
30849,tf.data.experimental.make_csv_dataset cannot decompress files,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.5 (but also tested in RedHat 7)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: CPU

**Describe the current behavior**
`tf.contrib.data.make_csv_dataset` cannot decompress gzip files

**Describe the expected behavior**
When `compression_type` is set to GZIP, it should decompress a gzip file

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python3
import tensorflow as tf
import pandas as pd

sample_iris = {
    'Id': [1,2,3],
    'SepalLengthCm': [5.1, 4.9, 4.7],
    'SepalWidth': [3.5, 3.0, 3.2],
    'Sepcies': ['Iris-setosa', 'Iris-setosa', 'Iris-setosa']
}

df = pd.DataFrame(sample_iris)
df.to_csv('Iris-compressed.csv.gz', compression='gzip', index=False)

train_files = ['Iris-compressed.csv.gz']
train_batch_size = 4
select_columns = ['Id','SepalLengthCm','SepalWidthCm','Species']

dataset = tf.data.experimental.make_csv_dataset(
    train_files,
    train_batch_size,
    column_names=None,
    column_defaults=None,
    label_name='Species',
    select_columns=select_columns,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=True,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=100,
    shuffle_seed=None,
    prefetch_buffer_size=tf.data.experimental.AUTOTUNE,
    num_parallel_reads=1,
    sloppy=True,
    num_rows_for_inference=100,
    compression_type=tf.constant('GZIP')
)
```

As a sanity check,

```bash
gunzip Iris-compressed.csv.gz && cat Iris-compressed.csv
```

works as expected, returning 

```
1,5.1,3.5,Iris-setosa
2,4.9,3.0,Iris-setosa
3,4.7,3.2,Iris-setosa
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""test.py"", line 38, in <module>
    compression_type=tf.constant('GZIP')
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py"", line 547, in make_csv_dataset_v1
    compression_type, ignore_errors))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py"", line 434, in make_csv_dataset_v2
    column_names = _infer_column_names(filenames, field_delim, use_quote_delim)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py"", line 164, in _infer_column_names
    column_names = next(csv.reader(f, **csv_kwargs))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 220, in __next__
    return self.next()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 214, in next
    retval = self.readline()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 179, in readline
    return self._prepare_value(self._read_buf.ReadLineAsString())
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 98, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/compat.py"", line 117, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/compat.py"", line 87, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
```"
30847,DistributedDataset iteration fails with data of type string,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS 
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0
- GPU model and memory: 8 x Tesla P100-PCIE-16GB 

**Describe the current behavior**
I have noticed an issue while iterating over a DistributedDataset (using a tf.distribute.MirroredStrategy) that contains data of type string, with eager execution enabled.
Iterating works perfectly well, but a RuntimeError is raised once the end of the dataset is reached (cf logs below).

**Describe the expected behavior**
The exception is never raised if the dataset does not contain string data, iteration stops and the rest of the code is executed. 

**Code to reproduce the issue**
```
import tensorflow as tf
tf.enable_eager_execution()
print('TensorFlow version: {}'.format(tf.__version__))

raw = tf.random.uniform([256, 20], maxval=10, dtype=tf.int32)

strategy = tf.distribute.MirroredStrategy()
print('Number of replicas: {}'.format(strategy.num_replicas_in_sync))

dataset_1 = tf.data.Dataset.from_tensor_slices(raw)
dataset_1 = dataset_1.batch(64)

dataset_2 = tf.data.Dataset.from_tensors(['This is a test']).repeat(256).batch(64)

dist_dataset_1 = strategy.experimental_distribute_dataset(dataset_1)
dist_dataset_2 = strategy.experimental_distribute_dataset(dataset_2)

print('Iterating over datataset_2')
for i, example in enumerate(dataset_2):
    print('Batch #{}'.format(i))

print('Iterating over distributed dataset_1')
for i, example in enumerate(dist_dataset_1):
    print('Batch #{}'.format(i))
    
print('Iterating over distributed datataset_2')
for i, example in enumerate(dist_dataset_2):
    print('Batch #{}'.format(i))
```
**Other info / logs**
```
TensorFlow version: 1.14.0
Number of replicas: 8
Iterating over datataset_2
Batch #0
Batch #1
Batch #2
Batch #3
Iterating over distributed dataset_1
Batch #0
Batch #1
Batch #2
Batch #3
Iterating over distributed datataset_2
Batch #0
Batch #1
Batch #2
Batch #3
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-18-730d6b0450e9> in <module>
     25 
     26 print('Iterating over distributed datataset_2')
---> 27 for i, example in enumerate(dist_dataset_2):
     28     print('Batch #{}'.format(i))

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in __next__(self)
    225   def __next__(self):
    226     try:
--> 227       return self.get_next()
    228     except errors.OutOfRangeError:
    229       raise StopIteration

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in get_next(self, name)
    254       return data
    255 
--> 256     global_has_value, replicas = _get_next_as_optional(self, self._strategy)
    257     results = []
    258     for i, worker in enumerate(self._input_workers.worker_devices):

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in _get_next_as_optional(iterator, strategy, name)
    160     with ops.device(worker):
    161       worker_has_value, next_element = (
--> 162           iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
    163       # Collective all-reduce requires explict devices for inputs.
    164       with ops.device(""/cpu:0""):

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in get_next_as_list(***failed resolving arguments***)
    719               data.has_value(),
    720               lambda: data.get_value(),
--> 721               lambda: _dummy_tensor_fn(data.value_structure))
    722           result.append(real_data)
    723           # pylint: enable=cell-var-from-loop

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)
   1955         result = true_fn()
   1956       else:
-> 1957         result = false_fn()
   1958       if not strict:
   1959         result = _UnpackIfSingleton(result)

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in <lambda>()
    719               data.has_value(),
    720               lambda: data.get_value(),
--> 721               lambda: _dummy_tensor_fn(data.value_structure))
    722           result.append(real_data)
    723           # pylint: enable=cell-var-from-loop

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in _dummy_tensor_fn(value_structure)
    637   for feature_shape, feature_type in zip(value_structure._flat_shapes,
    638                                          value_structure._flat_types):
--> 639     result.append(create_dummy_tensor(feature_shape, feature_type))
    640 
    641   if isinstance(value_structure, structure.NestedStructure):

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in create_dummy_tensor(feature_shape, feature_type)
    630 
    631     # Create the dummy tensor.
--> 632     dummy_tensor = array_ops.zeros(tensor_shape.TensorShape(dims), feature_type)
    633     return dummy_tensor
    634 

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)
   1869         # Create a constant if it won't be very big. Otherwise create a fill op
   1870         # to prevent serialized GraphDefs from becoming too large.
-> 1871         output = _constant_if_small(zero, shape, dtype, name)
   1872         if output is not None:
   1873           return output

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _constant_if_small(value, shape, dtype, name)
   1827   try:
   1828     if np.prod(shape) < 1000:
-> 1829       return constant(value, shape=shape, dtype=dtype, name=name)
   1830   except TypeError:
   1831     # Happens when shape is a Tensor, list with Tensor elements, etc.

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    244   """"""
    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 246                         allow_broadcast=True)
    247 
    248 

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    252   ctx = context.context()
    253   if ctx.executing_eagerly():
--> 254     t = convert_to_eager_tensor(value, ctx, dtype)
    255     if shape is None:
    256       return t

~/.conda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    113     return t
    114   else:
--> 115     return ops.EagerTensor(value, handle, device, dtype)
    116 
    117 

RuntimeError: Error copying tensor to device: /job:localhost/replica:0/task:0/device:GPU:0. Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.
```
"
30846, mismatch in the description of BatchDot and TensorFlow's implementation. ,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_dot

## Description of issue (what needs changing):
I raised [an issue on the plaidML repo](https://github.com/plaidml/plaidml/issues/358), and after some back and forth we determined the documentation for BatchDot doesn't quite match the actual implementation in the tensorflow code.

### Clear description
A BatchDot with x.shape=(1,2,6,2) and y.shape=(1,2,2,3) and axes = (3, 1)has an output shape of (1,2,6,3)) whereas by the TF definition for output shape ""A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1)."" sounds like it should have an output shape of (1,2,6,2,3).

### Submit a pull request?
I am not planning to submit a PR at this time, but I may do it later"
30843,Unable to train model on multiple GPUs using MirroredStrategy in TF2.0,"I have two Tesla T4 GPUs from Google. I am using this [example](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) in order to train it on multiple GPUs with defined the `model` inside `tf.distribute.MirroredStrategy()` as described [here](https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras). The model trains without `tf.distribute.MirroredStrategy()` on single GPU normally but not when I want to train it on multiple GPUs. You can find the error log below. 
Another dummy example program in the docs for using `MirroredStrategy()` is [here](https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras). On executing this, it trains and utilises both GPUs. Between these two examples, I noticed one thing which is the use of `padded_batch` and `batch`. It works fine in the latter case. I also have a model of mine where I am using `padded_batch`. This model also can not be trained and although the error there is a bit different. 
**Please suggest any ways to get rid of this problem. THANK YOU!**

> Train on None steps
Epoch 1/10
W0718 14:21:33.186410 139786698037056 cross_device_ops.py:764] Efficient allreduce is not supported for 1 IndexedSlices
W0718 14:21:38.304410 139786698037056 cross_device_ops.py:764] Efficient allreduce is not supported for 1 IndexedSlices
2019-07-18 14:21:39.456127: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name: replica_1/sequential/bidirectional/StatefulPartitionedCall_replica_1/StatefulPartitionedCall_8 Expected: {forward_node_name}:{index}
2019-07-18 14:21:40.179202: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_356864_357366' and '__inference___backward_standard_lstm_356864_357366_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_360209' both implement 'lstm_3542e53d-8ce9-47ba-b422-dc9202236064' but their signatures do not match.
2019-07-18 14:21:40.669817: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-18 14:21:40.752732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-18 14:21:51.150641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:111] Filling up shuffle buffer (this may take a while): 17050 of 50000
2019-07-18 14:22:00.643784: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:162] Shuffle buffer filled.
2019-07-18 14:22:00.664615: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
2019-07-18 14:22:00.664706: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
	 [[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_1/_50]]
2019-07-18 14:22:00.664771: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
	 [[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
	 [[replica_1/metrics/accuracy/AssignAddVariableOp_1/_41]]
2019-07-18 14:22:00.665117: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-18 14:22:00.665167: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
	 [[{{node replica_1/sequential/bidirectional/StatefulPartitionedCall}}]]
2019-07-18 14:22:00.666533: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_101 , and the dst node is while_1_RetVal
2019-07-18 14:22:00.679274: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
Traceback (most recent call last):
  File ""colab.py"", line 94, in <module>
    model.fit(train_data, epochs=10, validation_data=test_data)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 643, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_distributed.py"", line 681, in fit
    steps_name='steps_per_epoch')
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 294, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py"", line 813, in execution_function
    return [out.numpy() for out in distributed_function(input_fn)]
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/def_function.py"", line 428, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py"", line 1335, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py"", line 589, in _filtered_call
    (t for t in nest.flatten((args, kwargs), expand_composites=True)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py"", line 671, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/function.py"", line 445, in call
    ctx=ctx)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/tensorflow/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""/home/rishabh/.local/lib/python2.7/site-packages/six.py"", line 737, in raise_from
    raise value
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
	 [[node replica_1/sequential/bidirectional/StatefulPartitionedCall (defined at usr/lib/python2.7/threading.py:801) ]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_1/_50]]
  (1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_102 , and the dst node is while_2_RetVal
	 [[node replica_1/sequential/bidirectional/StatefulPartitionedCall (defined at usr/lib/python2.7/threading.py:801) ]]
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_360209]
Function call stack:
distributed_function -> distributed_function"
30842,change protobuf version to 3.8 then build failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 14.04):
- GCC 4.8.4
- build CPU only

**Describe the problem**
when build tensorflow r1.5, upgrade protobuf version by changing proto buf urls, sha256,strip prefix value . Just  As followed:
        ""http://mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",        ""https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",
PROTOBUF_SHA256 = ""b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59""
 PROTOBUF_STRIP_PREFIX = ""protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0""


**build commnd**:
bazel build //tensorflow:libtensorflow_cc.so

the error message is :
---------------------------
ERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//lib:versions.bzl': The repository could not be resolved
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//lib:versions.bzl': The repository could not be resolved
INFO: Elapsed time: 0.652s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow
-------------------------

how could I fix the problem  ?
"
30841,cannot detect communication between clusters when CollectiveAllReduceStrategy ,"tensorflow 1.12
CUDA 10
Ubuntu 16.04

Here is the part of code in the main script:
  strategy=tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.n_gpus)#devices=[""/gpu:0"", ""/gpu:1""]
  if FLAGS.strategy==""ps"":
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': {
            ""worker"": [""10.60.131.148:8081"", ""10.60.131.149:8082""],
            ""ps"": [""10.60.131.150:8083""]
        },
        'task': {'type': FLAGS.type, 'index': FLAGS.index}
    })
    strategy = tf.contrib.distribute.ParameterServerStrategy(num_gpus_per_worker=FLAGS.n_gpus)
  elif FLAGS.strategy==""collective"":
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': {
            ""worker"": [""10.60.131.148:8888"",""10.60.131.149:8888"", ""10.60.131.150:8888""],
        },
        'task': {'type': FLAGS.type, 'index': FLAGS.index}
    })
    strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.n_gpus)

  run_config = tf.estimator.RunConfig(
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      train_distribute=strategy
      )
  estimator = tf.estimator.Estimator(
      model_fn=model_fn,
      config=run_config,
      params={""batch_size"": FLAGS.train_batch_size}
  )

  if FLAGS.do_train:
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=True)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)

I have three machines each has 3 gpus.
When I run CollectiveAllReduceStrategy, set type as 'worker' and index, the num of gpus for each worker as 3.I run the script independently on three clusters,  and use iftop -n to monitor the network, I did notice any communication between the clusters.
No errors, and from the log I did't know if it now runs the correctly way. Or just single machine 3 gpus, running independently.

Any suggestions or reference links?
"
30840,2.0.0-beta1 no op for reassign variable to different shape (e.g. a validate_shape=False option),"```
import tensorflow as tf

print(tf.__version__)
tf.random.set_seed(0)

v = tf.Variable(
    tf.random_normal_initializer()(shape=(10,)),
    validate_shape=False) # Only for first initialization, not for reinitialization

v2 = tf.random.normal((5,))

print(v)

# tf.compat.v1.assign(v, v2, validate_shape=False)
v.assign(v2)

print(v)
```

Outputs:

```
2.0.0-beta1
2019-07-18 14:41:15.418014: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=
array([ 0.07555313,  0.0211461 , -0.02098475, -0.05180186, -0.06184139,
        0.02351365, -0.00069874,  0.05944292,  0.03012667,  0.02998556],
      dtype=float32)>
Traceback (most recent call last):
  File ""test.py"", line 15, in <module>
    v.assign(v2)
  File ""/Users/olanymoe/anaconda3/envs/tf2/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1145, in assign
    self._shape.assert_is_compatible_with(value_tensor.shape)
  File ""/Users/olanymoe/anaconda3/envs/tf2/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 1110, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (10,) and (5,) are incompatible
```

Similarly, the `compat.v1` seems to ignore the `validate_shape` argument for variables in 2.0 ( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/state_ops.py#L228 ).

I'm working on a model which is being served as a saved_model. Is there a way of changing shapes of variables in 2.0? In 1.14 it worked using assign ops with `validate_shape=False`. The 2.0 documentation seems to hint at this functionality, but I cannot find any compatible ops:

> If you want to change the shape of a variable later you have to use an assign Op with validate_shape=False.

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable"
30839,Tensorflow 2.0 - Build From Source not creating Python Wrappers such as _pywrap_tensorflow_internal.lib,"**System information**
- OS : Windows 10
- TensorFlow installed from : source 
- TensorFlow : 2.0.0
- Python version : 3.7.3
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): VS2017
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia 960m

When building tensorflow v1.14 from source, using configure.py script then building the resulting bazel --config=opt, in the bazel-bin/tensorflow output directory, it creates a /python directory containing python wrapper libraries such as _pywrap_tensorflow_internal.lib (which is required when creating custom ops).

Following the same procedure with v2.0.0, it doesn't produce this (using exactly the same config settings) i.e. for whatever reason the build script is now building the python wrappers libraries.

The source code is in the same location as for v1.14, and checking the prebuilt version that can be installed via pip, this directory and associated library are deployed.

Is an additional flag now required to create this additional set of python wrapper libraries?
"
30838,"Unable to quantize buffer or min/max value for input 1 in op MUL in subgraph 0, node: 8","**System information**
- OS: Linux Ubuntu 19.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0.130/ 7.4.1
- GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
Running the official tutorial on post training quantization:
https://www.tensorflow.org/lite/performance/post_training_quantization
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb

results in following error message:
`Unable to quantize buffer or min/max value for input 1 in op MUL in subgraph 0, node: 8`

**Describe the expected behavior**
No error

**Code to reproduce the issue**
Run the attached notebook with attached graph.pb and csv:
https://drive.google.com/open?id=1JI45QqwhgR2v4i8GCShkH5eod0TTB7FI

Download YTF Dataset as from here: https://www.cs.tau.ac.il/~wolf/ytfaces/


"
30837,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.","Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE."
30835,Keras Adam optimizer unsupported by GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **preinstalled in colab**
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.8**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: **GPU on Colab**

**Describe the current behavior**
I run a Keras Adam optimizer with a CNN network. The code works fine with CPU. If I turn on GPU in the notebook, and rerun the same code, I get an exception.

**Describe the expected behavior**
No exception

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Activate GPU.

```
import tensorflow as tf
from tensorflow import keras

print(tf.version.VERSION)

training_samples = 100
input_shape = (16, 512, 1)

dataset = tf.data.Dataset.from_tensor_slices((tf.random_uniform([32, 16, 512, 1], dtype=tf.float32), tf.random_uniform([32], dtype=tf.float32)))
dataset = dataset.shuffle(32).repeat()

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    initializer = 'he_uniform'
    nb_filts = [8, 16, 32, 400]
    out_size = 1
    model = tf.keras.models.Sequential()
    model.add(keras.layers.Conv2D(nb_filts[0], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer, input_shape=input_shape))
    model.add(keras.layers.Conv2D(nb_filts[0], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Conv2D(nb_filts[1], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Conv2D(nb_filts[1], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Conv2D(nb_filts[2], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Conv2D(nb_filts[2], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(1024, activation='relu',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Dense(nb_filts[3], activation='relu',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Dense(out_size))

    optimizer = tf.keras.optimizers.Adam(1e-3)
    model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mean_squared_error', 'mean_absolute_error'])

with strategy.scope():
    batch_size = 32
    nb_epochs = 1
    history = model.fit(dataset.batch(batch_size, drop_remainder=True), epochs=nb_epochs, steps_per_epoch=training_samples // batch_size, verbose=1)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

14 frames
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node Adam/NcclAllReduce}}with these attrs: [shared_name=""c0"", T=DT_FLOAT, num_devices=1, reduction=""sum""]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  <no registered kernels>

	 [[Adam/NcclAllReduce]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by node Adam/NcclAllReduce (defined at <ipython-input-8-7f03033581ef>:4) with these attrs: [shared_name=""c0"", T=DT_FLOAT, num_devices=1, reduction=""sum""]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  <no registered kernels>

	 [[Adam/NcclAllReduce]]
```"
30834,tf 2.0 Keras GaussianNoise to supports dtype=tf.float64,"**System information**
- TensorFlow version (you are using): `2.0.0-beta1`
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

* This (`model_float32`) works
```python
import tensorflow as tf
print(tf.__version__)
def model_float32():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(10, use_bias=False, input_shape=(10,)
        ,dtype=tf.float32))

    model.add(tf.keras.layers.GaussianNoise(0.0003))
    return model

testmodel_32 =model_float32()
```
without error.
```
2.0.0-beta1
```
 * However, the (`model_float64`) dose not work.
```python
import tensorflow as tf
print(tf.__version__)
def model_float64():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(10, use_bias=False, input_shape=(10,)
        ,dtype=tf.float64))

    model.add(tf.keras.layers.GaussianNoise(0.0003))
    return model

testmodel_64 =model_float64()
```
The error output is shown as below.
```python
2.0.0-beta1

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    526                 as_ref=input_arg.is_ref,
--> 527                 preferred_dtype=default_dtype)
    528           except TypeError as err:

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
   1236     if ret is None:
-> 1237       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1238 

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
   1035         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
-> 1036         (dtype.name, t.dtype.name, str(t)))
   1037   return t

ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(""random_normal:0"", shape=(None, 10), dtype=float32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-11-450afed79e6d> in <module>
     10     return model
     11 
---> 12 testmodel_64 =model_float64()

<ipython-input-11-450afed79e6d> in model_float64()
      7         ,dtype=tf.float64))
      8 
----> 9     model.add(tf.keras.layers.GaussianNoise(0.0003,dtype=tf.float64))
     10     return model
     11 

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    191       # If the model is being built continuously on top of an input layer:
    192       # refresh its output.
--> 193       output_tensor = layer(self.outputs[0])
    194       if len(nest.flatten(output_tensor)) != 1:
    195         raise TypeError('All layers in a Sequential model '

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    660                     not base_layer_utils.is_in_eager_or_tf_function()):
    661                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 662                     outputs = call_fn(inputs, *args, **kwargs)
    663                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    664                     # circular dependencies.

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/noise.py in call(self, inputs, training)
     68           shape=array_ops.shape(inputs), mean=0., stddev=self.stddev)
     69 
---> 70     return K.in_train_phase(noised, inputs, training=training)
     71 
     72   def get_config(self):

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in in_train_phase(x, alt, training)
   4051 
   4052   # else: assume learning phase is a placeholder tensor.
-> 4053   x = switch(training, x, alt)
   4054   return x
   4055 

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in switch(condition, then_expression, else_expression)
   3986     else:
   3987       else_expression_fn = else_expression
-> 3988     x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)
   3989   else:
   3990     # tf.where needs its condition tensor

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)
   1145   if (util.EnableControlFlowV2(ops.get_default_graph()) and
   1146       not context.executing_eagerly()):
-> 1147     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   1148 
   1149   # We needed to make true_fn/false_fn keyword arguments for

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py in cond_v2(pred, true_fn, false_fn, name)
     77             true_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access
     78         add_control_dependencies=add_control_dependencies,
---> 79         op_return_value=pred)
     80     false_graph = func_graph_module.func_graph_from_py_func(
     81         false_name,

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    714                                           converted_func)
    715 
--> 716       func_outputs = python_func(*func_args, **func_kwargs)
    717 
    718       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/noise.py in noised()
     66     def noised():
     67       return inputs + K.random_normal(
---> 68           shape=array_ops.shape(inputs), mean=0., stddev=self.stddev)
     69 
     70     return K.in_train_phase(noised, inputs, training=training)

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    882     with ops.name_scope(None, op_name, [x, y]) as name:
    883       if isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor):
--> 884         return func(x, y, name=name)
    885       elif not isinstance(y, sparse_tensor.SparseTensor):
    886         try:

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py in add(x, y, name)
    385   try:
    386     _, _, _op = _op_def_lib._apply_op_helper(
--> 387         ""Add"", x=x, y=y, name=name)
    388   except (TypeError, ValueError):
    389     result = _dispatch.dispatch(

~/miniconda3/envs/keras_tf2.0_b_cpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    561                   ""%s type %s of argument '%s'."" %
    562                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--> 563                    inferred_from[input_arg.type_attr]))
    564 
    565           types = [values.dtype]

TypeError: Input 'y' of 'Add' Op has type float32 that does not match type float64 of argument 'x'.

​
```


**Will this change the current api? How?**
`tf.keras.layers.GaussianNoise` to support `dtype argument`
**Who will benefit with this feature?**
Any one who use `float64` datatype.
**Any Other info.**
"
30832,Cannot convert from tensorflow to tensorflow lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
- TensorFlow installed from (source or binary): From Conda
- TensorFlow version (or github SHA if from source): 1.14.0


**Text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, WHERE. Here is a list of operators for which you will need custom implementations: DecodeBmp, DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, Merge, NonMaxSuppressionV3, ParseSingleExample, Substr, Switch.
```

GraphDef :  
https://github.com/michisaak/ObjectDetection/blob/master/saved_model.pb

**Logs**
Traceback (most recent call last):
  File ""/home/michaili/anaconda2/envs/retrain/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/michaili/anaconda2/envs/retrain/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/michaili/anaconda2/envs/retrain/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/michaili/anaconda2/envs/retrain/lib/python3.5/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/michaili/anaconda2/envs/retrain/lib/python3.5/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/michaili/anaconda2/envs/retrain/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md

"
30831,How to get the detail type of auto eigen variable in op,The auto variable is annoying in eigen as the interpreter cannot decide how to interpret it. See the detail [here](https://stackoverflow.com/questions/57057123/how-to-get-the-scalar-value-returned-by-eigen-tensors-sqrt-function/57075471#57075471)
30830,The model of tf1 cannot be converted to a model of tflite in tf2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta
- Python version: 3.6.8
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 10
- GPU model and memory: GTX-1080Ti

**Describe the current behavior**
![微信图片_20190718111406](https://user-images.githubusercontent.com/11308780/61426503-2df33400-a94d-11e9-9a56-226c1e6c7a06.png)

**Describe the expected behavior**
Convent successfully.

**Code to reproduce the issue**
``` python
import tensorflow as tf

modelPath = 'saved/old-model.h5'
model = tf.keras.models.load_model(modelPath, compile=False)
converter = tf.lite.TFLiteConverter.from_keras_model(
    model
)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

**Other info / logs**
Here are the model link
[model.h5](https://drive.google.com/file/d/1nwb0IOh6UVwJmV0QJgsiYawCKT0RVpxI/view?usp=sharing)"
30829,how can I create a model without tf.keras in tf2.0,"if I use tf2.0 and not use tf.keras, which advanced api(like tf.layers.* or tf.contrib.* in  tf1.x ) I can use for creating model"
30828,error: static assertion failed: Specified Data Type not supported,"`auto gt_pts_trans = tensorflow::ops::Transpose(root, gt_pts, NULL);`
I used 'tensorflow::ops::Transpose()' and occurred error despite gt_pts is DT_FLOAT

> error: static assertion failed: Specified Data Type not supported
> static_assert(IsValidDataType::value, ""Specified Data Type not supported"");
> error: ‘v’ is not a member of ‘tensorflow::DataTypeToEnum’
> Tensor t(DataTypeToEnum::v(), TensorShape());
"
30822,tf.keras: Model with multiple outputs cause error,"Hi

I'm training an auto-encoder, and this is the model summary.

```
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ENCODER_INPUT (InputLayer)      [(None, 15)]         0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 15, 100)      400000      ENCODER_INPUT[0][0]              
__________________________________________________________________________________________________
encoder (Encoder)               (None, 50)           189746      embedding[0][0]                  
__________________________________________________________________________________________________
decoder (Decoder)               (None, 15, 4000)     673696      encoder[0][0]                    
__________________________________________________________________________________________________
sentiment (Sentiment)           (None, 1)            2601        encoder[0][0]                    
==================================================================================================
Total params: 1,266,043
Trainable params: 1,266,043
Non-trainable params: 0
```


when I call
` autoencoder.fit(x=x, y={'decoded_mean': x_one_hot, 'pred': y}, epochs=1) `

it will return error:
```
~/anaconda3/envs/tf2_p37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    449     except KeyError as e:
    450       raise ValueError('No data provided for ""' + e.args[0] + '"". Need data '
--> 451                        'for each key in: ' + str(names))
    452   elif isinstance(data, (list, tuple)):
    453     if isinstance(data[0], (list, tuple)):

ValueError: No data provided for ""decoder"". Need data for each key in: ['decoder', 'sentiment']
```

However, this code works, but I really want to modify the name of the losses:
`autoencoder.fit(x=x, y=[x_one_hot, y], epochs=1)`



In addition, If I use run following code, It will directly kill everything without showing any errors.
```
db = tf.data.Dataset.from_tensor_slices((x, {'decoded_mean': x_one_hot, 'pred': y}))
db = db.shuffle(1000).batch(batch_size)
auotencoder.fit(db, epochs=1)
```

"
30820,Can xla compile tf.estimator.DNNLinearCombinedEstimator?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source 1.13
- TensorFlow version: 1.13
- Python version: 2.7
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source):4.8
- CUDA/cuDNN version: n/a
- GPU model and memory:n/a



**Describe the problem**
trying to AOT compile a model trained using XLA 
error :  {{node dnn/input_from_feature_columns/input_layer/xyz/axyz_embedding_weights/SparseReshape}}
	.  Registered:  <no registered kernels>
){{node dnn/input_from_feature_columns/input_layer/ad__ad_id_embedding/ad__ad_id_embedding_weights/SparseReshape}}

Is there any ETA on this being supported ? 

How can AOT xla compile : tf.estimator.DNNLinearCombinedEstimator , is this even possible with current support ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel-bin/tensorflow/compiler/aot/tfcompile --graph=mygrapht.pb --config=graphv.config.pbtxt --cpp_class=""mynamespace::MyComputation""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30818,tf.py_function in tf.data.Dataset pipeline doesn't work with TPUEstimator,"Environment is Google Colab with TPU runtime.

**Describe the current behavior**
I have an input pipeline that contains tf.py_function as one of its processing steps when training a model with TPUEstimator. When I run the code, I get the following error:

```
No registered 'EagerPyFunc' OpKernel for CPU devices compatible with node {{node EagerPyFunc}}
	.  Registered:  <no registered kernels>

	 [[EagerPyFunc]]
	 [[input_pipeline_task0/MakeIterator]]
```

According to the documentation (https://www.tensorflow.org/guide/using_tpu) ""The input pipeline generated by your input_fn is run on CPU."" Running the same input code with a standard Estimator on CPU/GPU works just fine. Manually placing the Dataset + all processing steps on the CPU with `tf.device('/cpu:0')` also fails with the same error when training with TPUEstimator.

**Describe the expected behavior**
I should be able to run Python code on the CPU as part of my input pipeline when training on TPUs.

**Code to reproduce the issue**
https://colab.research.google.com/drive/15KuVkukIWdN753ffNjY6jVzT1gyhsBRQ

The notebook linked above is nearly identical to https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpuestimator.ipynb. I've just added the following line to the Training Data code cell:

```idx = tf.py_function(lambda x: x, [idx], tf.int32)```"
30816,tensorflow latency 2-3x worse with multiple input tensors,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12, 1.11, 1.13
- Python version: 2.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a (but 4.8)
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have trained a model and then i 2 versions of frozen graph .
1) version 1 : single placeholder which takes in tfexample and rune the ParseExample operator and does inference
2)Version 2 : instead of single placeholder , i have about ~300 placeholders . one placholder per feature (i.e since the features are already parsed , i do not create tfexample , but rather feed in individual tensors i.e a feed dictionalry where for each feature column i feed in the respective tensor)

The number of ops in both graphs are approximately same (multiple placeholders being slightly less ) ~5-6k ops/nodes

Latency of version 2 is 2-3x higher then version 1 

**Describe the expected behavior**

Expected behavior is latency when i parse the feature before hand and feed in per feature tensor , there is lesser work for tensorflow to do , so it should be faster or at worse the same ? why do we observe 2-3x higher latency ?

is this due to the way feed dictionary gets copied from python to c++ ?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30815,Feedback on Tensorflow 2.0 beta - documentation issues?,"Hi,

Please accept the following as feedback on my experience of Tensorflow 2.0 (beta):
1. Read the [migration guide](https://www.tensorflow.org/beta/guide/migration_guide) and figured I would give it a go.
2. Installed without issues on my Mac Conda environment.  This was to be the highlight of my successes.
3. The [documentation site](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf) isn't searchable by version and given the frequency and amount of change here it would be nice to find a way to locate where various things have moved to.
4.  By way of example, PhasedLSTMCell, I knew the contrib module is gone in 2.0 but its still in the GitHub 2.0 branch which is misleading.  Further misleading is the comments [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/__init__.py): 
> Created in contrib, eventual plans to move to core.

No indication of where one might currently find it.
5. Figured it might be in in [Addons](https://github.com/tensorflow/addons) but no luck there.
6. I wanted to use tf.keras for the first time.  Maybe its my Pip/Conda environment but no matter what I did I could not get it to import unless I did import tensorflow._python._keras.  Did I miss this in the docs because I swear I didn't read it anywhere.  
After some more poking around I decided I had enough exposure to 2.0 and went back to 1.14 - it did give me some minutes of excitement.

However, I would love 2.0 to be speedily and widely adopted.  The API looks a lot cleaner (from what I read of it, I didn't get to use any ultimately) and I think some improvements around how the documentation is accessible would help uptake.  I'm willing to contribute to help this along if I can.
"
30814,Precision change of tf.variable while doing some ops,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed : via conda
- TensorFlow version (use command below): 1.14.0-rc1
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0.130/7.6 (driver 410.78)
- GPU model and memory: Tesla V100 16GB

**Describe the current behavior**
The output difference is non zero
**Describe the expected behavior**
It should be zero
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import numpy as np
import tensorflow as tf

tf.reset_default_graph()
with tf.device('/cpu:0'):    
    inp = tf.Variable(np.random.normal(size=(10,5)), dtype=tf.float64)
""""""
with tf.device('/gpu:0'):    
    inp = tf.Variable(np.random.normal(size=(10,5)), dtype=tf.float64)       
""""""
var_test = tf.Variable(np.zeros((3,5), dtype=np.float64), dtype=tf.float64)
plh = tf.placeholder(shape=(3,), dtype=tf.int32)
   
op = var_test.assign(tf.gather(inp, plh))
with tf.control_dependencies([op]):
    out_var = var_test + tf.constant(np.float64(0.07))
    out_var = out_var - tf.constant(np.float64(0.07))
    update_op = tf.scatter_update(inp, plh, out_var)

idx = np.arange(10)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    out_ip_before = sess.run(inp)
    print(out_ip_before)
    print('sum of all params (before):', np.sum(out_ip_before))
    for i in range(10):
        idx_batch = np.random.permutation(idx)[:3]
        out = sess.run(update_op, feed_dict={plh:idx_batch})
        
    out_ip_after = sess.run(inp)
    print('sum of all params (after):', np.sum(out_ip_after))
    print(out_ip_after)
    diff_after_before = out_ip_before - out_ip_after
    print('before-after:', np.sum(diff_after_before))
    print(diff_after_before)
```
**Other info / logs**
Could have come up with a better issue title :)"
30813,variable scope is changed by force when reopened (with regard to use Adam),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.12
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 1080 ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

with tf.variable_scope('scope'):
    a = tf.get_variable('a', [4,3,2], tf.float32)
    b = tf.get_variable('b', [4,3,2], tf.float32)
    c = tf.reduce_sum(a + b)
   
    d = tf.train.AdamOptimizer()
    #g = d.compute_gradients(c, tf.trainable_variables())
    #h = d.apply_gradients(g)

with tf.variable_scope('scope'):
    g = d.compute_gradients(c, tf.trainable_variables())
    h = d.apply_gradients(g)

print tf.global_variable()
```

the scope of beta1_power and beta2_power is not 'scope' but 'scope_1'.
Instead of reopening the variable_scope, if I run those two commented lines, the scope of beta1_power and beta2_power will be 'scope' not 'scope_1'


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30811,Saving of BatchNormalization layer fails,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

**Describe the current behavior**
When I try to save a BatchNormalization layer as in the example code it fails with the following error:

```
Traceback (most recent call last):
  File ""test_bn.py"", line 31, in <module>
    tf.saved_model.save(infer, saved_model_dir, signature_dict)
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 840, in save
    meta_graph_def, saveable_view, signatures)
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 536, in _fill_meta_graph_def
    object_map, resource_map, asset_info = saveable_view.map_resources()
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 270, in map_resources
    ""supported."").format(concrete_function.name, capture))
ValueError: Attempted to save a function b'__inference_batch_normalization_layer_call_and_return_conditional_losses_414' which references a symbolic Tensor Tensor(""batch_normalization_trainable:0"", dtype=bool) that is not a simple constant. This is not supported.
```

**Describe the expected behavior**
Saving succeeds without error.

**Code to reproduce the issue**
The following testcase can be used to reproduce the issue:

```
import tensorflow as tf

class Outer(tf.keras.Model):
    def __init__(self):
        super().__init__()

        self.bn = tf.keras.layers.BatchNormalization()

    def call(self, x, train_bn=False):
        return self.bn(x, training=train_bn)

class Infer(tf.Module):
    def __init__(self):
        super().__init__()

        # Decorate the inference function with tf.function
        self.infer_ = tf.function(self.infer, input_signature=[
             tf.TensorSpec([1, 64, 64, 8], tf.float32, 'prev_img')])

        self.outer = Outer()

    def infer(self, input):
        return self.outer(input, train_bn=False)

# Create model
infer = Infer()

# Save the trained model
signature_dict = {'infer': infer.infer_}
saved_model_dir = '/tmp/saved_model'
tf.saved_model.save(infer, saved_model_dir, signature_dict)
```"
30810,AttributeError: module 'tensorflow' has no attribute 'init_scope',"**System information**

**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
 -No
**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
-Windows
**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**
-No
**TensorFlow installed from (source or binary):**
-Pip
**TensorFlow version (use command below):** 
-Tensorflow-gpu 1.9.0
**Python version:**
-Python 3.6.6
**CUDA/cuDNN version:**
-CUDA 10.0.0
-cuDNN 7.6.1
**GPU model and memory:**
-NVIDIA GeForce GTX 1070 Ti  (8gb dedicated)

**Describe the current behavior**
 - Try to train a model based on  faster_rcnn_nass_coco.config. Get an  - error AttributeError: module 'tensorflow' has no attribute 'init_scope'.
Same code works if I use ssd_mobilenet_v1_pets.config

**Code to reproduce the issue**
`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_nass_coco.config`


**Other info / logs**
_Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\Vitalie\Anaconda3\envs\python-cvcourse\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\Vitalie\Anaconda3\envs\python-cvcourse\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""C:\Users\Vitalie\Downloads\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 821, in predict
    prediction_dict = self._predict_first_stage(preprocessed_inputs)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 872, in _predict_first_stage
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1250, in _extract_rpn_feature_maps
    feature_map_shape[2])]))
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\core\anchor_generator.py"", line 103, in generate
    anchors_list = self._generate(feature_map_shape_list, **params)
  File ""C:\Users\Vitalie\Downloads\models-master\research\object_detection\anchor_generators\grid_anchor_generator.py"", line 111, in _generate
    with tf.init_scope():
AttributeError: module 'tensorflow' has no attribute 'init_scope'_"
30809,Add keras.layers.Layer.add_callback,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): `1.13.1`
- Are you willing to contribute it (Yes/No): maybe, but I won't have time in the next week or two

**Describe the feature and the current behavior/state.**

Currently, custom Keras layers have `add_loss` and `add_metric`. It would be nice if layers could also contribute their own callback functions as well. 

**Will this change the current api? How?**

It would add a method `add_callback` to the `keras.layers.Layer` class. 

**Who will benefit with this feature?**

The main use case that I see this for is layers that have some warm start or epoch-dependent parameters. By adding them directly inside the layer, it keeps layers completely composable. 

Currently, we need to do something like this:

```python
input = x
my_layer = MyLayer(...)
x = my_layer(x)
...
model = Model(input, x)
model.compile(...)
model.fit(..., callbacks=[WarmStartCallback(my_layer.alpha)])
```

But it would be much cleaner to be able to do this:

```python
class MyLayer(Layer):
    def __init__(self, warm_start=True):
        self.alpha = tf.Variable(...)
        if warm_start:
            self.add_callback(WarmStartCallback(self.alpha))


input = x
x = MyLayer(...)(x)
...
model = Model(input, x)
model.compile(...)
model.fit(...)
```

**Any Other info.**

Off hand, here's a sketch of major changes that I see being needed:

```python
# tf.keras.engine.base_layer
class Layer:
    def __init__(self, ...):
        ...
        self._metrics = []

        # +
        self._callbacks = []
        ...

    @property
    def callbacks(self):
        return self._callbacks + self._gather_children_attribute('callbacks')

    def add_callback(self, callback):
        # maybe some checks, conversions, etc
        ...
        self._callbacks.append(callback)

# tf.keras.engine.training
class Model:
    def fit(..., callbacks=None, ...):
        ...
        # +
        layer_callbacks = self._gather_children_attribute('callbacks') 
        callbacks = layer_callbacks + callbacks if callbacks else layer_callbacks

        func = self._select_training_loop(x)
        ...

```"
30808,Serialization of keras object fails if called with different input sizes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

**Describe the current behavior**
When I try to save a function of tf.Module as saved_model that calls another function with different input shapes, it fails with the following error:

```
W0717 17:37:44.384423 139641189812032 save.py:129] Skipping full serialization of object <__main__.Outer object at 0x7f009a2e94e0>, because an error occurred while tracing layer functions. Error message: in converted code:                                                                                               
                                                                                                                                                                                                                                                                                                                             
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:539 call_and_return_conditional_losses  *                                                                                                                                                                      
        return layer_call(inputs), layer.get_losses_for(inputs)                                                              
    test_signature.py:32 call  *
        return self.inner(x, dummy=dummy), self.inner(x_small, dummy=dummy)                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:708 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py:48 wrapped_call
        outputs, losses = call_fn(inputs)                                                                                           
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:506 __call__
        self.call_collection.add_trace(*args, **kwargs)                                                                                
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:467 add_trace
        fn.original_get_concrete_function(*args, **kwargs)                                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:515 original_get_concrete_function
        return super(LayerCall, self).get_concrete_function(*args, **kwargs)                                                            
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:692 get_concrete_function
        concrete = self._stateful_fn.get_concrete_function(*args, **kwargs)                                                        
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1750 get_concrete_function
        (str(args), str(self.input_signature)))                                                                                      
                            
    ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'down/Identity:0' shape=(None, 32, 32, 8) dtype=float32>,)), input_signature ((TensorSpec(shape=(None, 64, 64, 8), dtype=tf.float32, name='input_1'),))
```

**Describe the expected behavior**
The model can be saved successfully.

**Code to reproduce the issue**
The following testcase allows to reproduce the issue:

```
import tensorflow as tf

class Inner(tf.keras.Model):
    def __init__(self):
        super().__init__()

        self.conv1 = tf.keras.layers.Conv2D(8,
            (3, 3),
            kernel_initializer=tf.keras.initializers.he_normal(),
            padding='same',
            name='conv1')

    def call(self, x, dummy=False):
        x = self.conv1(x)
        return x

class Outer(tf.keras.Model):
    def __init__(self):
        super().__init__()

        self.down = tf.keras.layers.Conv2D(8,
            (3, 3),
            strides=(2, 2),
            kernel_initializer=tf.keras.initializers.he_normal(),
            padding='same',
            name='down')

        self.inner = Inner()

    def call(self, x, dummy=False):
        x_small = self.down(x)
        return self.inner(x, dummy=dummy), self.inner(x_small, dummy=dummy)

class Infer(tf.Module):
    def __init__(self):
        super().__init__()

        # Decorate the inference function with tf.function
        self.infer_ = tf.function(self.infer, input_signature=[
             tf.TensorSpec([1, 64, 64, 8], tf.float32, 'prev_img')])

        self.outer = Outer()

    def infer(self, input):
        return self.outer(input, False)

# Create model
infer = Infer()

# Save the trained model
signature_dict = {'infer': infer.infer_}
saved_model_dir = '/tmp/saved_model'
tf.saved_model.save(infer, saved_model_dir, signature_dict)
```"
30807,Memory Leak in `tf.estimator.experimental.InMemoryEvaluatorHook`,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6.8
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Tesla K80 11441MiB
- **Exact command to reproduce**: python min.py

### Describe the problem
There seems to be some kind of memory leak in `tf.estimator.experimental.InMemoryEvaluatorHook`

My example is a little bit messy - because I had to copy over the `InMemoryEvaluatorHook` from a future version of TensorFlow and modify it very slightly (because I'm not able to upgrade) So technically, this might be a bug with TF1.14.0 which is where I copied the source code from.  

Essentially, if I feed a large constant tensor (a 3.6GB embedding in my case) in the scaffold of the training model, CPU memory usage seems to balloon, expanding every time an evaluation is run, eventually exceeding 64GB.  I checked memory usage simply by using `htop` and looking for my python instance.

Are there are any alternatives to what I'm doing here that would result in reasonable memory usage?

### Source code / logs
```python
# min.py
import tensorflow as tf
from hooks import InMemoryEvaluatorHook
import numpy as np

def train_input_fn():
  dataset = tf.data.Dataset.range(100)
  # Make sequence data
  dataset = dataset.map(lambda x: {'x': [x]*10})
  dataset = dataset.repeat(100)
  return dataset

def eval_input_fn():
  dataset = tf.data.Dataset.range(100)
  # Make sequence data
  dataset = dataset.map(lambda x: {'x': [x]*10})
  return dataset

def model_fn(features, labels, mode, params):
  seq = features['x']
  with tf.device('/cpu:0'):
    arr = np.random.rand(3000000, 300)
    var = tf.get_variable('big', arr.shape, trainable=False)
    emb = tf.nn.embedding_lookup(var, seq)
  logits = tf.layers.dense(emb, 2)
  # Don't care about loss but have to provide something
  loss = tf.reduce_mean(logits)
  if mode == tf.estimator.ModeKeys.TRAIN:
    def init_fn(scaffold, sess):
      sess.run(var.initializer, {var.initial_value: arr})
    trainable_vars = tf.trainable_variables()
    saver = tf.train.Saver(var_list=trainable_vars)
    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver)
    global_step = tf.train.get_or_create_global_step()
    optimizer = tf.train.GradientDescentOptimizer(0.1)
    train_op = optimizer.minimize(loss, global_step=global_step)
    output_spec = tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      scaffold=scaffold,
      train_op=train_op)
  elif mode == tf.estimator.ModeKeys.EVAL:
    output_spec = tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss)
  return output_spec

estimator = tf.estimator.Estimator(model_fn=model_fn)
evaluator = InMemoryEvaluatorHook(
    estimator,
    eval_input_fn,
    every_n_iter=300)
estimator.train(train_input_fn, hooks=[evaluator])
```
```python
# hooks.py
import os

import tensorflow as tf
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.training import training


class InMemoryEvaluatorHook(training.SessionRunHook):
  """"""Hook to run evaluation in training without a checkpoint.
  Example:
  python
  def train_input_fn():
    ...
    return train_dataset
  def eval_input_fn():
    ...
    return eval_dataset
  estimator = tf.estimator.DNNClassifier(...)
  evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(
      estimator, eval_input_fn)
  estimator.train(train_input_fn, hooks=[evaluator])
  
  Current limitations of this approach are:
  * It doesn't support multi-node distributed mode.
  * It doesn't support saveable objects other than variables (such as boosted
    tree support)
  * It doesn't support custom saver logic (such as ExponentialMovingAverage
    support)

  COPIED FROM TENSORFLOW REPO (FUTURE VERSION 1.14.0), PERMALINK BELOW
  https://github.com/tensorflow/estimator/blob/faa1058f2664b952c246e9097e898fab8042b0ce/tensorflow_estimator/python/estimator/hooks/hooks.py#L66
  """"""

  def __init__(self,
               estimator,
               input_fn,
               steps=None,
               hooks=None,
               name=None,
               every_n_iter=100):
    """"""Initializes a `InMemoryEvaluatorHook`.
    Args:
      estimator: A `tf.estimator.Estimator` instance to call evaluate.
      input_fn:  Equivalent to the `input_fn` arg to `estimator.evaluate`. A
        function that constructs the input data for evaluation.
        See [Createing input functions](
        https://tensorflow.org/guide/premade_estimators#create_input_functions)
        for more information. The function should construct and return one of
        the following:
          * A 'tf.data.Dataset' object: Outputs of `Dataset` object must be a
            tuple (features, labels) with same constraints as below.
          * A tuple (features, labels): Where `features` is a `Tensor` or a
            dictionary of string feature name to `Tensor` and `labels` is a
            `Tensor` or a dictionary of string label name to `Tensor`. Both
            `features` and `labels` are consumed by `model_fn`. They should
            satisfy the expectation of `model_fn` from inputs.
      steps: Equivalent to the `steps` arg to `estimator.evaluate`.  Number of
        steps for which to evaluate model. If `None`, evaluates until `input_fn`
        raises an end-of-input exception.
      hooks: Equivalent to the `hooks` arg to `estimator.evaluate`. List of
        `SessionRunHook` subclass instances. Used for callbacks inside the
        evaluation call.
      name:  Equivalent to the `name` arg to `estimator.evaluate`. Name of the
        evaluation if user needs to run multiple evaluations on different data
        sets, such as on training data vs test data. Metrics for different
        evaluations are saved in separate folders, and appear separately in
        tensorboard.
      every_n_iter: `int`, runs the evaluator once every N training iteration.
    Raises:
      ValueError: if `every_n_iter` is non-positive or it's not a single machine
        training
    """"""
    if every_n_iter is None or every_n_iter <= 0:
      raise ValueError('invalid every_n_iter=%s.' % every_n_iter)
    if (estimator.config.num_ps_replicas > 0 or
        estimator.config.num_worker_replicas > 1):
      raise ValueError(
          'InMemoryEvaluator supports only single machine (aka Local) setting.')
    self._estimator = estimator
    self._input_fn = input_fn
    self._steps = steps
    self._name = name
    self._every_n_iter = every_n_iter
    self._eval_dir = os.path.join(self._estimator.model_dir, 'eval'
                                  if not name else 'eval_' + name)

    self._graph = None
    self._hooks = _check_hooks_type(hooks)
    self._hooks.extend(self._estimator._convert_eval_steps_to_hooks(steps))
    self._timer = training.SecondOrStepTimer(every_steps=every_n_iter)

  def begin(self):
    """"""Build eval graph and restoring op.""""""
    self._timer.reset()
    self._iter_count = 0
    self._graph = ops.Graph()
    with self._graph.as_default():
      (self._scaffold, self._update_op, self._eval_dict,
       self._all_hooks) = self._estimator._evaluate_build_graph(
           self._input_fn, self._hooks, checkpoint_path=None)

      if self._scaffold.saver is not None:
        raise ValueError('InMemoryEvaluator does not support custom saver')
      if self._scaffold.init_fn is not None:
        raise ValueError('InMemoryEvaluator does not support custom init_fn')

      self._var_name_to_eval_var = {
          v.name: v for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)
      }
      self._var_name_to_placeholder = {
          v.name: array_ops.placeholder(v.dtype)
          for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)
      }

  def after_create_session(self, session, coord):  # pylint: disable=unused-argument
    """"""Does first run which shows the eval metrics before training.""""""
    if ops.get_collection(ops.GraphKeys.SAVEABLE_OBJECTS):
      raise ValueError(
          'InMemoryEvaluator does not support saveables other than global '
          'variables.')
    self._var_name_to_train_var = {
        v.name: v for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)
    }
    var_names_to_transfer = set(self._var_name_to_placeholder.keys()) & set(
        self._var_name_to_train_var.keys())
    # Filter training var names that do not exist in evaluation
    self._var_name_to_train_var = {
        v_name: self._var_name_to_train_var[v_name]
        for v_name in var_names_to_transfer
    }
    # Filter eval var names that do not exist in training
    self._var_name_to_eval_var = {
        v_name: self._var_name_to_eval_var[v_name]
        for v_name in var_names_to_transfer
    }

    with self._graph.as_default():
      self._var_feed_op = control_flow_ops.group([
          state_ops.assign(self._var_name_to_eval_var[v_name],
                           self._var_name_to_placeholder[v_name])
          for v_name in var_names_to_transfer
      ])

    self._evaluate(session)

  def _evaluate(self, train_session):
    var_name_to_value = train_session.run(self._var_name_to_train_var)
    placeholder_to_value = {
        self._var_name_to_placeholder[v_name]: var_name_to_value[v_name]
        for v_name in var_name_to_value
    }

    def feed_variables(scaffold, session):
      del scaffold
      session.run(self._var_feed_op, feed_dict=placeholder_to_value)

    scaffold = training.Scaffold(
        init_fn=feed_variables, copy_from_scaffold=self._scaffold)

    with self._graph.as_default():
      self._estimator._evaluate_run(
          checkpoint_path=None,
          scaffold=scaffold,
          update_op=self._update_op,
          eval_dict=self._eval_dict,
          all_hooks=self._all_hooks,
          output_dir=self._eval_dir)

    self._timer.update_last_triggered_step(self._iter_count)

  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument
    """"""Runs evaluator.""""""
    self._iter_count += 1
    if self._timer.should_trigger_for_step(self._iter_count):
      self._evaluate(run_context.session)

  def end(self, session):  # pylint: disable=unused-argument
    """"""Runs evaluator for final model.""""""
    self._evaluate(session)


def _check_hooks_type(hooks):
  """"""Returns hooks if all are `SessionRunHook`, raises TypeError otherwise.""""""
  hooks = list(hooks or [])
  for h in hooks:
    if not isinstance(h, training.SessionRunHook):
      raise TypeError('Hooks must be a SessionRunHook, given: {}'.format(h))
  return hooks
```"
30806,Inference on Larger Data Size on Edge ,"**System information**
- Have I written custom code (Custom Code using resources and my own design):
- OS Platform and Distribution (Linux Ubuntu 16.04):
- Mobile device (trying inference on google coral dev board)
- TensorFlow installed from (pip3 install tensorflow==2.0.0-beta1):
- TensorFlow version (2.0.0-beta1):
- Python version: 3.6.9
- Bazel version (not used):
- GCC/Compiler version (not used):
- CUDA/cuDNN version: not used
- GPU model and memory: not used 
- Run On CPU 

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    2
Core(s) per socket:    4
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 94
Model name:            Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz
Stepping:              3
CPU MHz:               800.210
CPU max MHz:           3500.0000
CPU min MHz:           800.0000
BogoMIPS:              5183.86
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              6144K

32 GB RAM

```
# The full neural network code!
###############################
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import UpSampling1D

#Reference:https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb
def UpSamplingCustom2D(scale=(2, 2)):
  if isinstance(scale, int):
    scale = (scale, scale)

  def upsampling(x):
    shape = x.shape
    print(""Upsampling : "", shape)
    x = keras.layers.Concatenate(-2)([x] * scale[0])
    x = keras.layers.Reshape([shape[1] * scale[0], shape[2], shape[3]])(x)
    x = keras.layers.Concatenate(-1)([x] * scale[1])
    x = keras.layers.Reshape([shape[1] * scale[0], shape[2] * scale[1], shape[3]])(x)
    return x

  return upsampling


image_size = 28
output_image_size = image_size * 2

train_images = np.random.rand(10,image_size,image_size,3)
train_images_labels = np.random.rand(10,output_image_size,output_image_size,3)


test_images = np.random.rand(10,image_size,image_size,3)
test_images_labels = np.random.rand(10,output_image_size,output_image_size,3)

inputs = keras.layers.Input(shape=(image_size, image_size, 3))
x = keras.layers.Dense(image_size, activation='relu')(inputs)
x = keras.layers.Dense(image_size, activation='relu')(x)
x = UpSamplingCustom2D()(x)
decoded = keras.layers.Dense(3, activation='relu')(x)
model = keras.models.Model(inputs, decoded)

#Compile the model.
model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
)

#Train the model.
model.fit(
  train_images,
  train_images_labels,
  epochs=1,
  batch_size=32,
)

#Evaluate the model.
model.evaluate(
  test_images,
  test_images_labels
)



#Save the model to disk.
model.save_weights('model.h5')

#Load the model from disk later using:
model.load_weights('model.h5')

#Predict on the first 5 test images.
predictions = model.predict(test_images)

print(predictions[0].shape)


def representative_dataset_gen():
  for i in range(5):
    yield [train_images[i: i + 1].astype(np.float32)]

keras_file = ""upsampling2d.h5""
tf.keras.models.save_model(model, keras_file)

#Convert to TensorFlow Lite model.
if (tf.__version__ == '1.14.0'):
  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
if (tf.__version__ == '2.0.0-beta1'):
  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
tflite_fname = ""upsampling2d_"" + str(tf.__version__) + "".tflite""
open(tflite_fname, ""wb"").write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=tflite_fname)
interpreter.allocate_tensors()

input_detail = interpreter.get_input_details()[0]
output_detail = interpreter.get_output_details()[0]

print(""Input Details : "", input_detail)
print(""Output Details : "", output_detail)

def quantize(real_value):
  std, mean = input_detail['quantization']
  return (real_value / std + mean).astype(np.uint8)


sample_input = quantize(test_images[0]).reshape(input_detail['shape'])
print(""Sample Input Shape : "", sample_input.shape)
print(""Inference Sample Input Shape : "", sample_input.shape)

interpreter.set_tensor(input_detail['index'], sample_input)
interpreter.invoke()

#original_image = test_images[0].reshape((28, 28))
pred_original_model = model.predict(test_images[:1])
pred_quantized_model = interpreter.get_tensor(output_detail['index'])

print(""Prediction : "",pred_quantized_model.shape)
```

Because of quantization issue for UpSampling2D, I used a snippet from 
`https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb`

My issue is when I use image sizes from 28 till 256 (I just used powers of 2 and used 28 just to check), it worked fine for inferencing. I tested inferencing on the CPU itself using the interpreter package. But 512 or anything above freezes the machine and gives the following error. 

`2019-07-17 08:33:18.567609: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory.
2019-07-17 08:33:20.626046: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory.
2019-07-17 08:33:26.517300: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 10737418240 exceeds 10% of system memory.
2019-07-17 08:33:27.836867: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21474836480 exceeds 10% of system memory.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)
`
Is there a limit for the array size that can be used at inferencing after making the input array flatten. Or am I doing something wrong here? 


"
30804,tf.keras.layers.Conv2D fails because it captures tensor from inner function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

**Describe the current behavior**
Since commit `546308e322a6b95542ba9f3cbb14136128aaad1e` a `tf.keras.layers.Conv2D` in my training code fails with the following error:

```
Traceback (most recent call last):                                                                                                                                        
  File ""./train.py"", line 351, in <module>                                                                                                   
    total_loss = train_step()                                                                                                                    
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 417, in __call__                   
    self._initialize(args, kwds, add_initializers_to=initializer_map)                                                                                
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 360, in _initialize                  
    *args, **kwds))                                                                                                                                  
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1709, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)                                                                            
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2013, in _maybe_define_function             
    graph_function = self._create_graph_function(args, kwargs)                                                                                     
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1899, in _create_graph_function            
    capture_by_value=self._capture_by_value),                                                                                                    
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 795, in func_graph_from_py_func      
    func_outputs = python_func(*func_args, **func_kwargs)                                                                                             
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 310, in wrapped_fn                      
    return weak_wrapped_fn().__wrapped__(*args, **kwds)                                                                                               
  File ""/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 785, in wrapper                       
    raise e.ag_error_metadata.to_exception(type(e))                                                                                                   
ValueError: in converted code:                                                                                                                        
                                                                                                                                                      
    ./train.py:328 train_step  *                                                                                                                      
        per_replica_losses = distribution_strategy.extended.call_for_each_replica(joint_train_step_fn, args=())                                  
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1776 call_for_each_replica                                                                                                                                                        
        return self._call_for_each_replica(fn, args, kwargs)                                                                                                        
    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:173 call  *                                                                   
        x = self.module_3a(x, train_batch_norm=train_batch_norm)                                                                                   
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                               
        outputs = call_fn(inputs, *args, **kwargs)                                                                                                 
    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:97 call  *                                                                      
        x = self.conv2(x)                                                                                                                        
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                                
        outputs = call_fn(inputs, *args, **kwargs)                                                                                                  
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:198 call                          
        outputs = self._convolution_op(inputs, self.kernel)                                                                                                                                                                                                                                 
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:1078 __call__                                            
        return self.conv_op(inp, filter)                                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:634 __call__                 
        return self.call(inp, filter)                                                                                                                                                                                                                        
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:610 _with_space_to_batch_call                         
        block_shape=self.dilation_rate)                                                                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:3084 required_space_to_batch_paddings              
        pad_start = base_paddings[:, 0]                                                                                                                                                                                                                      
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:694 _slice_helper                                                                                                                                              
        name=name)                                                                                                                                                                                                                                           
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:860 strided_slice                                                                                                                                              
        shrink_axis_mask=shrink_axis_mask)                                                                                                                                                                                                                   
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:10397 strided_slice                                                                                                                                        
        shrink_axis_mask=shrink_axis_mask, name=name)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:793 _apply_op_helper
        op_def=op_def)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:528 create_op
        inp = self.capture(inp)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:589 capture
        % (tensor, tensor.graph, self))

    ValueError: Trying to capture a tensor from an inner function. This can be caused by accessing a tensor defined inside a loop or conditional body, or a subfunction, from a calling function, without going through the proper return value mechanism. Consider using TensorFlow mechanisms such as TensorArrays to retur
n tensors from inner functions or loop / conditional bodies. Tensor: Tensor(""conv2/stack:0"", shape=(2, 2), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0); tensor graph: FuncGraph(name=build_graph, id=140610256740760); this graph: FuncGraph(name=train_step, id=140614894336264)
```

This error only occurs with `dilation_rate` set to 2. With `dilation_rate == 1` the training script runs.
It also works if I revert `546308e322a6b95542ba9f3cbb14136128aaad1e`.

**Describe the expected behavior**
The training runs with `dilation_rate == 2`.

**Code to reproduce the issue**
So far I could not come up with a small testcase to reproduce the issue. I will continue to try, but until then I only have the backtrace and the problematic commit."
30803,Padding causing 64x increase in memory size on TPU,"I'm trying to train a variation of a u-net on a TPU. However, I can't seem to get it to fit into memory. I'm using tf.keras with tensorflow 1.13.2. Here's a portion of the error logs proving a list of the ops that are the greatest memory hogs. CrossReplicaSum appears to be especially bad, but so are some multiplication ops. I don't quite know how to go about debugging this as the model won't even compile so I don't know what layers these ops actually correspond to.

```
RuntimeError: Compilation failed: Compilation failure: Ran out of memory in memory space hbm. Used 27.90G of 16.00G hbm. Exceeded hbm capacity by 11.90G.

Total hbm usage >= 27.90G:
    reserved        528.00M
    program          27.38G
    arguments       unknown size

Output size unknown.

Program hbm requirement 27.38G:
    reserved          12.0K
    scoped             1.0K
    HLO temp         27.38G (5.6% utilization, 0.0% fragmentation (1.14M))

  Largest program allocations in hbm:

  1. Size: 8.00G
     Operator: op_type=""CrossReplicaSum"" op_name=""tpu_140280287273760/CrossReplicaSum""
     Shape: f32[256,512,128,2]{3,2,1,0}
     Unpadded size: 128.00M
     Extra memory due to padding: 7.88G (64.0x expansion)
     XLA label: %cross-replica-sum = f32[256,512,128,2]{3,2,1,0} cross-replica-sum(f32[256,512,128,2]{3,2,1,0} %bitcast.1), replica_groups={{0,1,2,3,4,5,6,7}}, barrier=""custom:0"", to_apply=%sum.902, metadata=
{op_type=""CrossReplicaSum"" op_name=""tpu_140280287273760/CrossRep...
     Allocation type: HLO temp
     ==========================

  2. Size: 8.00G
     Operator: op_type=""Mul"" op_name=""tpu_140280287273760/mul_1""
     Shape: f32[8,32,512,128,2]{4,3,2,1,0}
     Unpadded size: 128.00M
     Extra memory due to padding: 7.88G (64.0x expansion)
     XLA label: %fusion.4 = (f32[8,32,512,128,2]{4,3,2,1,0}, f32[8,32,512,128,2]{4,3,2,1,0}) fusion(f32[8]{0} %fusion.1265, f32[32,512,128,2]{3,2,1,0} %reshape.319, f32[32,512,128,2]{3,2,1,0} %copy.5), kind=k
Loop, calls=%fused_computation.4, metadata={op_type=""Mul"" op_nam...
     Allocation type: HLO temp
     ==========================

  3. Size: 8.00G
     Operator: op_type=""Mul"" op_name=""tpu_140280287273760/mul_1""
     Shape: f32[8,32,512,128,2]{4,3,2,1,0}
     Unpadded size: 128.00M
     Extra memory due to padding: 7.88G (64.0x expansion)
     XLA label: %fusion.4 = (f32[8,32,512,128,2]{4,3,2,1,0}, f32[8,32,512,128,2]{4,3,2,1,0}) fusion(f32[8]{0} %fusion.1265, f32[32,512,128,2]{3,2,1,0} %reshape.319, f32[32,512,128,2]{3,2,1,0} %copy.5), kind=k
Loop, calls=%fused_computation.4, metadata={op_type=""Mul"" op_nam...
     Allocation type: HLO temp
     ==========================

    4. Size: 1.00G
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_1/Conv2D""
     Shape: f32[32,512,128,32]{3,0,2,1}
     Unpadded size: 256.00M
     Extra memory due to padding: 768.00M (4.0x expansion)
     XLA label: %fusion.12 = f32[32,512,128,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1107, f32[32,512,128,32]{3,0,2,1} %fusion.13, bf16[4,4,32,32]{3,2,1,0} %reshape.3), kind=kOutput, calls=%fused_com
putation.12, metadata={op_type=""Conv2D"" op_name=""tpu_14028028727...
     Allocation type: HLO temp
     ==========================

  5. Size: 1.00G
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d/Conv2D""
     Shape: f32[32,512,128,32]{3,0,2,1}
     Unpadded size: 256.00M
     Extra memory due to padding: 768.00M (4.0x expansion)
     XLA label: %fusion.13 = f32[32,512,128,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1105, bf16[32,512,128,2]{0,3,2,1} %copy.4, f32[4,4,2,32]{3,2,1,0} %get-tuple-element.1106), kind=kOutput, calls=%f
used_computation.13, metadata={op_type=""Conv2D"" op_name=""tpu_140...
     Allocation type: HLO temp
     ==========================

  6. Size: 256.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_4/Conv2D""
     Shape: f32[32,256,64,64]{3,0,2,1}
     Unpadded size: 128.00M
     Extra memory due to padding: 128.00M (2.0x expansion)
     XLA label: %fusion.32 = f32[32,256,64,64]{3,0,2,1} fusion(f32[64]{0} %get-tuple-element.1165, bf16[4,4,32,64]{3,2,1,0} %reshape.6, f32[32,256,64,32]{3,0,2,1} %get-tuple-element.964), kind=kOutput, calls=
%fused_computation.32, metadata={op_type=""Conv2D"" op_name=""tpu_1...
     Allocation type: HLO temp
     ==========================

  7. Size: 256.00M
     Operator: op_type=""Conv2DBackpropInput"" op_name=""tpu_140280287273760/conv2d_transpose_4/conv2d_transpose""
     Shape: f32[32,256,64,32]{3,0,2,1}
     Unpadded size: 64.00M
     Extra memory due to padding: 192.00M (4.0x expansion)
     XLA label: %fusion.23 = f32[32,256,64,32]{3,0,2,1} fusion(f32[4,4,32,128]{3,2,1,0} %get-tuple-element.1186, f32[32]{0} %get-tuple-element.1185, bf16[32,128,32,64]{3,0,2,1} %get-tuple-element.965, f32[32,
128,32,64]{3,0,2,1} %fusion.48), kind=kOutput, calls=%fused_comp...
     Allocation type: HLO temp
     ==========================

  8. Size: 256.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_34/Conv2D""
     Shape: f32[32,256,64,32]{3,0,2,1}
     Unpadded size: 64.00M
     Extra memory due to padding: 192.00M (4.0x expansion)
     XLA label: %fusion.22 = f32[32,256,64,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1161, f32[32,256,64,32]{3,0,2,1} %fusion.23, bf16[4,4,32,32]{3,2,1,0} %reshape.12), kind=kOutput, calls=%fused_comp
utation.22, metadata={op_type=""Conv2D"" op_name=""tpu_140280287273...
     Allocation type: HLO temp
     ==========================

      9. Size: 128.00M
     Operator: op_type=""LeakyRelu"" op_name=""tpu_140280287273760/leaky_re_lu/LeakyRelu""
     Shape: bf16[32,256,64,32]{3,0,2,1}
     Unpadded size: 32.00M
     Extra memory due to padding: 96.00M (4.0x expansion)
     XLA label: %fusion.38 = (bf16[32,256,64,32]{3,0,2,1}, f32[32,256,64,32]{3,0,2,1}) fusion(f32[32]{0} %get-tuple-element.1151, f32[32,512,128,32]{3,0,2,1} %fusion.14, bf16[4,4,32,32]{3,2,1,0} %reshape.5),
kind=kOutput, calls=%fused_computation.38, metadata={op_type=""Le...
     Allocation type: HLO temp
     ==========================

  10. Size: 64.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_32/Conv2D""
     Shape: f32[32,128,32,64]{3,0,2,1}
     Unpadded size: 32.00M
     Extra memory due to padding: 32.00M (2.0x expansion)
     XLA label: %fusion.46 = f32[32,128,32,64]{3,0,2,1} fusion(f32[64]{0} %get-tuple-element.1157, f32[32,128,32,64]{3,0,2,1} %fusion.47, bf16[4,4,64,64]{3,2,1,0} %reshape.10), kind=kOutput, calls=%fused_comp
utation.46, metadata={op_type=""Conv2D"" op_name=""tpu_140280287273...
     Allocation type: HLO temp
     ==========================

  11. Size: 64.00M
     Operator: op_type=""Conv2DBackpropInput"" op_name=""tpu_140280287273760/conv2d_transpose_3/conv2d_transpose""
     Shape: f32[32,128,32,64]{3,0,2,1}
     Unpadded size: 32.00M
     Extra memory due to padding: 32.00M (2.0x expansion)
     XLA label: %fusion.47 = f32[32,128,32,64]{3,0,2,1} fusion(f32[4,4,64,256]{3,2,1,0} %get-tuple-element.1184, f32[64]{0} %get-tuple-element.1183, bf16[32,64,16,128]{3,0,2,1} %get-tuple-element.967, f32[32,
64,16,128]{3,0,2,1} %fusion.99), kind=kOutput, calls=%fused_comp...
     Allocation type: HLO temp
     ==========================

  12. Size: 64.00M
     Operator: op_type=""InfeedDequeueTuple"" op_name=""infeed-train""
     Shape: bf16[32,512,128,2]{0,3,2,1}
     Unpadded size: 8.00M
     Extra memory due to padding: 56.00M (8.0x expansion)
     XLA label: %copy.4 = bf16[32,512,128,2]{0,3,2,1} copy(bf16[32,512,128,2]{3,2,1,0} %reshape.318), metadata={op_type=""InfeedDequeueTuple"" op_name=""infeed-train""}
     Allocation type: HLO temp
     ==========================

  13. Size: 64.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_8/Conv2D""
     Shape: f32[32,128,32,128]{3,0,2,1}
     Unpadded size: 64.00M
     XLA label: %fusion.56 = f32[32,128,32,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1173, f32[4,4,64,128]{3,2,1,0} %copy.77, f32[32,128,32,64]{3,0,2,1} %get-tuple-element.966), kind=kOutput, calls=
%fused_computation.56, metadata={op_type=""Conv2D"" op_name=""tpu_1...
     Allocation type: HLO temp
     ==========================


     14. Size: 32.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_12/Conv2D""
     Shape: f32[32,64,16,256]{3,0,2,1}
     Unpadded size: 32.00M
     XLA label: %fusion.78 = f32[32,64,16,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1113, f32[4,4,128,256]{3,2,1,0} %get-tuple-element.1114, f32[32,64,16,128]{3,0,2,1} %get-tuple-element.968), kind=
kOutput, calls=%fused_computation.78, metadata={op_type=""Conv2D""...
     Allocation type: HLO temp
     ==========================

  15. Size: 32.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_13/Conv2D""
     Shape: f32[32,64,16,256]{3,0,2,1}
     Unpadded size: 32.00M
     XLA label: %fusion.77 = f32[32,64,16,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1115, f32[32,64,16,256]{3,0,2,1} %fusion.78, f32[4,4,256,256]{3,2,1,0} %get-tuple-element.1116), kind=kOutput, cal
ls=%fused_computation.77, metadata={op_type=""Conv2D"" op_name=""tp...
     Allocation type: HLO temp
     ==========================

  16. Size: 32.00M
     Operator: op_type=""LeakyRelu"" op_name=""tpu_140280287273760/leaky_re_lu_1/LeakyRelu""
     Shape: bf16[32,128,32,64]{3,0,2,1}
     Unpadded size: 16.00M
     Extra memory due to padding: 16.00M (2.0x expansion)
     XLA label: %fusion.89 = (bf16[32,128,32,64]{3,0,2,1}, f32[32,128,32,64]{3,0,2,1}) fusion(f32[64]{0} %get-tuple-element.1171, f32[32,256,64,64]{3,0,2,1} %fusion.33, bf16[4,4,64,64]{3,2,1,0} %reshape.9), k
ind=kOutput, calls=%fused_computation.87, metadata={op_type=""Lea...
     Allocation type: HLO temp
     ==========================

  17. Size: 16.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_16/Conv2D""
     Shape: f32[32,32,8,512]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %fusion.107 = f32[32,32,8,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1121, f32[4,4,256,512]{3,2,1,0} %get-tuple-element.1122, f32[32,32,8,256]{3,0,2,1} %get-tuple-element.970), kind=k
Output, calls=%fused_computation.105, metadata={op_type=""Conv2D""...
     Allocation type: HLO temp
     ==========================

  18. Size: 16.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_17/Conv2D""
     Shape: f32[32,32,8,512]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %fusion.106 = f32[32,32,8,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1123, f32[32,32,8,512]{3,0,2,1} %fusion.107, f32[4,4,512,512]{3,2,1,0} %get-tuple-element.1124), kind=kOutput, cal
ls=%fused_computation.104, metadata={op_type=""Conv2D"" op_name=""t...
     Allocation type: HLO temp
     ==========================

  19. Size: 16.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_30/Conv2D""
     Shape: f32[32,64,16,128]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %fusion.97 = f32[32,64,16,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1153, f32[32,64,16,128]{3,0,2,1} %fusion.98, f32[4,4,128,128]{3,2,1,0} %get-tuple-element.1154), kind=kOutput, cal
ls=%fused_computation.95, metadata={op_type=""Conv2D"" op_name=""tp...
     Allocation type: HLO temp
     ==========================

    19. Size: 16.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_30/Conv2D""
     Shape: f32[32,64,16,128]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %fusion.97 = f32[32,64,16,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1153, f32[32,64,16,128]{3,0,2,1} %fusion.98, f32[4,4,128,128]{3,2,1,0} %get-tuple-element.1154), kind=kOutput, cal
ls=%fused_computation.95, metadata={op_type=""Conv2D"" op_name=""tp...
     Allocation type: HLO temp
     ==========================

  20. Size: 16.00M
     Operator: op_type=""Conv2DBackpropInput"" op_name=""tpu_140280287273760/conv2d_transpose_2/conv2d_transpose""
     Shape: f32[32,64,16,128]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %fusion.98 = f32[32,64,16,128]{3,0,2,1} fusion(f32[4,4,128,512]{3,2,1,0} %get-tuple-element.1182, f32[128]{0} %get-tuple-element.1181, bf16[32,32,8,256]{3,0,2,1} %get-tuple-element.969, f32[32
,32,8,256]{3,0,2,1} %fusion.179, f32[32,64,16,128]{3,0,2,1} %rng...
     Allocation type: HLO temp
     ==========================

  21. Size: 16.00M
     Operator: op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_5/dropout/random_uniform/RandomUniform""
     Shape: f32[32,64,16,128]{3,0,2,1}
     Unpadded size: 16.00M
     XLA label: %rng.5 = f32[32,64,16,128]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_5/dropout/rando
m_uniform/RandomUniform""}
     Allocation type: HLO temp
     ==========================

  22. Size: 8.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_21/Conv2D""
     Shape: f32[32,16,4,1024]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %fusion.186 = f32[32,16,4,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1133, f32[32,16,4,1024]{3,0,2,1} %fusion.187, f32[4,4,1024,1024]{3,2,1,0} %get-tuple-element.1134), kind=kOutput
, calls=%fused_computation.180, metadata={op_type=""Conv2D"" op_na...
     Allocation type: HLO temp
     ==========================

  23. Size: 8.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_28/Conv2D""
     Shape: f32[32,32,8,256]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %fusion.177 = f32[32,32,8,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1147, f32[32,32,8,256]{3,0,2,1} %fusion.178, f32[4,4,256,256]{3,2,1,0} %get-tuple-element.1148), kind=kOutput, cal
ls=%fused_computation.171, metadata={op_type=""Conv2D"" op_name=""t...
     Allocation type: HLO temp
     ==========================

  24. Size: 8.00M
     Operator: op_type=""Conv2DBackpropInput"" op_name=""tpu_140280287273760/conv2d_transpose_1/conv2d_transpose""
     Shape: f32[32,32,8,256]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %fusion.178 = f32[32,32,8,256]{3,0,2,1} fusion(f32[4,4,256,1024]{3,2,1,0} %get-tuple-element.1180, f32[256]{0} %get-tuple-element.1179, bf16[32,16,4,512]{3,0,2,1} %get-tuple-element.971, f32[3
2,16,4,512]{3,0,2,1} %fusion.229, f32[32,32,8,256]{3,0,2,1} %rng...
     Allocation type: HLO temp
     ==========================

  25. Size: 8.00M
     Operator: op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_4/dropout/random_uniform/RandomUniform""
     Shape: f32[32,32,8,256]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %rng.4 = f32[32,32,8,256]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_4/dropout/random
_uniform/RandomUniform""}
     Allocation type: HLO temp
     ==========================

  26. Size: 8.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_20/Conv2D""
     Shape: f32[32,16,4,1024]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %fusion.187 = f32[32,16,4,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1131, f32[4,4,512,1024]{3,2,1,0} %get-tuple-element.1132, f32[32,16,4,512]{3,0,2,1} %get-tuple-element.972), kin
d=kOutput, calls=%fused_computation.181, metadata={op_type=""Conv...
     Allocation type: HLO temp
     ==========================

  27. Size: 8.00M
     Operator: op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout/dropout/random_uniform/RandomUniform""
     Shape: f32[32,32,8,256]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %rng = f32[32,32,8,256]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout/dropout/random_uni
form/RandomUniform""}
     Allocation type: HLO temp
     ==========================

  28. Size: 8.00M
     Operator: op_type=""LeakyRelu"" op_name=""tpu_140280287273760/leaky_re_lu_2/LeakyRelu""
     Shape: bf16[32,64,16,128]{3,0,2,1}
     Unpadded size: 8.00M
     XLA label: %fusion.219 = (bf16[32,64,16,128]{3,0,2,1}, f32[32,64,16,128]{3,0,2,1}) fusion(f32[128]{0} %get-tuple-element.1111, f32[32,128,32,128]{3,0,2,1} %fusion.57, f32[4,4,128,128]{3,2,1,0} %get-tuple
-element.1112), kind=kOutput, calls=%fused_computation.209, meta...
     Allocation type: HLO temp
     ==========================

  29. Size: 4.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_26/Conv2D""
     Shape: f32[32,16,4,512]{3,0,2,1}
     Unpadded size: 4.00M
     XLA label: %fusion.227 = f32[32,16,4,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1143, f32[32,16,4,512]{3,0,2,1} %fusion.228, f32[4,4,512,512]{3,2,1,0} %get-tuple-element.1144), kind=kOutput, cal
ls=%fused_computation.217, metadata={op_type=""Conv2D"" op_name=""t...
     Allocation type: HLO temp
     ==========================

  30. Size: 4.00M
     Operator: op_type=""Conv2DBackpropInput"" op_name=""tpu_140280287273760/conv2d_transpose/conv2d_transpose""
     Shape: f32[32,16,4,512]{3,0,2,1}
     Unpadded size: 4.00M
     XLA label: %fusion.228 = f32[32,16,4,512]{3,0,2,1} fusion(f32[32,8,2,1024]{3,0,2,1} %fusion.339, f32[4,4,512,1024]{3,2,1,0} %get-tuple-element.1178, f32[512]{0} %get-tuple-element.1177, f32[32,16,4,512]{
3,0,2,1} %rng.3), kind=kOutput, calls=%fused_computation.218, me...
     Allocation type: HLO temp
     ==========================


  31. Size: 4.00M
     Operator: op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_3/dropout/random_uniform/RandomUniform""
     Shape: f32[32,16,4,512]{3,0,2,1}
     Unpadded size: 4.00M
     XLA label: %rng.3 = f32[32,16,4,512]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_3/dropout/random
_uniform/RandomUniform""}
     Allocation type: HLO temp
     ==========================

  32. Size: 4.00M
     Operator: op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_1/dropout/random_uniform/RandomUniform""
     Shape: f32[32,16,4,512]{3,0,2,1}
     Unpadded size: 4.00M
     XLA label: %rng.1 = f32[32,16,4,512]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=""RandomUniform"" op_name=""tpu_140280287273760/dropout_1/dropout/random
_uniform/RandomUniform""}
     Allocation type: HLO temp
     ==========================

  33. Size: 4.00M
     Operator: op_type=""LeakyRelu"" op_name=""tpu_140280287273760/leaky_re_lu_3/LeakyRelu""
     Shape: bf16[32,32,8,256]{3,0,2,1}
     Unpadded size: 4.00M
     XLA label: %fusion.301 = (bf16[32,32,8,256]{3,0,2,1}, f32[32,32,8,256]{3,0,2,1}) fusion(f32[32,32,8,256]{3,0,2,1} %rng, f32[256]{0} %get-tuple-element.1119, f32[32,64,16,256]{3,0,2,1} %fusion.79, f32[4,4
,256,256]{3,2,1,0} %get-tuple-element.1120), kind=kOutput, calls...
     Allocation type: HLO temp
     ==========================

  34. Size: 2.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_24/Conv2D""
     Shape: f32[32,8,2,1024]{3,0,2,1}
     Unpadded size: 2.00M
     XLA label: %fusion.338 = f32[32,8,2,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1139, f32[4,4,1024,1024]{3,2,1,0} %get-tuple-element.1140, f32[32,8,2,1024]{3,0,2,1} %fusion.421), kind=kOutput,
calls=%fused_computation.326, metadata={op_type=""Conv2D"" op_name...
     Allocation type: HLO temp
     ==========================

  35. Size: 2.00M
     Operator: op_type=""Conv2D"" op_name=""tpu_140280287273760/conv2d_23/Conv2D""
     Shape: f32[32,8,2,1024]{3,0,2,1}
     Unpadded size: 2.00M
     XLA label: %fusion.421 = f32[32,8,2,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1137, f32[32,8,2,1024]{3,0,2,1} %rng.2, f32[32,16,4,1024]{3,0,2,1} %fusion.188, f32[4,4,1024,1024]{3,2,1,0} %get-
tuple-element.1138), kind=kOutput, calls=%fused_computation.407,...
     Allocation type: HLO temp
     ==========================
```"
30802,Using tf.function while enumerating a dataset causes an infinite loop,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Using `tf.function` when enumerating a dataset will cause an infinite loop.

**Describe the expected behavior**
Using `tf.function` when enumerating a dataset should not change the looping behavior.

**Code to reproduce the issue**
The code snippet below will hang after the last function call. I'm not printing anything because calling `tf.print` results in a syntax error on colab and I know that these snippets are being run on colab by you. When printing the variable `i`, it's clear that the loop just never stops, i.e. `i` increase indefinitely.

```python
import tensorflow as tf

ds = tf.data.Dataset.from_tensor_slices([1,2,3,4])

def test_loop_without_enumerate_without_decorator(ds):
    for val in ds:
        pass

@tf.function
def test_loop_without_enumerate_with_decorator(ds):
    for val in ds:
        pass

def test_loop_with_enumerate_without_decorator(ds):
    for i, val in enumerate(ds):
        pass

@tf.function
def test_loop_with_enumerate_with_decorator(ds):
    for i, val in enumerate(ds):
        pass

print(""Without tf.function and without enumerate"")
test_loop_without_enumerate_without_decorator(ds)

print(""Without tf.function and with enumerate"")
test_loop_with_enumerate_without_decorator(ds)

print(""With tf.function and without enumerate"")
test_loop_without_enumerate_with_decorator(ds)

print(""With tf.function and with enumerate"")
test_loop_with_enumerate_with_decorator(ds)
```

**Other info / logs**
Output of the above snippet:
```
Without tf.function and without enumerate
Without tf.function and with enumerate
With tf.function and without enumerate
With tf.function and with enumerate
**HANGS HERE**
```
"
30801,fit_generator with use_multiprocessing=True causes memory usage grow (leakage) on TF 1.13+,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9 (GCP AI Platform Jobs and notebooks), Ubuntu 18.04 (custom) 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): preinstalled via official GCP deep learning VM image, also via pip and conda
- TensorFlow version (use command below): 1.13 and 1.14
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: NVIDIA Tesla K80 and GFX 1080

**Describe the current behavior**
We use `fit_generator` with a custom build on `tensorflow.keras.utils.Sequence`, but we also have seen similar behavior with `tensorflow.keras.preprocessing.image.ImageDataGenerator`. Namely, when we have the default `use_multiprocessing=False`, the training process is slow but the memory usage saturates at some point and everything works fine. When we use `use_multiprocessing=True`, despite the number of workers or `max_queue_size` the memory keeps growing linearly up to the point when the process gets killed by the system. See the enclosed memory graph from GCP AI Platform Job monitoring: 
![ai_job_mem](https://user-images.githubusercontent.com/1014323/61370638-2bcc9f00-a893-11e9-829e-7ed1ee16039d.png)
The same code worked fine on TF 1.12, i.e. the memory saturated at some point, but broke from 1.13, and we see the same effect with 1.14. 
**Describe the expected behavior**
The memory shouldn't grow indefinitely as it did in TF 1.12. See the memory timeseries for TF 1.12 below:
![ai_job_mem_ok](https://user-images.githubusercontent.com/1014323/61373323-ca5bfe80-a899-11e9-857d-47c85f985a78.png)

**Code to reproduce the issue**
Our Data Generator looks as follows:
```python
class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, affect_net_dir, data_file, batch_size, aug=True):
        self.affect_net_dir = affect_net_dir
        with FileIO(os.path.join(self.affect_net_dir, data_file), 'r') as fr:
            self.data = pd.read_csv(fr)
        self.batch_size = batch_size
        self.aug = aug
        self.indexes = np.arange(self.data.shape[0])
        np.random.shuffle(self.indexes)

    @property
    def steps(self):
        return int(np.floor(self.data.shape[0] / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]

        x = np.zeros(shape=(self.batch_size, 224, 224, 3))
        y = np.zeros(shape=(self.batch_size, 11))

        for i, b_index in enumerate(batch_indexes):
            file_path = self.data.iloc[b_index]['subDirectory_filePath']
            with FileIO(os.path.join(self.affect_net_dir, file_path), 'rb') as fr:
                face_img = Image.open(BytesIO(fr.read()))
            face_x = self.data.iloc[b_index]['face_x']
            face_y = self.data.iloc[b_index]['face_y']
            face_width = self.data.iloc[b_index]['face_width']
            face_height = self.data.iloc[b_index]['face_height']
            expression = self.data.iloc[b_index]['expression']
            # crop face from image
            face_img = face_img.crop([face_x, face_y, face_x + face_width, face_y + face_height])
            face_img = face_img.resize(size=(224, 224), resample=Image.LANCZOS)
            face_img = tf.keras.preprocessing.image.img_to_array(face_img)

            # face augmentation if needed
            if self.aug:
                if np.random.rand() >= 0.5:
                    face_img = face_aug.random_brightness(face_img)
                if np.random.rand() >= 0.5:
                    face_img = face_aug.random_hue(face_img)
                if np.random.rand() >= 0.5:
                    face_img = face_aug.random_saturation(face_img)
                if np.random.rand() >= 0.5:
                    face_img = face_aug.random_contrast(face_img)
                if np.random.rand() >= 0.5:
                    face_img = face_aug.horizontal_mirror(face_img)

            x[i] = face_img
            y[i] = tf.keras.utils.to_categorical(expression, num_classes=11)

        return tf.keras.applications.resnet50.preprocess_input(x), y

    def __len__(self):
        return int(np.floor(self.data.shape[0] / self.batch_size))
```
But it is quite problem specific and we use it with a large data set O(100k) samples. 

You can reproduce a similar memory growing behaviour with the Flower example https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb. When I run it on GCP AI Platform notebook with TF 1.14 and NVIDIA Tesla K80, with multiprocessing on, i.e. (train the model section):
```python
epochs = 80

history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size))),
    use_multiprocessing=True,
    workers=3,
)
```
I get the below memory plot in Stackdriver:
![ai_job_mem_sample](https://user-images.githubusercontent.com/1014323/61373689-bf559e00-a89a-11e9-95e5-397ce8c1113f.png)
It is not as profound but the same trace remains quite flat when train with `use_multiprocessing=False`. I guess in our case the number of iteration within one epoch, combined with the linear memory growth kills the machine.

"
30800,tf.function doesn't see methods of class,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from binary
- TensorFlow version: 2.0beta
- Python version: 3.6
- CUDA/cuDNN version: 10.0 

**Describe the current behavior**

I am working with GAN. I have `Trainer` class which has the following methods:  `pass_one_step_gan`, `__pass_critic` and `__pass_generator`. It looks like around that:
```python
@tf.function
def pass_one_step_gan(self, inputs, target_imgs, it, stage, training):
        with tf.GradientTape() as g_tape, tf.GradientTape() as c_tape:
            new_imgs = self.model(inputs, training=training)

            if training:
                c_loss, c_loss_with_gp = self.__pass_critic(target_imgs, new_imgs)
                g_loss = self.__pass_generator(new_imgs, it)

```

I got the following error:
```
.../trainer.py:254 __pass_one_step_gan  *
    c_loss, c_loss_with_gp = self.__pass_critic(
    .../tensorflow/python/autograph/impl/api.py:329 converted_call
            f = getattr(owner, f)
    AttributeError: 'Trainer' object has no attribute '__pass_critic'
```

I tried to wrap `__pass_critic` in `tf.function`, but it didn't help.

**Describe the expected behavior**

It should work, but doesn't.

**Code to reproduce the issue**

See above.

**Other info / logs**

I don't know why but it seems that python doesn't see my method and, thus, tf.function can't track it. "
30799,Unable to find implementation of Select TensorFlow operators,"I am trying to use Select tensorflow operators ( https://www.tensorflow.org/lite/guide/ops_select ). I have successfully created AAR file. But android studio is unable to find implementation of 
**implementation 'org.tensorflow:tensorflow-lite-with-select-tf-ops:0.1.100'**

> ERROR: Failed to resolve: org.tensorflow:tensorflow-lite-with-select-tf-ops:0.1.100


as described in (https://www.tensorflow.org/lite/guide/ops_select). 
Kindly tell me how should I implement select-tf-ops tflite version ?
Thanks
"
30798,slim,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
30797,.predict with numpy arrays fails if predictions are time major instead of batch major,"In TF 1.14, .predict fails if predictions are time major i.e. their dimensions are [NB_TIME_STEPS, BATCH_SIZE, N] and not batch major as usual i.e. [BATCH_SIZE, NB_TIME_STEPS, N].
The error is : 
```
  File ""......\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 169, in aggregate
    self.results[i][batch_start:batch_end] = batch_out
ValueError: could not broadcast input array from shape (NB_TIME_STEPS,BATCH_SIZE,N) into shape (BATCH_SIZE,BATCH_SIZE,N)
```
The cause of the bug is that when TF pre-allocates the destination ndarray in self.results, it always assume batch major dimensions (hardcode).
Code in `tensorflow/python/keras/engine/training_utils.py` :
```
class OutputsAggregator(Aggregator):
  def create(self, batch_outs):
```
```
        elif isinstance(batch_out, np.ndarray):
          # If the output is a ndarray, append an output array pre-allocated
          # to the expected shape of the output.
          shape = (self.num_samples_or_steps,) + batch_out.shape[1:]  <-- HERE IS THE BUG
          self.results.append(np.zeros(shape, dtype=batch_out.dtype))
```
I guess the problematic code is `batch_out.shape[1:]`.
The `[1:]` gets rid of batch size dimension but this is wrong in case of time major ndarray where it removes time dimensions instead. That is why `shape` is (BATCH_SIZE,BATCH_SIZE,N)."
30794,ModuleNotFoundError: No module named 'tensorflow.contrib',"import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression

convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')

convnet = conv_2d(convnet, 32, 2, activation='relu')
convnet = max_pool_2d(convnet, 2)

convnet = conv_2d(convnet, 64, 2, activation='relu')
convnet = max_pool_2d(convnet, 2)

convnet = fully_connected(convnet, 1024, activation='relu')
convnet = dropout(convnet, 0.8)

convnet = fully_connected(convnet, 2, activation='softmax')
convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')

model = tflearn.DNN(convnet, tensorboard_dir='log')


I am facing below issue 

ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-6-8507f364adee> in <module>
----> 1 import tflearn
      2 from tflearn.layers.conv import conv_2d, max_pool_2d
      3 from tflearn.layers.core import input_data, dropout, fully_connected
      4 from tflearn.layers.estimator import regression
      5 

~\Anaconda3\Lib\site-packages\tflearn\__init__.py in <module>
      2 
      3 # Config
----> 4 from . import config
      5 from .config import is_training, get_training_mode, init_graph
      6 

~\Anaconda3\Lib\site-packages\tflearn\config.py in <module>
      3 import tensorflow as tf
      4 
----> 5 from .variables import variable
      6 
      7 # -------------------

~\Anaconda3\Lib\site-packages\tflearn\variables.py in <module>
      5 import tflearn
      6 
----> 7 from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope
      8 from tensorflow.python.framework import ops
      9 from tensorflow.python.ops import variable_scope

ModuleNotFoundError: No module named 'tensorflow.contrib'"
30793,"2019-07-17 12:43:20.046568: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:180] An array, resize/ResizeNearestNeighbor, still does not have a known data type after all graph transformations have run.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14
- Python version:3.6
- Bazel version (if compiling from source):0.24
- GCC/Compiler version (if compiling from source):7.4.0
- CUDA/cuDNN version:10.0/7.6
- GPU model and memory:Gtx 1060, 16 GB ram



@suharshs Hello I am trying to convert mobilenet model to tflite with following commands.

`bazel-bin/tensorflow/contrib/lite/toco/toco toco --input_file=/home/abdullah/models-master/research/frozen_casted.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=let_see.tflite --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,1024,1024,3 --default_ranges_min=0 --default_ranges_max=255 --architecture=mobilenet_v2 --std_dev_values=128 --mean_values=1`

But I am getting the following error:

`2019-07-17 12:43:19.954318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor
2019-07-17 12:43:19.963358: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 813 operators, 1238 arrays (0 quantized)
2019-07-17 12:43:19.988357: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 801 operators, 1216 arrays (0 quantized)
2019-07-17 12:43:20.017312: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 801 operators, 1216 arrays (0 quantized)
2019-07-17 12:43:20.036581: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 132 operators, 328 arrays (0 quantized)
2019-07-17 12:43:20.039009: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 132 operators, 328 arrays (0 quantized)
2019-07-17 12:43:20.041679: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 132 operators, 328 arrays (0 quantized)
2019-07-17 12:43:20.044543: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 132 operators, 328 arrays (0 quantized)
2019-07-17 12:43:20.046568: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:180] An array, resize/ResizeNearestNeighbor, still does not have a known data type after all graph transformations have run.
Aborted (core dumped)
`
I am using **tensorflow1.14 from source**. Kindly tell me why this issue is occurring ?"
30791,Use XLA to Tacotron2 is slower than without XLA,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:V9.0.176 / 7
- GPU model and memory: P40

**Describe the current behavior**
Before using XLA, the [Tacotron 2](https://github.com/Rayhane-mamah/Tacotron-2) runs about 0.5s, however after using XLA, it increases to about 0.7s which is much slower.
```python
config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1              
self.session = tf.Session(config=config) 
```
I use XLA like above. Any thoughts would be appreciated. Thanks."
30790,how to debug the tensorflow source code,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
 does anyone know how to debug the tensorflow source code with the test module;
 I have tried to use bazel build -c dbg while_loop_test, but it seems that it will compile all the project, and it will take much time. anyone knows how to debug only a small module.
 Thanks!!!
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30788,"I improving the recognition rate about TensorFlow 2 programmer,Google","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04 x64
- TensorFlow installed from (source or binary):binary
- TensorFlow version:Tensorflow 2.0.0-beta0
- Python version:Python 3.6
- Installed using virtualenv? pip? conda?:Activate virtualenv,then pip install TensorFlow
- CUDA/cuDNN version:
- GPU model and memory: 1 vCore,1024RAM,Bandwidth 1000G 



**Describe the problem**
I improving the recognition rate, I mean My train the model have hight sussessful than the guide
**Provide the exact sequence of commands / steps that you executed before running into the problem**
URL:https://github.com/nongxi/Machine-Learning-tensorfolw-/blob/master/TensorFlow%202.py
code:
import tensorflow as tf
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='sigmoid),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
log:
Train on 60000 samples
Epoch 1/5
2019-07-17 02:51:34.258226: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
2019-07-17 02:51:34.287935: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21
60000/60000 [==============================] - 4s 66us/sample - loss: 0.0537 - accuracy: 0.9822
Epoch 2/5
60000/60000 [==============================] - 4s 73us/sample - loss: 0.0475 - accuracy: 0.9845
Epoch 3/5
60000/60000 [==============================] - 5s 77us/sample - loss: 0.0451 - accuracy: 0.9852
Epoch 4/5
60000/60000 [==============================] - 4s 72us/sample - loss: 0.0421 - accuracy: 0.9858
Epoch 5/5
60000/60000 [==============================] - 4s 71us/sample - loss: 0.0412 - accuracy: 0.9860
<tensorflow.python.keras.callbacks.History object at 0x7fec4cdd90b8>
>>> model.evaluate(x_test, y_test)
10000/10000 [==============================] - 1s 51us/sample - loss: 0.0678 - accuracy: 0.9813
[0.06783692461897153, 0.9813]"
30787,can't call tf.keras.Model created with functional api with no input tensors,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I know how to create tf.keras.Model with functional api for sure. I created a tf.keras.Model with no input tensors with functional api. When I call it, tensorflow complains that
>AttributeError: Tensor.op is meaningless when eager execution is enabled.

**Describe the expected behavior**

The tf.keras.Model should return expected value rather than raising a error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```Python
#!/usr/bin/python3

import tensorflow as tf;

def model():
  a = tf.keras.layers.Lambda(lambda x: tf.ones(x))((2,3,4));
  return tf.keras.Model(inputs = [], outputs = a);

b = model()();
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

>Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    b = model()();
  File ""test.py"", line 7, in model
    return tf.keras.Model(inputs = [], outputs = a);
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 133, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 161, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 267, in _init_graph_network
    base_layer_utils.create_keras_history(self._nested_outputs)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 199, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 225, in _create_keras_history_helper
    op = tensor.op  # The Op that created this Tensor.
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 987, in op
    ""Tensor.op is meaningless when eager execution is enabled."")
AttributeError: Tensor.op is meaningless when eager execution is enabled.


"
30786,"Feature Request: Adding a ""LayerListWrapper""-like class for adding list of layers (Non-sequential) to keras model.","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow 2.0.0b0-gpu
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Typical scenario: Assume that one needs to add a list that contains variable number of `tf.keras.layers.Layer` to a keras Model. By specifying something like `self.list_of_layers = [...]` in the `__init__` method`tf.keras.Model` subclass objects, the model won't track the trainable variables within layers in the list. For now this can be hacked by appending it to self._layers (e.g. `self._layers += self.list_of_layers`) to make the model object track the variables created in the layers in the list. 

It would be helpful if there were a class like pytorch's `ModuleLists` that wraps list of layers up and enables the model to ""register"" theses layers and the corresponding `tf.Variable` objects automatically. 

**Will this change the current api? How?**
Yes. New wrapper that accepts list of tf.keras.layers.Layer objects will be added. 
Not sure there will be any side effects
**Who will benefit with this feature?**
People who need to add non-sequential layers to `tf.keras.Model` subclass objects. 
People who are accustomed to the its equivalents in other deep learning framworks (e.g. `ModuleLists` in Pytorch). 
**Any Other info.**
This is also proposed in [this stackoverflow thread](https://stackoverflow.com/questions/56117745/how-to-use-a-list-of-layers-in-tensorflow-2-0).

A simple user case
```Python
class TextCNN(tf.keras.Model):
    def __init__(self, embedding, n_kernels, channel_per_kernel, n_cls):
        super(TextCNN, self).__init__()
        self.embedding = embedding
        self.convs = tuple(tf.keras.layers.Conv1D(channel_per_kernel, n_gram, strides=1, padding=""same"") for n_gram in range(2, 2 + n_kernels))
        # Temporary hack due to not finding a class like `ModuleLists` in pytorch. 
        self._layers += self.convs
        self.temporal_pooling = tf.keras.layers.GlobalMaxPool1D(data_format=""channels_last"")
        self.output_fc = tf.keras.layers.Dense(n_cls)
    
    def call(self, x, training=False):
        output = self.embedding(x)
        output = [conv(output) for conv in self.convs]
        output = tf.concat(output, axis=-1)
        output = self.temporal_pooling(output)
        output = self.output_fc(output)
        return output
```"
30784,Where can I find binary tensorflow-gpu 1.12 for PPC64LE,"The community-supported tensorflow-gpu is 1.14 which require CUDA 10 but I only have CUDA 9, does anyone know where can I find built tensorflow-gpu 1.12?"
30782,"Allow_soft_placement=True does not solve ""no supported kernel for GPU devices is available"" problem","I have got this error: 
` InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'model/inference/encoder_LSTM/bidirectional_rnn/fw/fw/Assert/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'
`

other people said that setting tf.ConfigProto to ""allow_soft_placement=True"" will probably solve this problem.

But it does not. Any other idea? Please help.

"
30780,tf.keras.losses.categorical_crossentropy ignores batch size and dont have name keywordarg,"I think this function is obsolete in Tensorflow since the class CategoricalCrossEntropy does the job in the standard way (i.e, compute loss by batch and associate with a name to be easier to debug). 

I suggest to drop this from next 1.14 version if iam not taking something wrong. 

Thank you."
30779,TF-TRT slower than plain TF on FasterRCNN type networks. ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda-10.1
- GPU model and memory: T4


**Describe the current behavior**
I have noticed that   throughput of FasterRCNN type object detector is slower with TF-TRT than just TF. 
 
**Describe the expected behavior**
Was expecting to see  higher throughput  with TF-TRT.

**Code to reproduce the issue**
Try any off the shelf FasterRCNN network 

**Other info / logs**
None
"
30776,freeze model for inference with output_node_name,"I want to compile the TensorFlow Graph to Movidius Graph. I have used Model Zoo's `ssd_mobilenet_v1_coco` model to train it on my own dataset.

Then I ran 
```
python object_detection/export_inference_graph.py \
				--input_type=image_tensor \
				--pipeline_config_path=/home/redtwo/nsir/ssd_mobilenet_v1_coco.config \
                                --trained_checkpoint_prefix=/home/redtwo/nsir/train/model.ckpt-3362 \
				--output_directory=/home/redtwo/nsir/output
```
which generates me `frozen_interference_graph.pb` & `saved_model/saved_model.pb`


![11](https://user-images.githubusercontent.com/8083613/61330462-3fe5b180-a83d-11e9-99a5-7b63aa1b7d2a.png)

![12](https://user-images.githubusercontent.com/8083613/61330465-4116de80-a83d-11e9-89cb-2d69e213391b.png)


Now to convert this saved model into *Movidius graph*. There are commands given


Export GraphDef file
```
python3 ../tensorflow/tensorflow/python/tools/freeze_graph.py \
		--input_graph=inception_v3.pb \
		--input_binary=true \
		--input_checkpoint=inception_v3.ckpt \
		--output_graph=inception_v3_frozen.pb \
		--output_node_name=InceptionV3/Predictions/Reshape_1
```
Freeze model for inference
```
python3 ../tensorflow/tensorflow/python/tools/freeze_graph.py \
		--input_graph=inception_v3.pb \
		--input_binary=true \
		--input_checkpoint=inception_v3.ckpt \
		--output_graph=inception_v3_frozen.pb \
		--output_node_name=InceptionV3/Predictions/Reshape_1
```

which can finally be feed to **NCS Intel Movidius SDK**

```
mvNCCompile -s 12 inception_v3_frozen.pb -in=input -on=InceptionV3/Predictions/Reshape_1
```

All of this is given at Intel Movidius Website here: https://movidius.github.io/ncsdk/tf_modelzoo.html


#### Question: My model was already trained i.e. `output/frozen_inference_graph`. Why do I again freeze it using `/slim/export_inference_graph.py` or it's the `output/saved_model/saved_model.py` that will go as input to `slim/export_inference_graph.py`??

All I want is **output_node_name=Inceptionv3/Predictions/Reshape_1**. How to get this output_name_name directory structure & anything inside it? I don't know what all it contains 

------------------------

### System information
- **What is the top-level directory of the model you are using**: 
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: TensorFlow installed with pip
- **TensorFlow version (use command below)**: 1.13.1
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: V10.1.168/7.*
- **GPU model and memory**: 2080Ti 11Gb
- **Exact command to reproduce**: "
30774,What replaces tf.data.get_output_shapes in TF 2?,"TF 2 Datasets don't appear to have the output_shape property and there is no tf.data.get_output_shapes. 

What do we use instead?"
30773,[TF 2.0] Can not compile model more than once without running out of memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190702
- Python version: 3.6.7
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: V10.0.130, 7.3.1
- GPU model and memory: Surface Book 1 Nvidia GPU

**Describe the current behavior**

Build a Keras model, compile it, run it.
Try and rebuild model with new parameters.
Result: OOM on GPU. Memory has not been freed or re-used.

**Describe the expected behavior**

Compile a model more than once without the GPU running out of memory.
More specifically, be able to hyper-parameter tuning without restarting the Jupyter kernel.

I've tried:
-  set_memory_growth on the GPU
- del model + gc.collect
- clear_session
none of them help.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
#Import basics and check everything works
import tensorflow as tf
from tensorflow import keras

AUTOTUNE = tf.data.experimental.AUTOTUNE

print(""Versions:"", tf.version.VERSION, tf.version.GIT_VERSION)
print(""GPU availablilty:"", tf.test.is_gpu_available())
print(""Eager execution:"", tf.executing_eagerly())

#Quick test
x = [[2.]]
m = tf.matmul(x, x)
print(""hello, {}"".format(m))

def make_model(input_shape, n_hidden1=2049, n_hidden2=500, n_hidden3=180, batch_n_mom=0.99, dropout_rate=0.1):

    from tensorflow.keras.initializers import he_normal
    
    stacked_ae = keras.models.Sequential([
        keras.layers.Flatten(input_shape=input_shape),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        
        keras.layers.Dense(n_hidden1, activation=""selu"", name=""he1"", kernel_initializer=he_normal(seed=27)),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        keras.layers.Dropout(dropout_rate),
        
        keras.layers.Dense(n_hidden2, activation=""selu"", name=""he2"", kernel_initializer=he_normal(seed=42)),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        
        keras.layers.Dense(n_hidden3, activation=""selu"", name=""he3"", kernel_initializer=he_normal(seed=65)),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        
        keras.layers.Dense(n_hidden2, activation=""selu"", name=""hd2"", kernel_initializer=he_normal(seed=42)),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        
        keras.layers.Dense(n_hidden1, activation=""selu"", name=""hd1"", kernel_initializer=he_normal(seed=27)),
        keras.layers.BatchNormalization(axis=1, momentum=batch_n_mom),
        keras.layers.Dropout(dropout_rate),
        
        keras.layers.Dense(input_shape[0] * input_shape[1], name=""output"", kernel_initializer=he_normal(seed=62)),
        keras.layers.Reshape(input_shape)
    ])
    
    return stacked_ae

import numpy as np

#Data doesn't matter
x_train = np.ones((32,60,80))
y_train = np.ones((32,60,80))

#Once runs ok
input_shape = [60,80]
ae_model = make_model(input_shape)
ae_model.compile(loss=""mse"",
                 optimizer=keras.optimizers.Adam(learning_rate=0.001, decay=1e-6),
                metrics=['accuracy'])
print(ae_model.summary())

#Do something with the model
history = ae_model.fit(x=x_train, y=y_train,  epochs=1, steps_per_epoch=1)

#Second run, new model
ae_model = make_model(input_shape, n_hidden1=2150)
ae_model.compile(loss=""mse"",
                 optimizer=keras.optimizers.Adam(learning_rate=0.001, decay=1e-6),
                metrics=['accuracy'])
print(ae_model.summary())

#Run again. GPU OOM.
history = ae_model.fit(x=x_train, y=y_train,  epochs=1, steps_per_epoch=1)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

2019-07-16 16:24:06.019147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-07-16 16:24:12.775543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-16 16:24:12.789530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-07-16 16:24:12.799822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-07-16 16:24:12.813163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 517 MB memory) -> physical GPU (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-07-16 16:24:12.847183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GPU major: 5 minor: 0 memoryClockRate(GHz): 0.993
pciBusID: 0000:01:00.0
2019-07-16 16:24:12.868383: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-16 16:24:12.887076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-07-16 16:24:12.902106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GPU major: 5 minor: 0 memoryClockRate(GHz): 0.993
pciBusID: 0000:01:00.0
2019-07-16 16:24:12.925257: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-07-16 16:24:12.946163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-07-16 16:24:12.958309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-16 16:24:12.977399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-07-16 16:24:12.988725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-07-16 16:24:13.001442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 517 MB memory) -> physical GPU (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0, compute capability: 5.0)

"
30772,"""--config mkl"" => mkl_version.h: No such file or directory","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 8.2.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the problem**

Building TF master with ""--config mkl"" fails because it is looking for Intel MKL (not MKL-DNN).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config opt --config mkl //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
In file included from external/mkl_dnn/src/cpu/rnn/ref_rnn.hpp:28,
                 from external/mkl_dnn/src/cpu/rnn/cell_gru_lbr.cpp:24:
external/mkl_dnn/src/cpu/rnn/../gemm/os_blas.hpp:33:10: fatal error: mkl_version.h: No such file or directory
 #include ""mkl_version.h""
          ^~~~~~~~~~~~~~~
compilation terminated.


Note that mkl_version.h is an Intel MKL header, not an MKL-DNN header. It looks like the current MKL-DNN binary blob requires MKL by default."
30770,AttributeError: The layer has never been called and thus has no defined input shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker container
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version (use command below): 'v2.0.0-beta0-16-g1d91213fe7', '2.0.0-beta1'
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I'm currently trying to get a PMML version of a Tensorflow 2.0 model, and seem to be getting an unrelated error.

AttributeError: The layer has never been called and thus has no defined input shape.

I've looked through the things that theoretically should cause this error, and none seem to be relevant. Here's my code:

```python
def create_and_train(x_training,y_training,n_cols_in,modelparams):
    layers = [tf.keras.layers.Dense(n_cols_in,activation=""relu""),
    tf.keras.layers.Dropout(.5)]
    for param in modelparams:
        layers.extend([tf.keras.layers.Dense(param,activation=""sigmoid""),tf.keras.layers.Dropout(.5)])
    layers.append(tf.keras.layers.Dense(1,activation=""sigmoid""))
    model = tf.keras.models.Sequential(layers)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
    model.fit(x_training, y_training, epochs = epochs)
    with open(""NN""+""_"".join([str(m) for m in modelparams])+"".pmml"",""w"") as pmml_file:
        pmml = KerasToPmml(model)
        pmml.export(pmml_file)
```

The model gets trained, and if I tell it to give me an AUC for the training set or even a test set before the PMML export, it does so without a problem. However, PMML export requires input shape, which Tensorflow claims doesn't exist."
30769,tf.__version__ raises an AttributeError exception,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
```python
tf.version.VERSION='2.0.0-dev20190716'
tf.version.GIT_VERSION='v1.12.1-6367-gb51a1b258b'
```
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.__version__` raises an `AttributeError` exception.

**Describe the expected behavior**
`tf.__version__` should return the version (=`tf.version.VERSION`) to respect [PEP 396](https://www.python.org/dev/peps/pep-0396/).

**Code to reproduce the issue**
```python
import tensorflow as tf

print(tf.__version__)
```

**Other info / logs**
Here's the exception:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-9-d1382151fdd6> in <module>
      1 import tensorflow as tf
      2
----> 3 print(tf.__version__)

AttributeError: module 'tensorflow' has no attribute '__version__'
```"
30768,Cryptic error message when assigning to a variable in a tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=2.0.0-dev20190716
GIT_VERSION=v1.12.1-6367-gb51a1b258b
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When assigning to a TF variable with the `=` operator in a TF Function, I get a really cryptic error message.
This issue is about the error message, not the error itself (I should have called the variable's assign() method).

**Describe the expected behavior**
I was hoping for a more explicit error message.

**Code to reproduce the issue**

```python
import tensorflow as tf

v = tf.Variable(0.)

@tf.function
def f(x):
    global v
    v = v + x  # no problem if use v.assign(v + x)
    return v

f(42.0)
```

**Other info / logs**
```
2019-07-16 21:02:26.141110: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-16 21:02:26.154309: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe6734a6e50 executing computations on platform Host. Devices:
2019-07-16 21:02:26.154329: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-16 21:02:26.222272: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-16 21:02:26.223137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar0 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar0/N10tensorflow3VarE does not exist.
	 [[{{node ReadVariableOp}}]]
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-1-a1cd73b43975> in <module>
      9     return v
     10
---> 11 f(42.0)

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    433               *args, **kwds)
    434       # If we did not create any variables the trace we have is good enough.
--> 435       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    436
    437     def fn_with_cond(*inner_args, **inner_kwds):

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
    633          if isinstance(t, (ops.Tensor,
    634                            resource_variable_ops.BaseResourceVariable))),
--> 635         self.captured_inputs)
    636
    637   def _call_flat(self, args, captured_inputs):

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs)
    743     # Only need to override the gradient in graph mode and when we have outputs.
    744     if context.executing_eagerly() or not self.outputs:
--> 745       outputs = self._inference_function.call(ctx, args)
    746     else:
    747       self._register_gradient()

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args)
    457             attrs=(""executor_type"", executor_type,
    458                    ""config_proto"", config),
--> 459             ctx=ctx)
    460       # Replace empty list with None
    461       outputs = outputs or None

~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/miniconda3/envs/tf2/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

FailedPreconditionError:  Error while reading resource variable _AnonymousVar0 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar0/N10tensorflow3VarE does not exist.
	 [[node ReadVariableOp (defined at /Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1657) ]] [Op:__inference_f_15]

Function call stack:
f
```"
30767,Minimum number of epochs before termination for tf.keras.callbacks.EarlyStopping(),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.14
- Are you willing to contribute it (Yes/No):
Yes



**Describe the feature and the current behavior/state.**
This feature is not yet implemented but is simple to implement. It requires an additional flag, storing one more `int`, and few more `if`-statements in tf.keras.callbacks.EarlyStopping().

**Will this change the current api? How?**
Yes, in that one more variable will be available to be assigned when calling the EarlyStopping callback.

**Who will benefit with this feature?**
Anyone who wishes to use EarlyStopping but wants to let the model train for a certain number of epochs before EarlyStopping is activated.

**Any Other info.**
N/A"
30766,implementation of tf.dense_tensor_sparse_matmul() such that a matrix product AB can be computed where A is dense and B is sparse.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.14
- Are you willing to contribute it (Yes/No):
Yes

**Describe the feature and the current behavior/state.**
The feature is not implemented in a transparent fashion. The operation can be performed by using tf.sparse.sparse_tensor_dense_matmul() and some matrix algebra. It is a python-only implementation and does not require going into the C/CUDA code.

**Will this change the current api? How?**
It will add a new function call within tf.sparse.

**Who will benefit with this feature?**
Most people who use the Sparse library.

**Any Other info.**
I've already implemented and tested this feature against NumPy's matrix math library. I wanted to bring it up and see if this is a desirable feature.
"
30764,Cannot cross-compile for Raspberry Pi with official instructions in MacOS,"
**System information**
- OSX 10.14.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 2.7
- Installed using virtualenv? pip? conda?: docker
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

I am following [this official guide](https://www.tensorflow.org/install/source_rpi) to cross-compile for the Raspberry Pi. I am doing:

       tensorflow/tools/ci_build/ci_build.sh PI \
       tensorflow/tools/ci_build/pi/build_raspberry_pi.sh

The process hungs (all CPUs being used 100% for a long long time) while compiling. Each time I kill it and the next time hungs at a different place. The scenario is always very similar.. I paste below some examples:

### Example1


	[1,152 / 1,160] Compiling tensorflow/core/kernels/data/experimental/group_by_reducer_dataset_op.cc; 131s local ... (4 actions running)
	ERROR: /workspace/tensorflow/core/kernels/BUILD:3114:1: C++ compilation of rule '//tensorflow/core/kernels:batch_matmul_op' failed (Exit 4): arm-linux-gnueabihf-gcc failed: error executing command
	  (cd /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
	  exec env - \
	    LD_LIBRARY_PATH='' \
	    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
	    PWD=/proc/self/cwd \
	    PYTHON_BIN_PATH=/usr/bin/python \
	    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
	    TF_DOWNLOAD_CLANG=0 \
	    TF_NEED_CUDA=0 \
	    TF_NEED_OPENCL_SYCL=0 \
	    TF_NEED_ROCM=0 \
	  /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -DCURL_STATICLIB -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-opt/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/armeabi-opt/genfiles/external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/armeabi-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/armeabi-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/armeabi-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-opt/bin/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/armeabi-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/genfiles/external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/curl -iquote bazel-out/armeabi-opt/genfiles/external/curl -iquote bazel-out/armeabi-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/armeabi-opt/genfiles/external/boringssl -iquote bazel-out/armeabi-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/armeabi-opt/genfiles/external/jsoncpp_git -iquote bazel-out/armeabi-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/armeabi-opt/genfiles/external/aws -iquote bazel-out/armeabi-opt/bin/external/aws -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/armeabi-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/armeabi-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/armeabi-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/armeabi-opt/genfiles/external/curl/include -isystem bazel-out/armeabi-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/armeabi-opt/genfiles/external/boringssl/src/include -isystem bazel-out/armeabi-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/armeabi-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/armeabi-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/armeabi-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-s3/include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/batch_matmul_op_real.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.o)
	Execution platform: @bazel_tools//platforms:host_platform
	cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
	arm-linux-gnueabihf-gcc: internal compiler error: Killed (program cc1plus)
	Please submit a full bug report,
	with preprocessed source if appropriate.
	See <http://gcc.gnu.org/bugs.html> for instructions.
	INFO: Elapsed time: 781.885s, Critical Path: 218.64s
	INFO: 108 processes: 108 local.
	FAILED: Build did NOT complete successfully
	FAILED: Build did NOT complete successfully
	>>> elapsed time 16m45s

### Example2
	ERROR: /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/com_github_nanopb_nanopb/BUILD.bazel:1:1: C++ compilation of rule '@com_github_nanopb_nanopb//:nanopb' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command
	  (cd /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
	  exec env - \
	    LD_LIBRARY_PATH='' \
	    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
	    PWD=/proc/self/cwd \
	    PYTHON_BIN_PATH=/usr/bin/python \
	    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
	    TF_DOWNLOAD_CLANG=0 \
	    TF_NEED_CUDA=0 \
	    TF_NEED_OPENCL_SYCL=0 \
	    TF_NEED_ROCM=0 \
	  /Users/mariano/Trabajo/Instantiations/InstantiationsShared/TensorFlow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mariano/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb/_objs/nanopb/pb_decode.pic.d -fPIC '-DPB_FIELD_32BIT=1' -iquote external/com_github_nanopb_nanopb -iquote bazel-out/armeabi-opt/genfiles/external/com_github_nanopb_nanopb -iquote bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_github_nanopb_nanopb/pb_decode.c -o bazel-out/armeabi-opt/bin/external/com_github_nanopb_nanopb/_objs/nanopb/pb_decode.pic.o)
	Execution platform: @bazel_tools//platforms:host_platform
	external/com_github_nanopb_nanopb/pb_decode.c:18:23: fatal error: pb_common.h: No such file or directory
	 #include ""pb_common.h""
	                       ^
	compilation terminated.
	INFO: Elapsed time: 380.977s, Critical Path: 35.72s
	INFO: 390 processes: 390 local.
	FAILED: Build did NOT complete successfully
	FAILED: Build did NOT complete successfully


**Any other info / logs**
The very very same instructions, running on a Linux Mint 18.3 work perfectly. Docker version on OSX is ""Docker version 18.09.2, build 6247962"" and in Mint ""Docker version 18.09.6, build 481bc77"".  The documentation says nothing about which Docker version to use and it does says that MacOS is supported: 

> it's easier to build TensorFlow on a more powerful host machine running Linux, macOS, or Windows.


Any ideas?"
30763,Eager and Graph API should be more similar,"**System information**
- TensorFlow version (you are using): 2.0b1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Eager mode just works. However, the moment you do @tf.function decorators to improve performance, you have to convert a bunch of commands from eager mode into graph mode. 

there are **two ways to do everything in tensorflow: the eager way and the graph way.** This means we have no ""single source of truth"" (do i use eager which just works but is slow and has memory issues, or do i use graph mode which is lightning fast but requires a ton of extra work to ""translate between two dialects"" tensorflow eager into tensorflow graph)

**Will this change the current api? How?**
Yes. This will dramatically simplify the tensorflow API, as there will be fewer redundant commands between graph mode and eager mode 

**Who will benefit with this feature?**
Everyone who switches between eager and graph mode would save days of work on every project if this change were implemented

**Any Other info.**
tensorflow 2.0 is a golden opportunity to simplify TF we have less code to debug. I wish the team would merge eager and graph mode as much as possible:

1. tf.shape() vs tensor.shape
2. basic logic often fails in graph mode (for example, if x = ""somestring"") and there's not many good ways to know why
3. input signature inference would save everyone a lot of time... retracing happens a lot in tf.function without signatures, especially on sequence data with variable shapes, so we have to write a bunch of (None, None ) to fix this ? that's not really informative. this should be automated. Maybe if tf.function sees a shape varying, it can handle this case automatically
4. a nice way to set dtype at the top of the file would be handy
5. a nice way to turn tensorboard tracing on globally would also be handy

```
def trace_function(fn, *args):
    tf.summary.trace_on(graph=True, profiler=True)
    y = fn(*args)
    with writer.as_default():
        tf.summary.trace_export(name=""trace_function"", step=0, profiler_outdir=log_dir)
    return y
```"
30758,MAXIMUM and RESIZE_NEAREST_NEIGHBOR not available for tensorflow-lite-gpu,"**System information**
- OS Platform and Distribution: Android Oreo
- TensorFlow installed from: source
- TensorFlow version (or github SHA if from source):
Using both tensorflow-lite and tensorflow-lite-gpu libraries
implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'

**Provide the text output from tflite-gpu**

```
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:
    MAXIMUM: Operation is not supported.
    RESIZE_NEAREST_NEIGHBOR: Operation is not supported.
    First 3 operations will run on the GPU, and the remaining 74 on the CPU.TfLiteGpuDelegate Prepare: Output tensor is not found in the graph.Node number 77 (TfLiteGpuDelegate) failed to prepare.
```

Do you plan on implementing the MAXIMUM and RESIZE_NEAREST_NEIGHBOR ops to run on the gpu build of tensorflow-lite-gpu? I'm using a neural network, which up-samples from feature vectors.
"
30757,Multiply complex number with a scalar,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

When I multiply a `tf.complex` with any other like `tf.float` it would be helpful to take the float as a complex number with imag part =0 (treat it as a scalar).

**Will this change the current api? How?**
 When using `tf.matmult()` between a complex and a real number it will automatically cast the real one to complex and perform the operation instead of trhowing:
`TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type complex64 of argument 'a'.`
 
**Who will benefit with this feature?**

I am currently trying to do a Complex-Valued Neural Network and I am having some headache with that. So I think it will helpful when doing a CVNN

**Any Other info.**
"
30755,Android: Failed to resolve: tensorflow-lite,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.2 HighSierra

- TensorFlow installed from (source or binary): gradle dependency 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
- TensorFlow version: 0.0.0-nightly
- Android Studio : 3.2.1
- Android compile / targetSdkVersion  = 28
- Gradle Version: 4.6


**Describe the problem**

Tensorflow Lite dependency is not resolved 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Run assembleDebug command -> 
_Could not find tensorflow-lite.aar (org.tensorflow:tensorflow-lite:0.0.0-nightly).
  Searched in the following locations:
      https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/0.0.0-nightly/tensorflow-lite-0.0.0-nightly.aar_



**Any other info / logs**

"
30754,What's the state of RNNs and tf.lite for Tensorflow 2.0?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0-beta1


**Provide the text output from tflite_convert**

```
Traceback (most recent call last):
  File ""test_rnn_tflite.py"", line 12, in <module>
    tflite_model = converter.convert()
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 392, in convert                                                                                                  
    **converter_kwargs)
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl                                                                                     
    input_data.SerializeToString())
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos                                                                                   
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-07-16 14:25:07.645148: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       
2019-07-16 14:25:07.645184: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       
2019-07-16 14:25:07.645191: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       
2019-07-16 14:25:07.645280: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: StatefulPartitionedCall                                                                                        
2019-07-16 14:25:07.653916: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)                                                                                     
Fatal Python error: Aborted

Current thread 0x00000001104045c0 (most recent call first):
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute                                                                                   
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40 in run                                                                                                     
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main                                                                                      
  File ""/Users/ben/miniconda3/envs/wakeword/bin/toco_from_protos"", line 10 in <module>
```

Code to reproduce:
```python
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.LSTM(10, input_shape=(16000, 40))]
)
model.build()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = set(
    [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
)

tflite_model = converter.convert()
```


**Any other info / logs**

There are several issues about support of RNNs in tf.lite, most of them are rather old and about TF 1.x. What's the current state of supporting RNNs in tf.lite for TF 2.x?"
30753,TF >=1.13 doesn't support CUDA 9.0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): build from source
- TensorFlow version: `1.13.1, 1.14.0, 2.0.0a, 2.0.0b1`
- Python version: 3.5
- Installed using virtualenv? pip? conda?: ...
- Bazel version (if compiling from source): `0.25.2`
- GCC/Compiler version (if compiling from source): `5.4.0`
- CUDA/cuDNN version: `CUDA 9.0, cudnn 7.6.1`
- GPU model and memory: Tesla V100



**Describe the problem**
I want to compile newer versions of TF from source to support CUDA 9.0 . I tried building all versions after 1.12.0 and always get the same error. Bazel build sucessfully finishes but after I install the resulting pip file and try to import TF it doesn't use GPU's
`Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda-9.0/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64`
```
2019-07-16 14:37:23.856339: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.9.0
2019-07-16 14:37:23.857504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.9.0
2019-07-16 14:37:23.858807: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.9.0
2019-07-16 14:37:23.859066: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.9.0
2019-07-16 14:37:23.859437: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda-9.0/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64
2019-07-16 14:37:23.860627: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.9.0
2019-07-16 14:37:23.864452: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-16 14:37:23.864478: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I use `configure` script with all parameters at default except specifying CUDA version. Then I build with:
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
This problem was already reported before: #30389  
I also found discussion, where people had the same issue with cusolver library:
https://groups.google.com/forum/#!topic/kaldi-help/8OThZ1lwE9I
"
30750,tf.sparse.to_dense does not work on sparse tensor with string values. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS (Bionic Beaver)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
Trying to convert a SparseTensor of type string into the corresponding dense tensor using tf.sparse.to_dense throughs an exception: TypeError: Expected string passed to parameter 'default_value' of op 'SparseToDense', got 0 of type 'int' instead. Error: Expected string, got 0 of type 'int' instead.

**Describe the expected behavior**
Like with using an integer valued SparseTensor I expect tf.sparse.to_dense to return a dense tensor with string values. 

**Code to reproduce the issue**
```
import tensorflow as tf
sample_int    = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])
sample_string = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=['a', 'b'], dense_shape=[3, 4])
tf.sparse.to_dense( sample_int )
tf.sparse.to_dense( sample_string )
```
There is also a colab notebook where you can execute the code directly: [https://colab.research.google.com/drive/1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh](https://colab.research.google.com/drive/1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh)

**Other info / logs**
I came across this issue while reading in some TFRecords that included VarLenFeatures with string-typed lists. Everything worked fine with the VarLenFeatures with integers. Right now I have not even found a way to convert the SparseTensor's string values into a new Tensor as a workaround. 
"
30749,Pass Input to tensorflow lite model in Android,"I have created a neural network that take numerical data as input and saved it as tensorflow lite model using python. I am trying to pass input to the model in Android. Shape of ndarray is 1*3

Sample of the input in python is as follows

`np.array([[-0.276786765 ,8.41897583008  ,-0.0222015380859]])`

But i do not know to create the same input in java to pass it to model.

I tried using nd4j library. But still not able to write the proper code to create the input which is required by the model."
30748,"Requested GPU:0, but only XLA_GPU:0 exsits, tf-gpu1.14.0","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.6.8 / 3.7.3
- Installed using virtualenv? pip? conda?: `pip`
- CUDA/cuDNN version: 10/7.4
- GPU model and memory:
![image](https://user-images.githubusercontent.com/5757359/61288486-e3987880-a7cf-11e9-8f1d-efb1a7130879.png)





**Describe the problem**
1. Installed `tensorflow-gpu` latest version by performing: `pip install --upgrade --force-reinstall tensorflow-gpu`
2. Run `tf.device('/gpu:0')`

The error seems to say I have 4 gpus, but they don't match GPU, only XLA_GPU. I have no idea why, earlier versions of tensorflow do say I have GPU, but claim other bugs.

Error:
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation pooled_center_map/center_map/AvgPool: {{node pooled_center_map/center_map/AvgPool}}was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3 ]. Make sure the device specification refers to a valid device.


Even if I try:
`tf.device('/job:localhost/replica:0/task:0/device:XLA_GPU:0')`

I get:
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation sub_stages/sub_conv1/weights/Initializer/random_uniform/RandomUniform: Could not satisfy explicit device specification '' because the node {{colocation_node sub_stages/sub_conv1/weights/Initializer/random_uniform/RandomUniform}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:XLA_GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3, /job:localhost/replica:0/task:0/device:XLA_CPU:0]. 
"
30747,Penalty in the contractive auto-encoder,How can I compute a contraction penalty with Tensorflow? The contraction penalty is the squared of Frobenius norm ‖ · ‖F of the Jacobian matrix of the encoder.
30746,Couldn't find and import the 'graph_transforms' module in tensorflow1.14,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source): 24.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
import tensorflow.tools.graph_transforms as graph_transforms
```
Error message:
```
AttributeError: 'module' object has no attribute 'graph_transforms'
```

**Describe the expected behavior**
Successfully import the graph_transforms module.
 This code succesfully run with tf 1.13 but failed on tf 1.14.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow.tools.graph_transforms as graph_transforms
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The installed 'graph_transforms' module seems has been moved to tensorflow_core folder in site-packages.
Is this as expected? 

"
30745,How to pad input sequences for implementing CuDNN LSTM in TF2.0?,"For implementing CuDNN LSTM, according to [docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM) there are 6 requirements as follows- 

> activation == 'tanh'
recurrent_activation == 'sigmoid'
recurrent_dropout == 0
unroll is False
use_bias is True
Inputs are not masked or strictly right padded.

According to my understanding, the last one say to have input sequences not right padded. However, in text classification I want to pad sequences and I am using [the function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#padded_batch), now I do not know how to left pad using this function, as I could check it always right pad the input sequences, which means one can never use CuDNN LSTM for training on GPU(s) if they are padding the inputs. However, it can be done easily using `tf.keras..pad_sequences` function, defined [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences). "
30744,"why GetLocalGPUInfo() using cuda runtime api, I guess it should use cudart_stub instead.","Base on below commit, cuda Runtime API should wrapped in dynamic load， but I still see  tensorflow/core/grappler/clusters/utils.cc using cuda Runtime API.

commit: 7c5aed39f0148f93cebef850d939558a938b41e7
Remove tensorflow core runtime's explicit dependency on cuda runtime.
- Add stub implementation which is wrapped in dynamic loader
- Replace the runtime dependency with stub in tf_cuda_library

Adopted csigg@'s method in cl/230858143 to work around the issue where the cuda runtime calls in Eigen::initializeDeviceProp() cannot be wrapped with the macro method.

PiperOrigin-RevId: 232063778"
30743,How to build Tensorflow lib files with bazel from source code on Windows?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 pro Visual studio 2015
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): trying to build from source
- TensorFlow version: gpu-1.13.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0/ cudnn 7.4.2
- GPU model and memory: Geforce 1080Ti



**Describe the problem**
I'm trying to build Tensorflow-gpu lib files(tensorflow.dll and .lib files) with bazel from source code on Win10, but I have encountered serveral problems.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After build configure
```cmd
C:\tensorflow>python configure.py
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
nul
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 412c8d20-03eb-46e5-bf73-fd54952f1e01
You have bazel 0.21.0 installed.
Please specify the location of python. [Default is C:\Users\Administrator\AppData\Local\Programs\Python\Python36\python.exe]:


Found possible Python library paths:
  C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```
I ran 
```cmd
C:\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/lib_package:libtensorflow
```

**Any other info / logs**
After a long time of compiling, the build did indeed succeed but the lib file it generates is 
    <code>libtensorflow.so</code>

I expect the output lib file for windows to be <code>tensorflow.dll</code> and <code>tensorflow.lib</code> but I think this <code>.so</code> file is not useful on Windows, so I'd like to know:

1. if I can build a <code>tensorflow.dll</code> and <code>tensorflow.lib</code> file on Win10 or not?

2. and if not, how do I suppose to use this <code>libtensorflow.so</code> on Windows and deal with all the <code>include</code> file?
"
30742,AttributeError: module 'tensorflow' has no attribute 'get_default_graph',"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
30741,Mask R-CNN not learning on pet dataset even without any changes to the code,"I am attempting to retrain pretrained model on COCO to detect images from Pet Dataset. I followed tutorials and managed to start the training. But according to results, the model is not trained properly.

Steps to recreate:

 1. `git clone https://github.com/cocodataset/cocoapi.git`
 2. `conda create --name tf_test python=3.6`
 3. `conda activate tf_test`
 4. `pip install tensorflow`
 5. `pip install pillow, lxml, jupyter, matplotlib, opencv, contextlib2`
 6. download protoc-3.9.0-win32.zip extract to Program Files\Google Protobuf\
 7. add Program Files\Google Protobuf\bin to path
 8. `for /f %i in ('dir /b object_detection\protos\*.proto') do protoc object_detection\protos\%i --python_out=.`
 9. created environment variable PYTHONPATH with 3 entries: TF\models, TF\models\research\slim, TF\models\research\object_detection
 10. `conda install -c anaconda cython`
 11. `pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI`
 12. restart
 13. `cd TF\models\research\object_detection`
 14. `jupyter notebook`
 15. open object_detection_tutorial.ipynb
 16. added imports:
	from object_detection.utils import label_map_util
	from object_detection.utils import visualization_utils as vis_util
 17. went through demo in object_detection without any problems (I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2)

Directory structure:

 - TF
     + models
     + training 
         * pet_dataset
         * pet_tf_records
         * model
         * checkpoints

Then:

 18. downloaded pet dataset https://www.robots.ox.ac.uk/~vgg/data/pets/
 19. paste contents into pet_dataset
 20. `cd TF`
 21. `conda activate tf_test`
 22. manually set faces_only to False in create_pet_tf_record.py
 23. 
```
python models\research\object_detection\dataset_tools\create_pet_tf_record.py
	--data_dir=training\pet_dataset 
	--output_dir=training\pet_tf_records 
	--mask_type png 
	--label_map_path=models\research\object_detection\data\pet_label_map.pbtxt
```

 24. downloaded mask_rcnn_inception_v2_coco from github to TF\training\model
 25. moved config from models/research/object_detection/sample/configs/ to TF\training\model
 26. changed class number to 37
 27. changed paths accordingly in config file

 28. 
```
python models\research\object_detection\model_main.py 
	--model_dir=training\checkpoints 
	--num_train_steps 1000 
	--pipeline_config_path=training\model\mask_rcnn_inception_v2_coco.config
```
 29. let train for 20 hours

I got following results, when examining in tensorboard, mAP gradually drops from the beginning of the training:
I got following results:

```
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.003
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.019
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.030
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.030
I0716 07:35:44.988143  3956 evaluation.py:275] Finished evaluation at 2019-07-16-07:35:44
I0716 07:35:44.988143  3956 estimator.py:2039] Saving dict for global step 196: DetectionBoxes_Precision/mAP = 0.00063327036, DetectionBoxes_Precision/mAP (large) = 0.00067928224, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0025016854, DetectionBoxes_Precision/mAP@.75IOU = 0.0002254508, DetectionBoxes_Recall/AR@1 = 0.018629802, DetectionBoxes_Recall/AR@10 = 0.030065296, DetectionBoxes_Recall/AR@100 = 0.030065296, DetectionBoxes_Recall/AR@100 (large) = 0.030065296, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.11927358, Loss/BoxClassifierLoss/localization_loss = 0.10073859, Loss/BoxClassifierLoss/mask_loss = 2.1088743, Loss/RPNLoss/localization_loss = 0.11095625, Loss/RPNLoss/objectness_loss = 0.54912615, Loss/total_loss = 2.988967, global_step = 196, learning_rate = 0.0002, loss = 2.988967
```

Please, do you have any suggestions where did i go wrong?"
30740,"If padding='same', which side is padded with 0?","If set `padding='same'` for `Conv1D`, which side (the start or the end) is padded with 0 if the `kernel_size=2` and `stride=1`?
I checked the code, but didn't find any clue. Hope this simple question can be clarified here. Thanks.
"
30738,saved_model.save bug in 2.0.0b0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): maybe binary (pip)
- TensorFlow version (use command below): 2.0.0b0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: P40/24451MB

You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I save saved model.pb using tf.saved_model.save() in 2.0.0b0.
and it gives this error.
```
W0716 04:34:59.213974 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49adb5da0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:34:59.375897 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49add7940>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:34:59.670762 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade14a8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:34:59.837181 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade1fd0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.001157 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade7b00>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.162233 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad6f668>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.321973 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad761d0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.503687 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad80198>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.669191 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad80cc0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:00.855475 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad87cc0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:01.017992 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad92828>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:01.204613 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad99828>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:01.524981 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ada1390>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:01.706990 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad28390>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:01.868694 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad28eb8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.059983 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad30eb8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.229396 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad35a20>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.413010 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad3da20>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.573688 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad45588>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.756634 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad4f588>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:02.916903 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad550f0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None
W0716 04:35:03.265970 140620342499136 saved_model.py:722] Skipping full serialization of object <deepnormal.Model object at 0x7fe4b82cd240>, because an error occurred while tracing layer functions. Error message: in converted code:


    TypeError: tf__wrapped_call() takes 1 positional argument but 2 were given

Traceback (most recent call last):
  File ""train.py"", line 106, in <module>
    train(dataset, FLAGS.epochs)
  File ""train.py"", line 78, in train
    tf.saved_model.save(model, path)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py"", line 835, in save
    meta_graph_def, saveable_view, signatures)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py"", line 531, in _fill_meta_graph_def
    object_map, resource_map, asset_info = saveable_view.map_resources()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py"", line 266, in map_resources
    tensor_util.constant_value(capture))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py"", line 455, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
```
**Describe the expected behavior**
When I reinstall tensorflow with 2.0.0a0 it works and available for tf serving server.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
Maybe the model should have `batchnorm` layer
tf.saved_model.save(model, path)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30736,GPU device creation fails when using the CUDA Malloc Allocator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14/master
- Python version: 3.6
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2080

**Describe the current behavior**
GPU device creation fails when the cuda malloc allocator is selected, with the error ""No allocator statistics"".  This is because of an allocator stats check introduced between releases 1.13.1 and 1.14 in the gpu common runtime  [BaseGPUDeviceFactory::CreateGPUDevice function](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1310-L1312).  This check fails when using the cuda malloc allocator because the virtual Allocator method GetStats was never overridden.

**Describe the expected behavior**
GPU device creation should succeed if the user specifies use of the cuda malloc allocator.

**Code to reproduce the issue**
Use the TF_GPU_ALLOCATOR environment variable to select the cuda malloc allocator:
`$ export TF_GPU_ALLOCATOR=""cuda_malloc""`

Then, in a python shell, try to use the gpu:
```
$ python

>>> import tensorflow as tf
>>> tf.test.is_gpu_available()
```"
30734,timeline export has bug for multiple gpu case,"Timeline export has bug when using multiple tower on multiple gpu: the exported ops are almost all placed on gpu:0, while the other device(gpu:1, gpu:2) has almost no ops. I dumped the device placement, and find out that the placement is not the same as those in timeline.json. 
"
30732,How do you export this model into a tensorflow model,Im following this code to train a CNN image classifier to classify Philippines currency. How would you export this model? to get the file to be interpreted by tensorflow
30730,Unable to find documentation for framework/ common run time files,"Hello TF team,

I've been combing the documentation for any reference to OpKernels, Graphs defs, the executor, rendezvous, ect... the important abstractions that seem to be near the core of tensorflow. So far I haven't been able to find any kind of high level overview of how they all come together. I'm working on a feature that involves adding a new device and I don't want to miss any important abstractions in my implementation.

Does anyone know where I could find documentation that is concerned with how the tensorflow framework / run time work?
Thanks,
Scott
"
30729,Best practice of using tensor-core on TensorFlow r1.12,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: cuda 9.2
- GPU model and memory: Volta 100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I'm wondering what is the best practice of leveraging V100 GPU on TensorFlow r1.12. I understand that Nvidia support it nicely with their TensorFlow container and TF-nightly build may have good support as well. For older version of TensorFlow, (r1.12 or r1.13), is ""--use_fp16=True"" and ""--xla_compile=True"" the best practice of leveraging tensor-cores?

Right now my benchmark results indicates that fp16/xla can give 844 images/sec (resnet50, batch size = 128) while fp32/no xla gives 412.5. It is no where near the reported 3x improvement. 

Thank you,"
30728,AttributeError: '_TfDeviceCaptureOp' object has no attribute '_set_device_from_string',"Tensorflow Version: 1.14.0
python3.6
Num of GPU's : 3

--- Sample keras code to reproduce this error ---
```
import tensorflow as tf
from keras.applications import Xception
from keras.utils import multi_gpu_model
import numpy as np

num_samples = 1000
height = 224
width = 224
num_classes = 1000

model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

gpus = tf.config.experimental.list_physical_devices('GPU')
print(gpus)
parallel_model = multi_gpu_model(model, gpus=2)
```


Error Message:
```

device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
device_string:  /device:GPU:0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/keras/utils/multi_gpu_utils.py"", line 227, in multi_gpu_model
    outputs = model(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 564, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 721, in run_internal_graph
    layer.call(computed_tensor, **kwargs))
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/normalization.py"", line 185, in call
    epsilon=self.epsilon)
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 1858, in normalize_batch_in_training
    if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 291, in _has_nchw_support
    explicitly_on_cpu = _is_current_explicit_device('CPU')
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 266, in _is_current_explicit_device
    device = _get_current_tf_device()
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 247, in _get_current_tf_device
    g._apply_device_functions(op)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 4581, in _apply_device_functions
    op._set_device_from_string(device_string)
AttributeError: '_TfDeviceCaptureOp' object has no attribute '_set_device_from_string'
(arg: 2) 

```"
30727,TOCO unable to convert unsupported operation using --allow_custom_ops,"

**System information**
- Have I written custom code **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) **PIXEL 2 api 27**
- TensorFlow installed from (source or binary): **Binary (pip installed)**
- TensorFlow version (use command below): **tensorflow=='1.6.0-rc1'**
- Python version:**3.68**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.0/7/4**
- GPU model and memory:**GTX 1060 6GB, 16 GB RAM**



@aquariusjay I am trying to convert mobilenet_v2 model trained on pascal voc dataset (https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md).

I want to infer with 1024x1024 resolution therefore I exported model with following commands.

` python export_model.py \
--checkpoint_path model/model.ckpt-30000 \
--export_path ./frozen_inference_graph.pb \
--model_variant=""mobilenet_v2"" \
--num_classes=21 \
--crop_size=1024 \
--crop_size=1024 \
--inference_scales=1.0
`


And then I convert model to tflite using toco command.

`toco --graph_def_file=/home/abdullah/models-master/research   /frozen_casted.pb --input_format=TENSORFLOW_GRAPHDEF                                                         
--output_format=TFLITE  
--output_file=deeplabv3_mnv2_pascal_trainval.tflite   
--inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8
--input_arrays=ImageTensor 
--output_arrays=SemanticPredictions
--input_shapes=1,1024,1024,3 --default_ranges_min=0 
--default_ranges_max=255 --allow_custom_ops  
--std_dev_values=128 --mean_values=1
`

And then model is converted to tflite without errors :
`
    2019-07-16 00:28:18.188009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Equal
2019-07-16 00:28:18.188052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Equal
2019-07-16 00:28:18.188067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: LogicalOr
2019-07-16 00:28:18.188093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Unpack
2019-07-16 00:28:18.188134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: LogicalAnd
2019-07-16 00:28:18.192256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: ResizeNearestNeighbor
2019-07-16 00:28:18.203603: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 813 operators, 1237 arrays (0 quantized)
2019-07-16 00:28:18.232795: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 801 operators, 1215 arrays (0 quantized)
2019-07-16 00:28:18.269909: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 801 operators, 1215 arrays (0 quantized)
2019-07-16 00:28:18.293777: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 132 operators, 325 arrays (0 quantized)
2019-07-16 00:28:18.296414: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 132 operators, 325 arrays (0 quantized)
2019-07-16 00:28:18.298905: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 28311552 bytes, theoretical optimal value: 25165824 bytes.
2019-07-16 00:28:18.299732: W tensorflow/contrib/lite/toco/tflite/operator.cc:661] Ignoring unsupported attribute type with key '_output_shapes'
2019-07-16 00:28:18.299743: W tensorflow/contrib/lite/toco/tflite/operator.cc:661] Ignoring unsupported attribute type with key 'T'
2019-07-16 00:28:18.299946: W tensorflow/contrib/lite/toco/tflite/operator.cc:661] Ignoring unsupported attribute type with key 'T'
2019-07-16 00:28:18.299956: W tensorflow/contrib/lite/toco/tflite/operator.cc:661] Ignoring unsupported attribute type with key '_output_shapes'
`


But when I run the model on tflite it gives the following error.

    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find           
    custom op for name 'ExpandDims' with version 1 Registration failed.

Following are the details of environment.

    For conversion I used tensorflow tensorflow=='1.6.0-rc1'

    And tflite implementation in build.gradle is implementation 'org.tensorflow:tensorflow-lite:+'

I don't know where I am failing, please help me.

"
30726,"cudaGetDevice() failed after upgrading to v 1.14, downgraded back to 1.12 still having issue","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.12
- **Python version**:3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:Tesla K80 and ?
- **Exact command to reproduce**:model_main.py


### Describe the problem
I am running on AWS instance and upgraded to tensorflow 1.14 to build arm7 tflite runtime wheel for a pi.  That is completed and works.  I am now trying to retrain my model on my AWS instance and am getting the cudaGetDevice() failed error.  I have now downgraded back to tensorflow 1.12 to try and fix and get the same issue.  I have upgraded and downgraded nvidia drivers and nvidia-smi command works.  Below is the traceback error.

### Source code / logs
Traceback (most recent call last):
  File ""/home/ubuntu/sc2.2/models/research/object_detection/model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/ubuntu/sc2.2/models/research/object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1468, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session
    config=config)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py"", line 185, in _restore_checkpoint
    sess = session.Session(self._target, graph=self._graph, config=config)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1551, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 676, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
"
30725,CIFAR-10 tutorial for multi-GPU fails because full shape isn't passed to prefetch_queue,The [CIFAR-10 Multi-GPU Tutorial](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) has a bug in it when run from the command line. I am using Tensorflow `1.13.1` and Tensorflow-Datasets `1.0.2`. You simply need to put an explicit call to tf.reshape before passing it into the prefetch_queue to add the outer num_samples shape. I've got a quick PR that fixes this.
30722,TensorFlow Lite build fatal error: gtest/gtest.h: No such file or directory,"**System information**
- Raspberry Pi 3
- TensorFlow installed from (source or binary): source
- TensorFlow version:  1.14.0

**Describe the problem**

When natively compiling TensorFlow Lite 1.14.0 in Raspberry Pi 3, I am getting the following compile error:
fatal error: gtest/gtest.h: No such file or directory

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the error log:
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/test_main.cc -o /home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o
tensorflow/lite/kernels/test_main.cc:17:25: fatal error: gtest/gtest.h: No such file or directory
 #include <gtest/gtest.h>
                         ^
compilation terminated.
tensorflow/lite/tools/make/Makefile:241: recipe for target '/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o' failed
make: *** [/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:22:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:42,
                 from tensorflow/lite/kernels/sub.cc:18:
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function ‘static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)’:
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:484:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:487:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
pi@raspberrypi:~/tensorflow $ 
"
30721,TensorFlow import error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.7.4 x64
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Intel core i7 6700, 16GB ram

**Describe the problem**
Hi,
I would like to import tensorflow on pycharm. I checked related posts but I think that I missed a step somewhere. Please excuse me I am new to python and may have forgotten something trivial. 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

I installed tensorflow and it succeeded, then run the following code: (I disabled unused import statements but it seems that it is still an issue).

```
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt
print(tf.__version__)
```


**Any other info / logs**

I get the following error message:

```
Traceback (most recent call last):
  File ""C:/Users/Leo Picard/PycharmProjects/tensorflow/tensorflow_init.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Python\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python\lib\site-packages\tensorflow\python\__init__.py"", line 47, in <module>
    import numpy as np
  File ""C:\Python\lib\site-packages\numpy\__init__.py"", line 142, in <module>
    from . import core
  File ""C:\Python\lib\site-packages\numpy\core\__init__.py"", line 23, in <module>
    WinDLL(os.path.abspath(filename))
  File ""C:\Python\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 193] %1 is not a valid Win32 application
```
"
30717,No Speedup or Size Savings After FP16 / INT8 with TensorRT,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): [nvidia-docker `tensor:19.02-py3` (Ubuntu 16.04)](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02)
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0.130 / 7.4.2
- GPU model and memory: Tesla V100 32GB

**Describe the current behavior**
I am trying to optimize (decrease) the inference time and model size of my Tiny Yolov3 model. I currently have it as a frozen graph. When I run the timing and evaluation script from [here](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py), fps actually decreases as the optimizations are tried, and file sizes stay constant.

**Describe the expected behavior**
I expect file sizes and inference time to decrease.

**Code to reproduce the issue**
I used [this](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02) nvidia-docker image.

[This](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py) script with a small preprocessing change:

```
def resize_image(image, target_height, target_width):
    image = image.resize((target_width, target_height), Image.BICUBIC).convert('RGB')
    return np.array(image)

def preprocess_image(file_name, output_height=416, output_width=416,
                     num_channels=3):
    return resize_image(Image.open(file_name), output_height, output_width).astype('float32')
```

I also removed the printing of predictions, as the script is built for a classifier rather than a detector.

**Other info / logs**
Command executed (image file is attached; model is [here](https://drive.google.com/file/d/1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v/view)):
```
python timed.py --frozen_graph=tiny-yolov3_frozen.pb \
--image_file=11075842.jpg \
--native --fp32 --fp16 --int8 \
--output_dir=/workspace \
--input_node=inputs --output_node=output_boxes
```
```
==========================
network: native_tiny-yolov3_frozen.pb,   batchsize 256, steps 100
  fps   median: 993.1,  mean: 959.8,    uncertainty: 8.9,       jitter: 7.7
  latency       median: 0.25778,        mean: 0.26986,  99th_p: 0.43523,        99th_uncertainty: 0.05783

==========================
network: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 256, steps 100
  fps   median: 912.7,  mean: 896.3,    uncertainty: 8.2,       jitter: 4.6
  latency       median: 0.28048,        mean: 0.29338,  99th_p: 0.42830,        99th_uncertainty: 0.43877

==========================
network: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 256, steps 100
  fps   median: 823.5,  mean: 831.9,    uncertainty: 8.4,       jitter: 96.2
  latency       median: 0.31087,        mean: 0.31129,  99th_p: 0.43070,        99th_uncertainty: 0.07565

==========================
network: tftrt_int8_tiny-yolov3_frozen.pb,       batchsize 256, steps 100
  fps   median: 803.3,  mean: 820.6,    uncertainty: 9.9,       jitter: 19.9
  latency       median: 0.31867,        mean: 0.31814,  99th_p: 0.60985,        99th_uncertainty: 0.01011
```"
30716,Provide Raspberry Pi #aarch64 cross-compile instructions,"**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): yes, at least as a tester


**Describe the feature and the current behavior/state.**

There is no pre-build binary for Raspberry Pi / ARM and the instructions to build from scratch [here](https://www.tensorflow.org/install/source_rpi#build_from_source). However, there is nothing for any kind of SBC (like Raspberry Pi, ODroid, Pine64, etc) with a aarch64 architecture. 

I cannot use the Python wheel. I need the C library to bind to a different language. 

**Will this change the current api? How?**

No. 

**Who will benefit with this feature?**

Anyone using a SBC with a aarch64 OS. 

"
30715,Low GPU usage of RNN layers under MirroredStrategy,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 14.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0b1`
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 
- GPU model and memory: TITAN X

**Describe the current behavior**

RNN layers have poor performance and low GPU usage when used with `MirroredStrategy`. By monitoring `nvidia-smi`, part of the execution seems to run sequentially on each GPU.

**Describe the expected behavior**

With `MirroredStrategy`, the model is expected to be run in parallel.

**Code to reproduce the issue**

Consider the dummy training code below. It generates examples with random shapes and apply a stack of `LSTMCell` on batches of sequences on 3 GPUs. If you replace the RNN layer by e.g. a stack of Dense layers, the parallelism is visibly improved.

```python
import tensorflow as tf

class MyModel(tf.keras.layers.Layer):

    def __init__(self):
        super(MyModel, self).__init__()
        cell = tf.keras.layers.StackedRNNCells(
            [tf.keras.layers.LSTMCell(512) for _ in range(12)])
        self.rnn = tf.keras.layers.RNN(cell)

    def call(self, inputs):
        return self.rnn(inputs)

dataset = tf.data.Dataset.from_tensor_slices(
    tf.random.uniform([10000], minval=1, maxval=80, dtype=tf.int32))
dataset = dataset.shuffle(10000)
dataset = dataset.map(lambda t: tf.zeros([t, 512]))
dataset = dataset.padded_batch(
    64, padded_shapes=tf.compat.v1.data.get_output_shapes(dataset))
dataset = dataset.repeat()
dataset = dataset.prefetch(1)

devices = [""/gpu:0"", ""/gpu:1"", ""/gpu:2""]
strategy = tf.distribute.MirroredStrategy(devices=devices)

with strategy.scope():
    dataset = strategy.experimental_distribute_dataset(dataset)
    model = MyModel()
    optimizer = tf.keras.optimizers.Adam()

def step(inputs):
    outputs = model(inputs)
    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)(
        tf.zeros_like(outputs), outputs)
    variables = model.trainable_variables
    gradients = optimizer.get_gradients(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return loss

@tf.function
def train():
    with strategy.scope():
        for inputs in dataset:
            loss = strategy.experimental_run_v2(step, args=(inputs,))

train()
```

cc @jkamalu."
30714,Running inference on Coral TPU stick results in Segmentation fault,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Docker image tensorflow/tensorflow:gpu-latest
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15+
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti

I want to deploy a simple tflite model on a machine with an ARM processor and the Coral TPU Stick, but run into problems and inconsistencies during the process. Due to the procedure being quite long, I decided to post the whole process, so you track every step that I made. Overall goal is to fit a function of the form `f(x) = 2x - 1`.

**1. Train a model with quantization aware training**

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

print(tf.__version__)

train_input = np.array([ -1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
train_truth = np.array([ -3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)

def build_keras_model():
	return keras.models.Sequential([
		keras.layers.Dense(units=1, input_shape=[1]),
	])

### train the model
train_graph = tf.Graph()
train_sess = tf.Session(graph=train_graph)

keras.backend.set_session(train_sess)

with train_graph.as_default():
	keras.backend.set_learning_phase(1)
	train_model = build_keras_model()

	tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)
	train_sess.run(tf.global_variables_initializer())

	train_model.compile(
		optimizer='sgd',
		loss='mean_squared_error'
	)
	train_model.fit(train_input, train_truth, epochs=250)

	saver = tf.train.Saver()
	saver.save(train_sess, 'linear.ckpt')
```
This produces the checkpoint. Next, I created an eval graph and loaded the checkpoint.

**2. Create the eval graph and freeze the model**

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

def build_keras_model():
	return keras.models.Sequential([
		keras.layers.Dense(units=1, input_shape=[1]),
	])

# eval
eval_graph = tf.Graph()
eval_sess = tf.Session(graph=eval_graph)

keras.backend.set_session(eval_sess)

with eval_graph.as_default():
	keras.backend.set_learning_phase(0)
	eval_model = build_keras_model()

	tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)

	eval_graph_def = eval_graph.as_graph_def()
	saver = tf.train.Saver()
	saver.restore(eval_sess, 'linear.ckpt')

	frozen_graph_def = tf.graph_util.convert_variables_to_constants(
		eval_sess,
		eval_graph_def,
		[eval_model.output.op.name]
	)

	with open('frozen_model.pb', 'wb') as f:
		f.write(frozen_graph_def.SerializeToString())
```
I resorted to use the python API for freezing it because the command line tool throws errors. Executing this, I get a protocol buffer called `frozen_model.pb`.

**3. Convert the protocol buffer into a tflite model**
```
tflite_convert \
--output_file=model.tflite \
--graph_def_file=frozen_model.pb \
--inference_type=QUANTIZED_UINT8 \
--input_arrays=dense_input \
--output_arrays=dense/BiasAdd \
--mean_values=0 \
--std_dev_values=255 \
--default_ranges_min=-128 \
--default_ranges_max=127
```
Resorting the the command line tool, this produces a tflite model. Although I am not sure how to determine good values for mean, std_dev and the default ranges, the conversion seems to work. I end up with a model, that I was able to load and interpret with the Python API. Apparently, everything has worked as expected.

**4. Use the edgetpu_compiler to create a tflite file that utilizes the TPU stick**

`edgetpu_compiler model.tflite`

This command creates a file called `model_edgetpu.tflite`. No errors, I even get a log file stating that everything worked as expected.

**5. Transferring everything to the ARM machine, running the interpreter there**

I tried to do inference on the target system, using the following C++ snippet to create an interpreter as described on the Coral site.

```
#include <stdio.h>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/tools/gen_op_registration.h""
#include ""edgetpu.h""

int main(){

    // load model
    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""model_edgetpu.tflite"");

    if(!model){
        printf(""Failed to mmap model\n"");
        exit(0);
    }

    // create TPU context handle
    edgetpu::EdgeTpuContext* edgetpu_context = edgetpu::EdgeTpuManager::GetSingleton()->NewEdgeTpuContext().release();

    tflite::ops::builtin::BuiltinOpResolver resolver;
    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());

    // Build the interpreter
    std::unique_ptr<tflite::Interpreter> interpreter;

    if (tflite::InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk){
            printf(""Failed to build interpreter."");
    }

    interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);
    interpreter->SetNumThreads(1);

    // Allocate memory
    if(interpreter->AllocateTensors() != kTfLiteOk){
            printf(""Failed to allocate tensors."");
    }

    // Prepare input(s)
    float a = 0.0;
    interpreter->typed_input_tensor<float>(0)[0] = a;

    // Invoke the interpreter
    interpreter->Invoke();

    // Get output(s)
    float* output = interpreter->typed_output_tensor<float>(0);

    // Should print -1.0
    printf(""Result is: %f\n"", *output);

    return 0;
}
```

After compiling my executable without any errors, I get an Segmentation fault:
```
root@localhost:/demo# ./tpu_demo 
INFO: Initialized TensorFlow Lite runtime.
Segmentation fault
```

Until this point, no errors have occured during the whole process. So I guess something went wrong during the creation of the tflite model. I also noticed that the non-edge_compiler version of the the tflite model also results in a Segmentation fault on the ARM system, even though I was able to use with the Python API.

Any help is appreciated! Thanks."
30713,Gradient tape with tf.math.reduce_euclidean_norm disconnects,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (Conda)
- TensorFlow version (use command below): 2.0.0b1                  
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A (Running on CPU)


**Describe the current behavior**
Using GradientTape to track the gradients using tf.math.reduce_euclidean_norm directly the gradient disconnects and returns 'None'.

**Describe the expected behavior**
I expect a gradient to be returned. If I decompose the function into the three constituent sequential operators (square the elements, sum them [reducing the dimension] and square rooting the resulting tensor) I get the gradient connecting up as expected.

**Code to reproduce the issue**
import tensorflow as tf

x = tf.constant([3.0,1.2,17,0])


''' Calculate euclidian distance of an nD vector by tf implementation'''
with tf.GradientTape() as t2:
  t2.watch(x)
  z2 = tf.math.reduce_euclidean_norm(x, axis = -1)

dz2_dx = t2.gradient(z2,x)
print(""z: {}, \nGradients: {}"".format(z2, dz2_dx))


''' Calculate euclidian distance of an nD vector by decomposed operations'''
with tf.GradientTape() as t:
  t.watch(x)
  x_sq = tf.math.square(x)
  x_sum_sq = tf.math.reduce_sum(x_sq, axis = 0)
  z = tf.math.sqrt(x_sum_sq)

dz_dx = t.gradient(z, x)
print(""z: {}, \nGradients: {}"".format(z, dz_dx))



**Other info / logs**
The example code gives:
z: 17.30433464050293, 
Gradients: None
z: 17.30433464050293, 
Gradients: [0.17336696 0.06934679 0.9824128  0.        ]

With the reduce_euclidean_distance() operator the gradient is dropped."
30712,Duplicate variables for models whose layers share weights,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14 and 2
- Python version: 3.6
- CUDA/cuDNN version: 10/7
- GPU model and memory: NA

**Describe the current behavior**

Imagine you extend the tf.keras.layers.Layer with the class MyLayer, which is a wrapper for the tf.keras.layers.Dense layer class (this class extension could also be a Model, but that's not important).

Now imagine you instantiate a Dense layer `dense` (w/ no bias) and initialize two independent MyLayer instances `dense1` and `dense2` using `dense`...

Now, we extend the Model class to take two MyLayer instances on init. We do this because we want to use dense1 and dense2 to perform different tasks but we want their weights to be shared (in a realistic example, MyLayer would be split into MyLayer1 and MyLayer2, which would perform different operations on `call` but would both depend on `dense`). We run data through our model pipeline and compute a loss, gathering gradients by using either the GradientTape or keras optimizers. However, when we gather and apply gradients with respect to Model.trainable_variables, even though our model has one unique weight, the kernel for `dense`, Model.trainable_variables will return two variables, and the gradients will be calculated and thus applied twice, which I would say is a bug, insofar as it's unexpected.

**Describe the expected behavior**

Models whose layers have shared/tied weights should not return duplicate weights when accessing the trainable_variables property. A super simple work-around is to call list(set(model.trainable_variables)) but the real issue is the unexpected behavior: ""Why would I ever think that model.trainable_variables would return duplicates of the same variable ?!"" 

The two calls to test in the code below should have the same output.

**Code to reproduce the issue**
`
code

    import numpy as np
    import tensorflow as tf

    tf.enable_eager_execution()

    class MyDense(tf.keras.layers.Layer):
      def __init__(self, dense, **kwargs):
        super().__init__(**kwargs)
        self.dense = dense
      def call(self, inputs):
        return self.dense(inputs)

    class MyModel(tf.keras.Model):
      def __init__(self, dense1, dense2, **kwargs):
        super().__init__(**kwargs)
        self.dense1 = dense1
        self.dense2 = dense2
      def call(self, inputs):
        return self.dense1(inputs) + self.dense2(inputs)

    def test(unique):
      x = tf.ones(shape=(10, 5))
      y = tf.ones(shape=(10, 1)) + 2

      dense = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.constant(0.2), use_bias=False)
      dense1 = MyDense(dense)
      dense2 = MyDense(dense)
      model = MyModel(dense1, dense2)

      adam = tf.keras.optimizers.Adam(0.001)

      with tf.GradientTape() as tape:
        y_hat = model(x)
        loss = tf.keras.losses.mse(y, y_hat)
        variables = model.trainable_variables if unique else list(set(model.trainable_variables))
        print(f""# trainable variables: {len(variables)}"")

        pre_weight = variables[0][0]

        grads = tape.gradient(loss, variables)
        adam.apply_gradients(list(zip(grads, variables)))

        post_weight = variables[0][0]

        return post_weight - pre_weight

    ex1 = test(True)
    print(ex1)
    print(""\n+=+=+=+=\n"")
    ex2 = test(False)
    print(ex2)

`

"
30711,Keras custom metrics raises error when update_state returns an op.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to build a custom metric for Keras, which worked with tensorflow 1.12. Now after upgrading to python 1.14 I get the error shown below.  I am returning the result of tf.group in the update_state method of the metric which is of course an Op. What puzzles me is that `tensorflow.python.keras.utils.metric_utils.update_confusion_matrix_variables` which is used by many of the other builtin metrics like Precision, does the exact same thing. To make sure that the error is not caused by my own implementation I copied the implementation of tf.keras.metrics.Precision into my own file and tried to run it. It get the same error, however when I substitute this ""custom"" metric with the builtin, it works. The code to reproduce this is shown below.

*Describe the expected behavior**
The custom metric should work as expected.


**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.python.keras.metrics import Metric
from tensorflow.python.keras.utils import metrics_utils
import tensorflow.keras.backend as K
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import init_ops
import numpy as np
from tensorflow.python.keras.utils.generic_utils import to_list


class Precision(Metric):
    """"""This is a 1:1 copy of the code in tensorflow.python.keras.metrics.""""""
    def __init__(self,
                 thresholds=None,
                 top_k=None,
                 class_id=None,
                 name=None,
                 dtype=None):

        super(Precision, self).__init__(name=name, dtype=dtype)
        self.init_thresholds = thresholds
        self.top_k = top_k
        self.class_id = class_id

        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF
        self.thresholds = metrics_utils.parse_init_thresholds(
            thresholds, default_threshold=default_threshold)
        self.true_positives = self.add_weight(
            'true_positives',
            shape=(len(self.thresholds),),
            initializer=init_ops.zeros_initializer)
        self.false_positives = self.add_weight(
            'false_positives',
            shape=(len(self.thresholds),),
            initializer=init_ops.zeros_initializer)

    def update_state(self, y_true, y_pred, sample_weight=None):
        return metrics_utils.update_confusion_matrix_variables({
            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,
            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives},
            y_true,
            y_pred,
            thresholds=self.thresholds,
            top_k=self.top_k,
            class_id=self.class_id,
            sample_weight=sample_weight)

    def result(self):
        result = math_ops.div_no_nan(
            self.true_positives,
            self.true_positives + self.false_positives)
        return result[0] if len(self.thresholds) == 1 else result

    def reset_states(self):
        num_thresholds = len(to_list(self.thresholds))
        K.batch_set_value(
            [(v, np.zeros((num_thresholds,))) for v in self.variables])

    def get_config(self):
        config = {
            'thresholds': self.init_thresholds,
            'top_k': self.top_k,
            'class_id': self.class_id
        }
        base_config = super(Precision, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


if __name__ == '__main__':
    x = tf.keras.Input((10,))
    y_hat = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.models.Model(inputs=[x], outputs=[y_hat])
    model.compile(
        optimizer=tf.keras.optimizers.SGD(0.01),
        loss='binary_crossentropy',
        metrics=[Precision()])
    # However the builtin metric works:
    # model.compile(
    #     optimizer=tf.keras.optimizers.SGD(0.01),
    #     loss='binary_crossentropy',
    #     metrics=[tf.keras.metrics.Precision()])

    X = np.random.uniform(-1, 1, size=(100, 10)).astype(np.float32)
    y = np.random.choice([0, 1], size=(100,)).astype(np.float32)
    model.fit(X, y)

```

**Other info / logs**
```
Traceback (most recent call last):
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 676, in convert
    x = ops.convert_to_tensor_or_composite(x)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1479, in convert_to_tensor_or_composite
    value=value, dtype=dtype, name=name, as_ref=False)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1518, in internal_convert_to_tensor_or_composite
    accept_composite_tensors=True)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1224, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6696, in _operation_conversion_error
    (op.name, dtype, name, as_ref))
TypeError: Can't convert Operation 'group_deps' to Tensor (target dtype=None, name=None, as_ref=False)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""metrics_bug.py"", line 75, in <module>
    metrics=[Precision()])
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 330, in compile
    masks=self._prepare_output_masks())
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2170, in _handle_metrics
    target, output, output_mask))
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2118, in _handle_per_output_metrics
    mask)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2094, in _call_metric_fn
    strategy=self._distribution_strategy)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py"", line 1054, in call_replica_local_fn
    return fn(*args, **kwargs)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 873, in call_metric_function
    return metric_fn(y_true, y_pred, sample_weight=weights)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py"", line 170, in __call__
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 73, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 414, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 357, in _initialize
    *args, **kwds))
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1349, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1652, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1545, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 720, in func_graph_from_py_func
    expand_composites=True)
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 515, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 515, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/Users/denis/anaconda2/envs/dev-p36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 682, in convert
    (str(python_func), type(x)))
TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function Function._defun_with_scope.<locals>.wrapped_fn at 0xb34ec5d08>, found return value of type <class 'tensorflow.python.framework.ops.Operation'>, which is not a Tensor.
```

"
30710,No way to generate HTML docs from source,"## Description of issue (what needs changing):

According to https://github.com/tensorflow/tensorflow/issues/1574 there was no (open source) way to generate HTML docs from the TensorFlow source code in March of 2016. Is that still the case? I haven't been able to find any way to generate HTML docs from the source.

Generating offline HTML docs is useful for having access to offline docs [without having to scrape www.tensorflow.org](https://github.com/ppwwyyxx/dash-docset-tensorflow)."
30706,HistogramFixedWidth in TFLite ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0beta


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. 

If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). 

Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). 

Here is a list of operators for which you will need custom implementations: HistogramFixedWidth.
```

Code to reproduce:

```
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

root = tf.train.Checkpoint()
root.nbins = tf.Variable(nbins)
root.value_range = tf.Variable(value_range)
root.f = tf.function(lambda x: tf.histogram_fixed_width(x, root.value_range, nbins=root.nbins))

input_data = tf.convert_to_tensor(new_values)
concrete_func = root.f.get_concrete_function(input_data)

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
tflite_model = converter.convert()
```"
30705,./bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory,"Hi guys.
i built Bazel from scratch from here:
https://docs.bazel.build/versions/master/install-compile-source.html

Now i want to run 
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

and then lite my model.but when i run it this error appears:
./bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory

but i have bazel-bin


Can any one help me please?"
30703,Compiling 1.14 with MPI support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Centos 6.9
- 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source):0.25.2
- GCC/Compiler version (if compiling from source): 4.9,3
- CUDA/cuDNN version:10.0.130/7
- GPU model and memory: cuda



Compiling with MPI support gives the following build errors:
INFO: From Compiling tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc:
external/com_google_absl/absl/strings/string_view.h(495): warning: expression has no effect
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(109): error: identifier ""CudaLaunchKernel"" is undefined
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(110): error: identifier ""CudaLaunchKernel"" is undefined
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(111): error: identifier ""CudaLaunchKernel"" is undefined
3 errors detected in the compilation of ""/tmp/tmpxft_00038d5b_00000000-6_ring.cu.cpp1.ii"".

Standard ./configure but answer yes to MPI support


Compiles fine without MPI. Have tried with both openmpi/3.1.3 and cuda enabled openmpi/3.1.3

"
30702,Variation in Tensorflow execution latency,"**System information:**

Neural Network: 	LeNet 300-100
Target Platform: 	XEON PLATINUM 8000 SERIES - AMAZON AWS EC2 C5 INSTANCE
Supports:  A		VX-512, 72 Cores, 2-4 Threads; Up to 2 Instance Cores and 2 vCPUs
Baseline Framework: 	TensorFlow (AWS Image Version)
AMI ID: 		Deep Learning AMI (Ubuntu) Version 23.1 (ami-07262a45de118922e)
Python version:		p27


**Describe the problem:**

			To collect baseline results for TensorFlow model of lenet_300_100 ( Find the Accuracy and inference latency). By using MNIST dataset with a batch size of 16 and full(10000) number of batches.

Analysis Tensorflow inference runtime results, There is inconsistent on subsequent sessions. Please review the attached “ref_times” for  inconsistent runtime for various batches.
![Runtime](https://user-images.githubusercontent.com/51189885/61205763-943e4380-a70e-11e9-8ead-568ddd829416.png)
[ref_times.txt](https://github.com/tensorflow/tensorflow/files/3391578/ref_times.txt)
Please review the following logs and advice on this.

**Source code / logs:**

**Time session source code:**

with tf.Session(graph=graph, config=config) as sess:
    curr_batch = 0
    warm_up = 5
    for i in range(warm_up):
        sess.run(output, feed_dict={input_: zeros})
    while True:
        batch = input_reader.receive_batch()
        if batch is None:
            break
        start = monotonic.monotonic()
        pred = sess.run(output, feed_dict={input_: batch})
        sys.stderr.write('Batch Inference Time(s): {}\n'.format(
            monotonic.monotonic() - start))
        output_writer.output_send(pred)

Please let know need another details? Please review all and advice me on this. 

"
30701,build tensorflow failed with bazel on windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win7 X64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:2.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):0.25.1
- GCC/Compiler version (if compiling from source):no
- CUDA/cuDNN version:no
- GPU model and memory:no



**Describe the problem**
I try to build C++ lib and dll file with Bazel, as the following steps:
1. run ""python configure.py"", and set up options.
2.run ""bazel build --config=opt //tensorflow/tools/lib_package:libtensorflow""
It throw errors:
```
""Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
INFO: An error occurred during the fetch of repository 'io_bazel_rules_docker'
INFO: Call stack for the definition of repository 'io_bazel_rules_docker':
 - C:/users/administrator/_bazel_administrator/zfk46uyn/external/bazel_toolchain
s/repositories/repositories.bzl:37:9
 - D:/tensorflow/tensorflow/WORKSPACE:29:1
ERROR: error loading package '': Encountered error while reading extension file
'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//reposi
tories': Traceback (most recent call last):
        File ""C:/users/administrator/_bazel_administrator/zfk46uyn/external/baze
l_tools/tools/build_defs/repo/git.bzl"", line 234
                _clone_or_update(ctx)
        File ""C:/users/administrator/_bazel_administrator/zfk46uyn/external/baze
l_tools/tools/build_defs/repo/git.bzl"", line 74, in _clone_or_update
                fail((""error cloning %s:\n%s"" % (ctx....)))
error cloning io_bazel_rules_docker:
+ cd C:/users/administrator/_bazel_administrator/zfk46uyn/external
+ rm -rf C:/users/administrator/_bazel_administrator/zfk46uyn/external/io_bazel_
rules_docker C:/users/administrator/_bazel_administrator/zfk46uyn/external/io_ba
zel_rules_docker""
```

Thanks for the help!
"
30700,training become too slow,"I`m working on:
Windows 10,
64 bit,
Anaconda3 version 5.3.0 
python 3.6.8,
tensorflow version 1.10.0


"
30699,Performance slowdown in Non-AVX targets with default build options,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code written. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile. Slowdown seen on both windows PC and NI Linux Real Time OS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14-RC0
- Python version: 3.5
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 4.7
- CUDA/cuDNN version: NA. Not building with CUDA
- GPU model and memory: NA

**Describe the current behavior**

The performance of TF 1.14-RC0 is more than 3 time slower compared to TF 1.8 binaries. We are using a standard SSD mobile net model for loading and running. 

We used same build options for TF 1.8 and TF 1.14-RC0. So I believe there is some change in default build options that is affecting the performance on Non-AVX targets. (Please refer 'Steps Followed' for build options used) 

**Steps Followed**

We are building with all default settings except with following changes: 
- we set 'march' optimization flag empty. we do not want to optimize for native system. (In 1.8, we did not set 'march' optimization flag)
- we say NO to XLA JIT support.

We tried following command to build :

1) bazel build --config=opt //tensorflow:tensorflow
2) bazel build --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl //tensorflow:tensorflow


"
30697,Freeze_graph not supporting tf.control_dependencies,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (pypi: tensorflow-gpu==1.14.0)
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA-10/CUDNN-7
- GPU model and memory: NVIDIA 1080

**Describe the current behavior**
This is the simple code I want to freeze, where `out` node is as the output node names:
```py
  print_op = tf.print(tf.ones([2, 2]))
  with tf.control_dependencies([print_op]):
    out = tf.zeros([3,])
```
By freezing the `out` node with `freeze_graph.freeze_graph(...)`, what I got from the protobuf only contains nodes like `StringFormat`, `PrintV2` and `Ones`. No else information to show there execution order. Seems like the execution order managed by the `tf.control_dependencies` is lost from the frozen graph.

**Code to reproduce the issue**

Freeze any large or small graph that contains using `tf.control_dependencies`, and check how it is described in the frozen protobuf data.
"
30695,ModelCheckpoint should have a way to delete non-best model checkpoints as it goes,"**System information**
- TensorFlow version (you are using): 2.0.0b0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
tensorflow.keras.callbacks.ModelCheckpoint should have an (optional) way to clean up non-best model checkpoints as it goes. This would help avoid disk consumption issues for larger models and other uses cases like doing a lot of training during hyperparameter searches.

**Will this change the current api? How?**
Yes. My proposal is to add a 'delete_previous_best_checkpoints' named argument to the constructor for tensorflow.keras.callbacks.ModelCheckpoint with the default set to False for backward compatibility. The docs would be something like this:
delete_previous_best_checkpoints: If True then the previous best checkpoint will be deleted after a new best checkpoint is saved. Only non-best checkpoints created by the current training session will be deleted. Resumed training from CheckpointA will not delete CheckpointA or any previous checkpoint.

**Who will benefit with this feature?**
Folks training large models in disk consumption conscious environments (think AWS budget). Or folks doing hyperparameter searches where it's easy to create hundreds of checkpoints quickly.

**Any Other info.**
I'm happy to submit a PR for this."
30693,ImportError: cannot import name 'lite',"on ubuntu 18.4

using this input 

`from tensorflow.contrib import lite
converter = lite.TFLiteConverter.from_keras_model_file( 'model.h5')
tfmodel = converter.convert()
open (""model.tflite"" , ""wb"") .write(tfmodel)`

i got this error

`Traceback (most recent call last):
  File ""h5totflite.py"", line 1, in <module>
    from tensorflow.contrib import lite
ImportError: cannot import name 'lite'`"
30691,Strange bug with uint64 and tf.constants,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13
- Python version: 2.7

**Describe the current behavior**
Run the following code
```
import tensorflow as tf

xx = tf.constant([ 54043195528445964 , 72057594037927941 , 54043195528445957, 54043195528445954, 108086391056891910], dtype=tf.int64)
yy = tf.cast(xx, dtype=tf.uint64)


qqq=tf.constant([ 1,  2 , 3, 4, 5])
www=tf.constant([ 1,  2 , 3, 4, 5])

sess = tf.Session()
with sess.as_default():
    print(sess.run(xx))
    print(sess.run(yy))
```
the output is 
```
[ 54043195528445964  72057594037927941  54043195528445957
  54043195528445954 108086391056891910]
[0 0 0 0 0]
```
while it should have been
```
[ 54043195528445964  72057594037927941  54043195528445957
  54043195528445954 108086391056891910]
[ 54043195528445964  72057594037927941  54043195528445957
  54043195528445954 108086391056891910]
```

However, the following code
```
import tensorflow as tf

xx = tf.constant([ 54043195528445964 , 72057594037927941 , 54043195528445957, 54043195528445954, 108086391056891910], dtype=tf.int64)
yy = tf.cast(xx, dtype=tf.uint64)

sess = tf.Session()
with sess.as_default():
    print(sess.run(xx))
    print(sess.run(yy))
```
prints the correct output
```
[ 54043195528445964  72057594037927941  54043195528445957
  54043195528445954 108086391056891910]
[ 54043195528445964  72057594037927941  54043195528445957
  54043195528445954 108086391056891910]
```

This is really weird
"
30690,.h5 to tflite conversion error,"working on GOOGLE COLAB

tensorflow version 1.2

INPUT :- 
`from tensorflow.contrib import lite

converter = lite.TFLiteConverter.from_keras_model_file( 'model.h5' ) # Your model's name

model = converter.convert()

file = open( 'model.tflite' , 'wb' )
 
file.write( model )`

ERROR:-
`ImportError                               Traceback (most recent call last)
<ipython-input-1-e02e9998849c> in <module>()
      1 
----> 2 from tensorflow.contrib import lite
      3 converter = lite.TFLiteConverter.from_keras_model_file( 'model.h5' ) # Your model's name
      4 
      5 model = converter.convert()`

`ImportError: cannot import name 'lite'`"
30689,Need help training model on TPU on Colab,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Hey everyone, 

I'm training a Neural Machine Translation model on Colab using `tf.keras` and when I try to connect to the TPU, it gives me this error:
```
W0714 02:35:35.778475 140626016266112 keras_support.py:217] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.
I0714 02:35:35.780029 140626016266112 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.17.222.66:8470) for TPU system metadata.
I0714 02:35:35.794606 140626016266112 tpu_system_metadata.py:148] Found TPU system:
I0714 02:35:35.796221 140626016266112 tpu_system_metadata.py:149] *** Num TPU Cores: 8
I0714 02:35:35.797333 140626016266112 tpu_system_metadata.py:150] *** Num TPU Workers: 1
I0714 02:35:35.798576 140626016266112 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8
I0714 02:35:35.799489 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18249094776997611553)
I0714 02:35:35.801580 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1473395334155129707)
I0714 02:35:35.803380 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12488202382755017065)
I0714 02:35:35.804275 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 11169959297202392905)
I0714 02:35:35.806226 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17277375797260286420)
I0714 02:35:35.808259 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14241437196071514035)
I0714 02:35:35.811089 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6205429048552507469)
I0714 02:35:35.813731 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6051595501763147539)
I0714 02:35:35.816036 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4392780516891638609)
I0714 02:35:35.817032 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17401714988011142246)
I0714 02:35:35.819163 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3282974203306179719)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-> 1356       return fn(*args)
   1357     except errors.OpError as e:

11 frames
InvalidArgumentError: From /job:worker/replica:0/task:0:
In ReadVariableOp the following variables were found uninitialized: dense_3/bias, dense_3/kernel, embedding_3/embeddings, lstm_6/bias, lstm_6/kernel, lstm_6/recurrent_kernel, lstm_7/bias, lstm_7/kernel, lstm_7/recurrent_kernel
	 [[{{node ReadVariables_10092430516030672714/_1}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-> 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

InvalidArgumentError: From /job:worker/replica:0/task:0:
In ReadVariableOp the following variables were found uninitialized: dense_3/bias, dense_3/kernel, embedding_3/embeddings, lstm_6/bias, lstm_6/kernel, lstm_6/recurrent_kernel, lstm_7/bias, lstm_7/kernel, lstm_7/recurrent_kernel
	 [[{{node ReadVariables_10092430516030672714/_1}}]]
```

My model is as such:

```python
def NMT_Model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
    model.add(tf.keras.layers.LSTM(units))
    model.add(tf.keras.layers.RepeatVector(out_timesteps))
    model.add(tf.keras.layers.LSTM(units, return_sequences=True))
    model.add(tf.keras.layers.Dense(out_vocab, activation='softmax'))
    
    return model

model = NMT_Model(ger_vocab_size, eng_vocab_size, ger_sequence_length, eng_sequence_length, 512)
rms = tf.train.RMSPropOptimizer(learning_rate=0.01)
model.compile(loss='sparse_categorical_crossentropy', optimizer=rms, metrics=['acc'])
model.summary()
```

Can I please know how I can go about successfully connecting to a TPU and running a straightforward `tf.keras` model on it? Any help would be highly appreciated!

If there are any other details I can give, please do let me know!

Cheers!"
30687,"""ambiguous call to overloaded function"" error during compiling tensorflow on Windows ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): Visual Studio 2017/2019
- CUDA/cuDNN version:10.0/7 and 10.1/7
- GPU model and memory: Geforce GTX 1080, 8GB



**Describe the problem**
I get the following error during the compile:

.\tensorflow/stream_executor/kernel.h(534): error C2668: 'stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam': ambiguous call to overloaded function
.\tensorflow/stream_executor/kernel.h(557): note: could be 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<T>(stream_executor::KernelArgsArray<4> *,const T &,void *) const'
with
[
T=stream_executor::DeviceMemory<tensorflow::uint8>
]
.\tensorflow/stream_executor/kernel.h(532): note: or 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<T,>(stream_executor::KernelArgsArray<4> *,const T &) const'
with
[
T=stream_executor::DeviceMemory<tensorflow::uint8>
]
.\tensorflow/stream_executor/kernel.h(534): note: while trying to match the argument list '(stream_executor::KernelArgsArray<4> *, const T)'
with
[
T=stream_executor::DeviceMemory<tensorflow::uint8>
]
.\tensorflow/stream_executor/kernel.h(528): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>(stream_executor::KernelArgsArray<4> *,const T &,const tensorflow::uint8 &,const tensorflow::uint64 &,const stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled
with
[
T=stream_executor::DeviceMemory<tensorflow::uint8>
]
.\tensorflow/stream_executor/kernel.h(528): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>(stream_executor::KernelArgsArray<4> *,const T &,const tensorflow::uint8 &,const tensorflow::uint64 &,const stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled
with
[
T=stream_executor::DeviceMemory<tensorflow::uint8>
]
.\tensorflow/stream_executor/kernel.h(527): note: while compiling class template member function 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackParams(stream_executor::KernelArgsArray<4> *,stream_executor::DeviceMemory<tensorflow::uint8> &,tensorflow::uint8 &,tensorflow::uint64 &,stream_executor::DeviceMemory<tensorflow::uint64> &) const'
.\tensorflow/stream_executor/stream_executor_pimpl.h(882): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackParams(stream_executor::KernelArgsArray<4> *,stream_executor::DeviceMemory<tensorflow::uint8> &,tensorflow::uint8 &,tensorflow::uint64 &,stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled
tensorflow/stream_executor/cuda/redzone_allocator.cc(269): note: see reference to class template instantiation 'stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>' being compiled


The error is found at kernel.h file: (https://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/stream_executor/kernel.h

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I did everything as explained in the tensorflow website.


**Any other info / logs**
I get the error whether I am compiling with CUDA 10.0 or 10.1, vs 2017 or vs 2019."
30686,`tf.reduce_*` called on the result of `tf.TensorArray.concat` of unknown rank return corrupted Tensor,"**System information**
- TensorFlow version (use command below): 2.0
- Python version: 3

**Describe the current behavior**

See the reproducing code. When `tf.reduce_mean` is called on the output of a `TensorArray.concat` which had a fully dynamic shape (including rank), it produces a `Tensor` of scalar shape, but whose value is a 1-element vector.

**Describe the expected behavior**

The resulting `Tensor` should have a scalar value, consistent with its static shape.

**Code to reproduce the issue**

Note that the bug does not reproduce if the unknown-shape Tensor is passed directly to `reduce_mean`, which would suggest some interaction between the two.

```
import numpy as np

def unknown_shape_constant(val):
  # This is one way to create tensors of unknown shape.
  return tf.py_function(lambda: np.array(val), (), tf.int32)

@tf.function
def reduce_mean_bug():
  arr = tf.TensorArray(tf.int32, size=1, dynamic_size=True)
  arr = arr.write(0, unknown_shape_constant([[1], [3]]))
  c = arr.concat()
  m1 = tf.reduce_mean(c)

  arr = tf.TensorArray(tf.int32, size=1, dynamic_size=True)
  arr = arr.write(0, tf.constant([[1], [3]]))
  c = arr.concat()
  m2 = tf.reduce_mean(c)

  return m1, m2

m1, m2 = reduce_mean_bug()
assert m1.shape == m2.shape  # Fails
```
"
30685,`TensorArray` objects used as `Dataset.reduce` state lose inferred shapes,"**System information**
- TensorFlow version (use command below): 2.0
- Python version: 3

**Describe the current behavior**

`TensorArray` objects passed as accumulators to `Dataset.reduce` lose inferred shapes. Subsequent calls to `TensorArray.concat` returns a fully unknown shape.

**Describe the expected behavior**

The element shape of the `TensorArray` should be partially known, consistent with the behavior of an equivalent `tf.while_loop`.

**Code to reproduce the issue**
```
@tf.function
def compute():
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    def body(i, arr):
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i += 1
        return i, arr
    def cond(i, arr):
      return i < 10
    _, arr = tf.while_loop(cond, body, (0, arr))

    c = arr.concat()
    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
    return c

@tf.function
def compute_ds():
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    def body(state, _):
        i, arr = state
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i += 1
        return i, arr
    en_ds = tf.data.Dataset.range(10).enumerate()
    _, arr = en_ds.reduce((0, arr), body)

    c = arr.concat()
    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
    return c

print('*** With tf.while_loop')
_ = compute()
print()
print('*** With tf.Dataset.reduce')
_ = compute_ds()
```
```
*** With tf.while_loop
TensortArray.concat() shape: TensorShape([None, 1]) rank: 2

*** With tf.Dataset.reduce
TensortArray.concat() shape: TensorShape(None) rank: None
```"
30684,Compiling libtensorflow-core.a using NDK16b or later?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Snapdragon 820 , Android
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
1.13

- Python version:
3
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
4.9
- CUDA/cuDNN version:
NA
 GPU model and memory:
NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Compilation Error
Due 

e/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o
In file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:
/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory
 #include_next <wchar.h>



**Describe the expected behavior**

**Code to reproduce the issue**
Checkout r1.13 and export NDK_ROOT=/opt/google/r16b
**Other info / logs**
nuc2@nuc2-NUC7i5BNH:~/alok/tf-static-android/tensorflow$ export e/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o
In file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:
/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory
 #include_next <wchar.h>

"
30682,TFLite speech example failing to train with -output_representation='spec'or'mfcc' examples/lite/examples/speech_commands/ml,"My Environment 
Working sample Link : https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ml
Virtual environment : Anaconda Navigator
Editor : VS Code
Mode of execution : VS Code integrated Terminal with Conda envs
OS : Mac OSX

Tensorflow Version : 1.13.1 (as required in the samples requirement)

When i download the sample and run it as it is as per the ReadMe file, everything works perfectly fine. 
But when i try to change the ""-output_representation"" parameter value to 'spec' or 'mfcc' it doesn't work. I get the error `ValueError: total size of new array must be unchanged` in the `model.py line no : 59 x = Reshape([800, 20])(x)`. After a quick traceback i found that the spectrogram fingerprint size is taken as 257x98 for every second. So i change that line to `x = Reshape([257, 98])(x)` and it successfully passed through this line. 
But instead i get the following 
```
Traceback (most recent call last):
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1659, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 3 from 1 for 'conv1d_12/convolution/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,192], [1,3,192,256].
```
This happens in the line no 99: `x = _reduce_conv(x, 256, 3)`.

When i downloaded the ios example model and opened it in Netron i can clearly see that it uses audio spectrogram. 

What are all the changes that are to be done to train the model with 'spec', 'mfcc' and the 'mfcc_and_raw' ?"
30680,ReadTensorFromImageFile() C++ function too slow for large size images,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10
- TensorFlow installed from (source or binary):from source
- TensorFlow version (use command below):1.8.0
- Python version:3.5
-Microsoft Visual Studio 2017

**Describe the current behavior**
I am using tensoirflow object detection api in C++ , trained model with version 1.8.0, now i am running it on a laptop with no GPU and in Microsoft Visual studio 2017. The ReadTensorFromImageFile function takes alot of time and i would like it if it can work faster. i am loading 8K image and it takes around two seconds running through the ReadTensorFromImageFile() function. Is there a way to make it faster ??
**Describe the expected behavior**
I would like this function ReadTensorFromImageFile() to work faster for 8K size images.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30679,can not convert model to tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30678,i tried convert tf model to tflite but get this exception.Exception: Placeholder normalized_input_image_tensor should be specied by input_arrays.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30676,Exception: Placeholder normalized_input_image_tensor should be specied by input_arrays,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
30675,Hi guys.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
30674,Failed to build TFLite model benchmark tool on windows due to include 'dirent.h',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016 x64
- TensorFlow installed from (source or binary):source
- TensorFlow version:master 
- Bazel version (if compiling from source):0.25
- GCC/Compiler version (if compiling from source):MSVC 14
- CUDA/cuDNN version:not used

**Describe the problem**

Failed to build [TFLite Model Benchmark Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) with MSVC 14 on windows.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I prepared [the CI environment with Azure DevOps](https://dev.azure.com/mlops/tflite/) to reproduce this error.
I utilized the following script to build TFLite model benchmark tool on windows.

```yaml
variables:
  BAZEL_VERSION: ""0.25.2""
  TF_REPOSITORY: https://github.com/tensorflow/tensorflow.git
  TF_NEED_JEMALLOC: 1
  TF_NEED_GCP: 0
  TF_NEED_HDFS: 0
  TF_NEED_AWS: 0
  TF_NEED_KAFKA: 0
  TF_ENABLE_XLA: 0
  TF_NEED_GDR: 0
  TF_NEED_VERBS: 0
  TF_NEED_OPENCL_SYCL: 0
  TF_NEED_OPENCL: 0
  TF_NEED_CUDA: 0
  TF_CUDA_CLANG: 0
  TF_NEED_ROCM: 0
  TF_NEED_MKL: 0
  TF_DOWNLOAD_MKL: 0
  TF_DOWNLOAD_CLANG: 0
  TF_NEED_MPI: 0
  TF_NEED_S3: 0
  TF_SET_ANDROID_WORKSPACE: 0
  TF_NEED_COMPUTECPP: 0
  TF_NEED_TENSORRT: 0
  TF_CONFIGURE_IOS: 0
  - job: build_tflite_win
    pool:
      vmImage: 'vs2017-win2016'
    timeoutInMinutes: 360
    variables:
      BAZEL_SH: ""C:\\tools\\msys64\\usr\\bin\\bash.exe""
      BAZEL_VC: ""C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC""
      CC_OPT_FLAGS: ""/arch:AVX /fp:fast""
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x' 
        addToPath: true   
    - powershell: |
        cinst msys2 --params ""/NoUpdate"" -y --no-progress
        cinst bazel -Version $env:BAZEL_VERSION -y --no-progress -i
        C:\tools\msys64\usr\bin\pacman -S --noconfirm patch unzip
        git clone --branch=$(Build.SourceBranchName) --no-progress --depth=1 $env:TF_REPOSITORY
        $env:PYTHON_BIN_PATH=(Get-Command python).Source
        $env:PYTHON_LIB_PATH=""$(python -c 'import site; print(site.getsitepackages()[0])')""
        python ./tensorflow/configure.py
      displayName: 'Install'
    - powershell: |
        cd ./tensorflow
        Set-Item Env:Path ""C:\\tools\\msys64\\usr\\bin\\;$Env:Path""
        bazel --output_base $(Build.BinariesDirectory) build -c opt --color=yes --verbose_failures 
 //tensorflow/lite/tools/benchmark:benchmark_model
      condition: succeededOrFailed()
      displayName: 'Build TFLite Model Benchmark Tool'
    - task: PublishPipelineArtifact@0
      inputs:
        artifactName: tflite_benchmark_win
        targetPath: $(Build.SourcesDirectory)/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/
      condition: succeeded()
      displayName: 'Publish TFLite Model Benchmark Tool'

```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

All build logs is [uploaded here](https://dev.azure.com/mlops/tflite/_build/results?buildId=147&view=logs&j=e30d995d-3917-5417-8fca-0e6fe874aac5).
This error seems to be caused by 'dirent.h' as follows.

```
  | Execution platform: @bazel_tools//platforms:host_platform |  
  | tensorflow/lite/tools/evaluation/utils.cc(18): fatal error C1083: Cannot open include file: 'dirent.h': No such file or directory |  
  | Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build |  
  | INFO: Elapsed time: 4.617s, Critical Path: 3.72s |  
  | INFO: 1 process: 1 local. |  
  | FAILED: Build did NOT complete successfully
```

The POSIX header **'dirent.h' is not part of the C standard and is not portable between platforms.**
Would you like to modify the inclusion?"
30673,Support Conv3D in profiler for FLOPS counting,"**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
The [TensorFlow Profiler](https://www.tensorflow.org/api_docs/python/tf/profiler/profile) currently does not support counting operations / FLOPS of Conv3D layers. I request that we add this feature. This [nice readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#profile-model-float-operations) describes how to add this, if I am not mistaken.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Everyone that uses 3D convolutions and wants to profile the compute complexity of a model.

**Any Other info.**
Code to quickly see that Conv3D operations are indeed not supported:
```python
import tensorflow as tf

with tf.Graph().as_default() as graph:
    inputs = tf.ones(shape=(1, 5, 32, 32, 3))
    tf.layers.conv3d(
        inputs,
        filters=6,
        kernel_size=(3, 3, 3),
        strides=(1, 1, 1),
        padding=""valid"",
        use_bias=True,
    )

profiling_options = tf.profiler.ProfileOptionBuilder.float_operation()
tf_profile = tf.profiler.profile(graph, options=profiling_options)
```
output
```
Profile:
node name | # float_ops
_TFProfRoot (--/17.17k flops)
  conv3d/BiasAdd (16.20k/16.20k flops)
  conv3d/kernel/Initializer/random_uniform (486/973 flops)
    conv3d/kernel/Initializer/random_uniform/mul (486/486 flops)
    conv3d/kernel/Initializer/random_uniform/sub (1/1 flops)
```"
30672,Backpropagate through tf.data,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Gradients are not propagated through tf.data

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This code snippet returns [None]:

import tensorflow as tf
_ = tf.layers.conv1d(tf.ones((1,1,2)), 1, 1, use_bias=False, name='name')
data = tf.data.Dataset.from_tensors(tf.ones((10,2))).repeat().batch(4)\
    .map(lambda x: tf.layers.conv1d(x, 1, 1, use_bias=False, name='name', reuse=True))
i = data.make_initializable_iterator()
x = i.get_next()
g = tf.gradients(x, tf.trainable_variables())
print (g)

>>> [None]

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30670,libcuda.so.1: cannot open shared object file: no such file or directory,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow version: 1.13
- Python version: 3.7
- Installed using virtualenv? pip? conda?: Conda
- CUDA/cuDNN version: cudatoolkit version 10 and cudnn version 7.6 (according to conda list)
- GPU model and memory: 2080ti 11gb 🥇 


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I create a new conda environment and run conda install tensorflow-gpu. Next I start up a python terminal and import tensorflow as tf. My error message received is in next section.

**Any other info / logs**

> Traceback (most recent call last):
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py"", line 242, in load_module
>     return load_dynamic(name, filename, file)
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py"", line 342, in load_dynamic
>     return _load(spec)
> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py"", line 242, in load_module
>     return load_dynamic(name, filename, file)
>   File ""/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py"", line 342, in load_dynamic
>     return _load(spec)
> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
"
30666,tf.keras.layers.Conv2D does not initialize kernel and bias when called inside name_scope,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: Python 2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: No GPU

**Describe the current behavior**

It throws this
FailedPreconditionError: Error while reading resource variable name9/conv_linear/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/gcnn2d_d1/conv_linear/bias)

when trying to run the graph with a feed dict to get variable value


**Describe the expected behavior**

Expect to see a normal run since the variables are initialized inside the Conv2D function

**Code to reproduce the issue**

graph = tf.Graph()
with graph.name_scope('name9'):
  with graph.as_default():
    sign_in = tf.placeholder(tf.float32,(data_shape[0],data_shape[1],data_shape[2],data_shape[3]), name='signal_in')

    conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137)  )(sign_in)

data_tensor = np.random.rand(10,40,2,1)
feed_dict = {
  graph.get_tensor_by_name('signal_in:0'):data_tensor
}

op_value = session.run('/name9/conv_linear:0', feed_dict=feed_dict)  
 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30665,Non-OK-status: CudaLaunchKernel Internal: invalid configuration argument,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2, V9.2.148
- GPU model and memory: two gpus, GTX 1080 Ti, each 11178MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to run the follow sample code(keras tensorflow backend) my my GPU

```
left = Input(shape=(128, 3072), dtype='float32', name='Input-Left')
right = Input(shape=(128, 3072), dtype='float32', name='Input-Right')
lstm = Bidirectional(LSTM(units=768,
                          activation='tanh'),
                      name='Bidirectional-LSTM')
l_lstm = lstm(left)
r_lstm = lstm(right)
subtracted = Subtract(name='Subtract')([l_lstm, r_lstm])
abs_subtracted = Lambda(function=backend.abs)(subtracted)
mul = Multiply(name='multiplication')([l_lstm, r_lstm])
concat = concatenate([abs_subtracted, mul])
output = Dense(units=1)(concat)
model = Model(inputs=[left, right],
              outputs=output)
model = multi_gpu_model(model, gpus=2)
model.compile(loss='mean_squared_error',
              optimizer='Adam',
              metrics=['acc'])
import numpy as np
x1 = np.random.rand(100, 128, 3072)
x2 = np.random.rand(100, 128, 3072)
y = np.random.rand(100)
model.fit(x = [x1, x2],
         y=y,
         epochs=10)
```

**Describe the expected behavior**

expected it run normally without error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

When I ran the code on my GPUs, I got the following error, However, if I uninstall tensorflow-gpu, there was no error, but I can only use cpu.

```
Using TensorFlow backend.
2019-07-12 15:38:28,543 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

2019-07-12 15:38:29,759 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

2019-07-12 15:38:29,837 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

2019-07-12 15:38:29,840 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-07-12 15:38:29.856975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-12 15:38:29.860863: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-12 15:38:29.974060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.976417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.977100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x97d2a40 executing computations on platform CUDA. Devices:
2019-07-12 15:38:29.977112: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-07-12 15:38:29.977117: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-07-12 15:38:29.978574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696330000 Hz
2019-07-12 15:38:29.979175: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x984ed80 executing computations on platform Host. Devices:
2019-07-12 15:38:29.979185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-12 15:38:29.980287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.980881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
2019-07-12 15:38:29.980907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.981642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:02:00.0
2019-07-12 15:38:29.981842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-12 15:38:29.982531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-12 15:38:29.983173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-12 15:38:29.983363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-12 15:38:29.984149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-12 15:38:29.984728: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-12 15:38:29.985920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-12 15:38:29.985961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.986728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.987487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.988224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:29.988948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-07-12 15:38:33.252621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 15:38:33.252666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-07-12 15:38:33.252672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y 
2019-07-12 15:38:33.252677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N 
2019-07-12 15:38:33.253003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:33.253769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:33.254223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:33.254958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10054 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-07-12 15:38:33.255392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-12 15:38:33.255840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10054 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-07-12 15:38:33.714112: F ./tensorflow/core/kernels/random_op_gpu.h:227] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument
Aborted (core dumped)
```"
30664,tf.data.Dataset.window encountered `AttributeError: '_VariantDataset' object has no attribute 'numpy'`,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0b1
- Python version: 2.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version:n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
ubuntu@ubuntu:/v# python
Python 2.7.15+ (default, Nov 27 2018, 23:36:35) 
[GCC 7.3.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.version.VERSION
'2.0.0-beta1'
>>> x = tf.data.Dataset.range(5)
>>> y = tf.data.Dataset.range(5).window(2)
>>> for i in x:
...   print(i.numpy())
... 
0
1
2
3
4
>>> for i in y:
...   print(i.numpy())
... 
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
AttributeError: '_VariantDataset' object has no attribute 'numpy'
>>> 
```
**Describe the expected behavior**

In the above example `for i in y:print(i.numpy())` should produce result without AttributeError.
```
for i in y:
...   print(i.numpy())
... 
0
1
2
3
4
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
tf.version.VERSION
x = tf.data.Dataset.range(5)
y = tf.data.Dataset.range(5).window(2)
for i in x:
  print(i.numpy())
for i in y:
  print(i.numpy())

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30659,PLEASE add cdist,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0b1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
calculating distance matrices efficiently with tensorflow is a huge pain involving reading tons of stack overflow threads and re-implementing the same stuff. the solutions on stack overflow only cover euclidean distances and give MxM matrices even if you want city-block distance and MxMxD tensors ... it is extremely frustrating to experiment with optimal transport theory with tensorflow when such an obvious thing as CDIST is missing. 

**Will this change the current api? How?**
Yes, it will be easy to calculate distance matrices

**Who will benefit with this feature?**
anyone doing molecular dynamics, optimal transport, wasserstein, shape matching, graph theory, etc etc

**Any Other info.**
PDIST is also potentially useful
https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html
https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html

Block-wise version would be IDEAL as huge distance matrices often overflow gpu memory"
30658,TPUEstimator function requires `train_batch_size` to be set when `use_tpu` is True,"The [TPUEstimator constructor](https://github.com/JayMody/estimator/blob/e794e634ba52f9e8b04971e74fe24b91857f2228/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py#L2590) requires `train_batch_size` to be set if `use_tpu` is True. 

In cases when I only want to use a TPU estimator to predict, that means I have to pass in an arbitrary value in for `train_batch_size`. Looking deeper into the code, I can't pinpoint why `train_batch_size` needs to be set when on a TPU, but I'm assuming it's required somewhere deeper in the code. 

It was very confusing for me, especially since the documentation is conflicting [pull request #37](https://github.com/tensorflow/estimator/pull/37#issue-297115525). 

Maybe an option should be added that if `train_batch_size` is not set, but one of the other two (eval and predict) are, then provide a warning and pass an arbitrary value for `train_batch_size`. Otherwise maybe be more clear with the documentation that `train_batch_size` must always be set when on a TPU, even if you are only predicting or evaluating."
30657,No TRTEngineOps after Conversion (Tiny Yolov3),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0 / 7.6.0
- GPU model and memory: Tesla V100 32GB

**Describe the current behavior**
I am trying to convert a Tiny Yolov3 frozen graph into a frozen graph with some operations replaced with TRTEngineOps so that they are run with TensorRT. My graph has many [nodes that are supported by TF-TRT](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops) yet none are simplified into a TRTEngineOp.

After conversion, this is the count of different `node.op` in the resulting frozen graph:
```
{
    ""Placeholder"": 1,
    ""Const"": 84,
    ""Mul"": 7,
    ""Transpose"": 26,
    ""Conv2D"": 13,
    ""FusedBatchNorm"": 11,
    ""LeakyRelu"": 11,
    ""MaxPool"": 6,
    ""ResizeNearestNeighbor"": 1,
    ""Identity"": 4,
    ""BiasAdd"": 2,
    ""ConcatV2"": 5,
    ""Reshape"": 2,
    ""SplitV"": 3,
    ""Sigmoid"": 6,
    ""Exp"": 2,
    ""Add"": 4,
    ""Sub"": 2
}
```

And, information regarding the conversion process:
```
graph_size(MB)(native_tf): 34.0
graph_size(MB)(trt): 34.0
num_nodes(native_tf): 300
num_nodes(tftrt_total): 190
num_nodes(trt_only): 0
time(s) (trt_conversion): 8.8426
```

**Describe the expected behavior**
I expect some groups of nodes / subgraphs to be converted into TRTEngineOps so that they can run in TensorRT. According to the [supported ops](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops), the following operators could / should have been somewhat simplified into TRTEngineOps: `Const`, `Mul`, `Conv2D`, `FusedBatchNorm`, `MaxPool`, `Identity`, `BiasAdd`, `ConcatV2`, `Reshape`, `Sigmoid`, `Exp`, `Add`, and `Sub`. In fact, this is nearly all the operators, save for `Placeholder`, `Transpose`, `LeakyRelu`, `ResizeNearestNeighbor`, and `SplitV`.

**Code to reproduce the issue**
(functionality used from [example here](https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/object_detection))
```
import tensorflow as tf
import time

from tensorflow.python.compiler.tensorrt import trt_convert as trt

def optimize_model(frozen_graph,
                   use_trt=True,
                   force_nms_cpu=True,
                   replace_relu6=True,
                   remove_assert=True,
                   is_dynamic_op=True,
                   precision_mode='FP32',
                   max_batch_size=1,
                   minimum_segment_size=1,
                   max_workspace_size_bytes=2 << 32,
                   maximum_cached_engines=100,
                   calib_images_dir=None,
                   num_calib_images=None,
                   calib_batch_size=1,
                   calib_image_shape=None,
                   output_path=None):
    # apply graph modifications
    if force_nms_cpu:
        frozen_graph = f_force_nms_cpu(frozen_graph)
    if replace_relu6:
        frozen_graph = f_replace_relu6(frozen_graph)
    if remove_assert:
        frozen_graph = f_remove_assert(frozen_graph)
    # get input names
    output_names = ['inputs', 'output_boxes']

    # optionally perform TensorRT optimization
    if use_trt:
        graph_size = len(frozen_graph.SerializeToString())
        num_nodes = len(frozen_graph.node)
        start_time = time.time()

        converter = trt.TrtGraphConverter(
            input_graph_def=frozen_graph,
            nodes_blacklist=output_names,
            max_workspace_size_bytes=max_workspace_size_bytes,
            precision_mode=precision_mode,
            minimum_segment_size=minimum_segment_size,
            is_dynamic_op=is_dynamic_op,
            maximum_cached_engines=maximum_cached_engines,
            max_batch_size=max_batch_size)
        frozen_graph = converter.convert()

        end_time = time.time()
        print(""graph_size(MB)(native_tf): %.1f"" % (float(graph_size)/(1<<20)))
        print(""graph_size(MB)(trt): %.1f"" %
            (float(len(frozen_graph.SerializeToString()))/(1<<20)))
        print(""num_nodes(native_tf): %d"" % num_nodes)
        print(""num_nodes(tftrt_total): %d"" % len(frozen_graph.node))
        print(""num_nodes(trt_only): %d"" % len([1 for n in frozen_graph.node if str(n.op)=='TRTEngineOp']))
        print(""time(s) (trt_conversion): %.4f"" % (end_time - start_time))

    return frozen_graph

input_graph_path = '/path/to/frozen_graph.pb'

with tf.io.gfile.GFile(input_graph_path, 'rb') as f:
    orig_graph_def = tf.compat.v1.GraphDef()
    orig_graph_def.ParseFromString(f.read())

frozen_graph_def = optimize_model(orig_graph_def,
                                  precision_mode=PRECISION,
                                  max_batch_size=64,
                                  is_dynamic_op=False)

for node in frozen_graph_def.node:
    print(node.op, node.name)
```

**Other info / logs**
Click [here](https://drive.google.com/file/d/1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v/view?usp=sharing) for the graph in question.

Additionally, regardless of which precision of `FP32`, `FP16`, and `INT8` I use for precision, the size of the resulting models is `exactly` the same number of bytes. I observe no inference speed-up either.
"
30655,About Quantization Full Integer: Test with Yolo Tiny v2,"Hello, 

We tested the integer quantization in the scenarios of the table below, but we did not succeed in testing with Yolo Tiny v2, we would like to know what kind of optimization there is in mobilenet, inception and resnet so that we can apply in Yolo so that I can quantize and obtain a smaller inference time for the quantized model.

![image](https://user-images.githubusercontent.com/8125226/61136270-b11e1100-a499-11e9-8d52-efb148f6ab10.png)


"
30653,tf.data.Dataset.map() ignores eager execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.5 (18F132)
- TensorFlow installed from (source or binary): tensorflow==2.0.0b1 from https://pypi.org/
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

By default, eager execution should be enabled in TF 2.0; so each tensor's value can be accessed by calling `.numpy()`.

When a function relying on accessing a tensor's value is passed as a parameter to `tf.data.Dataset.map()`, it seems that internally the tensor is no longer an `EagerTensor` and accessing its value fails.

**Describe the expected behavior**

I'm not sure whether this behavior is intended or not but either way it should be document and/or fixed.

Purpose: text-based operations on `tf.data.Dataset`, such as custom splitting, filtering, etc. ideally without the need to write a custom operation.

**Code to reproduce the issue**

MWE to reproduce the behavior in colab:

```python
!pip install tensorflow-gpu==2.0.0b1
!pip install tensorflow-datasets==1.0.2

import tensorflow as tf


# transform a string tensor to upper case
def upper_case_fn(t: tf.Tensor) -> tf.Tensor:
    print(type(t))
    return tf.constant(t.numpy().decode('utf-8').upper())


# the same with tf op
def upper_case_tf(t: tf.Tensor) -> tf.Tensor:
    print(type(t))
    return tf.strings.upper(t)


# sanity check
c1 = upper_case_fn(tf.constant('casing_fn'))
print(c1)

c2 = upper_case_tf(tf.constant('casing_tf'))
print(c2)

# dataset
d = tf.data.Dataset.from_tensor_slices([
    tf.constant('hello'),
    tf.constant('world')
])

# working with OP
d2 = d.map(lambda x: upper_case_tf(x))
for _ in d2:
    print(_)

# failing with Python fn
d2 = d.map(lambda x: upper_case_fn(x))
```


**Other info / logs**

```
Installing collected packages: tensorflow-datasets
Successfully installed tensorflow-datasets-1.0.2
<class 'tensorflow.python.framework.ops.EagerTensor'>
tf.Tensor(b'CASING_FN', shape=(), dtype=string)
<class 'tensorflow.python.framework.ops.EagerTensor'>
tf.Tensor(b'CASING_TF', shape=(), dtype=string)
<class 'tensorflow.python.framework.ops.Tensor'>
tf.Tensor(b'HELLO', shape=(), dtype=string)
tf.Tensor(b'WORLD', shape=(), dtype=string)
<class 'tensorflow.python.framework.ops.Tensor'>

---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

<ipython-input-1-44edb2261a09> in <module>()
     36 
     37 # failing with Python fn
---> 38 d2 = d.map(lambda x: upper_case_fn(x))

11 frames

<ipython-input-1-44edb2261a09> in upper_case_fn(t)
      8 def upper_case_fn(t: tf.Tensor) -> tf.Tensor:
      9     print(type(t))
---> 10     return tf.constant(t.numpy().decode('utf-8').upper())
     11 
     12 

AttributeError: 'Tensor' object has no attribute 'numpy'
```
"
30652,Failed to convert tensorflow frozen graph to pbtxt file,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.14
- Python version: 3.6
- Bazel version (if compiling from source): 4
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: No GPU used

**Describe the current behavior**
I want to extract pbtxt file given an input of tensorflow frozen inference graph. In order to do this I am using the below script :

```
import tensorflow as tf

#from google.protobuf import text_format
from tensorflow.python.platform import gfile

def converter(filename): 
  with gfile.FastGFile(filename,'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    tf.import_graph_def(graph_def, name='')
    tf.train.write_graph(graph_def, 'pbtxt/', 'protobuf.pbtxt', as_text=True)
    print(graph_def)
  return


#converter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')  # here you can write the name of the file to be converted
# and then a new file will be made in pbtxt directory.

converter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')
```

As an example, I am using ssd mobilenet architecture. Using the above code I get the output as pbtxt but I cannot use it. For reference see the image below

<img width=""1440"" alt=""Screenshot 2019-07-12 at 6 34 05 PM"" src=""https://user-images.githubusercontent.com/17012391/61133402-06c6dd80-a4db-11e9-81e8-e38e02d329c9.png"">

> RIGHT: Image of original pbtxt file of mobile-net architecture 

> LEFT: Image of pbtxt file obtained by using above script.

When I use The official pbtxt on the RIGHT I get correct results. But, I do not get any prediction when I use LEFT pbtxt which I generated using above script

I am using these predictions on open cv DNN module

`tensorflowNet = cv2.dnn.readNetFromTensorflow('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb', 'pbtxt/protobuf.pbtxt')`

How do I convert mobilenet frozen inference graph into proper pbtxt format so that I can get inference ?


**Describe the expected behavior**

It is expected that the above script produces pbtxt file similar to the one provided by google for its mobilenet ssd model.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

#from google.protobuf import text_format
from tensorflow.python.platform import gfile

def converter(filename): 
  with gfile.FastGFile(filename,'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    tf.import_graph_def(graph_def, name='')
    tf.train.write_graph(graph_def, 'pbtxt/', 'protobuf.pbtxt', as_text=True)
    print(graph_def)
  return



# and then a new file will be made in pbtxt directory.

converter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')
```

References: https://gist.github.com/Arafatk/c063bddb9b8d17a037695d748db4f592
"
30649,tf.distribute.MirroredStrategy incompatible with tf.estimator training when defining tf.train.Scaffold with saver ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**: 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0/7.1
- GPU model and memory: TitanXp 12G x 4

**Describe the current behavior**
When I use `tf.estimator` together with `tf.distribute.MirroredStrategy()` for single worker multiple GPUs training, I meet the following error if I try to define `tf.train.Scaffold` for `tf.estimator.EstimatorSpec()` to configure the saver parameters. Everything works fine JUST I remove the scafflold and the multiple gpu training for estimator is referred this [tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/distribute_strategy.ipynb#scrollTo=_098zB3vVhuV).
```
...
File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 126, in _require_cross_replica_or_default_context_extended
    raise RuntimeError(""Method requires being in cross-replica context, use ""
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()
```

**Code to reproduce the issue**
Here is my minimum snippet of code to reproduce this error.
```python
import tensorflow as tf 
from tensorflow.python.keras.applications import MobileNetV2

l = tf.keras.layers

def input_fn(): 
    dataset = tf.data.Dataset.from_tensor_slices({""feature"": tf.random_normal(shape=(1, 224, 224, 3), dtype=tf.float32),
                                                  ""label"": tf.random.uniform(shape=[1], minval=0, maxval=2, dtype=tf.int32)})
    dataset = dataset.repeat()
    dataset = dataset.batch(2)
    return dataset

def model_fn(features, labels, mode):
    input_tensor = features['feature']
    label = features['label']
    if mode == tf.estimator.ModeKeys.TRAIN:
        model = MobileNetV2(input_shape=(224, 224, 3), classes=2, weights=None)
        output = model(input_tensor)

        loss = tf.losses.sparse_softmax_cross_entropy(label, output)
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())
        # define scaffold
        saver = tf.train.Saver(
            sharded=True,
            keep_checkpoint_every_n_hours=1,
            save_relative_paths=True)
        tf.add_to_collection(tf.GraphKeys.SAVERS, saver)
        scaffold = tf.train.Scaffold(saver=saver)
        # remove scaffold this code could work
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, scaffold=scaffold)

# multiple gpu configuration for estimator    
devices = [""/device:GPU:0"", ""/device:GPU:1""]
strategy = tf.distribute.MirroredStrategy(devices=devices)

config = tf.estimator.RunConfig(model_dir=""test_multi_gpu"",)
                                train_distribute=strategy)
estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)
estimator.train(input_fn=input_fn, steps=1000)
```

**Other info / logs**
The total error log is here:
```
Traceback (most recent call last):
  File ""test_multi_gpus.py"", line 45, in <module>
    estimator.train(input_fn=input_fn, steps=1000)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1156, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1219, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1299, in _actual_train_model_distributed
    self.config))
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1555, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 693, in _call_for_each_replica
    fn, args, kwargs)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 195, in _call_for_each_replica
    coord.join(threads)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 911, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1146, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""test_multi_gpus.py"", line 34, in model_fn
    save_relative_paths=True)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 497, in _build_internal
    per_device = self._GroupByDevices(saveables)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 404, in _GroupByDevices
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 404, in <genexpr>
    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object.py"", line 52, in tensor
    return self._tensor() if callable(self._tensor) else self._tensor
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py"", line 1358, in tensor
    return strategy.extended.read_var(sync_on_read_variable)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 768, in read_var
    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py"", line 1424, in _get_cross_replica
    axis=None)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 832, in reduce
    return super(StrategyV1, self).reduce(reduce_op, value, axis)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 552, in reduce
    _require_cross_replica_or_default_context_extended(self._extended)
  File ""/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 126, in _require_cross_replica_or_default_context_extended
    raise RuntimeError(""Method requires being in cross-replica context, use ""
RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()
```
"
30647,Unresolved External Symbols Windows C++Tensorflow v1.14.0,"### System information
- Windows 10
- Built from source
- Tensorflow v1.14.0
- Bazel v0.25.2
- MSVC 14.16.27023
- CUDA 10.0 Cudnn 7.6.0
- GTX 1060

### Describe the problem
I have built tenosrflow c++ library using the following commands 

```bash
bazel build //tensorflow:tensorflow_cc.lib
```

```bash
bazel build //tensorflow:tensorflow_cc.dll
```

however, after including the necessary headers and building/compiling my c++ code, I got the following 20 unresolved externals. 

### Source code / logs
Included tensorflow headers: 
```
#include ""tensorflow/cc/ops/const_op.h""
#include ""tensorflow/cc/ops/image_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/str_util.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""

```
Missing external symbols:

```
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)"" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z) referenced in function ""public: __cdecl tensorflow::Input::Input(struct tensorflow::Input::Initializer const &)"" (??0Input@tensorflow@@QEAA@AEBUInitializer@01@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)"" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function ""public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)"" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::Scope::~Scope(void)"" (??1Scope@tensorflow@@QEAA@XZ) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)"" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: class tensorflow::Status __cdecl tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const "" (?ToGraphDef@Scope@tensorflow@@QEBA?AVStatus@2@PEAVGraphDef@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const "" (?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function ""public: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpName<char const *>(char const *)const "" (??$WithOpName@PEBD@Scope@tensorflow@@QEBA?AV01@PEBD@Z)
1>main.obj : error LNK2019: unresolved external symbol ""class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)"" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::DecodeBmp::DecodeBmp(class tensorflow::Scope const &,class tensorflow::Input)"" (??0DecodeBmp@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::DecodeGif::DecodeGif(class tensorflow::Scope const &,class tensorflow::Input)"" (??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::DecodeJpeg::DecodeJpeg(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodeJpeg::Attrs const &)"" (??0DecodeJpeg@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::DecodePng::DecodePng(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodePng::Attrs const &)"" (??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::ResizeBilinear::ResizeBilinear(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)"" (??0ResizeBilinear@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)"" (??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)"" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::Squeeze::Squeeze(class tensorflow::Scope const &,class tensorflow::Input)"" (??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::Cast::Cast(class tensorflow::Scope const &,class tensorflow::Input,enum tensorflow::DataType)"" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::Div::Div(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)"" (??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)"" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function ""class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)"" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::SessionOptions::SessionOptions(void)"" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function ""class tensorflow::Status __cdecl LoadGraph(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::Session,struct std::default_delete<class tensorflow::Session> > *)"" (?LoadGraph@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VSession@tensorflow@@U?$default_delete@VSession@tensorflow@@@std@@@4@@Z)
1>main.obj : error LNK2019: unresolved external symbol ""class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)"" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z) referenced in function ""class tensorflow::Status __cdecl LoadGraph(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::Session,struct std::default_delete<class tensorflow::Session> > *)"" (?LoadGraph@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VSession@tensorflow@@U?$default_delete@VSession@tensorflow@@@std@@@4@@Z)
1>C:\Sources\Projects\UDAE_cc_interface\x64\Debug\UDAE_cc_interface.exe : fatal error LNK1120: 20 unresolved externals
```

I'm using Microsoft Visual Studio and linked tensorflow_cc.lib library and included the headers .. 
What are the other libraries I should link against other than the generated tensorflow c++ library, protobuf, abseil and eigen? I tried to export these missing symbols but didn't work .. 
"
30646,"Filling shuffle buffer, this happens before the beginning of every epoch. Is there a way to avoid it?","I am following [this tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches), however I am using my own dataset stored as a csv file consisting 1265800 elements. My question is:
Before the beginning of every epoch, it shows `Filling up shuffle buffer (this may take a while)`. I think it means that it is shuffling the dataset before feeding it to the model for training. Is there a way to not to shuffle this before every epoch because it takes time before proceeding to the next epoch.
Also, this behaviour is not seen if I run the example model on Google Colab. I also read [this issue](https://github.com/tensorflow/tensorflow/issues/29957) but didn't help.
Thank you for your inputs."
30645,How to convert TFmodel to TFLite Model????,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Command Line
TensorFlow version: 1.14.0
Python version: 3.7.3
Installed using virtualenv? pip? conda?: conda
Bazel version (if compiling from source): 0.28.0
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version: NO
GPU model and memory:NO

I found different ways to convert Tensorflow model to Tensorflow Lite model
using Bazel
using Toco
using tflite_convert.py

All these method give error
I need a proper suggestion to convert TFModel to TFLiteModel
The error says 

**tflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file**

**ValueError: Invalid tensors 'input' were found.**

Thank you

"
30644,When trying to convert .pb to tflite ,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No
TensorFlow installed from (source or binary): Command Line
TensorFlow version: 1.14.0
Python version: 3.7.3
Installed using virtualenv? pip? conda?: conda
Bazel version (if compiling from source): 0.28.0
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version: NO
GPU model and memory: NO
/tensorflow/lite/python
I m using tflite_convert.py from this folder /tensorflow/lite/python.
I run the code in Command Line as:
python ./tflite_convert.py --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=Mul—output_array= final_result--inference_type=FLOAT --input_data_type=FLOAT

I get

usage: tflite_convert.py [-h] --output_file OUTPUT_FILE
(--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)
[--output_format {TFLITE,GRAPHVIZ_DOT}]
[--inference_type {FLOAT,QUANTIZED_UINT8}]
[--inference_input_type {FLOAT,QUANTIZED_UINT8}]
[--input_arrays INPUT_ARRAYS]
[--input_shapes INPUT_SHAPES]
[--output_arrays OUTPUT_ARRAYS]
[--saved_model_tag_set SAVED_MODEL_TAG_SET]
[--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
[--std_dev_values STD_DEV_VALUES]
[--mean_values MEAN_VALUES]
[--default_ranges_min DEFAULT_RANGES_MIN]
[--default_ranges_max DEFAULT_RANGES_MAX]
[--post_training_quantize]
[--drop_control_dependency]
[--reorder_across_fake_quant]
[--change_concat_input_ranges {TRUE,FALSE}]
[--allow_custom_ops] [--target_ops TARGET_OPS]
[--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]
[--dump_graphviz_video]
tflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file


plz, help."
30643,Convert TFModel to TFLite Model,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Command Line
- TensorFlow version: 1.14.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.28.0
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: NO
- GPU model and memory:NO

/tensorflow/lite/python
I m using tflite_convert.py from this folder /tensorflow/lite/python.
I run the code in Command Line as:
python ./tflite_convert.py --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=Mul—output_array= final_result--inference_type=FLOAT --input_data_type=FLOAT

I get

usage: tflite_convert.py [-h] --output_file OUTPUT_FILE
                         (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)
                         [--output_format {TFLITE,GRAPHVIZ_DOT}]
                         [--inference_type {FLOAT,QUANTIZED_UINT8}]
                         [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
                         [--input_arrays INPUT_ARRAYS]
                         [--input_shapes INPUT_SHAPES]
                         [--output_arrays OUTPUT_ARRAYS]
                         [--saved_model_tag_set SAVED_MODEL_TAG_SET]
                         [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                         [--std_dev_values STD_DEV_VALUES]
                         [--mean_values MEAN_VALUES]
                         [--default_ranges_min DEFAULT_RANGES_MIN]
                         [--default_ranges_max DEFAULT_RANGES_MAX]
                         [--post_training_quantize]
                         [--drop_control_dependency]
                         [--reorder_across_fake_quant]
                         [--change_concat_input_ranges {TRUE,FALSE}]
                         [--allow_custom_ops] [--target_ops TARGET_OPS]
                         [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]
                         [--dump_graphviz_video]
**tflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file**

I need help to solve this error
"
30642,‘scatter_nd_update’ doesn't work with string,"**System information**
I Reproduced this issue in newest [tensorflow official docker image](https://hub.docker.com/r/tensorflow/tensorflow).
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14.0
- Python version:2.7.15+

**Describe the current behavior**
In my model, I need to maintain an extremely long 2-D variable tensor，which has several columns and many rows, and its dtype is string. In every training step, I need to update only several individual rows of that tensor. [tf.scatter_nd_update](https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update) meets my requirements perfectly,
except that it doesn't work with string in fact. As a contrast,  [tf.scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) does work. Since the document doesn't mention that `ref` can't be string, I think it may be a bug.

**Describe the expected behavior**
I hope `tf.scatter_nd_update` support string `ref`,and I really need this feature in my project. So if it can't be fixed quickly, **any walk-around suggestions (include modify some source code) is also welcome.**  

**Code to reproduce the issue**
```
import tensorflow as tf
ref = tf.Variable([‘qq’,’ww’,’ee’,’rr’,’’,’’,’’,’’])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant(['aa', 'dd', 'cc', 'bb'])
update = tf.scatter_nd_update(ref, indices, updates)
with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    print(sess.run(update))
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ScatterNdUpdate' used by node ScatterNdUpdate (defined at <stdin>:1) with these attrs: [_class=[""loc:@Variable""], use_locking=true, Tindices=DT_INT32, T=DT_STRING]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]

	 [[ScatterNdUpdate]]
```
"
30641,No OpKernel was registered to support Op 'GatherV2' with these attrs.,"Hi there! Facing with this problem after training pytorch model, converting to pb file and runing on android with tensorflow 1.13.1. Can anybody guide me what I am doing wrong?
"
30640,DEBUG: /home/nd/.cache/bazel/_bazel_nd/905339168ec5fb0a3da01fdaab718117/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12 (trying to install)
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9/7 (trying to install) 
- GPU model and memory: Nvidia Quardo K4000

**Describe the problem**
While running tensorflow with backend i was getting problem ` Ignoring visible gpu device (device: 0, name: Quadro K4000, pci bus id: 0000:05:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.` So to make it compatible i followed tutorial from [here](https://medium.com/@mccann.matt/compiling-tensorflow-with-cuda-3-0-support-42d8fe0bf3b5). After running instruction `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` getting error which is mentioned in attached in image. Please help me to solve the problem. 
![Error](https://user-images.githubusercontent.com/30615882/61120986-c60b9c00-a4bb-11e9-85dd-1db2d53bc099.png)


"
30639,tf.while_loop with tf.keras.layers.LSTM broken,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): the july 12 p36 gpu 2.0 nightly preview
- Python version: 3.6
- CUDA/cuDNN version: 10/7
- GPU model and memory: 3 GeForce GTX w/8 GB 

**Describe the current behavior**

First, I want to mention that the LSTM not working with distributed strategies is already being looked into here: https://github.com/tensorflow/tensorflow/issues/29189 -- I wanted to highlight this as a separate issue, because it likely has a different source...

Basically, when dynamically decoding a sequence with an LSTM and tf.while_loop, the code breaks (see logs below for more detail). This does not happen with an RNN(LSTMCell) configuration, but the LSTM is the only CuDNN access point, aside from GRU (which also does not work in this configuration).

**Describe the expected behavior**

The code should use the optimized CuDNN LSTM implementation and behave as the RNN(LSTMCell) approach i.e. not fail.

**Code to reproduce the issue**
https://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py

**Other info / logs**

2019-07-12 11:37:10.386140: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-12 11:38:30.548248: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_49 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.
2019-07-12 11:38:37.853257: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_49 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.
2019-07-12 11:38:39.689929: W tensorflow/core/common_runtime/process_function_library_runtime.cc:672] Ignoring multi-device function optimization failure: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_77 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall:9 incompatible with expected variant.
2019-07-12 11:38:45.280991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-12 11:38:45.755520: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal
2019-07-12 11:38:45.755562: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal
	 [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]
	 [[If_9/else/_2424/gradients/while_grad/while_grad/body/_11561/gradients/TensorArrayV2Read/TensorListGetItem_grad/TensorListLength/TensorListPopBack/_1920]]
2019-07-12 11:38:45.755854: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal
	 [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]
[I 11:38:49.971 NotebookApp] Saving file at /SEQ3_LSTM_CUDA.ipynb
"
30638,CUDA10.1 with tensorflow-gpu==2.0.0b1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution:Linux manjaro (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip install tensorflow-gpu==2.0.0-b1
- TensorFlow version:2.0.0b1
- Python version:3.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:rtx2070



**Describe the problem**
tf.test.is_gpu_available():
I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2019-07-12 16:39:38.345749: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2019-07-12 16:39:38.345852: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2019-07-12 16:39:38.345953: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2019-07-12 16:39:38.346052: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2019-07-12 16:39:38.346153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
30636,[Mirrored Strategy] You dataset iterator ran out of data; interrupting training. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- TensorFlow installed from (source or binary):Pip tensorflow-gpu 2.0 beta1
- TensorFlow version (use command below):2.0 beta1
- Python version:3.7
- GPU model and memory:Titan RTX x 2 (2 x 24GB) / P100 x 2 (2 x 16GB)

Error in keras.Model.fit.
When using the mirroredstrategy with tensorflow dataset in both training and validation.
Single GPU card works fine, whether using the mirroredstertegy or not. (When using the mirroredstertegy, set the devices = /gpu:0). This problem only occurs when using multiple gpu cards.

The error displayed:
[training_arrays.py 325] Your dataset iterator ran out of data; interrupting training. Make sure that your iterator can geretate at least ""validation_steps * epochs"" batches.

Currently the only worked solutation for me is manully set the ""validation_steps"" in keras.Model.fit.

Tensorflow dataset repeat or/and take, will not work. By setting the validation batch size to 2 (1 for each GPU) also does not work

Simliar issues in here [https://github.com/tensorflow/tensorflow/issues/25254](https://github.com/tensorflow/tensorflow/issues/25254), but closed"
30635,Java version upgrade from 1.13.1 to 1.14.0 got error,"<em>Java version upgrade from 1.13.1 to 1.14.0 got error</em>

**System information**
- Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-1013-azure x86_64):
- TensorFlow 1.14.0:
- Java 1.8:

Log with 1.13.1(Worked):
 ``` .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.3.RELEASE)

2019-07-12 07:15:28.366  INFO 21194 --- [           main] com.springWeb.App                        : Starting App v0.0.1-SNAPSHOT on RaTceUbt2019 with PID 21194 (/home/RaTce2019/od/springWeb-0.0.1-SNAPSHOT.jar started by RaTce2019 in /home/RaTce2019/od)
2019-07-12 07:15:28.375  INFO 21194 --- [           main] com.springWeb.App                        : No active profile set, falling back to default profiles: default
2019-07-12 07:15:31.058  INFO 21194 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8765 (http)
2019-07-12 07:15:31.143  INFO 21194 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-07-12 07:15:31.143  INFO 21194 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.16]
2019-07-12 07:15:31.171  INFO 21194 --- [           main] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib]
2019-07-12 07:15:31.362  INFO 21194 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2019-07-12 07:15:31.362  INFO 21194 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2885 ms
2019-07-12 07:15:31.940  INFO 21194 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2019-07-12 07:15:32.424  INFO 21194 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8765 (http) with context path ''
2019-07-12 07:15:32.432  INFO 21194 --- [           main] com.springWeb.App                        : Started App in 4.883 seconds (JVM running for 5.79)
2019-07-12 07:15:36.232  INFO 21194 --- [nio-8765-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2019-07-12 07:15:36.233  INFO 21194 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2019-07-12 07:15:36.249  INFO 21194 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 16 ms
2019-07-12 07:15:38.465576: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: models/pepsi/ssd/saved_model
2019-07-12 07:15:38.574596: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-07-12 07:15:38.614832: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2019-07-12 07:15:38.621862: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-07-12 07:15:38.623574: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7fe1c435d720 executing computations on platform Host. Devices:
2019-07-12 07:15:38.623614: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-12 07:15:38.716123: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.
2019-07-12 07:15:38.716231: I tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: models/pepsi/ssd/saved_model/variables/variables.index
2019-07-12 07:15:38.716265: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 250701 microseconds.
```

Log with 1.14.0(Failed):
```
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.3.RELEASE)

2019-07-12 06:59:43.499  INFO 19308 --- [           main] com.springWeb.App                        : Starting App v0.0.1-SNAPSHOT on RaTceUbt2019 with PID 19308 (/home/RaTce2019/od/springWeb-0.0.1-SNAPSHOT.jar started by RaTce2019 in /home/RaTce2019/od)
2019-07-12 06:59:43.507  INFO 19308 --- [           main] com.springWeb.App                        : No active profile set, falling back to default profiles: default
2019-07-12 06:59:46.150  INFO 19308 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8765 (http)
2019-07-12 06:59:46.236  INFO 19308 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-07-12 06:59:46.237  INFO 19308 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.16]
2019-07-12 06:59:46.279  INFO 19308 --- [           main] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib]
2019-07-12 06:59:46.447  INFO 19308 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2019-07-12 06:59:46.447  INFO 19308 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2844 ms
2019-07-12 06:59:47.007  INFO 19308 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2019-07-12 06:59:47.481  INFO 19308 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8765 (http) with context path ''
2019-07-12 06:59:47.490  INFO 19308 --- [           main] com.springWeb.App                        : Started App in 4.798 seconds (JVM running for 5.729)
2019-07-12 07:00:05.710  INFO 19308 --- [nio-8765-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2019-07-12 07:00:05.710  INFO 19308 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2019-07-12 07:00:05.723  INFO 19308 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 13 ms
2019-07-12 07:00:08.082 ERROR 19308 --- [nio-8765-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler dispatch failed; nested exception is java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1562914806051-0/libtensorflow_jni.so: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory] with root cause

java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1562914806051-0/libtensorflow_jni.so: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory
	at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[na:1.8.0_212]
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[na:1.8.0_212]
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[na:1.8.0_212]
	at java.lang.Runtime.load0(Runtime.java:809) ~[na:1.8.0_212]
	at java.lang.System.load(System.java:1086) ~[na:1.8.0_212]
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101) ~[libtensorflow-1.14.0.jar!/:na]
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66) ~[libtensorflow-1.14.0.jar!/:na]
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70) ~[libtensorflow-1.14.0.jar!/:na]
	at org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170) ~[libtensorflow-1.14.0.jar!/:na]
	at com.springWeb.controller.Controller.detectImage(Controller.java:73) ~[classes!/:0.0.1-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_212]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_212]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_212]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_212]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:800) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:908) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:660) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:741) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:834) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1415) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_212]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_212]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]

2019-07-12 07:00:26.038 ERROR 19308 --- [nio-8765-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: Could not initialize class org.tensorflow.SavedModelBundle] with root cause

java.lang.NoClassDefFoundError: Could not initialize class org.tensorflow.SavedModelBundle
	at com.springWeb.controller.Controller.detectImage(Controller.java:73) ~[classes!/:0.0.1-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_212]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_212]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_212]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_212]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:189) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:800) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1038) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:908) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:660) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882) ~[spring-webmvc-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:741) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-5.1.5.RELEASE.jar!/:5.1.5.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200) ~[tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:834) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1415) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_212]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_212]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-9.0.16.jar!/:9.0.16]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]
```
This is my pom.xml:
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>

	<groupId>springWeb</groupId>
	<artifactId>springWeb</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>springWeb</name>
	<url>http://maven.apache.org</url>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<java.version>1.8</java.version>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
	</properties>

	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>2.1.3.RELEASE</version>
	</parent>

	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>tensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>proto</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.json</groupId>
			<artifactId>json</artifactId>
			<version>20180813</version>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
			</plugin>
		</plugins>
	</build>
</project>

`And what I did was changed version from 1.13.1 to 1.14.0.`

And I tried official Java start up got the same error:
**System information**
- macOS High Sierra Version 10.13.6 (17G6030):
- TensorFlow 1.14.0:
- Java 1.8:

pom.xml:
<project xmlns=""http://maven.apache.org/POM/4.0.0""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>

	<groupId>tftest</groupId>
	<artifactId>tftest</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>tftest</name>
	<url>http://maven.apache.org</url>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
	</properties>

	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>3.8.1</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>tensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>libtensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
	</dependencies>
</project>


App.java:
```package tftest.tftest;

import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;

public class App {
	public static void main(String[] args) throws Exception {
		try (Graph g = new Graph()) {
			final String value = ""Hello from "" + TensorFlow.version();
			// Construct the computation graph with a single operation, a constant
			// named ""MyConst"" with a value ""value"".
			try (Tensor t = Tensor.create(value.getBytes(""UTF-8""))) {
				// The Java API doesn't yet include convenience functions for adding operations.
				g.opBuilder(""Const"", ""MyConst"").setAttr(""dtype"", t.dataType()).setAttr(""value"", t).build();
			}
			// Execute the ""MyConst"" operation in a Session.
			try (Session s = new Session(g);
					// Generally, there may be multiple output tensors,
					// all of them must be closed to prevent resource leaks.
					Tensor output = s.runner().fetch(""MyConst"").run().get(0)) {
				System.out.println(new String(output.bytesValue(), ""UTF-8""));
			}
		}
	}
}
```

Log:
```
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib: dlopen(/private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib, 1): Library not loaded: @rpath/libtensorflow_framework.1.dylib
  Referenced from: /private/var/folders/0c/nz0823ps085b_1zcq6l8ry441kqw10/T/tensorflow_native_libraries-1562915587706-0/libtensorflow_jni.dylib
  Reason: image not found
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1938)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1821)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:479)
	at tftest.tftest.App.main(App.java:10)
```"
30634,[Documentation][TF 1.14] Missing documentation for `batch_dims` in `tf.gather`,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/gather

## Description of issue (what needs changing):

In TF 1.14, `tf.batch_gather` is marked as deprecated, and the keyword `batch_dims` has been added to `tf.gather` to handle the batch version. Though, the documentation of `tf.gather` has only been updated with the **type** of `batch_dims`, but not **how to use** it neither **what it does**.

The old [`tf.batch_gather`](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/batch_gather) function was documented in TF 1.13. Though, `tf.gather` is a bit more complex than the old `tf.batch_gather`, so maybe the [documentation of the underlying `_batch_gather`](https://github.com/tensorflow/tensorflow/blob/2aca283764bbbd54a5556319eb7cc0ed323c81f1/tensorflow/python/ops/array_ops.py#L3514) function could be used.

These two existing documentations could be used to complete the existing documentation of `tf.gather`.

### Correct links

Yes.

### Parameters defined

Yes

### Returns defined

Yes

### Raises listed and defined

Yes

### Usage example

Not for using `batch_dims`.

### Submit a pull request?

No.
"
30633,tf.sysconfig.get_link_flags() issue,"Env:
tensorflow 1.14.0
macos

tf.sysconfig.get_link_flags() returns
```
['-L/Users/kimmyzhang/anaconda2/lib/python2.7/site-packages/tensorflow',
 '-l:libtensorflow_framework.1.dylib']
```

""-l:libtensorflow_framework.1.dylib"" will cause a custom op compilation failure:
```
ld: library not found for -l:libtensorflow_framework.1.dylib
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

It should be -ltensorflow_framework."
30632,How do I create a custom OP using TF1.14?,"I am a TensorFlow beginner. The environment I use is ubuntu18.04, python3.7, TensorFlow-GPU1.14, CUDA10.0.

I want to build a custom OP (using GPU), I saw from the official website tutorial that you need to include #include ""tensorflow/core/util/cuda_kernel_helper.h"" in the source code.
But when I was ""make"", I got an error: ""cuda_kernel_helper.h"" was not found.

In addition, I used the python environment built by anaconda. I found gpu_kernel_helper.h in the folder in the environment. I guess this is a replacement file for cuda_kernel_helper.h. When I replace the file name in my source code, another one appears. Error: ""third_party/gpus/cuda/include/cuda_fp16.h"" file not found.
After that, I only found the ""eigen3"" folder in the “third_party” folder and did not find the ""gpus"" folder.
I installed TensorFlow-gpu using pip. Is there a problem with my installation method?

I hope to get help to solve this problem. thank you all."
30629,unable to build pip package from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.2 commit
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**
```
 File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 28, in <module>
    from tensorflow.python.framework import function
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 36, in <module>
    from tensorflow.python.ops import resource_variable_ops
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 39, in <module>
    from tensorflow.python.ops import variables
  File ""/home/p.patel/.cache/bazel/_bazel_root/1bd8cf0b6a86ac78985c768eb3530621/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/variables.py"", line 133, in <module>
    ""* `ONLY_FIRST_TOWER`: Deprecated alias for `ONLY_FIRST_REPLICA`.\n  "")
AttributeError: attribute '__doc__' of 'type' objects is not writable
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30623,compile_nsync.sh does not exist,"compile_nsync.sh does not exist
yet again I give it up
FUCK GOOGLE
FUCK TENSORFLOW"
30622,errorrr,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory:2344



**Describe the problem**
import tensorflow as tf
Traceback (most recent call last):
File ""C:\Users\heyx\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\heyx\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\heyx\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\heyx\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\heyx\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30621,RPI3 - g++-4.8 does'nt exist on debian buster (june 2019),"no way I am gonna fill this
guys, update your doc please ;-)"
30620,Feature: Add safe division in reciprocal operation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14 / 2.0 alpha
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Hello, 
I wanted to add a feature to the `tf.math.reciprocal` operation to optionally perform ""safe"" reciprocal. If the input is zero, it returns zero instead of NaN. I found this feature useful when implementing one of the recent [PR](https://github.com/tensorflow/tensorflow/pull/30152/files#diff-614af081276f42dbb98e5d731d7a1a86R158). This is similar to the [`tf.math.div_no_nan`](https://github.com/tensorflow/tensorflow/pull/21621) operation/PR. 

Should this feature request be approved, I can either add an optional argument to the current `tf.math.reciprocal()` operation which indicates whether to perform safe division. The default for this would be False, thus retaining the current interface. OR I can create a new tf.math operation called `tf.math.reciprocal_no_nan()` similar to the safe division op. Would love inputs on this!

**Will this change the current api? How?**
Depending on the two options laid out above, the first option would change the current API slightly, while the second option adds a new operation. 

**Who will benefit with this feature?**
Any researcher designing algorithms involving reciprocals as well as engineers/developers who would otherwise write wrapper functions to perform safe reciprocals. 

**Any Other info.**
"
30619,TF-TRT example does not convert a graph with INT8 precision,"System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cudnn version: 10.0/7.4.1
- GPU model and memory: Titan XP

I simply followed an example in the following directory to generate a resnet (resnet_v1_50) model quantized with INT8

- tensorflow/tensorrt/tftrt/examples/image-classification/image_classification.py

I cannot find any particular error messages in the log. Here is the summary of the graphs.

- num_nodes(native_tf): 741
- num_nodes(tftrt_total): 474
- num_nodes(trt_only): 0
- graph_size(MB)(calib): 97.8
- graph_size(MB)(native_tf): 97.8
- graph_size(MB)(trt): 97.8
- time(s)(saving_frozen_graph): 0.5
- time(s)(trt_calibration): 7.9
- time(s)(trt_conversion): 1.6

The number of nodes in tftrt_total is reduced from native_tf, but cannot see any node of trt_only.
I expected TRTEngineOp in the graph but I cannot see it when I read the cached INT8 graph. Most of data types are DL_FLOAT, not  DL_INT8. 

Based on this observation, I guess the cached graph is not a graph quantized with INT8. 
Any clue why it does not generate a quantized graph? I repeated this for different models such as vgg_16 and inception_v3, but I did not have a luck.

"
30617,Save and Load keras model in multi-gpu environment changing the input and output tensor names,"API Used:
Kerras==2.2.4
Tensorflow=1.13.1

Problem Statement:
After I load my saved keras model for retraining, it changes the input and output tensor names.

Code to replicate:
```
from keras.applications import MobileNet
from keras.layers import Input
from keras.utils import multi_gpu_model
from keras.models import model_from_json, load_model

input_tensor = Input(shape=(224, 224, 3), name=""input"")
pre_trained_model = MobileNet(input_shape=(224,224,3),
                                       alpha=1,
                                       depth_multiplier=1,
                                       dropout=1e-3,
                                       include_top=True,
                                       weights='imagenet',
                                       input_tensor=input_tensor,
                                       classes=1000)
last_layer = pre_trained_model.layers[-1].output
output_tensor = Dense(10, activation='softmax', name='output')(last_layer)
model = Model(inputs=input_tensor, outputs=output_tensor)
model = multi_gpu_model(model, gpus=4)
model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics='accuracy',
        )

---- Training ----

architecture_file=""model_arch.json""
weight_file=""model_weights.h5""
model.save_weights(weight_file)
with open(architecture_file, 'w') as f:
     f.write(model.to_json())

```
After the first training, when I froze the model and loaded into the testing environment, I could feed to ```input``` node and get output from ```output```


Then reload and train again
```
 with open(architecture_file, 'r') as f:
      model = model_from_json(f.read())
      model.load_weights(weight_file, by_name=True)

model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics='accuracy',
        )

---- training ----
architecture_file=""model_arch.json""
weight_file=""model_weights.h5""
model.save_weights(weight_file)
with open(architecture_file, 'w') as f:
     f.write(model.to_json())
````

After the second training from the saved model, when I froze the model and loaded into the testing environment,
I could feed to input to ```input``` node the input node has changed to ```input_1``` "
30616,"[Documentation] In TF 1.13.1, tf.keras.experimental.export does not exist despite being documented","
## URL(s) with the issue:

https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/keras/experimental/export

## Description of issue (what needs changing):

Ok, so let's say for some arbitrary reason out of your control (*cough* sagemaker *cough*) you are pegged to TensorFlow <= 1.13.1. The cool new `tf.keras.experimental.export` feature looks a _lot_ easier than building all that stuff yourself, so you go to try and use it.

Unfortunately, you get hit with the following error:
```
Traceback (most recent call last):
  File ""documented_example.py"", line 10, in <module>
    saved_to_path = tf.keras.experimental.export(
AttributeError: 'module' object has no attribute 'export'
```

### Correct links

It is not -- the part that is `Defined in tensorflow/python/keras/saving/saved_model.py.` links to `https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/saving/saved_model.py` which gets a 404!

### Parameters defined

Probably.

### Returns defined

Very possible

### Raises listed and defined

Yep.

### Usage example

Yes! In fact, it doesn't work.

For posterity here's that usage example copy-pasted into a Github gist: https://gist.github.com/zmjjmz/fcc73dad9f49d34f9c047c108fdd0e3f

### Submit a pull request?

Nope."
30612,Failed to load the native TensorFlow runtime.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):pip install tensorflow

- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:no gpu 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\python.exe"" ""G:/Data Camp/keras3.6/test.py""
Traceback (most recent call last):
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""G:/Data Camp/keras3.6/test.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\keras3.6\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1
"
30610,[r1.13.1] ImportError for  tf.keras.applications.imagenet_utils.preprocess_input,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS (Mojave) 10.14.4
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.7

- CUDA/cuDNN version: No GPU
- GPU model and memory: No GPU

**Describe the current behavior**
None of the following imports work are working on the current version:
```{python}
from tensorflow.keras.applications import imagenet_utils
from tensorflow.keras.applications.imagenet_utils import preprocess_input

from tensorflow.keras.applications import preprocess_input
```

**Describe the expected behavior**
The aforementioned imports should work. I reviewed the source code for branch r1.13 examining file `tensorflow/python/keras/applications/__init__.py`

https://github.com/tensorflow/tensorflow/blob/2aca283764bbbd54a5556319eb7cc0ed323c81f1/tensorflow/python/keras/applications/__init__.py#L74-L86

`preprocess_input` appears to be missing. 

**Code to reproduce the issue**
Provided above.
"
30609,(more) spurious deprecation warnings,"Similar to #27897

**System information**
- OS Platform and Distribution
 Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic
- TensorFlow installed from:
pip install tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.8

(This is happening in colab.sandbox.google.com)

**Describe the current behavior**

When using new APIs that replaced old APIs, you deprecation warnings as if you were still using the old API.

**Describe the expected behavior**

If I use the new APIs, I should not get deprecation warnings.

**Code to reproduce the issue**


```
from __future__ import absolute_import, division, print_function, unicode_literals
import functools

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds


TRAIN_DATA_URL = ""https://storage.googleapis.com/tf-datasets/titanic/train.csv""

train_file_path = tf.keras.utils.get_file(""train.csv"", TRAIN_DATA_URL)
LABEL_COLUMN = 'survived'
LABELS = [0, 1]

def get_dataset(file_path):
  dataset = tf.data.experimental.make_csv_dataset(
      file_path,
      batch_size=12, # Artificially small to make examples easier to show.
      label_name=LABEL_COLUMN,
      na_value=""?"",
      num_epochs=1,
      ignore_errors=True)
  return dataset

raw_train_data = get_dataset(train_file_path)

""""""
-------OUTPUT------------
WARNING: Logging before flag parsing goes to stderr.
W0711 17:34:31.453707 140627566475136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py:498: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
------END OUTPUT-------
""""""

CATEGORIES = {
    'sex': ['male', 'female'],
    'class' : ['First', 'Second', 'Third'],
    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],
    'alone' : ['y', 'n']
}

categorical_columns = []
for feature, vocab in CATEGORIES.items():
  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(
        key=feature, vocabulary_list=vocab)
  categorical_columns.append(tf.feature_column.indicator_column(cat_col))

MEANS = {
    'age' : 29.631308,
    'n_siblings_spouses' : 0.545455,
    'parch' : 0.379585,
    'fare' : 34.385399
}

def process_continuous_data(mean, data):
  # Normalize data
  data = tf.cast(data, tf.float32) * 1/(2*mean)
  return tf.reshape(data, [-1, 1])

numerical_columns = []

for feature in MEANS.keys():
  num_col = tf.feature_column.numeric_column(feature, normalizer_fn=functools.partial(process_continuous_data, MEANS[feature]))
  numerical_columns.append(num_col)

preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns)


def get_model(hidden_units=[100, 100]):

  model = tf.keras.Sequential([preprocessing_layer])
  for units in hidden_units:
    model.add(tf.keras.layers.Dense(units, activation='relu'))
 
  return model


train_data = raw_train_data.shuffle(500)

model = get_model()
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy'])

model.fit(train_data, epochs=20)

""""""
-------OUTPUT------

Epoch 1/20
W0711 17:34:32.313002 140627566475136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0711 17:34:32.347570 140627566475136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4215: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
W0711 17:34:32.350716 140627566475136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
""""""
```
"
30606,Additional command line arguments needed for tflite conversion of a very simple model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Docker image tensorflow/tensorflow:latest-gpu
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15+
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti

I am trying to deploy a simple, custom fully-quantized model with Coral Edge TPU support. When trying to convert the frozen graph into a tflite model, I run into errors, unless I provide more command line arguments. Considering how simple the model is (e.g. using very basic layers), I did not expect to run into problems.

**Code to reproduce the issue**

I create and train a model that fits the function f(x) = 2 x - 1 with the following code:

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

print(tf.__version__)

train_input = np.array([ -1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
train_truth = np.array([ -3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)

def build_keras_model():
	return keras.models.Sequential([
		keras.layers.Dense(units=1, input_shape=[1]),
	])

### train the model
train_graph = tf.Graph()
train_sess = tf.Session(graph=train_graph)

keras.backend.set_session(train_sess)

with train_graph.as_default():
	keras.backend.set_learning_phase(1)
	train_model = build_keras_model()

	tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)
	train_sess.run(tf.global_variables_initializer())

	train_model.compile(
		optimizer='sgd',
		loss='mean_squared_error'
	)
	train_model.fit(train_input, train_truth, epochs=250)

	saver = tf.train.Saver()
	saver.save(train_sess, 'linear.ckpt')
```

Afterwards, I create an eval graph and freeze the model with the help of this script:

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

def build_keras_model():
	return keras.models.Sequential([
		keras.layers.Dense(units=1, input_shape=[1]),
	])

# eval
eval_graph = tf.Graph()
eval_sess = tf.Session(graph=eval_graph)

keras.backend.set_session(eval_sess)

with eval_graph.as_default():
	keras.backend.set_learning_phase(0)
	eval_model = build_keras_model()

	tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)

	eval_graph_def = eval_graph.as_graph_def()
	saver = tf.train.Saver()
	saver.restore(eval_sess, 'linear.ckpt')

	frozen_graph_def = tf.graph_util.convert_variables_to_constants(
		eval_sess,
		eval_graph_def,
		[eval_model.output.op.name]
	)

	with open('frozen_model.pb', 'wb') as f:
		f.write(frozen_graph_def.SerializeToString())
```

When I try to convert the frozen graph to a tflite model, using the following command

```
tflite_convert \
--output_file=model.tflite \
--graph_def_file=frozen_model.pb \
--inference_type=QUANTIZED_UINT8 \
--input_arrays=dense_input \
--output_arrays=dense/BiasAdd \
--mean_values=0 \
--std_dev_values=255
```
I run into an error:

```
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py"", line 898, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-07-11 15:54:26.995204: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7 operators, 12 arrays (0 quantized)
2019-07-11 15:54:26.995314: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7 operators, 12 arrays (0 quantized)
2019-07-11 15:54:26.995465: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (1 quantized)
2019-07-11 15:54:26.995497: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 2 operators, 5 arrays (1 quantized)
2019-07-11 15:54:26.995512: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 5 arrays (1 quantized)
2019-07-11 15:54:26.995528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 2 operators, 5 arrays (1 quantized)
2019-07-11 15:54:26.995558: F tensorflow/lite/toco/graph_transformations/quantize.cc:149] Array dense/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.
Aborted (core dumped)
```

This error vanishes, when I provide two additional command line arguments with any value:
`--default_ranges_min=-128 --default_ranges_max=127`

First and foremost, shouldn't a simple model like this be convertible without these args? Also, I am still only guessing on how to determine the default ranges as well as the values for mean_values and std_dev_values. This should be added to the documentation for clarity."
30605,Share tf.distribute.strategy scopes across multiple sets of GPUs for multithreaded data gather and training,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): 1.14/2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
In some RL models, an agent will gather data in one thread/process while an update loop in a second thread calculates the gradients/updates the model. Often for model-based RL, the time spent performing forward passes through the model can be large, so this has many advantages if it can be a non-blocking procedure in parallel with the update calls. It would be useful to be able to use the new tf.distribute strategies to share weights across multiple *sets* of GPUs (for instance, one GPU performs continual forward passes through the model while the remaining 7 on the node perform the updates on the model) across threads/processes. In the current parameter server model, I can simply pass the tf.Server configuration and build the graph in multiple workers, sharing the same set of variables on the parameter server.

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
30604,Calling .map on a tf.data.Dataset causes unrelated imports to fail,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-6008-gd883916ee4 2.0.0-dev20190711 
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

When `.map` is called on a `tf.data.Dataset`, some available imports are getting lost. I've stumbled upon this being unable to import `tf.summary.create_file_writer` after mapping a function onto a dataset, even though I can import it in ipython. Several other methods of `tf.summary` are lost as well. I didn't check whether any other tensorflow models apart form `summary` loose imports.

**Describe the expected behavior**

Calling `.map` on a `tf.data.Dataset` should leave available imports intact.

**Code to reproduce the issue**
```
import tensorflow as tf

print(tf.summary.create_file_writer)

def map_fun(x):
    return x

ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])

ds = ds.map(map_fun)

print(tf.summary.create_file_writer)
```

**Other info / logs**
Output of the above code snippet:
```
<function create_file_writer_v2 at 0x13180d9d8>
2019-07-11 17:12:16.193865: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA                                                  
2019-07-11 17:12:16.208592: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc08db4f5b0 executing computations on platform Host. Devices:                                                                     
2019-07-11 17:12:16.208619: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>                                                                                            
Traceback (most recent call last):
  File ""bug_test.py"", line 12, in <module>
    print(tf.summary.create_file_writer)
AttributeError: module 'tensorflow_core.summary' has no attribute 'create_file_writer'
```"
30602,FFT ops consume a lot of GPU RAM,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 19.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')
- **Python version**: 2.7.16
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.0 / 7.6.1.34
- **GPU model and memory**: RTX 2080ti 11GB
- **Exact command to reproduce**:
`import tensorflow as tf`
`tf.enable_eager_execution()`
`d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)`
`D = tf.fft(d)`
`d = tf.ifft(D)`

### Describe the problem
The output of the above code is an error produced by the line `d = tf.ifft(D)`. The error is: `tensorflow.python.framework.errors_impl.InternalError: BlasScal failed : in.shape=[512] [Op:IFFT]`

The following code works as expected:
`import tensorflow as tf`
`tf.enable_eager_execution()`
`d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)`
`D = tf.fft(d)`
`with tf.device(""/cpu:0""):`
`    d = tf.ifft(D)`"
30601,Tensorflow 2.0/1.14 GPU build fails in debug mode,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda? 
- Bazel version (if compiling from source):0.25.2
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:10.0/7
- GPU model and memory: p40 



**Describe the problem**

tensorflow 1.14 gpu build doesn't succeed in debug mode

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=opt --config=cuda --compilation_mode=dbg //tensorflow/tools/pip_package:build_pip_package



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



1 error detected in the compilation of ""/tmp/tmpxft_000029e6_00000000-6_spacetodepth_op_gpu.cu.cpp1.ii"".
ERROR: /home/core/tensorflow/tensorflow/core/kernels/BUILD:4483:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/spacetodepth_op_gpu.cu.pic.o' was not created
ERROR: /home/core/tensorflow/tensorflow/core/kernels/BUILD:4483:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 394.154s, Critical Path: 51.10s
INFO: 4640 processes: 4640 local.
FAILED: Build did NOT complete successfully

"
30600,TFTRT: Can I specify the computation precision for a specific layer?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.14

**Describe the feature and the current behavior/state.**
Support blacklist of ops that cannot use lower precision when converting a graph into tftrt with precision mode of fp16 or int8. Or provide a way to specify a subgraph that cannot be optimized by tftrt.

**Who will benefit with this feature?**
All users that need a mixed precision of their model and want to optimize the model by tftrt.

**Any Other info.**

In my product I'm developing a model that needs full precision for a few layers (which include elementwise ops and conv ops). The layers need full precision because they are doing some math computation that cannot sacrifice precision. I also want to use tftrt to optimize the graph because most of other layers can use precision as low as fp16 or int8. However, i cannot find a way in tftrt to specify a blacklist of ops that cannot use lower precision. And there is no way to specify a subgraph that cannot be optimized by tftrt. Tensorrt supports specifying a precision for a layer but my model uses many ops that don't exist in tensorrt so I cannot convert it into tensorrt.

Thank you.

@trevor-m "
30599,"Error(i.e. fatal error C1189: #error:  STL1001: Unexpected compiler version, expected MSVC 19.20 or newer) while building Tensorflow with MSVC 2019 on windows ","System information
OS Platform : Windows 10 Pro
TensorFlow version: r1.3
Python version: Python 3.6.8 :: Anaconda, Inc.
Installed using virtualenv: conda
Bazel version : 0.26.1
MSVC 19

configuration :
Please specify the location of python. [Default is C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\python.exe]:
Found possible Python library paths:
C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\lib\site-packages
Please input the desired Python library path to use. Default is [C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\lib\site-packages]
Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N

Error Description :

 C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/spectrogram.obj /c tensorflow/lite/kernels/internal/spectrogram.cc
Execution platform: @bazel_tools//platforms:host_platform
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.21.27702\include\yvals_core.h(352): fatal error C1189: #error:  STL1001: Unexpected compiler version, expected MSVC 19.20 or newer.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1719.835s, Critical Path: 17.80s
INFO: 18 processes: 18 local.
FAILED: Build did NOT complete successfully"
30598,gfile for S3 uploads corrupt data when called concurrently,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):

Amazon DLAMI
- TensorFlow version (use command below):
1.13.1
- Python version:
3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have code that writes .npz files from multiple python processes. python code is single-threaded, and each process writes a separate .npz file. The files are sometimes corrupted on S3. I checked a sha256 checksum of the data in memory, and they differ from the checksum of the written data on S3.

**Describe the expected behavior**

the contents should be identical.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I can create a repro if needed.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The reason seems to be that the gfile writer doesn't create a unique tempfile when it is invoked within a millisecond.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L213

Here, the writer creates a tempfile. and the corresponding aws-sdk-cpp code is:

https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-core/source/platform/linux-shared/FileSystem.cpp#L262

The latter code only has a millisecond granularity, so if gfile is called within the same millisecond, it creates the same tempfile and they overwrite each other.

Also, the ""XXXXXX"" part in s3_file_system.c doesn't do anything, although it's not the cause of this problem.

I created a separate issue in

https://github.com/aws/aws-sdk-cpp/issues/1192





"
30596,Very bad performance using Gradient Tape,"System information

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 18.04.2
TensorFlow installed from (source or binary): binary pip
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8
CUDA/cuDNN version: 10.0/7
GPU model and memory: Tesla K80

I'm trying to learn Tensorflow 2.0, so I build a toy model and trained it using keres .fit 
method, everything worked well. 
But when I tried to implement the training loop from scratch, the training is happening very very slowly. Keras .fit method trained the model in 1 min 41 secs while the training code I've written taking more than 8 mins to train!!!

Below is my model definition:
```python
model = tf.keras.Sequential()
model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.AveragePooling2D())
model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
model.add(layers.AveragePooling2D())
model.add(layers.Flatten())
model.add(layers.Dense(units = 120, activation = 'relu'))
model.add(layers.Dense(units = 84, activation = 'relu'))
model.add(layers.Dense(units = 10, activation = 'softmax'))
```
Below I'm defining loss, optimizer and accuracy:
```python
optimizer = tf.keras.optimizers.Adam()
objective = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy()
```

And Below is my training loop:
```python
%%time
with tf.device('gpu:0'):
    for epoch in range(20):
        cumulative_loss = 0.0
        metric.reset_states()
        for images, labels in dataset:
            with tf.GradientTape() as tape:
                predictions = model(images, training=True)
                loss = objective(labels, predictions)

            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            cumulative_loss += loss
            metric.update_state(labels, predictions)

        print(""Epoch: {} Loss: {} Accuracy: {}"".format(epoch, cumulative_loss.numpy()/(batch + 1), metric.result()))
```
I'm runnig my notebook in Google Colab"
30595,TFTRT calib_graph/infer_graph has different output,"I am doing tftrt convert to accelerate tf model.
first, get calib_graph
calibration_graph = trt.create_inference_graph()

second run calib_graph
the output is a tensor with batch_size = 16, output like this

third, I get the int8 graph
int8Graph = trt.calib_graph_to_infer_graph(calibration_graph)
('length and output: ', [array([-0.999994  ,  0.99682164,  0.9992819 ,  0.95060456, -0.8617101 ,
        0.9993279 , -0.994945  ,  0.99960554, -0.99968964,  0.99943244,
        0.9990337 ,  0.9889959 ,  0.91563284,  0.99274397,  0.99358046,
        0.9925939 ], dtype=float32)])

But when I run the int8Graph using the same input data as step2, I get output like this, 
('length and output: ', array([0.952391  , 0.94719326, 0.9581959 , 0.86539173, 0.92513335,
       0.97020185, 0.88829505, 0.97703993, 0.87482417, 0.9612503 ,
       0.95443654, 0.8331325 , 0.9141325 , 0.92734456, 0.94608974,
       0.9845928 ], dtype=float32))

It's really confused, any one met this problem?


by the way, I run origin pb with the same data, I get 
('length and output: ', array([-0.999994  ,  0.99682164,  0.9992819 ,  0.9506048 , -0.8617081 ,
        0.9993279 , -0.9949451 ,  0.99960554, -0.99968964,  0.99943244,
        0.9990337 ,  0.9889959 ,  0.9156331 ,  0.99274397,  0.99358046,
        0.9925939 ], dtype=float32)),
same as my calib_graph result.

So, what's wrong with the int8_graph???
"
30594,CUDA_ERROR_OUT_OF_MEMORY AND UNABLE TO LOAD CUDA," Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-11 12:42:11.226602: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416
2019-07-11 12:42:11.399046: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Aborted (core dumped)
"
30593,"(Aborted (core dumped)) - Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Stock example script. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: On my laptop,
- TensorFlow installed from (source or binary): From source for c++ application. 
- TensorFlow version (use command below): v1.8.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.1
- GCC/Compiler version (if compiling from source): g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
- CUDA/cuDNN version: CPU only
- GPU model and memory: None.


**Describe the current behavior**
I have a [Standalone c++ build](https://medium.com/@tomdeore/standalone-c-build-tensorflow-opencv-6dc9d8a1412d) process for ubuntu and it works fine with my own code as well as with the c++ [example code](https://www.tensorflow.org/guide/extend/cc) on tensorflow. 

But when i run the same code on CentOS platform. I get following runtime error message and the executable core dumps:

```
2019-07-11 10:20:42.027023: F tensorflow/core/framework/function.cc:1329] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate
Aborted (core dumped)
```
My compilation command is as: 

```
g++ -std=gnu++11 -Wl,-rpath='$ORIGIN/lib' -fPIC -Iinclude -Llib  example.cc -ltensorflow_cc -ltensorflow_framework   -o example
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please follow the steps provided on [blog](https://medium.com/@tomdeore/standalone-c-build-tensorflow-opencv-6dc9d8a1412d). 
"
30592,Macros definition for using MKL from Eigen,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution Centos 7.2 (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1 Release
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: Build from source, no conda
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
In tensorflow/tensorflow/tensorflow.bzl, it defines the macros used for using MKL from Eigen as below:
`if_mkl([""-DINTEL_MKL=1"", ""-DEIGEN_USE_VML""])`

However, in Eigen's documentation, the macro is:
`-DEIGEN_USE_MKL_VML`
 
Is this intentional? I tried to read both tensorflow's and Eigen's source code but did not find a reason of that. 
Thanks.
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30591,Consistent use of else after return (pylint no-else-return),"While working on some pylint warnings in `tensorflow/python/keras/backend.py` I noticed that two different ways of ""if else return"" styles are used.

1. `if condition: return 0`
    `return 1`
2. `if condition: return 0`
    `else: return 1`

Is changing the behaiviour to one of the variants considered good practice?
I would like to clear things up (and improve the pylint score :D ).

"
30589,Train models under the director '/mxnet/example/gluon/',"I conduct distributed training on almost all the models provided under the directory _'/mxnet/example/gluon'_, however, when running the models with the asynchronous mechanism (--kvstore dist_async), none of the models is convergent. 
 For instance, I train the _resnet18_v1_ with _**cifar10**_ dataset, the training accuracy is around 0.09 under the asynchronous mechanism. Nevertheless, training with the synchronous mechanism can achieve the target accuracy.
I do not know the reason why the _asynchronous SGD_ is not applicable to these models, and what should I do to fix this? "
30587,keras define a trainable weights as layer,"Hi~I have some problems in use tf.keras to build model. Now I want to define a trainbale weight B with shape(64, 128), then do tensor matrix, how to define a trainable weight B in keras similar to `tf.get_variable`
```
A = Input(shape=(128,))
out = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([A, B])
model = Model(inputs=A, outputs=out)
```
I hope someone can help me.It's very grateful!"
30585,Add should_finialize(state) to tf.data.experimental.Reducer,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Current behavior: tf.data.experimental.Reducer will not finalize until the end of a dataset.
Wanted feature: I don't want to reduce the whole dataset. I hope that the Reducer could decide when to finalize based on its state. For example, add the following function inside Reducer and call the function after reduce_func().
def should_finialize(state):
  return True   --> stop reducing anymore, call finalize_func, and return results.
  return False  --> continue reduce.

**Will this change the current api? How?**
No. The default should_finialize always returns False. If anyone wants to change this behavior, he needs to override this function.

**Who will benefit with this feature?**
The tf.data input pipeline will be more versatile with this new feature. It is possible to do more operations without calling a python generator or py_func. For example, we need several different samples from the same class to be in one batch for tf.contrib.losses.metric_learning.triplet_semihard_loss. The labels of a batch may look like the following:
0, 0, 0, 0,
1, 1, 1, 1,
...
15, 15, 15, 15
There are 64 (=16*4) samples in this batch. I cannot find a pure tf.data way to form batches in this way. But with the new feature, we are able to do so.

**Any Other info.**
It seems that Jiri Simsa @jsimsa is very familiar with tf.data.experimental.Reducer."
30581,Restoring only a PART of the graph from warm_start_from in Estimator,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**
When restoring a TF estimator, we can use warm_start_from. However, that means all variables will be restored from the checkpoint. (see https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)
However, what if my model_fn is changed with some more variables being added? In that case, we can't use warm_start_from.
We hope those newly added variables can be initialized by some default initializer while others are loaded from the checkpoint. That way, the estimator can be incrementally defined. Much more flexible!

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
BERT users and Wide-n-deep estimator users.
**Any Other info.**
"
30580,TPU has XLA compilation issue on TF 1.14,"I am getting an issue with using XLA on the cloud TPU on tensorflow version 1.14

**System information**
- Using Google's cloud TPU with Tensorflow 1.14
```v1.14.0-rc1-22-gaf24dc91b5 1.14.0```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3379601/tf_env.txt)
System info (sanity check log message removed, full `tf_env.txt` attached above):
```

== check python ===================================================
python version: 3.5.3
python branch: 
python build version: ('default', 'Sep 27 2018 17:25:39')
python compiler version: GCC 6.3.0 20170516
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Sat Jun 22 23:33:41 PDT 2019
os release version: 4.14.111+
os platform: Linux-4.14.111+-x86_64-with-debian-9.9
linux distribution: ('debian', '9.9', '')
linux os distribution: ('debian', '9.9', '')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='cs-6000-devshell-vm-cb1acb17-794a-49c5-8346-cd612beb1d0d', release='4.14.111+', version='#1 SMP Sat Jun 22 23:33:41 PDT 2019', machine='x86_64', processor='')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy (1.16.4)
protobuf (3.8.0)
tensorflow (1.14.0)
tensorflow-estimator (1.14.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 1.14.0
tf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5
tf.version.COMPILER_VERSION = 4.8.5
Sanity check: -- EDITED OUT ---

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 1.14.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.5/dist-packages

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 5, 3, 'final', 0)

== bazel version  ===============================================
Build label: 0.27.1
Build time: Tue Jul 2 17:49:35 2019 (1562089775)
Build timestamp: 1562089775
Build timestamp as int: 1562089775

```

**Describe the current behavior**

When I run my model (which is a partial implementation of Transformer) using XLA, I get an error message.

The error message is the following:

```
WARNING: Logging before flag parsing goes to stderr.
W0710 13:59:32.655435 139705705027328 deprecation_wrapper.py:119] From issue.py:24: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.

W0710 13:59:32.655846 139705705027328 deprecation.py:323] From issue.py:25: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`.
W0710 13:59:32.655990 139705705027328 deprecation.py:323] From issue.py:26: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
W0710 13:59:32.663649 139705705027328 deprecation_wrapper.py:119] From issue.py:68: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0710 13:59:32.707169 139705705027328 deprecation_wrapper.py:119] From issue.py:85: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.

W0710 13:59:32.714751 139705705027328 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0710 13:59:33.743603 139705705027328 deprecation_wrapper.py:119] From issue.py:205: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

W0710 13:59:35.527529 139705705027328 deprecation_wrapper.py:119] From issue.py:206: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

W0710 13:59:35.536509 139705705027328 deprecation_wrapper.py:119] From issue.py:208: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

W0710 13:59:35.547362 139705705027328 deprecation_wrapper.py:119] From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-10 13:59:35.547951: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-10 13:59:35.799823: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-07-10 13:59:35.800045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5608f773ae90 executing computations on platform Host. Devices:
2019-07-10 13:59:35.800062: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
W0710 13:59:35.811006 139705705027328 deprecation_wrapper.py:119] From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-07-10 13:59:36.203915: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-10 13:59:36.572778: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.
2019-07-10 13:59:36.861743: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.
2019-07-10 13:59:37.125698: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.
2019-07-10 13:59:37.551537: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.
W0710 13:59:39.391257 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(iterator)`.
W0710 13:59:39.444635 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(iterator)`.
W0710 13:59:39.445140 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(iterator)`.
2019-07-10 13:59:44.506360: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at xla_ops.cc:343 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}
  .  Registered:  device='CPU'
){{node gradients/AddN_13/tmp_var}}
  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}
  .  Registered:  device='CPU'
){{node gradients/AddN_13/tmp_var}}
  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
   [[cluster]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""issue.py"", line 221, in <module>
    sys.exit(main(sys.argv))
  File ""issue.py"", line 217, in main
    hlo = get_hlo(transformer_model_fn, transformer_input_fn)
  File ""issue.py"", line 33, in get_hlo
    sess.run(loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}
  .  Registered:  device='CPU'
){{node gradients/AddN_13/tmp_var}}
  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
   [[cluster]]
```

**Describe the expected behavior**

I expect there to be no error message!


**Code to reproduce the issue**
Reproduce with `python3 issue.py`

[issue.py.zip](https://github.com/tensorflow/tensorflow/files/3379603/issue.py.zip)
issue.py is the following (also zipped above):
```
import sys
import tensorflow as tf
import numpy as np

from tensorflow.contrib.compiler.xla import compile

HIDDEN_SIZE = 2048
FILTER_SIZE = 8196
NUM_HEADS = 32
D_K = HIDDEN_SIZE // NUM_HEADS
D_K_ROOT = D_K ** (-0.5)
WORD_LEN = 512 


def get_hlo(model_fn, input_fn):

    def xla_model_fn(features, labels):
        spec = model_fn(features, labels, mode=""train"", params=None)
        with tf.control_dependencies([spec.train_op]):
            return tf.identity(spec.loss, name=spec.loss.op.name)

    train_ds = input_fn().repeat()

    iterator = tf.data.Iterator.from_structure(
        train_ds.output_types,
        train_ds.output_shapes,
    )
    loss, = compile(xla_model_fn, inputs=iterator.get_next())

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(iterator.make_initializer(train_ds))
        sess.run(loss)


def transformer_input_fn():
    feat_shape = [1, WORD_LEN, HIDDEN_SIZE]
    labels_shape = [1, 1, 1, WORD_LEN]

    model_inputs = [
        tf.image.convert_image_dtype(
            tf.reshape(
                tf.constant(
                    np.random.uniform(size=feat_shape)
                ),
                feat_shape,
            ),
            tf.float32,
        ),
    ]

    model_expected = [
        tf.image.convert_image_dtype(
            tf.reshape(
                tf.constant(np.random.uniform(size=labels_shape)),
                labels_shape,
            ),
            tf.float32,
        ),
    ]

    dataset = (model_inputs, model_expected)
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    return dataset


def transformer_model_fn(features, labels, mode, params):
    layer_norm_scale = tf.get_variable(
        ""layer_norm_scale"",
        [HIDDEN_SIZE],
        initializer=tf.ones_initializer(),
    )
    layer_norm_bias = tf.get_variable(
        ""layer_norm_bias"",
        [HIDDEN_SIZE],
        initializer=tf.zeros_initializer(),
    )

    def layer_normalization(net):
        mean = tf.reduce_mean(net, axis=[-1], keepdims=True)
        mean = tf.multiply(mean, -1)
        diff = tf.add(net, mean)
        var = tf.reduce_mean(tf.square(diff), axis=[-1], keepdims=True)
        var = tf.add(var, 1e-6)
        net = tf.multiply(diff, tf.rsqrt(var))
        return tf.add(tf.multiply(net, layer_norm_scale), layer_norm_bias)

    def transformer_fc(net, size, use_bias=True):
        return tf.keras.layers.Dense(
            size,
            activation=None,
            use_bias=use_bias,
        )(net)

    def feed_forward(net):
        net = transformer_fc(net, FILTER_SIZE)
        net = tf.nn.relu(net)
        net = tf.nn.dropout(net, rate=0.1)
        net = transformer_fc(net, HIDDEN_SIZE)
        return net

    def multi_headed_attention(q, k, v, bias):
        def split_heads(net):
            net = tf.reshape(
                net,
                [1, WORD_LEN, NUM_HEADS, D_K],
            )
            net = tf.transpose(net, [0, 2, 1, 3])
            return net

        def combine_heads(net):
            net = tf.transpose(net, [0, 2, 1, 3])
            net = tf.reshape(
                net, [1, WORD_LEN, HIDDEN_SIZE]
            )
            return net

        q = transformer_fc(q, HIDDEN_SIZE, use_bias=False)
        k = transformer_fc(k, HIDDEN_SIZE, use_bias=False)
        v = transformer_fc(v, HIDDEN_SIZE, use_bias=False)

        q = split_heads(q)
        k = split_heads(k)
        v = split_heads(v)

        q = tf.multiply(q, D_K_ROOT)
        x = tf.matmul(q, k, transpose_b=True)
        x = tf.add(x, bias)

        x = tf.nn.softmax(x)
        x = tf.nn.dropout(x, rate=0.1)
        x = tf.matmul(x, v)

        x = combine_heads(x)
        x = transformer_fc(x, HIDDEN_SIZE, use_bias=False)
        return x

    def encoder_block(net, enc_bias):
        residual = layer_normalization(net)
        residual = multi_headed_attention(
            residual, residual, residual, enc_bias
        )
        residual = tf.nn.dropout(residual, rate=0.1)
        net = tf.add(residual, net)

        residual = layer_normalization(net)
        residual = feed_forward(residual)
        residual = tf.nn.dropout(residual, rate=0.1)
        net = tf.add(residual, net)

        return layer_normalization(net)

    def encode(enc_input, enc_bias):
        net = tf.nn.dropout(enc_input, rate=0.1)
        net = encoder_block(net, enc_bias) 
        return net

    def decoder_block(net, enc_output, dec_bias, enc_dec_bias):
        residual = layer_normalization(net)
        residual = multi_headed_attention(
            residual, residual, residual, dec_bias
        )
        residual = tf.nn.dropout(residual, rate=0.1)
        net = tf.add(residual, net)

        residual = layer_normalization(net)
        residual = multi_headed_attention(
            residual, enc_output, enc_output, enc_dec_bias
        )
        residual = tf.nn.dropout(residual, rate=0.1)
        net = tf.add(residual, net)

        residual = layer_normalization(net)
        residual = feed_forward(residual)
        residual = tf.nn.dropout(residual, rate=0.1)
        net = tf.add(residual, net)

        return layer_normalization(net)

    def decode(dec_input, enc_output, dec_bias, enc_dec_bias):
        net = tf.nn.dropout(dec_input, rate=0.1)
        net = decoder_block(net, enc_output, dec_bias, enc_dec_bias)
        return net

    labels = tf.stop_gradient(labels)

    enc_input = features
    dec_input = features
    enc_bias = labels
    enc_dec_bias = labels
    dec_bias = labels
    target = features

    enc_output = encode(enc_input, enc_bias)
    dec_output = decode(dec_input, enc_output, dec_bias, enc_dec_bias)

    net = dec_output

    loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits_v2(
            labels=target,
            logits=net,
        )
    )
    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)
    global_step = tf.train.get_or_create_global_step()

    update_global_step = tf.assign(
        global_step, global_step + 1, name=""update_global_step""
    )
    return tf.estimator.EstimatorSpec(
        mode, loss=loss, train_op=tf.group(train_step, update_global_step)
    )


def main(args):
    hlo = get_hlo(transformer_model_fn, transformer_input_fn)


if __name__ == ""__main__"":
    sys.exit(main(sys.argv))

```

**Other info / logs**

The issue can actually be avoided if the `issue.py` file is modified slightly:

change
```
HIDDEN_SIZE = 2048
FILTER_SIZE = 8196 
NUM_HEADS = 32
```
to
```
HIDDEN_SIZE = 2048 // 4
FILTER_SIZE = 8196 // 4
NUM_HEADS = 32 // 4
```
on lines `7-9`

Note that this fix works on the Google cloud TPU, may not work on other machines?

The issue was also reproduced on multiple tf versions:

` tensorflow/tensorflow:1.13.0rc0-py3`: Was able to reproduce the issue
` tensorflow/tensorflow:1.13.0rc1-py3`: Was able to reproduce the issue
` tensorflow/tensorflow:1.13.0rc2-py3`: Was able to reproduce the issue
` tensorflow/tensorflow:1.13.1-py3`: Was able to reproduce the issue
` tensorflow/tensorflow:1.14.0-py3`: Was able to reproduce the issue
` tensorflow/tensorflow:2.0.0a0-py3`: No module named 'tensorflow.contrib' import error

All of these tf versions were run on vanilla tf docker images. The fix above where I divide the numbers by 4 also fixes the issue on all those versions as well.

tf version 2 and higher was not tested because of significant API changes, for this I need to spend some time creating a new python file with new API calls to get the issue reproduced

Images:

Issue reproduced using issue.py:
<img width=""1081"" alt=""tpu_err"" src=""https://user-images.githubusercontent.com/44978436/61009735-92e4d380-a328-11e9-9701-aed9d921ecde.png"">

Issue avoided by dividing variables by 4  (expected behavior):
<img width=""1430"" alt=""tf_no_issue"" src=""https://user-images.githubusercontent.com/44978436/61009751-9f692c00-a328-11e9-9e2c-9c2417211360.png"">

```
WARNING: Logging before flag parsing goes to stderr.
W0710 15:25:47.346601 140584685344512 deprecation_wrapper.py:119] From issue.py:24: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.

W0710 15:25:47.347033 140584685344512 deprecation.py:323] From issue.py:25: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`.
W0710 15:25:47.347194 140584685344512 deprecation.py:323] From issue.py:26: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
W0710 15:25:47.356488 140584685344512 deprecation_wrapper.py:119] From issue.py:68: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0710 15:25:47.417571 140584685344512 deprecation_wrapper.py:119] From issue.py:85: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.

W0710 15:25:47.425961 140584685344512 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0710 15:25:48.473849 140584685344512 deprecation_wrapper.py:119] From issue.py:205: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

W0710 15:25:49.911786 140584685344512 deprecation_wrapper.py:119] From issue.py:206: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

W0710 15:25:49.917340 140584685344512 deprecation_wrapper.py:119] From issue.py:208: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

W0710 15:25:49.928683 140584685344512 deprecation_wrapper.py:119] From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-10 15:25:49.929245: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-10 15:25:49.939914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-07-10 15:25:49.940074: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563e2d4be50 executing computations on platform Host. Devices:
2019-07-10 15:25:49.940087: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
W0710 15:25:49.940532 140584685344512 deprecation_wrapper.py:119] From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-07-10 15:25:50.432006: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
W0710 15:25:50.529966 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(iterator)`.
W0710 15:25:50.530351 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(iterator)`.
W0710 15:25:50.530480 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(iterator)`.
2019-07-10 15:26:03.872235: W tensorflow/core/framework/allocator.cc:107] Allocation of 123777024 exceeds 10% of system memory.
```"
30579,Choosing device on NNAPI,"I would like to call specifically into the NPU of a device. Is there a way to call into NNAPI from the application level that will allow such behavior?


"
30577,TensorflowLiteSwift fails pod validation,"

**System information**
- OS Platform and Distribution: MacOS 10.14.5
- TensorflowLiteSwift Pod version: 0.2.0
- Commit: 477447155b
- Cocoapods version: 1.7.3
- Xcode version: 10.2.1

**Describe the problem**

When running the command `pod spec lint`, tests fail because `i386` architecture cannot be found. This happens not only on my machine, but also on our private pod system. Failing validation means we cannot use the pod for our apps without turning off the tests. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. cd to TensorflowLiteSwift directory
2. run `pod spec lint --verbose` 

**Any other info / logs**

[here's the output](https://github.com/tensorflow/tensorflow/files/3378927/log.txt)

I was able to resolve this issue by upping the the iOS deployment target in the podspec from 9.0 to 11.0. I believe `pod spec lint` wants to compile a 32 bit version of TensorFlowLiteC for iOS 9 and 10, but the TensorFlowLiteC binary only contains `x86_64` and no `i386`. A more robust solution might be to compile for `i386`. Or it may be possible to configure the podspec to only execute tests on arm and `x86_64`. "
30576,tf.function input signature error messages are not-helpful walls of text,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yes
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2 b 1
- Python version: 3.6.8

**Describe the current behavior**
if you make any errors with Input Signature on autograph / tf.function, the error message is essentially useless:

```        
neuromax.py:396 train  *
        change, total_change, episode, episodes_this_dataset = run_episode(type, n_atoms, target_positions, positions, features, masses, quantum_target, target_features, change, total_change, episode, episodes_this_dataset, gif)
    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:404 __call__
        return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1334 __call__
        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1612 _maybe_define_function
        *args, **kwargs)
    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1169 canonicalize_function_inputs
        self._flat_input_signature)
    /home/bion/anaconda3/envs/tfg/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1229 _convert_inputs_to_signature
        (str(inputs), str(input_signature)))

    ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'args_5:0' shape=(None,) dtype=string>, <tf.Tensor 'args_7:0' shape=(None,) dtype=int32>, <tf.Tensor 'args_8:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_9:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_10:0' shape=(None, None, 7) dtype=float32>, <tf.Tensor 'args_11:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'args_12:0' shape=<unknown> dtype=float32>, <tf.Tensor 'args_13:0' shape=(None, None, 7) dtype=float32>, <tf.Tensor 'args_0:0' shape=() dtype=float32>, <tf.Tensor 'args_4:0' shape=() dtype=float32>, <tf.Tensor 'args_1:0' shape=() dtype=int32>, <tf.Tensor 'args_2:0' shape=() dtype=int32>, False)), input_signature ((TensorSpec(shape=(1,), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 7), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(15,), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 7), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.bool, name=None)))
```


**Describe the expected behavior**
this error message should be formatted and it should highlight exactly which input is incompatible. Otherwise this can become a needle in a haystack and gets annoying real quick

**Code to reproduce the issue**
N/A

"
30574,Decode_wav sample rate output cannot be passed to tf.signal.linear_to_mel_weight_matrix,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Combined output of decode_wav with the sample in [signal/mfccs_from_log_mel_spectrograms](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Conda binary
- TensorFlow version (use command below): 2.0.0-dev20190702 (git version unknown)
- Python version: 3.6.7
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 
- GPU model and memory: Surface Book Nvidia GPU

**Describe the current behavior**
The output of decode_wav is tuple of (wav_data, sample_rate) 
Sample_rate is int32 but linear_to_mel_weight_matrix expects a float32 sample_rate.
If the sample rate is cast using tf.cast(sample_rate, float32) and then a TypeError is thrown with the message:

TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

**Describe the expected behavior**
Sample rate output of decode_wav can be used as input to linear_to_mel_weight_matrix.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

def load_and_mel_file(path_tensor):
    #From: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms
    pcm, sample_rate = tf.audio.decode_wav(path_tensor)
    sr_f = tf.cast(sample_rate, tf.float32) #Mismatch in types between output of decode_wav and input to linear_to_mel_weight_matrix
    print(pcm, sample_rate, sr_f)

    # A 1024-point STFT with frames of 64 ms and 75% overlap.
    stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                           fft_length=1024)
    spectrograms = tf.abs(stfts)

    # Warp the linear scale spectrograms into the mel-scale.
    num_spectrogram_bins = stfts.shape[-1]
    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
      num_mel_bins, num_spectrogram_bins, sr_f, lower_edge_hertz,
      upper_edge_hertz)
    mel_spectrograms = tf.tensordot(
      spectrograms, linear_to_mel_weight_matrix, 1)
    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
      linear_to_mel_weight_matrix.shape[-1:]))

    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)
    print(log_mel_spectrograms)
    
    return log_mel_spectrograms

path_ds = tf.data.Dataset.list_files(""*.wav"")
mel_ds = path_ds.map(load_and_mel_file)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> > TypeError                                 Traceback (most recent call last)
> <ipython-input-61-aaa96a9258a6> in <module>
>       1 #Build datasets
> ----> 2 train_ds = build_data_pairs_from_dir(train_dir)
>       3 test_ds = build_data_pairs_from_dir(test_dir)
> 
> <ipython-input-60-92023d8b5863> in build_data_pairs_from_dir(source_dir)
>      54 
>      55     #Convert to MEL
> ---> 56     clean_mel_ds = clean_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)
>      57     noisy_mel_ds = noisy_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)
>      58 
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in map(self, map_func, num_parallel_calls)
>    1887       return DatasetV1Adapter(
>    1888           ParallelMapDataset(
> -> 1889               self, map_func, num_parallel_calls, preserve_cardinality=False))
>    1890 
>    1891   @deprecation.deprecated(None, ""Use `tf.data.Dataset.map()"")
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
>    3333         self._transformation_name(),
>    3334         dataset=input_dataset,
> -> 3335         use_legacy_function=use_legacy_function)
>    3336     self._num_parallel_calls = ops.convert_to_tensor(
>    3337         num_parallel_calls, dtype=dtypes.int32, name=""num_parallel_calls"")
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
>    2677       resource_tracker = tracking.ResourceTracker()
>    2678       with tracking.resource_tracker_scope(resource_tracker):
> -> 2679         self._function = wrapper_fn._get_concrete_function_internal()
>    2680         if add_to_graph:
>    2681           self._function.add_to_graph(ops.get_default_graph())
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal(self, *args, **kwargs)
>    1418     """"""Bypasses error checking when getting a graph function.""""""
>    1419     graph_function = self._get_concrete_function_internal_garbage_collected(
> -> 1420         *args, **kwargs)
>    1421     # We're returning this concrete function to someone, and they may keep a
>    1422     # reference to the FuncGraph without keeping a reference to the
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
>    1412     if self.input_signature:
>    1413       args, kwargs = None, None
> -> 1414     graph_function, _, _ = self._maybe_define_function(args, kwargs)
>    1415     return graph_function
>    1416 
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
>    1716         graph_function = self._function_cache.primary.get(cache_key, None)
>    1717         if graph_function is None:
> -> 1718           graph_function = self._create_graph_function(args, kwargs)
>    1719           self._function_cache.primary[cache_key] = graph_function
>    1720         return graph_function, args, kwargs
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
>    1602             arg_names=arg_names,
>    1603             override_flat_arg_shapes=override_flat_arg_shapes,
> -> 1604             capture_by_value=self._capture_by_value),
>    1605         self._function_attributes)
>    1606 
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
>     784                                           converted_func)
>     785 
> --> 786       func_outputs = python_func(*func_args, **func_kwargs)
>     787 
>     788       # invariant: `func_outputs` contains only Tensors, CompositeTensors,
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in wrapper_fn(*args)
>    2671           attributes=defun_kwargs)
>    2672       def wrapper_fn(*args):  # pylint: disable=missing-docstring
> -> 2673         ret = _wrapper_helper(*args)
>    2674         ret = structure.to_tensor_list(self._output_structure, ret)
>    2675         return [ops.convert_to_tensor(t) for t in ret]
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in _wrapper_helper(*args)
>    2616         nested_args = (nested_args,)
>    2617 
> -> 2618       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
>    2619       # If `func` returns a list of tensors, `nest.flatten()` and
>    2620       # `ops.convert_to_tensor()` would conspire to attempt to stack
> 
> c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py in wrapper(*args, **kwargs)
>     220         except Exception as e:  # pylint:disable=broad-except
>     221           if hasattr(e, 'ag_error_metadata'):
> --> 222             raise e.ag_error_metadata.to_exception(type(e))
>     223           else:
>     224             raise
> 
> TypeError: in converted code:
> 
>     <ipython-input-60-92023d8b5863>:27 load_and_mel_file  *
>         linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
>     c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\ops\signal\mel_ops.py:155 linear_to_mel_weight_matrix
>         lower_edge_hertz, upper_edge_hertz, dtype)
>     c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\ops\signal\mel_ops.py:74 _validate_arguments
>         if sample_rate <= 0.0:
>     c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\framework\ops.py:692 __bool__
>         raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
> 
>     TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
> 


"
30571,Trace/Wallclock Performance Discrepancy ,"**Describe the bug**

Running a converted PyTorch model has some curious performance characteristics were running the `.pb` model directly in TensorFlow is ~2.5 times slower than the onnx runtime.

```bash
$ ./benchmark model
> onnx runtime took 4.83842
> tensorflow took 12.63854
```

Looking at the trace information shows that work is only being done for about 3.5 seconds so the question is what is `session.run` doing for the other ~9 seconds?  

![onnx-tf-rnn-perf](https://user-images.githubusercontent.com/4310904/60820118-b547e700-a198-11e9-8c3f-f48c54e3952e.png)

**To Reproduce**

```python
#!/usr/bin/env python

""""""
Benchmark ONNX model
""""""

import time
from argparse import ArgumentParser

import numpy as np
import numpy.testing

import onnxruntime as runtime

import tensorflow as tf
from tensorflow.python.client import timeline

def main(args):

    x = np.random.randn(args.chunksize, args.batchsize, 1).astype(np.float32)

    # == ONNX Runtime ==

    options = runtime.SessionOptions()
    session = runtime.InferenceSession(args.model + "".onnx"", options)

    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    t0 = time.time()
    onnxrt_out = session.run([output_name], {input_name: x})
    onnxrt_out = np.array(onnxrt_out[0])
    t1 = time.time()
    print(""> onnx runtime took %.5f"" % (t1 - t0))
 
    # == TensorFlow ==

    with tf.compat.v1.Session() as session:
        with tf.io.gfile.GFile(args.model + "".pb"", ""rb"") as graph:
            g = tf.compat.v1.GraphDef()
            g.ParseFromString(graph.read())
            tf.import_graph_def(g)

        graph = tf.get_default_graph()
        first = graph.get_tensor_by_name(""import/input:0"")
        last = graph.get_operations()[-1].outputs[0]

        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()

        t0 = time.time()
        tensorflow_out = session.run(
            last,
            feed_dict={first: x},
            options=run_options,
            run_metadata=run_metadata
        )
        t1 = time.time()

        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('trace_file.json', 'w') as f:
            f.write(ctf)

    print(""> tensorflow took %.5f"" % (t1 - t0))

    numpy.testing.assert_allclose(
        onnxrt_out,
        tensorflow_out,
        atol=1e-4
    )


if __name__ == ""__main__"":
    parser = ArgumentParser()
    parser.add_argument(""model"")
    parser.add_argument('--batchsize', type=int, default=1000)
    parser.add_argument('--chunksize', type=int, default=1000)
    main(parser.parse_args())
```

**Model files**

 - ONNX https://www.dropbox.com/s/jv0hk9h62eymax2/model.onnx
 - TensorFlow https://www.dropbox.com/s/t2n4ut7kwfjq8j2/model.pb

**Python, ONNX, Tensorflow version**

 - Python version: 3.5.3
 - ONNX version: 1.5.0
 - Tensorflow version: 1.14.0

**Additional context**

 - Trace file https://www.dropbox.com/s/vhaq5ykt6vlu5ij/trace_file.json

"
30566,Missing Graph Editor Documentation,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor
https://www.tensorflow.org/api_guides/python/contrib.graph_editor

## Description of issue (what needs changing):
The Graph Editor API Doc (first link above) links to the Graph Editor Guide (second link above) which is a 404 error.

### Clear description
The broken link to the Graph Editor Guide needs to either be fixed or removed and replaced with some actual, comprehensive documentation on using the Graph Editor. At the moment, there is no documentation on Graph Editor usage available anywhere.
"
30565,LogMatrix Gradients,"Hi, 
I am trying to optimize a function that requires to compute tf.linalg.logmatrix but this operation doesn't seem to have a gradient as this error came up : LookupError: gradient registry has no entry for: MatrixLogarithm

Best regards,

"
30564,Output of sysconfig.get_link_flags does not seem to be suitable for Mac,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 26 2018, 19:50:54)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): Apple LLVM version 9.0.0 (clang-900.0.39.2)
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When running the example of creating a custom op from https://www.tensorflow.org/guide/extend/op on a Mac the compilation stage (see (https://www.tensorflow.org/guide/extend/op#build_the_op_library) fails with the following error message:

```
ld: library not found for -l:libtensorflow_framework.1.dylib
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

This appears to be because the specified linker option `-l:libtensorflow_framework.1.dylib` (returned by `sysconfig.get_link_flags`) is not valid for `ld` on the Mac. The man page for `ld` says the following:

```
-lx         This option tells the linker to search for libx.dylib or libx.a in the library 
search path.  If string x is of the form y.o, then that file is searched for in the 
same places, but without prepending `lib' or appending `.a' or `.dylib' to the 
filename.
```

I believe that the correct format for this flag is `-ltensorflow_framework.1` (since the linker will prepend `lib` and append `.dylib`).

A workaround for this is to replace this line

```
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
```

With this:

```
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()).replace(""-l:libtensorflow_framework.1.dylib"", ""-ltensorflow_framework.1""))') )
```

Alternatively there is (I believe) a fix for the underlying cause here: https://github.com/pio-neil/tensorflow/pull/1/files

**Describe the expected behavior**

The linking process should be performed successfully and produce a library file called zero_out.so.

**Code to reproduce the issue**

On a Mac, create a file called zero_out.cc with the following code (copied from https://www.tensorflow.org/guide/extend/op):

```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""


using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

Then run:

```
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
```

You should see an error like this:

```
ld: library not found for -l:libtensorflow_framework.1.dylib
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```"
30561,Eager execution drastically increases `tf.keras.Model.fit` runtimes,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 19.1
- TensorFlow installed from: source
- TensorFlow version: v2.0.0-beta1-0-g8e423e3d56
- Python version: 3.6.8


- Bazel version: 0.26.0
- GCC version: 7.4.0 
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5

**Describe the current behavior**

My issue regards a performance degradation induced by enabling Eager execution, in a context when no Eager tensor should be created, apart from the model's weights (to which I do not need access). As of now, my installation (compiled from source based on yesterday's status of the r2.0 branch) does not have TF 2.0 behaviours enabled by default, thus I compared the run-times for a supervised learning task depending on whether I `enable_v2_behavior` or simply `enable_resource_variables`. It turns out the former yields a significant decrease in performance.

On my initial example (which I do not include as it relies on custom data and model layers), the run time went from two minutes to five. In Eager mode, the first epoch was really slow, while the following ones proved increasingly fast, stabilizing at 11 / 12 seconds (19 ms/step). By contrast, without eager, all epochs ran at the same speed, for 8 / 9 seconds (15 ms/step).

I reproduced this issue on an example that uses mock data (i.e. properly-shaped randomly-generated values, since learning performance is not at stake here) and uses exclusively layers taken from `tensorflow.keras` (no custom bits), whose code I present below. As it happens, not enabling Eager execution results in a 13m49s runtime, while enabling it increases it to 19m17s, i.e. an increase of about 40 percent.

The task at hand consists in fitting a binary classifier that takes variable-length sequences of vectors as input. For this, I set up a `tensorflow.data.Dataset` instance (which, for simplification, contains random data in the provided code) based on pre-generated numpy arrays of data, and use the `padded_batches` method to format my inputs as desired. In the mock example, the model consists of a layer of 100 LSTM cells with inputs masking (due to length variability) and default tanh activation topped with a single feed-forward unit with sigmoid activation.

**Describe the expected behavior**

I can understand that Eager execution would create a slight overhead, however here it proves huge, while no mechanism whatsoever requires it. I do not know what can be done about it _per se_, but if such an overhead is to be expected, I think it would be nice to be able to disable Eager in 2.0. And I mean _properly_ disabling it, not using `tf.compat` instructions that are bound to be wiped out at some point.

Alternatively, I believe that it could be great to enable using ""old-style"" (not Eager) tensors as layer weights through, _e.g._, a boolean option, so that in the settings when accessing those weights (as Eager tensors) is not required (which, I believe, is a majority of cases, especially when some keras methods allow to effectively pull out the weights as numpy arrays), no overhead would be implied by a (seemingly) useless Eager declaration.

Of course, I might be missing an existing feature allowing to do so, in which case I would be most glad to be pointed to a way to optimize run times when Eager execution is enabled (maybe by enforcing the isolation of the operations in a compiled graph?).

**Code to reproduce the issue**
```
# coding: utf-8

""""""Test script to measure runtime performances on mock data.

Set up the EAGER constant on line 18 to toggle the use of
Eager execution (provided it is not enabled by default,
otherwise change the instructions on lines 31 and 33).
Then, call `python3 <this_script.py>` to run it.
""""""

import os
import time

import numpy as np
import tensorflow as tf


EAGER = False


def main(eager):
    """"""Set up and fit a model on mock data, with or without Eager execution.

    The model consists of a layer of 100 LSTM cells topped with
    a single dense unit with sigmoid activation. Its fitting is
    bound not to yield actual accuracy improvements, as the data
    used is purely random, but aims at measuring performances as
    to runtime.
    """"""
    if eager:
        tf.enable_v2_behavior()
    else:
        tf.enable_resource_variables()
    # Set up the classifier model using custom embedding units.
    inputs = tf.keras.Input((None, 100), dtype=tf.float32)
    lengths = tf.keras.Input((), dtype=tf.int64)
    mask = tf.sequence_mask(lengths)
    output = tf.keras.layers.LSTM(100)(inputs, mask=mask)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)
    model = tf.keras.Model([inputs, lengths], output)
    # Set up the training and validation data.
    dataset = setup_mock_dataset()
    train, valid = dataset.take(500), dataset.skip(500)
    # Fit the model.
    model.compile('adam', 'binary_crossentropy', ['accuracy'])
    history = model.fit(
        train.repeat(), steps_per_epoch=500, epochs=10,
        validation_data=valid.repeat(), validation_steps=100
    )


def setup_mock_dataset(n_batches=600, batch_size=32):
    """"""Return a tf.data.Dataset yielding batches of random data.

    The input data consists of a couple of tensors, one with
    zero-padded sequences of vectors of size 100, the other
    with the true sequence lengths. The target data consists
    of sequence-wise binary values.
    """"""
    # Generate some random (mock) input and target data.
    n_samples = n_batches * batch_size
    lengths = 1 + np.random.choice(100, size=n_samples, replace=True)
    inputs = np.random.normal(size=(lengths.sum(), 100))
    targets = np.random.choice(2, size=(n_samples, 1), replace=True)
    # Set up a generator yielding shuffled training samples.
    def generator():
        """"""Yield individual training samples.""""""
        nonlocal inputs, targets, lengths, n_samples
        start = 0
        for i in range(n_samples):
            end = start + lengths[i]
            yield (inputs[start:end], lengths[i]), targets[i]
            start = end
    # Set up a tensorflow Dataset based on the previous.
    output_shapes = (
        (tf.TensorShape((None, 100)), tf.TensorShape(())), tf.TensorShape(1)
    )
    dataset = tf.data.Dataset.from_generator(
        generator, ((tf.float32, tf.int64), tf.int64), output_shapes
    )
    # Have the dataset output data batches, and return it.
    return dataset.padded_batch(batch_size, output_shapes)


if __name__ == '__main__':
    start = time.clock()
    main(EAGER)
    duration = time.clock() - start
    min, sec = duration // 60, duration % 60
    print('TOTAL DURATION: %im%i' % (min, sec))

```

**Additional logs**


Here are the script's print outs:

* With Eager execution enabled:

```
Epoch 1/10
2019-07-10 15:58:59.009116: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
500/500 [==============================] - 83s 165ms/step - loss: 0.6983 - accuracy: 0.5009 - val_loss: 0.6990 - val_accuracy: 0.5034
Epoch 2/10
500/500 [==============================] - 72s 144ms/step - loss: 0.6712 - accuracy: 0.5904 - val_loss: 0.7125 - val_accuracy: 0.4931
Epoch 3/10
500/500 [==============================] - 72s 143ms/step - loss: 0.6056 - accuracy: 0.6841 - val_loss: 0.7604 - val_accuracy: 0.4947
Epoch 4/10
500/500 [==============================] - 72s 144ms/step - loss: 0.4445 - accuracy: 0.8073 - val_loss: 0.9025 - val_accuracy: 0.5031
Epoch 5/10
500/500 [==============================] - 72s 145ms/step - loss: 0.2502 - accuracy: 0.9169 - val_loss: 1.1817 - val_accuracy: 0.5069
Epoch 6/10
500/500 [==============================] - 72s 144ms/step - loss: 0.1250 - accuracy: 0.9699 - val_loss: 1.4860 - val_accuracy: 0.5091
Epoch 7/10
500/500 [==============================] - 72s 145ms/step - loss: 0.0724 - accuracy: 0.9859 - val_loss: 1.6498 - val_accuracy: 0.5047
Epoch 8/10
500/500 [==============================] - 72s 143ms/step - loss: 0.0537 - accuracy: 0.9891 - val_loss: 1.7936 - val_accuracy: 0.5038
Epoch 9/10
500/500 [==============================] - 72s 143ms/step - loss: 0.0351 - accuracy: 0.9948 - val_loss: 1.8995 - val_accuracy: 0.5066
Epoch 10/10
500/500 [==============================] - 72s 144ms/step - loss: 0.0297 - accuracy: 0.9949 - val_loss: 2.0393 - val_accuracy: 0.5041
TOTAL DURATION: 19m17
```

* Without enabling Eager execution:

```
Epoch 1/10
500/500 [==============================] - 46s 91ms/step - loss: 0.6989 - acc: 0.5006 - val_loss: 0.6999 - val_acc: 0.4894
Epoch 2/10
500/500 [==============================] - 45s 91ms/step - loss: 0.6708 - acc: 0.5932 - val_loss: 0.7138 - val_acc: 0.4875
Epoch 3/10
500/500 [==============================] - 45s 90ms/step - loss: 0.6048 - acc: 0.6849 - val_loss: 0.7705 - val_acc: 0.4975
Epoch 4/10
500/500 [==============================] - 45s 90ms/step - loss: 0.4453 - acc: 0.8104 - val_loss: 0.9408 - val_acc: 0.4950
Epoch 5/10
500/500 [==============================] - 46s 92ms/step - loss: 0.2518 - acc: 0.9169 - val_loss: 1.2647 - val_acc: 0.5009
Epoch 6/10
500/500 [==============================] - 46s 91ms/step - loss: 0.1206 - acc: 0.9726 - val_loss: 1.5996 - val_acc: 0.4994
Epoch 7/10
500/500 [==============================] - 45s 91ms/step - loss: 0.0766 - acc: 0.9839 - val_loss: 1.8580 - val_acc: 0.4975
Epoch 8/10
500/500 [==============================] - 46s 91ms/step - loss: 0.0569 - acc: 0.9884 - val_loss: 1.9623 - val_acc: 0.4975
Epoch 9/10
500/500 [==============================] - 46s 92ms/step - loss: 0.0476 - acc: 0.9897 - val_loss: 2.0733 - val_acc: 0.4966
Epoch 10/10
500/500 [==============================] - 46s 91ms/step - loss: 0.0316 - acc: 0.9946 - val_loss: 2.2490 - val_acc: 0.5044
TOTAL DURATION: 13m49
```"
30560,TF2 Nightly-20190710 broken import,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04/19.10
- TensorFlow installed from (source or binary): Binary - **tf2-nightly-2.0-preview**
- TensorFlow version (use command below): 2.0 preview
- Python version: python3.6 (probably all others)
**Describe the current behavior**

**Describe the expected behavior**
Able to import TensorFlow

**Code to reproduce the issue**
```
pip install tf-nightly-2.0-preview
python -c ""import tensorflow as tf""
```

Results in:
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/sean-morgan/miniconda3/envs/tf-broken/lib/python3.6/site-packages/tensorflow/__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""/home/sean-morgan/miniconda3/envs/tf-broken/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 363, in <module>
    _API_MODULE = sys.modules[__name__].bitwise  # pylint: disable=undefined-variable
NameError: name 'sys' is not defined
```"
30557,Padded_batch in TF2.0 throws DataLossError: Attempted to pad to a smaller size than the input element. [Op:IteratorGetNextSync],"I am following [this tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) and exactly [here](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) in
`train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))`, it uses the length of the longest sentence in the dataset (which is 16). I want to pad the sentences further to length 8. So, after replacing this 
> **-1**  with **8**
in the line above, it throws the error 

> DataLossError: Attempted to pad to a smaller size than the input element. [Op:IteratorGetNextSync]
on executing `sample_text, sample_labels = next(iter(test_data))

sample_text, sample_labels or on fit method.
I am not sure if this is a bug or a normal behaviour. I want to achieve sentences with shorter paddings. If anyone can help me, I will be very thankful. 
It can be easily done with `tf.keras.preprocessing.sequence.pad_sequences()`but not sure about tf.Data in TF2.0. 

"
30556,Which Bazel versions are okay for Tensorflow source install?,"

## URL(s) with the issue:

https://www.tensorflow.org/install/source_windows


## Description of issue (what needs changing):

It is purely a logical issue:
Install Bazel 0.24.1, the build tool used to compile TensorFlow. Set up Bazel to build C++.
[...] Ensure you install Bazel 0.23.0 or lower.

0.24.1 > 0.23.0 
"
30555,Trying to create frozen graph with freeze_graph: IndexError: list index out of range,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Docker image
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15+
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti

I am trying to freeze a custom keras model (simple XOR) with the help of the freeze_graph script for later usage with the Coral TPU Stick. After a week of testing or so, I still can't get it to work because I am running into an error, caused by freeze_graph.

I think I am following the instructions correctly on creating a graph, so the script should work and produce a frozen graph. I am using the official `tensorflow:tensorflow:gpu-latest` docker image.

**Code**

With the following script, I create the graph in keras and save the quantized training graph.

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

training_data = np.array([[0,0],[0,1],[1,0],[1,1]], ""uint8"")
target_data = np.array([[0],[1],[1],[0]], ""uint8"")

model = Sequential()
model.add(Dense(16, input_dim=2, use_bias=False, activation='relu'))
model.add(Dense(1, use_bias=False, activation='sigmoid'))

session = tf.keras.backend.get_session()
tf.contrib.quantize.create_training_graph(session.graph)
session.run(tf.global_variables_initializer())

model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['binary_accuracy'])

model.fit(training_data, target_data, nb_epoch=1000, verbose=2)
print(model.predict(training_data).round())
model.summary()

saver = tf.train.Saver()
saver.save(keras.backend.get_session(), 'xor-keras.ckpt')

```

Then I load the graph and convert it into an eval graph with a second script:

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(16, input_dim=2, use_bias=False, activation='relu'))
model.add(Dense(1, use_bias=False, activation='sigmoid')) 


 #<Load the model into a new session>

session = tf.keras.backend.get_session()

saver = tf.train.Saver()
saver.restore(session, 'xor-keras.ckpt')

tf.contrib.quantize.create_eval_graph(session.graph)

tf.io.write_graph(session.graph, '.', 'xor-keras-eval.pb', as_text=False)
```

In my opinion, this should be valid input for the script, which I execute with the command:

`freeze_graph --input_graph='xor-keras-eval.pb' --input_checkpoint='xor-keras.ckpt' --output_graph='xor-keras-frozen.pb' --output_node_name='dense_1/Sigmoid' --input_binary=True`

This throws an error. Here the complete error traceback:

```
Traceback (most recent call last):
  File ""/usr/local/bin/freeze_graph"", line 10, in <module>
    sys.exit(run_main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 487, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 486, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 378, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 361, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py"", line 190, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 482, in _build_internal
    names_to_saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 342, in validate_and_slice_inputs
    for converted_saveable_object in saveable_objects_for_op(op, name):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 205, in saveable_objects_for_op
    variable, """", name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 82, in __init__
    self.handle_op = var.op.inputs[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2410, in __getitem__
    return self._inputs[i]
IndexError: list index out of range
```

I have tried executing my scripts with a custom docker image with tensorflow 1.13.1 and Python 3.6.8, but with the same result. I also found other threads with similar/the same issue #20886 and #24591, but none of the proposed solutions worked. I am quite confident that this is a bug, and want you to verify that I am doing it correctly."
30554,Can't set an initial state of the tf.keras.layers.Bidirectional,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: macOS 10.14.15 Darwin-18.6.0-x86_64-i386-64bit
- TensorFlow installed from: binary
- TensorFlow version: 1.14.0
- Python version: 3.6.0

**Describe the current behavior**

`tf.keras.layers.Bidirectional` wrapped around a `tf.keras.layers.LSTM` instance raises a `TypeError` when trying to pass it an initial state (through the `initial_state` argument of the `__call__`).

**Describe the expected behavior**

No error is raised and the (joint) initial state is properly distributed by the `Bidirectional` between the underlying forward and backward `LSTM` instances.

**Code to reproduce the issue**

```python
import tensorflow as tf

lstm = tf.keras.layers.LSTM(units=10)
bidirectional = tf.keras.layers.Bidirectional(lstm)

inputs = tf.placeholder(tf.float32, shape=[32, 15, 20])

fw_state = [tf.zeros([32, 10]), tf.zeros([32, 10])]
bw_state = [tf.zeros([32, 10]), tf.zeros([32, 10])]
initial_state = fw_state + bw_state

output = bidirectional(
    inputs=inputs,
    initial_state=initial_state)
```

**Other info / logs**

```
Traceback (most recent call last):
  File "".../test.py"", line 14, in <module>
    initial_state=initial_state)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 592, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 634, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 629, in call
    initial_state=forward_state, **kwargs)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2533, in call
    inputs, mask=mask, training=training, initial_state=initial_state)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 678, in call
    inputs, initial_state, constants)
  File "".../lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 787, in _process_inputs
    if len(initial_state) != len(self.states):
TypeError: object of type 'Tensor' has no len()
```

Similar issue https://github.com/tensorflow/tensorflow/issues/28761 is resolved in the tensorflow 2.0 code. The corresponding code fix in https://github.com/tensorflow/tensorflow/commit/2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e seems to be relevant for this issue in tensorflow 1.x, too."
30552,"VS linkage fails with ""unresolved external symbol""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: git tag v2.0.0_beta1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel release 0.24.1
- GCC/Compiler version (if compiling from source): ""C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC//bin/amd64/cl.exe""
- Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64
- CUDA/cuDNN version: 10.1/7.6.1
- GPU model and memory: GeForce GTX 1060 (6GB)



**Describe the problem**
Can't link the code. I get this error:
1>a.obj : error LNK2001: unresolved external symbol ""public: int __cdecl google::protobuf::RepeatedPtrField<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::size(void)const "" (?size@?$RepeatedPtrField@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@protobuf@google@@QEBAHXZ)
1>a.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::SessionOptions::SessionOptions(void)"" (??0SessionOptions@tensorflow@@QEAA@XZ)
1>a.obj : error LNK2001: unresolved external symbol ""class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)"" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z)
1>b.obj : error LNK2001: unresolved external symbol ""public: bool __cdecl google::protobuf::MessageLite::ParseFromArray(void const *,int)"" (?ParseFromArray@MessageLite@protobuf@google@@QEAA_NPEBXH@Z)

I checked with dumpbin and didn't find this symbols/exports.
I also tried to patch missing symbols, and it didn't help (https://github.com/guikarist/tensorflow-windows-build-script/blob/master/patches/tf_exported_symbols_msvc.lds)


**Provide the exact sequence of commands / steps that you executed before running into the problem**
I built a tensorflow.dll & tensorflow.lib using this command:
bazel build --config=opt //tensorflow:tensorflow_dll_import_lib

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30551,RuntimeError: Collective ops must be configured at program startup,"Hi,

I am trying to run mnist example using distributed training tf 2.0.I am getting below similar error. I request to help me with workaround for this error.

tf.distribute.experimental.CollectiveCommunication.NCCL

File ""/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 80, in init
communication=communication))
File ""/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 110, in init
self._initialize_strategy(cluster_resolver)
File ""/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 116, in _initialize_strategy
self._initialize_multi_worker(cluster_resolver)
File ""/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 208, in _initialize_multi_worker
device_filters=(""/job:%s/task:%d"" % (task_type, task_id),))
File ""/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/eager/context.py"", line 541, in configure_collective_ops
raise RuntimeError(""Collective ops must be configured at program startup"")
RuntimeError: Collective ops must be configured at program startup"
30548,init_from_checkpoint performing incorrectly,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GTX Titan X

**Describe the current behavior**
I have a variable stored in a ckpt called prsr/prior/conv1/weights and i want to do a map assignment to a variable called prsr/prior/conv1/weights_1:0. When I execute the init_from_checkpoint execution it gives the following error 

> ValueError: Assignment map with scope only name prsr/prior/conv1 should map to scope only prsr/prior/conv1/weights. Should be 'scope/': 'other_scope/'.

**Describe the expected behavior**
I think it should assigned correctly the variable from the .ckpt prsr/prior/conv1/weights into the current variable prsr/prior/conv1/weights_1:0 as indicated in the API official documentation:

```
# Initialize partitioned variables using variable's name
init_from_checkpoint('/tmp/model.ckpt',
                     {'old_scope_2/var3': 'new_scope_2/var3'})
```
"
30545,MSVC compiler gives warning C4190 when include Tensorflow header,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
No
- TensorFlow version (use command below):
TensorFlow 1.10
- Python version:
None
- Bazel version (if compiling from source):
None
- GCC/Compiler version (if compiling from source):
None
- CUDA/cuDNN version:
CUDA 10.0 / cuDNN 7.5
- GPU model and memory:
GTX 1070

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When include tensorflow header to use the C API in my project, it always gives me warnings warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C. It is a bug introduced by the Tensorflow C API header.

**Describe the expected behavior**
No warnings should be come from the Tensorflow header. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30539,Error importing tensorflow on windows 10 - OSError: [WinError 126] The specified module could not be found,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): cmd 
- TensorFlow GPU  version:  1.14.0
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.1, 7.6.0
- GPU model and memory: GTX 970 8GB RAM

Hey i've recently tried to installed tensorflow-gpu and before that i made sure that I have all Environment variables set up cuDNN, CUDA, drivers. 
eventually i installed tensorflow-gpu using pip (pip install --ignore-installed -- upgrade tensorflow-gpu

after the Installation I tried importing tensorflow as tf and It came up with this error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
>>> import tensorflow as tf
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\ירין\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive

**it says it cannot find 'cudart64_100.dll' but I have in my cuda folder the following dll 'cudnn64_7'**

Here is a pic of my Environment variables : https://imgur.com/a/UXub3LR"
30537,Colocation error with SparseApply* ops and ResourceVariables,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0/7.4.1
- GPU model and memory: GeForce GTX 980 Ti

**Describe the current behavior**

TensorFlow attempts to colocate ops with incompatible devices (when the ops are related to `SparseApply*`, e.g. `SparseApplyRMSProp`, operations), which results in an error.  This only occurs when using ResourceVariables (not RefVariables).

**Describe the expected behavior**

There should be no error, and ops should be assigned to their appropriate devices (CPU vs GPU).

**Code to reproduce the issue**
``` python
import tensorflow as tf
import numpy as np

do_error = True

with tf.Graph().as_default() as graph:
    my_var = tf.Variable(np.ones(5), use_resource=True)
    with tf.device(""/gpu:0"" if do_error else None):
        gather = tf.gather(my_var, [0, 2, 4])
    opt_op = tf.train.MomentumOptimizer(0.1, 0.1).minimize(gather)

with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(opt_op)
```

**Other info / logs**
Colocation debug info:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Variable/IsInitialized/VarIsInitializedOp: Could not satisfy explicit device specification '' because the node node Variable/IsInitialized/VarIsInitializedOp (defined at tmp.py:7) placed on device No device assignments were active during op 'Variable/IsInitiali
zed/VarIsInitializedOp' creation.  was colocated with a group of nodes that required incompatible device '/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0].
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]
Const: GPU CPU
VarHandleOp: GPU CPU
AssignVariableOp: GPU CPU
VarIsInitializedOp: GPU CPU
ResourceGather: GPU CPU
ReadVariableOp: GPU CPU
StridedSlice: GPU CPU
Unique: GPU CPU
Shape: GPU CPU
UnsortedSegmentSum: GPU CPU
Cast: GPU CPU
ResourceSparseApplyMomentum: CPU

Colocation members, user-requested devices, and framework assigned devices, if any:
  Variable/Initializer/initial_value (Const)
  Variable (VarHandleOp)
  Variable/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)
  Variable/Assign (AssignVariableOp)
  Variable/Read/ReadVariableOp (ReadVariableOp)
  Gather (ResourceGather) /device:GPU:0
  Variable/Momentum/Initializer/zeros (Const)
  Variable/Momentum (VarHandleOp)
  Variable/Momentum/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)
  Variable/Momentum/Assign (AssignVariableOp)
  Variable/Momentum/Read/ReadVariableOp (ReadVariableOp)
  Momentum/update_Variable/Unique (Unique)
  Momentum/update_Variable/Shape (Shape)
  Momentum/update_Variable/strided_slice/stack (Const)
  Momentum/update_Variable/strided_slice/stack_1 (Const)
  Momentum/update_Variable/strided_slice/stack_2 (Const)
  Momentum/update_Variable/strided_slice (StridedSlice)
  Momentum/update_Variable/UnsortedSegmentSum (UnsortedSegmentSum)
  Momentum/update_Variable/Cast (Cast)
  Momentum/update_Variable/Cast_1 (Cast)
  Momentum/update_Variable/ResourceSparseApplyMomentum (ResourceSparseApplyMomentum)

         [[node Variable/IsInitialized/VarIsInitializedOp (defined at tmp.py:7) ]]Additional information about colocations:No node-device colocations were active during op 'Variable/IsInitialized/VarIsInitializedOp' creation.
No device assignments were active during op 'Variable/IsInitialized/VarIsInitializedOp' creation.
```"
30536,Transformer training error with evaluation/test dataset newstest2014.de ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (only changed batch/filter size on modelparams.py)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): https://hub.docker.com/r/tensorflow/tensorflow nightly-gpu-py3
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA RTX 8000 x2 (wNVLINK)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When following the transformer [walkthrough](https://github.com/tensorflow/models/tree/master/official/transformer#detailed-instructions) There seems to be an issue with the newstest2014.de 'Download and preprocess datasets' step the newstest2014.de doesn't appear to be in valid text format. When its unpacked it cannot be opened as a text file (looks like another archive file) as the newstest2014.en can and was in previous versions. 

For example: when I run the following command (after completing the rest of the requisite steps): 

_python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET --bleu_source=$DATA_DIR/newstest2014.en --bleu_ref=$DATA_DIR/newstest2014.de --train_steps=250000 --steps_between_evals=10000 --export_dir=$EXPORT_DIR_ 

I get the error ( see attached/below for traceback) **UnicodeDecodeError: 'utf8' codec can't decode byte 0x8b in position 1: invalid start byte**

**Describe the expected behavior**
compute_bleu.py should work, but does not (only when i replace newstest2014.de with an older version (text format)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Follow the example walkthrough, you'll get invalid newstest2014.de (as of  7/8/19)

**Other info / logs** see attached for traceback as well
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

As it currently stands, training only worked when i replaced the invalid newstest2014.de file with the older version, therefore i think thats the error.

I0708 18:55:13.764734 140267571169088 translate.py:133] Writing to file /tmp/tmpie7053oh
Traceback (most recent call last):
  File ""transformer_main.py"", line 670, in <module>
    absl_app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""transformer_main.py"", line 664, in main
    run_transformer(flags.FLAGS)
  File ""transformer_main.py"", line 644, in run_transformer
    vocab_file=flags_obj.vocab_file)
  File ""transformer_main.py"", line 361, in run_loop
    estimator, bleu_source, bleu_ref, vocab_file)
  File ""transformer_main.py"", line 238, in evaluate_and_log_bleu
    estimator, subtokenizer, bleu_source, bleu_ref)
  File ""transformer_main.py"", line 222, in translate_and_compute_bleu
    uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)
  File ""/datasets/datasets/models/official/transformer/compute_bleu.py"", line 91, in bleu_wrapper
    tf.io.gfile.GFile(ref_filename).read()).strip().splitlines()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py"", line 128, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py"", line 98, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py"", line 117, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py"", line 87, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte

 
[traceback1.txt](https://github.com/tensorflow/tensorflow/files/3373946/traceback1.txt)
"
30535,Argument to force CuDNN implementation for tf.keras.layers.LSTM,"**System information**
- TensorFlow version (you are using): 2 beta1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Boolean `force_cudnn` argument for tf.keras.layers.LSTM. When True, would raise an error if the user's configurations or usage (masking) are incompatible with the CuDNN implementation. 

Currently, the LSTM layer logic dynamically uses the CuDNN implementation depending on the given arguments to the `__init__` and `call` functions. It does not report which it ends up using.

**Will this change the current api? How?**

This will add an argument to the tf.keras.layers.LSTM `__init__` function.

**Who will benefit with this feature?**

Anyone for whom the CuDNN implementation is both essential and viable and who is debugging their architecture for this reason. For example... ""My model is very slow, but I haven't run it before. I wonder if the LSTM implementation is optimized for GPU? I wish I could verify that it is."""
30534,[TF 2.0] Error using VGG-based loss with keras model.compile,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: 3.6
- CUDA/cuDNN version: cudatoolkit 10.0.130, cudnn 7.6.0
- GPU model and memory: NVIDIA GeForce GTX 1080 Ti

**Describe the current behavior**
The following error (full traceback below) is observed during the  keras compile call when using a pretrained VGG model in my loss function.

> AttributeError: 'Tensor' object has no attribute '_cpu_nograd'

This error only occurs in TF 2.0, and does not occur in TF 1.14.

It is worth noting that no error occurs when using a similar loss function in non-eager execution as in https://www.tensorflow.org/beta/tutorials/generative/style_transfer
 
**Code to reproduce the issue**

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import models, optimizers, metrics
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.applications.vgg19 import preprocess_input
import numpy as np

#Build Simple Model
inputs = keras.Input(shape=(32,32,3))
x = Conv2D(16, 3, padding = 'same', activation='relu')(inputs)
x = Conv2D(16, 3, padding = 'same', activation='relu')(x)
o = Conv2D( 3, 3, padding = 'same', activation='relu')(x)
model = keras.Model(inputs=inputs, outputs=o)

#Generate dummy data
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_train = x_train.astype('float32') / 255.
y_train = x_train.copy()
x_train = x_train+0.1*np.random.randn(*x_train.shape).astype('f')
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(128)

#Load VGG model
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape = [32,32,3])
vgg.trainable = False
content_layers = 'block5_conv2'

lossModel = models.Model([vgg.input], vgg.get_layer(content_layers).output, name = 'vggL')

def lossVGG(X,Y):
    Xt = preprocess_input(X*255)
    Yt = preprocess_input(Y*255)
    vggX = lossModel(Xt)
    vggY = lossModel(Yt)
    return tf.reduce_mean(tf.square(vggY-vggX))

optimizer = optimizers.Adam(learning_rate=0.0005)
loss = [lossVGG]
model.compile(optimizer = optimizer, loss = loss)
history = model.fit(train_ds, epochs = 20)
```

**Other info / logs**

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-6822dc73d458> in <module>
     40 optimizer = optimizers.Adam(learning_rate=0.0005)
     41 loss = [lossVGG]
---> 42 model.compile(optimizer = optimizer, loss = loss)
     43 history = model.fit(train_ds, epochs = 20)

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    335 
    336       # Creates the model loss and weighted metrics sub-graphs.
--> 337       self._compile_weights_loss_and_weighted_metrics()
    338 
    339       # Functions for train, test and predict will

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--> 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)
   1492       #                   loss_weight_2 * output_2_loss_fn(...) +
   1493       #                   layer losses.
-> 1494       self.total_loss = self._prepare_total_loss(masks)
   1495 
   1496   def _prepare_skip_target_masks(self):

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\training.py in _prepare_total_loss(self, masks)
   1552 
   1553           if hasattr(loss_fn, 'reduction'):
-> 1554             per_sample_losses = loss_fn.call(y_true, y_pred)
   1555             weighted_losses = losses_utils.compute_weighted_loss(
   1556                 per_sample_losses,

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\losses.py in call(self, y_true, y_pred)
    213       Loss values per sample.
    214     """"""
--> 215     return self.fn(y_true, y_pred, **self._fn_kwargs)
    216 
    217   def get_config(self):

<ipython-input-6-6822dc73d458> in lossVGG(X, Y)
     34 
     35     vggX = lossModel(Xt)
---> 36     vggY = lossModel(Yt)
     37 
     38     return tf.reduce_mean(tf.square(vggY-vggX))

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    594     # framework.
    595     if build_graph and base_layer_utils.needs_keras_history(inputs):
--> 596       base_layer_utils.create_keras_history(inputs)
    597 
    598     # Clear eager losses on top level model call.

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py in create_keras_history(tensors)
    197     keras_tensors: The Tensors found that came from a Keras Layer.
    198   """"""
--> 199   _, created_layers = _create_keras_history_helper(tensors, set(), [])
    200   return created_layers
    201 

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    241             constants[i] = op_input
    242           else:
--> 243             constants[i] = backend.function([], op_input)([])
    244       processed_ops, created_layers = _create_keras_history_helper(
    245           layer_inputs, processed_ops, created_layers)

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\backend.py in __call__(self, inputs)
   3517     # (otherwise it's just a no-op.)
   3518     return nest.pack_sequence_as(
-> 3519         self._outputs_structure, [x._cpu_nograd()._numpy() for x in outputs],  # pylint: disable=protected-access
   3520         expand_composites=True)
   3521 

~\AppData\Local\Continuum\miniconda3\envs\tensor2\lib\site-packages\tensorflow\python\keras\backend.py in <listcomp>(.0)
   3517     # (otherwise it's just a no-op.)
   3518     return nest.pack_sequence_as(
-> 3519         self._outputs_structure, [x._cpu_nograd()._numpy() for x in outputs],  # pylint: disable=protected-access
   3520         expand_composites=True)
   3521 

AttributeError: 'Tensor' object has no attribute '_cpu_nograd'
```"
30533,"[TF2.0] trainable=True on tf.keras.Embedding result in ""constant folding failed: Invalid argument: Unsupported type: 21""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: 3.7.3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: CUDA 10.0.130_411.31 / cuDNN 7.5.1.10
- GPU model and memory: GTX 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When enabling training on Embedding layer using keras subclassed model, an error is raised during model.fit():

`2019-07-09 11:00:33.231719: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`

This has a huge impact on training speed, that scales with the amount of data, in my full code, each epoch take more than 1 minute to complete(dataset of 869 tokenized strings, for train an encoder/decoder model), the code for reproduce this error dont have a noticeable impact because there is only one sample, already tokenized.

The error only is showed when an Embedding layer with trainable=True, is followed by an LSTM/GRU layer

**Describe the expected behavior**
LSTM/GRU supporting zero masking, and having no impact of training speed

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import numpy as np

class lstm(tf.keras.Model):
    def __init__(self):
        super(lstm, self).__init__()

        self.embedding = tf.keras.layers.Embedding(300, 2, mask_zero=True, trainable=True)
        self.encoder = tf.keras.layers.LSTM(2, return_sequences=True, return_state=False)

    def call(self, inputs):
        output = self.embedding(inputs)
        output = self.encoder(output)
        return output[0]

input_questions = np.array([[5, 12, 13, 189, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
output = np.array([[-0.00299482, 0.00096033]])

dataset = tf.data.Dataset.from_tensor_slices((input_questions, output)).shuffle(2).batch(1)

model = lstm()
model.compile(tf.keras.optimizers.Adadelta(1.0), tf.keras.losses.MeanSquaredError())
model.fit(dataset, epochs=10, verbose=2)
for sample, target in dataset.take(1):
    model(sample)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

What the minimal code example above show in the console for me

```
2019-07-09 11:10:25.468004: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll                                                                                                      2019-07-09 11:10:26.569016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:    name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733                                                    pciBusID: 0000:01:00.0                                                                                                  2019-07-09 11:10:26.588454: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                                                                                    2019-07-09 11:10:26.604012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0      2019-07-09 11:10:26.619218: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2                                                                       2019-07-09 11:10:26.643472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:    name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733                                                    pciBusID: 0000:01:00.0                                                                                                  2019-07-09 11:10:26.670895: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                                                                                    2019-07-09 11:10:26.688869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0      2019-07-09 11:10:28.438683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                            2019-07-09 11:10:28.466078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0                             2019-07-09 11:10:28.480348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N                             2019-07-09 11:10:28.500656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4712 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)  
                                                                              
Epoch 1/10                                                                                                             

WARNING: Logging before flag parsing goes to stderr.                                                                    
W0709 11:10:30.293739 15940 deprecation.py:323] From C:\ProgramData\Anaconda3\envs\tf2.0-gpu\lib\site-packages\tensorflow\python\keras\backend.py:3868: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.                                                                             
Instructions for updating:                                                                                              
Use tf.where in 2.0, which has the same broadcast rule as np.where                                                      

2019-07-09 11:10:38.997986: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21                                                                                        

2019-07-09 11:10:39.653444: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21      
                                                                                  
1/1 - 13s - loss: 1.5416e-05                                                                                            
Epoch 2/10                                                                                                              
1/1 - 1s - loss: 1.1537e-05                                                                                             
Epoch 3/10                                                                                                              
1/1 - 1s - loss: 8.7570e-06                                                                                             
Epoch 4/10                                                                                                              
1/1 - 1s - loss: 6.8737e-06                                                                                             
Epoch 5/10                                                                                                              
1/1 - 1s - loss: 5.6433e-06                                                                                             
Epoch 6/10                                                                                                              
1/1 - 1s - loss: 4.8554e-06                                                                                             
Epoch 7/10                                                                                                              
1/1 - 1s - loss: 4.3538e-06                                                                                             
Epoch 8/10                                                                                                              
1/1 - 1s - loss: 4.0314e-06                                                                                             
Epoch 9/10                                                                                                              
1/1 - 1s - loss: 3.8191e-06                                                                                             
Epoch 10/10                                                                                                             
1/1 - 1s - loss: 3.6735e-06                                                                                             

Press any key to continue . . .  
```

Edit 01: i think is somewhat ralated to [this](https://github.com/tensorflow/tensorflow/issues/29525#issuecomment-509154149) on https://github.com/tensorflow/tensorflow/issues/29525"
30532,How can i make sure the hash is consistent when saving the same model?,"I'm using the Keras model.save('my_model.h5') function. Every time i save it, the hash changes, even though no changes were made to the weights or settings etc. 

On this page: [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/save.py), it shows there's a KERAS_CACHE_KEY, is that the culprit? or is some other change/timestamp happening on save? 

I need to identify the specific model in the future and really just want to store it's hash for verification, but if i save the same model twice, i'll never know it was the same one. Can you please help point me in the right direction? "
30531,tf.keras becuase Tensorflow Version Update to 1.14.0 Not Running,"Tensorflow Version: 1.14.0
Exception : 
```
---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

<ipython-input-14-dcf9102c1887> in <module>()
----> 1 trainModel()

3 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __init__(self, cpu_model, strategy)
   1426           self._cpu_model.sample_weight_mode,
   1427           self._cpu_model._compile_weighted_metrics,
-> 1428           self._cpu_model.target_tensors,
   1429       )
   1430 

AttributeError: 'Model' object has no attribute 'target_tensors'
```
But My Code in Tensorflow Version 1.13.1 is Normal Working.

My Code:
```
···
	tuneModel = Model(inputs=base_model.input, outputs = output)
	tuneModel.compile(loss='sparse_categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])

···
```"
30530,Request for ComplexAbs and RFFT operations in tf.lite for Tensorflow 2.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0-dev20190709


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**
Full traceback:
```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-23-ddedc174c27b> in <module>
      4 #converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
      5 
----> 6 tflite_model = converter.convert()

~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)
    438         input_tensors=input_tensors,
    439         output_tensors=output_tensors,
--> 440         **converter_kwargs)
    441 
    442     if self._is_calibration_quantize():

~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    409   data = toco_convert_protos(model_flags.SerializeToString(),
    410                              toco_flags.SerializeToString(),
--> 411                              input_data.SerializeToString())
    412   return data
    413 

~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--> 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2019-07-09 16:34:24.077376: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT
2019-07-09 16:34:24.077408: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: ComplexAbs
2019-07-09 16:34:24.078122: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 47 operators, 85 arrays (0 quantized)
2019-07-09 16:34:24.078475: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 47 operators, 85 arrays (0 quantized)
2019-07-09 16:34:24.079060: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 70 arrays (0 quantized)
2019-07-09 16:34:24.079501: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 39 operators, 69 arrays (0 quantized)
2019-07-09 16:34:24.079910: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 39 operators, 69 arrays (0 quantized)
2019-07-09 16:34:24.080126: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 39 operators, 69 arrays (0 quantized)
2019-07-09 16:34:24.080490: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 384 bytes, theoretical optimal value: 192 bytes.
2019-07-09 16:34:24.080931: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.
Traceback (most recent call last):
  File ""/Users/ben/miniconda3/envs/wakeword/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.
```

I'm trying to convert a model with tf.lite and running into this error. There is another issue https://github.com/tensorflow/tensorflow/issues/27303 that requests the RFFT operator as well, but it seems to be for Tensorflow 1.x. There is a commit https://github.com/tensorflow/tensorflow/commit/c77e7e56de56c624116cf9eea340b4f96f032c85#diff-ed4b7d597384e8e4b1210b7558a16640 that whitelists the RFFT operation, however my conversion fails. Is this only implemented in Tensorflow 1.x right now?

I'm converting my model using this code:
```python
converter = tf.lite.TFLiteConverter.from_keras_model(test_model)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
```
I've tried using `converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]` and `converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]` but nothing changes."
30529,"How to know about the length of the data, TF2.0?","I have been following these two tutorials, [1](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) & [2](https://www.tensorflow.org/beta/tutorials/load_data/csv) in order to read a csv file to train a NN model. However, In [tutorial 1](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches), [here exactly](https://www.tensorflow.org/beta/tutorials/load_data/text#load_text_into_datasets), I did not understand how the author chose the values for **BUFFFER** and **TAKE**. It is not documented how to find the total number of examples/elements in the txt files because some file might consisting less or more examples/elements. 
Thank you!"
30528,tf.keras in TPU working callbacks call TensorBoard is ERROR,"**Tensorflow Version:** 1.13.1
**Working Platom:** colab TPU

**Use TensorBoard  Happen Exception：**
“AttributeError: 'TPUFunction' object has no attribute 'fetches'”

**Exception Detail：**
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)
   1139       # pylint: disable=protected-access
   1140       # add the histogram summary op if it should run this epoch
-> 1141       if self.merged not in self.model._eval_function.fetches:
   1142         self.model._eval_function.fetches.append(self.merged)
   1143         self.model._eval_function.fetch_callbacks[
```

**My Code Fragment：**

```
#TensorBoard
log_dir=""logs/fit/""+ datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_graph=True, write_images=True)

callbacks_list = [checkpoint,tensorboard_callback]

#Learning
history = tpu_model.fit_generator(generator=train_iter,epochs=100,validation_data=val_iter,callbacks=callbacks_list)
```
"
30527,"I want to connect two networks,but No gradients provided for any variable","I trained a network, and I want to add another network before the trained network

`with tf.name_scope('inNN'):
    T_lamb = tf.placeholder(tf.float32, [None, 2], name='inNN_inputs')
    Ts = tf.placeholder(tf.float32, [None, 1], name='T_labels')
    Ds = tf.placeholder(tf.float32, [None, 4], name='D_labels')
    DLs = tf.placeholder(tf.float32, [None, 5], name='D_lamb')

weight, bias = get_w_b()
D_logits_op = inNN(T_lamb, None)
prediction = fwNN(DLs, None, weight, bias)
with tf.variable_scope('T_loss'):
    T_loss_op = losses(prediction, Ts)
with tf.variable_scope('D_loss'):
    D_loss_op = losses(D_logits_op, Ds)
global_step = tf.Variable(0, trainable=0)

valid_op = evaluation(prediction, Ts)
with tf.variable_scope('train_step'):
    train_step_op = training(T_loss_op, LEARNING_RATE, global_step)  # error: No gradients provided for any variable, check your graph for ops that do not support gradients`

when I choose the D_loss_op in the training. The network works"
30526,"not show `raise ValueError` but `unsupported operand` error when using modular arithmetic as minus integer in TF 1.13.1, 1.14.0","1. tf 1.13.1
![image](https://user-images.githubusercontent.com/10525011/60887589-bc055580-a28f-11e9-8cfd-c08e960a227e.png)

2. tf1.14.0
![image](https://user-images.githubusercontent.com/10525011/60888239-4bf7cf00-a291-11e9-9f37-4c9781883e86.png)


3. tf2.0.0a0
![image](https://user-images.githubusercontent.com/10525011/60887966-a5133300-a290-11e9-9831-9470b875d5fe.png)

Why used `NotImplemented`in `__mod__` and `__rmod__` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L221? **It makes to be confused '%' operand is not working**

when people try modular arithmetic with minus integer, error with `raise ValueError(""Dimension %d must be >= 0"" % self._value)` is feel more good instead of error with `TypeError: unsupported operand type(s) for %:`

What else?
```python
  def __mod__(self, other):
    """"""Returns `self` modulo `other`.

    Dimension moduli are computed as follows:

    ```python
    tf.Dimension(m)    % tf.Dimension(n)    == tf.Dimension(m % n)
    tf.Dimension(m)    % tf.Dimension(None) == tf.Dimension(None)
    tf.Dimension(None) % tf.Dimension(n)    == tf.Dimension(None)
    tf.Dimension(None) % tf.Dimension(None) == tf.Dimension(None)
    ```

    Args:
      other: Another Dimension, or a value accepted by `as_dimension`.

    Returns:
      A Dimension whose value is `self` modulo `other`.
    """"""
    other = as_dimension(other)
    if self._value is None or other.value is None:
      return Dimension(None)
    else:
      return Dimension(self._value % other.value)
```
Thanks"
30525,how to apply quant to bert,"my bert is slow for prediction using cpu, can quant speed up its forward process? how to use?"
30524,Failed to convert ssdlite_mobilenet_v2_coco_2018_05_09 to tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colaboratory
- TensorFlow installed from (source or binary): idk
- TensorFlow version (or github SHA if from source): 1.14


**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------

ConverterError                            Traceback (most recent call last)

<ipython-input-5-fc0e59056dc1> in <module>()
----> 1 tflite_model = converter.convert()
      2 open(""converted_model.tflite"", ""wb"").write(tflite_model)

2 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    902           input_arrays_with_shape=self._input_arrays_with_shape,
    903           output_arrays=self._output_arrays,
--> 904           **converter_kwargs)
    905 
    906     if self._is_calibration_quantize():

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_graph_def(input_data, input_arrays_with_shape, output_arrays, *args, **kwargs)
    371   data = toco_convert_protos(model_flags.SerializeToString(),
    372                              toco_flags.SerializeToString(),
--> 373                              input_data.SerializeToString())
    374   return data
    375 

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--> 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2019-07-09 12:08:44.812119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess
2019-07-09 12:08:44.833564: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array ""detection_boxes"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)

```

Also, please include a link to a GraphDef or the model if possible:
https://drive.google.com/open?id=1HJ0bnrOok4Brd5IcB30HYn80itE4dH8_

**Any other info / logs**

I'm using model ssdlite_mobilenet_v2_coco_2018_05_09 from zoo. Have trained it and converted ckpt with command:
`export_tflite_ssd_graph.py --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-19780 --output_directory tflite_graph --add_postprocessing_op True`

I'm using Python API for conversion of model. My code:
```
import tensorflow as tf
graph_def_file = ""/content/tflite_graph.pb""
input_arrays = [""normalized_input_image_tensor""]
output_arrays = [""detection_boxes"", ""detection_classes"", ""detection_scores"", ""num_boxes""]

converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, 
  input_arrays, 
  output_arrays, 
  input_shapes={'normalized_input_image_tensor':[1, 300, 300, 3]}
  )
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```"
30523,Keras Tensorflow R  Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home/Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA = 10.1; cuDNN = 7.6
- GPU model and memory: Nvidia RTX 2060 6GB; RAM = 16GB; CPU = Intel I7 8750h 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Hi all,

I know this problem has arisen on here but I have yet to find a discussion around the problem with regards to R; all I see are python issues and solutions. In short I am using the Keras library with Tensorflow GPU as a backend in Rstudio for deep learning, specifically convnets. After a bit of trouble installing Cuda/cuDNN etc. I managed to get it all working, and installed Tensorflow-gpu in Anaconda (I am working within an Anaconda environment). This all worked fine and yesterday I was running my models on my GPU in R with no problems. 

Without making any changes whatsoever, I boot up today and run the same models and get an error telling me that cuDNN cannot initialise as Tensorflow could not create a cuDNN handle. I am not sure why this has emerged all of a sudden as I was having no problems yesterday.

**Describe the expected behavior**

As mentioned, yesterday the models were running perfectly fine and I expected the same to happen today as I have made no changes to my software/hardware in between.

**Code to reproduce the issue**

Any code that attempts to compile a Keras model produces this error. However I will provide below the exact code I used:

`img_height <- 32
img_width <- 32
channels <- 3

### Full convolutional model
model <- keras_model_sequential() %>%
  
  ### input layer
  layer_conv_2d(filters = 96,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"",
                input_shape = c(img_height, img_width, channels)) %>%
  layer_dropout(0.2) %>%

  ### hidden block 1
  layer_conv_2d(filters = 96,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"") %>%
  layer_conv_2d(filters = 96,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"",
                strides = 2) %>%
  layer_dropout(0.5) %>%
  
  ## hidden block 2
  layer_conv_2d(filters = 192,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"") %>%
  layer_conv_2d(filters = 192,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"") %>%
  layer_conv_2d(filters = 192,
                kernel_size = c(3,3),
                activation = ""relu"",
                padding = ""same"",
                strides = 2) %>%
  layer_dropout(0.5) %>%
  
  ### hidden block 3
  layer_conv_2d(filters = 192,
                kernel_size = c(3,3),
                padding = ""same"") %>%
  layer_activation(""relu"") %>%
  layer_conv_2d(filters = 192,
                kernel_size = c(1,1),
                padding = ""valid"") %>%
  layer_activation(""relu"") %>%
  layer_conv_2d(filters = 200,
                kernel_size = c(1,1),
                padding = ""valid"") %>%
  
  ### global pooling layer (in place of flattening)
  layer_global_average_pooling_2d() %>%
  
  ### softmax output (takes as input the filters from the last conv layer in place of FC layer)
  layer_activation(""softmax"")
  

### Compiler Architecture:

model %>% compile(
  optimizer = optimizer_adam(lr = 0.1), # high learning rate for large classes
  loss = ""categorical_crossentropy"",
  metrics = c(""accuracy"")
)

### view model architecture
model

### data augmentation
datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)

### validation data generator (should not be augmented)
validation_datagen = image_data_generator(
  rescale = 1/255
)

### define batch size 
batch_size <- 32

### define no. epochs
epochs <- 50

### define class mode
class_mode <- ""categorical""

### create generator for training data
train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(img_height, img_width),
  batch_size = batch_size,
  classes = classes,
  class_mode = class_mode
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(img_height, img_width),
  batch_size = (batch_size/9), # validation data is 1/9th of training so reduce the batch size
  classes = classes,
  class_mode = class_mode
)

### define no. training samples
train_samples <- train_generator$n # 7000 training data

### define no. validation samples
valid_samples <- validation_generator$n # 800 validation data

### fit the model to the data
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = as.integer(train_samples / batch_size),
  epochs = epochs,
  validation_data = validation_generator,
  validation_steps = as.integer(valid_samples / (batch_size/9))
)`

NB. This model is using the CUB200-2011 dataset.

**Other info / logs**
Full traceback of the issue is provided in the log below:

2019-07-09 10:56:45.085345: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-09 10:56:45.328872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.89GiB
2019-07-09 10:56:45.329507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-09 10:56:46.571437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-09 10:56:46.571725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-07-09 10:56:46.571847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-07-09 10:56:46.573101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4620 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-07-09 10:56:49.550878: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2019-07-09 10:56:49.551567: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
 Show Traceback
 
 Rerun with Debug
 Error in py_call_impl(callable, dots$args, dots$keywords) : 
  UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d_9/Conv2D}}]]
	 [[{{node ConstantFoldingCtrl/loss/activation_5_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0}}]] 

I have also attempted to run this on Ubuntu too in case the problem is related to Windows' compilation, with the exact same error:

2019-07-09 11:22:49.942666: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: 
SSE4.1 SSE4.2 AVX AVX2 FMA
2019-07-09 11:22:49.966646: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-07-09 11:22:49.967415: 
I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5611f27510e0 executing computations on platform Host. Devices:
2019-07-09 11:22:49.967431: 
I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-09 11:22:50.051695: 
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, 
so returning NUMA node zero
2019-07-09 11:22:50.051950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: 
GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2
pciBusID: 0000:01:00.0
totalMemory: 5.79GiB freeMemory: 5.43GiB
2019-07-09 11:22:50.051963: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-09 11:22:50.052384: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-09 11:22:50.052392: 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-07-09 11:22:50.052396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   
N 
2019-07-09 11:22:50.052445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5259 MB memory) 
-> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-07-09 11:22:50.053557: I tensorflow/compiler/xla/service/service.cc:150] 
XLA service 0x5611f28776a0 executing computations on platform CUDA. Devices:
2019-07-09 11:22:50.053569: I tensorflow/compiler/xla/service/service.cc:158]   
StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5
2019-07-09 11:22:54.658481: I tensorflow/stream_executor/dso_loader.cc:152] 
successfully opened CUDA library libcublas.so.10.0 locally
2019-07-09 11:23:28.099635: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] 
Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-07-09 11:23:28.114124: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] 
Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
 Show Traceback
 
 Rerun with Debug
 Error in py_call_impl(callable, dots$args, dots$keywords) : 
  
UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.

	 [[{{node conv2d/Conv2D}}]]
	 [[{{node loss/mul}}]] 

I have also provided an output of nvidia-smi to show the driver versions and installations of CUDA. Likewise, this also demonstrates how Tensorflow is detecting my GPU in Rstudio to show how the problem may not be related to that:

nvidia-smi
Tue Jul 09 11:05:43 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 425.25       Driver Version: 425.25       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   41C    P8     4W /  N/A |   5256MiB /  6144MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     13156    C+G   ...eam\bin\cef\cef.win7\steamwebhelper.exe N/A      |
|    0     16016      C   ...tensorflow_gpu\Library\bin\rsession.exe N/A      |
+-----------------------------------------------------------------------------+

Here I offer a more complete overview of the software used for this:

Software:
OS - Windows 10 Home/ Ubuntu 18.04 (Kabuntu)
GPU Drivers - 425.25 (Windows) /430.25 (Ubuntu)
Cuda - 10.1 (Windows) / 10.2 (Ubuntu)
Cudnn - 7.6 
TF - 1.13.1 (GPU version)
Keras - 2.2.4 
Python - 3.6.8 
Rstudio - 1.1.456
Base R - 3.6.1

I hope this is enough to help others understand my problem. As I said, I know many have reported this issue but I have yet to find a thread where the discussion centres around the R version of Keras rather than Python and would like to know if there is anything anyone knows of to help.
Thanks in advance :)
"
30521,Loading Keras model: TypeError: __init__() missing 1 required positional argument: 'fn',"- Installed with `pip install tensorflow-gpu`, and also tried with `pip install tensorflow` which yielded the same issue.
- tensorflow 1.14
- Keras-Applications 1.0.8 
- Keras-Preprocessing 1.1.0

After saving model with `tf.keras.models.save_model(model=self.predict_model, filepath=path)`, we try to load with `tf.keras.models.load_model(path, custom_objects={'tf': tf})` and it yields the error that I reported in the title.
What possible conflict of packages may I have? Or is this a known bug?
"
30515,Convert Tensorflow Model to Tensorflow Lite Format Model,"Convert Tensorflow Model to Tensorflow Lite Format [Conversion + Build]

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaconda Navigator
- TensorFlow version: 1.14.0
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: No 

I trained my custom model for tensorflow by following this tutorial
https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html
I made all the code work in macOS
I got the output with python code.

My query here is 
I need to run my model in unity through TensorflowLite Plugin
So, I have to convert my tensorflow model to tensorflow lite format
My model's size is 55.5Mb 
I need help, How to convert my model to tensorflow lite model 

"
30514,Operators not supported by the standard TensorFlow Lite runtime,"I tried a very small model that is as follows : 
model_input = Input(shape=(max_sequence_len,))
X = Embedding(num_words, word_embedding_dimensions, weights = [embedding_matrix], trainable = False)(model_input)
X = Flatten()(X)
X = Dense(1, activation='sigmoid')(X)
model = Model(inputs = model_input, outputs = X)
model.summary()

When converting my Keras model to tensorflow lite model I received message saying paste the following here - 

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.
Traceback (most recent call last):
  File ""C:\Users\udits\Anaconda3\Scripts\toco_from_protos-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\udits\Anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\udits\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:\Users\udits\Anaconda3\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.

"
30513,"TF2-gpu: tf.distribute cause crash when using RNN model,","**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from  binary:
- TensorFlow version (use command below): v1.12.1-5670-g718503b075 2.0.0-dev20190707
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA-10.0/ Cudnn7.6.1
- GPU model and memory: 3 Titan XP

tf.distribute cause crash when using RNN model, work fine when using CNN.

 if  ""with mirrored_strategy.scope():""  removed, The code below can work well.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

total_data_size = 10000
X = np.random.randint(100, size=(total_data_size, 100, 20)) / 100
X = X.astype(np.float32)
Y = np.random.randint(2, size=(total_data_size)).astype(
    np.int32)

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
dataset = dataset.batch(12)

mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(3, activation='sigmoid')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
    model.fit(dataset)
```

**logs**

Apply a constraint manually following the optimizer update step.
2019-07-09 12:01:15.720305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name: replica_1/sequential/lstm/StatefulPartitionedCall_replica_1/StatefulPartitionedCall_0 Expected: {forward_node_name}:{index}
2019-07-09 12:01:16.056015: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_8517_9019' and '__inference___backward_standard_lstm_8517_9019_specialized_for_replica_2_StatefulPartitionedCall_at___inference_distributed_function_9976' both implement 'lstm_e2ea6704-e320-4be8-b8e0-8ad71afc296b' but their signatures do not match.
2019-07-09 12:01:16.282647: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-09 12:01:16.325991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 12:01:16.940501: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-09 12:01:16.940501: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
2019-07-09 12:01:16.940560: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[{{node sequential/lstm/StatefulPartitionedCall}}]]
2019-07-09 12:01:16.940597: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_1_1/Const/_143]]
2019-07-09 12:01:16.940632: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_1/_110]]
2019-07-09 12:01:16.940810: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[{{node replica_2/sequential/lstm/StatefulPartitionedCall}}]]
2019-07-09 12:01:16.943854: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
Traceback (most recent call last):
  File ""test_run.py"", line 25, in <module>
    model.fit(dataset)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 668, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py"", line 680, in fit
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 294, in model_iteration
    batch_outs = f(actual_inputs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py"", line 854, in execution_function
    return [out.numpy() for out in distributed_function(input_fn)]
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 429, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1662, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 635, in _filtered_call
    self.captured_inputs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 733, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 459, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[node sequential/lstm/StatefulPartitionedCall (defined at /usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1654) ]]
  (1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:2 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal
	 [[node replica_2/sequential/lstm/StatefulPartitionedCall (defined at /usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1654) ]]
0 successful operations.
2 derived errors ignored. [Op:__inference_distributed_function_9976]

Function call stack:
distributed_function -> distributed_function

"
30512,`tf.keras.layers.RNN` is very different from `tf.nn.dynamic_rnn`,"In the documentation of [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn), it gives such a warning saying:

> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. 
> Instructions for updating: Please use keras.layers.RNN(cell), which is equivalent to this API

BUT when I checked [`tf.keras.layers.RNN(cell)`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN), I found an important feature of `tf.nn.dynamic_rnn` is lost, which enables users to deal with sequences of variable lengths.

```
tf.nn.dynamic_rnn(
    cell,
    inputs,
    sequence_length=None,
    initial_state=None,
    dtype=None,
    parallel_iterations=None,
    swap_memory=False,
    time_major=False,
    scope=None
)
```

Here `sequence_length` can be a tensor which indicating the length of the sequences. Let's see the API of `tf.keras.layers.RNN`:

```
__init__(
    cell,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    **kwargs
)
```

There is actually no such an argument. If I want to deal with variant-length sequences, I have to do a lot of work myself. So I still would like to use `tf.nn.dynamic_rnn` rather than `tf.keras.layers.RNN`.


After all, I think as the arguments and funcationality is different, 

> keras.layers.RNN(cell), which is equivalent to this API

is really a very big mistake. And please keep `tf.nn.dynamic_rnn` and remove it from deprecated functions."
30511,Unsupported data type (DT_BOOL) given to Fill op with output,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.14.0


**Provide the text output from tflite_convert**

```
2019-07-09 11:44:07.354104: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-09 11:44:09.016434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:02:00.0
2019-07-09 11:44:09.017804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
2019-07-09 11:44:09.017884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.019209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:83:00.0
2019-07-09 11:44:09.019280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.020624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:84:00.0
2019-07-09 11:44:09.020953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:09.023323: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 11:44:09.024759: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-09 11:44:09.025100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-09 11:44:09.027096: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-09 11:44:09.028651: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-09 11:44:09.033522: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-09 11:44:09.036355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.037640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.041685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.042985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.044219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-07-09 11:44:09.341294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.351801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.353487: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2295610 executing computations on platform CUDA. Devices:
2019-07-09 11:44:09.353521: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:09.353534: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:09.353544: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:09.353554: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:09.357486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499990000 Hz
2019-07-09 11:44:09.358742: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49190f0 executing computations on platform Host. Devices:
2019-07-09 11:44:09.358787: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-09 11:44:09.360901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:02:00.0
2019-07-09 11:44:09.362217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
2019-07-09 11:44:09.362286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.363528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:83:00.0
2019-07-09 11:44:09.363597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.364844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:84:00.0
2019-07-09 11:44:09.364909: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:09.364934: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 11:44:09.364955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-09 11:44:09.364977: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-09 11:44:09.364998: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-09 11:44:09.365019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-09 11:44:09.365042: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-09 11:44:09.367704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.368961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.372683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.373913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.375125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-07-09 11:44:09.375170: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:09.380638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-09 11:44:09.380661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3
2019-07-09 11:44:09.380672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y N N
2019-07-09 11:44:09.380685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N N N
2019-07-09 11:44:09.380695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N Y
2019-07-09 11:44:09.380704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N Y N
2019-07-09 11:44:09.385795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.386981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.389401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11497 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0, compute capability: 5.2)
2019-07-09 11:44:09.391171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11498 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
2019-07-09 11:44:09.392493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.393664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11498 MB memory) -> physical GPU (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0, compute capability: 5.2)
2019-07-09 11:44:09.394220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:09.395385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11498 MB memory) -> physical GPU (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-07-09 11:44:11.354423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.356293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.358071: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 4
2019-07-09 11:44:11.358226: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-07-09 11:44:11.361961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:02:00.0
2019-07-09 11:44:11.363679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
2019-07-09 11:44:11.363774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.365461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:83:00.0
2019-07-09 11:44:11.365554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.367225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:84:00.0
2019-07-09 11:44:11.367299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:11.367336: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 11:44:11.367365: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-09 11:44:11.367392: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-09 11:44:11.367420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-09 11:44:11.367448: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-09 11:44:11.367477: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-09 11:44:11.370909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.372651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.377451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.379093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.380741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-07-09 11:44:11.380985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-09 11:44:11.381010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3
2019-07-09 11:44:11.381024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y N N
2019-07-09 11:44:11.381041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N N N
2019-07-09 11:44:11.381055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N Y
2019-07-09 11:44:11.381068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N Y N
2019-07-09 11:44:11.387871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.389460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.392539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11497 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0, compute capability: 5.2)
2019-07-09 11:44:11.394104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11498 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
2019-07-09 11:44:11.394198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.395746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11498 MB memory) -> physical GPU (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0, compute capability: 5.2)
2019-07-09 11:44:11.395841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:11.397369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11498 MB memory) -> physical GPU (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-07-09 11:44:14.068140: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-09 11:44:14.068230: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2203 nodes (-39), 3277 edges (-74), time = 1373.11206ms.
2019-07-09 11:44:14.068249: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2203 nodes (0), 3277 edges (0), time = 725.973ms.
2019-07-09 11:44:14.068300: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: layer_norm_compute_TsrF4cwFbXs
2019-07-09 11:44:14.068329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 17 nodes (0), 19 edges (0), time = 0.917ms.
2019-07-09 11:44:14.068446: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 17 nodes (0), 19 edges (0), time = 0.531ms.
2019-07-09 11:44:14.068463: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: layer_norm_compute_grad_ZL8R5wqcdys
2019-07-09 11:44:14.068473: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 129 nodes (-1), 187 edges (-2), time = 11.934ms.
2019-07-09 11:44:14.068482: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 129 nodes (0), 187 edges (0), time = 6.312ms.
Traceback (most recent call last):
  File ""/home/hh1208-kang/venv_py2_tf1.13/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 898, in convert
    **converter_kwargs)
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/hh1208-kang/venv_py2_tf1.13/local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-07-09 11:44:19.974693: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-09 11:44:20.292118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.295593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.306852: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f20b60 executing computations on platform CUDA. Devices:
2019-07-09 11:44:20.306883: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:20.306893: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:20.306900: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:20.306911: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): GeForce GTX TITAN X, Compute Capability 5.2
2019-07-09 11:44:20.311504: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499990000 Hz
2019-07-09 11:44:20.312575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b8d9fb0 executing computations on platform Host. Devices:
2019-07-09 11:44:20.312601: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-09 11:44:20.314507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:02:00.0
2019-07-09 11:44:20.315699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
2019-07-09 11:44:20.315760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.316951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:83:00.0
2019-07-09 11:44:20.317011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.318192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:84:00.0
2019-07-09 11:44:20.318493: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:20.320081: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-09 11:44:20.321322: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-09 11:44:20.321617: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-09 11:44:20.323431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-09 11:44:20.324827: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-09 11:44:20.329388: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-09 11:44:20.331962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.333203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.336847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.338058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.340613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-07-09 11:44:20.340677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-09 11:44:20.345904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-09 11:44:20.345928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3
2019-07-09 11:44:20.345939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y N N
2019-07-09 11:44:20.345947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N N N
2019-07-09 11:44:20.345954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N Y
2019-07-09 11:44:20.345961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N Y N
2019-07-09 11:44:20.350971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.352193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.354601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0, compute capability: 5.2)
2019-07-09 11:44:20.357821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11404 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
2019-07-09 11:44:20.358888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.361068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11404 MB memory) -> physical GPU (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0, compute capability: 5.2)
2019-07-09 11:44:20.362854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-09 11:44:20.365024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11404 MB memory) -> physical GPU (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-07-09 11:44:20.601403: I tensorflow/lite/toco/import_tensorflow.cc:2201] Found and inlined TensorFlow functions.
2019-07-09 11:44:20.739944: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740059: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740088: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740111: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740134: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740159: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740182: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740205: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740228: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740250: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740272: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740313: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740336: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740359: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740382: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740404: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740427: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740450: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740495: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740518: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740540: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740562: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740586: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740609: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740631: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740653: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740676: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740699: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740723: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740746: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740769: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740815: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740838: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740861: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740884: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740931: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740954: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740976: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.740999: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741023: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741045: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741068: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741095: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741118: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741142: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741165: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741189: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741212: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741236: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741259: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741282: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741305: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741328: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741351: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741373: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741396: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741419: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741441: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741465: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741509: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741535: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741558: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741580: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741602: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741625: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741647: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741671: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741694: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741718: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741741: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741764: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741788: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741811: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741856: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741880: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741902: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741924: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741947: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741969: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.741992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742015: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742037: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742060: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742082: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742105: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742148: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742171: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742193: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742216: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742238: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742261: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742284: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742307: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742330: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742353: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742376: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742398: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742421: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742444: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742465: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742510: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742532: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742556: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742579: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742603: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742626: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742649: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742673: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742696: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742719: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742742: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742764: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742787: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742808: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742832: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742855: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742878: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742901: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742924: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742946: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742968: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.742993: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743015: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743038: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743060: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743082: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743105: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743127: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743150: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743172: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743195: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743217: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743239: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743261: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743283: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743305: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743327: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743349: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743372: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743394: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743417: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743439: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743461: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743482: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743503: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743524: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743545: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743566: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743588: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743610: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743631: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743654: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743676: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743697: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743719: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743741: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743762: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743784: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743806: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743827: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743851: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743873: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743896: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743917: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743940: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743963: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.743985: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744006: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744028: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744048: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744069: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744090: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744112: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744133: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744154: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744175: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744197: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744218: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744240: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744261: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744288: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744313: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744334: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744356: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744399: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744421: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744708: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744744: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744788: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744811: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744837: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744873: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.744901: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-09 11:44:20.745001: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond
2019-07-09 11:44:20.745064: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-07-09 11:44:20.745093: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-07-09 11:44:20.745119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-07-09 11:44:20.887269: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: MatrixBandPart
2019-07-09 11:44:20.892202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: IsFinite
2019-07-09 11:44:21.047492: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1559 operators, 2697 arrays (0 quantized)
2019-07-09 11:44:21.127085: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1366 operators, 2311 arrays (0 quantized)
2019-07-09 11:44:21.214208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1366 operators, 2311 arrays (0 quantized)
2019-07-09 11:44:21.217270: F **tensorflow/lite/toco/graph_transformations/resolve_constant_fill.cc:107] Unsupported data type given to Fill op with output ""zeros_2""**
Aborted (core dumped)
```

Also, please include a link to a GraphDef or the model if possible.
I cannot post the whole GraphDef but I think this part seems problem.

node {
  name: ""zeros_2""
  op: ""Fill""
  input: ""zeros_2/packed""
  input: ""zeros_2/Const""
  attr {
    key: ""T""
    value {
      **type: DT_BOOL**
    }
  }
  attr {
    key: ""_output_shapes""
    value {
      list {
        shape {
          dim {
            size: -1
          }
          dim {
            size: 1
          }
        }
      }
    }
  }
  attr {
    key: ""index_type""
    value {
      type: DT_INT32
    }
  }
}


**Any other info / logs**


'tflite_convert' complains about the data type 'DT_BOOL', while it dealt with 'DT_INT32', 'DT_FLOAT' in other Fill ops in different nodes.

How can I avoid this? I wonder if there exist a way that change the type, and is it safe or not.
"
30509,Official windows pip package build settings could be better,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
The build settings for the Tensorflow package produced and distributed via pip for windows aren't using the compiler flags that would result in optimal performance. I.e. the pip distributed package doesn't appear to use AVX2 instructions.

Given this, we suspect the package is also not built using LTCG, fp:fast, the new /Ob3 inlining, or the new exception handling /FH4 flags. A nice performance win (and binary size win in the case of /FH4) ought to come out of the small change of simply throwing the appropriate flags.


**Any other info / logs**
Output from running Tensorflow models on MLPerf: ""Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"""
30501, the ipynb 'Get started with TensorFlow 2.0 for experts' exhausts RAM memory when it runs on GPU,"When I run the notebook 'Get started with TensorFlow 2.0 for experts' using tensorflow GPU all RAM (12gb on Colab) gets consumed before training starts. If I run on colab without GPU it is slow but it runs to the end

**System information**
Colab (but also debian linux 10)

"
30499,TOCO input_shape not working as expexted,"Hello!
I have converted deeplabv3 model to tflite using TOCO with following command.
`toco --graph_def_file=""/home/abdullah/frozen_inference_graph.pb"" --output_file=""model1.tflite"" --output_format=""TFLITE"" --input_arrays='sub_7' --output_arrays=""ResizeBilinear_3"" --input_shape= 1,1024,1024,3
`
So, tflite model should take (1, 1024, 1024, 3) input. But when I try to test this model on my laptop for testing using this code it gives output of dimensions mismatch and is still expecting input of (1, 513, 513, 3).

Here is Inference code. 

    import tensorflow as tf
    import numpy as np
    from PIL import Image
    import matplotlib.pyplot as plt

    interpreter = tf.contrib.lite.Interpreter(model_path='/home/abdullah/Documents/Company Work/TFlite/model1.tflite')
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    quantization = None
    using_type = input_details[0]['dtype']


    size = 1024

    image = Image.open('/home/abdullah/Pictures/xyz.jpg')
    image = image.resize((size, size))

    input_shape = input_details[0]['shape']
    image = np.array(image)
    image = image.reshape(1, size, size, 3)
    image = image/127 -1
    input_data = image.astype(using_type)

    print(input_details)

    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    output_data = output_data.reshape(size, size, 21)


    labels = np.argmax(output_data, -1)
    labels = labels.reshape(size, size)


    plt.imshow(labels)
    plt.show()
    print(labels.shape)

    print(labels)
 
I am getting following error. 

`Traceback (most recent call last):
  File ""infer.py"", line 36, in <module>
    interpreter.set_tensor(input_details[0]['index'], input_data)
  File ""/home/abdullah/anaconda3/envs/tflow/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter.py"", line 151, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
  File ""/home/abdullah/anaconda3/envs/tflow/lib/python3.6/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 133, in SetTensor
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)
ValueError: Cannot set tensor: Dimension mismatch
`

But when I set size=513 it works like charm. 
Moreover, when I print **input_details = interpreter.get_input_details()**
It prints  
`[{'name': 'sub_7', 'index': 283, 'shape': array([  1, 513, 513,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
`

Kindly tell me how should I solve this.

**System information**

- OS Platform and Distribution : Ubuntu 18.04):
- TensorFlow installed from : Pip install tensorflow
- TensorFlow version : 1.10.1
- Python version: Python 3.6.8
- Bazel version : NA
- GCC/Compiler version : NA
- CUDA/cuDNN version : 10.0/7.4 
- GPU model and memory: Gtx 1060 and 16 gb RAM


"
30498,No `output_shape` after tf.keras layer/model build() and call. Is it intended?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below): `2.0.0-beta1` and `tf_nightly_gpu_2.0_preview-2.0.0.dev20190708`
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: without and with GPU (P100)

**Describe the current behavior**

Hi, all. When I tried to use `model.summary()`, the output shapes were printed as **multiple**. After few tries, I realized that the `output_shape` of keras layers/models are not determined even after the layer/model was built and called. Here are short examples.

1. Keras layer
```
import tensorflow as tf

dense = tf.keras.layers.Dense(2)
dense.build(input_shape=3)

input_tensor = tf.ones((5, 3), tf.float32)
output_tensor = dense(input_tensor)

# The line below raises ""AttributeError: The layer has never been called and thus has no defined output shape.""
print(dense.output_shape)
```

2. Keras model
```
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dense(4),
])

model.build(input_shape=(None, 3))

input_tensor = tf.ones((5, 3), tf.float32)
output_tensor = model(input_tensor)

# The line below raises ""AttributeError: The layer has never been called and thus has no defined output shape.""
print(model.output_shape)
```

**Describe the expected behavior**
I think keras layer/model should have `output_shape`, but they aren't. Please see if it's intended. I've just started to migrate from TF 1.x to TF 2.0 and to use keras APIs, so I might be wrong when using keras API.

**Code to reproduce the issue**
Described above.

**Other info / logs**
"
30495,/etc/bash.bashrc leads to scp fail,"I tried to copy my model files to a remote docker container using scp, but the ASCII welcome painting and warning sentences, i.e., the logo of ""tensorflow"" and ""WARNING: You are running this container as root ...... "", cause a transfer failure. As `scp` uses `ssh` as the backend, it will fail if you print something when init an interactive shell.
"
30494,Null Pointer Bug for InferenceContext* in Linux,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux CentOS 7 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8

**Describe the current behavior**
When adding a custom op in C++, the shape function passes a NULL InferenceContext pointer.

**Describe the expected behavior**
The pointer should not be NULL. (I have also tested it on Mac OS, and it works fine. But on Linux, it gives a null pointer.)

**Code to reproduce the issue**
This code (as a file `zero_out.cc`) is taken directly from `https://www.tensorflow.org/guide/extend/op`.
```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include <iostream>

using namespace tensorflow;
using shape_inference::InferenceContext;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](InferenceContext* c) {
      std::cout << ""c address: "" << c << std::endl;
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
      // Just leave it empty
  }
};

REGISTER_KERNEL_BUILDER(
    Name(""ZeroOut"")
    .Device(DEVICE_CPU),
    ZeroOutOp);

```
Compile the op with the commands:
```
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
```
Run the op:
```
import tensorflow as tf
zero_out_module = tf.load_op_library('./zero_out.so')
with tf.Session(''):
  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()
```
This prints the following:
```
c address: 0
Segmentation fault
```
"
30493,[Feature] To support casting float32 to int32 in HEX format,"**System information**
- TensorFlow version (you are using):
    1.13.x
- Are you willing to contribute it (Yes/No):
    Yes


**Describe the feature and the current behavior/state.**
I want something like `int32_tensor = tf.float_to_hexint(fp32_tensor)` which allows me to get the hex value representation and can be used to further compute with bit-wise operators.

**Will this change the current api? How?**
No, just extend a new op to handle it.

**Who will benefit with this feature?**
Any requirement that is needed to edit the bit value for a float32-type tensor.

**Any Other info.**
No."
30492,Custom OP Documentation: `-D_GLIBCXX_USE_CXX11_ABI=0` not required anymore?,"## URL(s) with the issue:

https://www.tensorflow.org/guide/extend/op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation
(Last Note of the paragraph).

## Description of issue (what needs changing):
The documentation states that customs ops for the binary pip packages should be compiled with `-D_GLIBCXX_USE_CXX11_ABI=0`. For me this actually had to be removed (python 3.7, tensorflow-gpu==1.14.0  from pip), so I guess tf-1.14 is now built with `gcc > 4`?
If someone can confirm, I could open a PR :)


"
30491,padding parameter of conv2d layer is not saved correctly with dilation > 1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
tf.layers.conv2d(padding='same') is saved as padding is VALID.

**Describe the expected behavior**
tf.layers.conv2d(padding='same') is saved as padding is SAME.

**Code to reproduce the issue**
i use the below code to generate .pb file, the key is that padding parameter is same, and dilation_rate is 2.
```
import tensorflow as tf
import numpy as np

x = tf.placeholder(tf.float32, shape=[1, None, None, 1])
conv = tf.layers.conv2d(x, 64, 5, activation=tf.nn.tanh, padding='same', dilation_rate=(2, 2), kernel_initializer=tf.keras.initializers.he_normal())
y = tf.identity(conv, name='y')


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ['y'])
    tf.train.write_graph(output_graph_def, '.', 'same.pb', as_text=False)
```

I use the following code to load and check the .pb file, but the padding parameter is not same, but VALID.
```
import tensorflow as tf
import numpy as np

with open('same.pb', 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    nodes = graph_def.node
    for node in nodes:
        if node.op == 'Conv2D':
            print(node.attr['padding'].s)
```"
30490,core dumped when convert to tflite,"Hi.
After too debuging,i tried to convert tflite model!but i get error.
my code is:


tflite_convert --graph_def_file=ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb --input_arrays=image_tensor --input_shapes=1,300,300,3 --output_arrays=concat --output_file=mobilenetv2_ssdlite.tflite




my error is:


2019-07-08 15:20:45.599944: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-08 15:20:45.623407: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192955000 Hz
2019-07-08 15:20:45.623793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3096840 executing computations on platform Host. Devices:
2019-07-08 15:20:45.623816: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-08 15:20:46.856808: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-07-08 15:20:46.856973: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-07-08 15:20:48.347129: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-08 15:20:48.347173: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6099 nodes (-30), 10134 edges (-41), time = 1151.25806ms.
2019-07-08 15:20:48.347178: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6099 nodes (0), 10134 edges (0), time = 197.626ms.
Traceback (most recent call last):
  File ""/home/davari/virtualenvironments/detection_on_mobile/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 987, in convert
    **converter_kwargs)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 411, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-07-08 15:20:49.738690: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738807: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.738820: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738828: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.738835: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738842: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.738848: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738856: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738867: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738873: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738879: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738887: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738893: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738899: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738914: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.738923: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-07-08 15:20:49.738935: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond
2019-07-08 15:20:49.738943: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.738959: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-07-08 15:20:49.738968: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-07-08 15:20:49.738977: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3
2019-07-08 15:20:49.738985: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3
2019-07-08 15:20:49.738992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-07-08 15:20:49.739018: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:49.739031: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3
2019-07-08 15:20:49.739040: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3
2019-07-08 15:20:49.739067: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:49.740378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740392: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740402: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740409: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740418: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740425: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740433: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740440: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740448: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740455: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740463: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740469: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740477: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740484: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740492: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-07-08 15:20:49.740498: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740505: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740519: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740526: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740533: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740539: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740546: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-07-08 15:20:49.740554: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740561: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740568: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-07-08 15:20:49.740577: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740585: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740591: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740600: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740606: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740613: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740621: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740627: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740633: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740640: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740646: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740653: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740660: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740666: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-07-08 15:20:49.740674: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-07-08 15:20:49.740684: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740693: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740707: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond
2019-07-08 15:20:49.740716: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.740749: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-07-08 15:20:49.740758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-07-08 15:20:49.740766: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-07-08 15:20:49.743703: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-07-08 15:20:49.743720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-07-08 15:20:49.743729: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-07-08 15:20:49.748554: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748585: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748594: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748602: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748611: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748619: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748626: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748634: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748642: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748650: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748658: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748668: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748676: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748684: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748692: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748708: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748716: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748724: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748732: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748740: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748748: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748764: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748772: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748780: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748787: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748796: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748803: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748811: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748819: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748827: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748835: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748843: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748851: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748860: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748868: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748876: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748884: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748892: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748900: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748917: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748925: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748932: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748940: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748948: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748956: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748964: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748972: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748980: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748988: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.748996: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749020: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749028: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749036: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749044: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749052: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749060: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749068: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749076: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749084: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749093: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749101: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749109: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749117: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749125: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749133: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749141: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749149: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749157: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749165: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749173: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749181: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749189: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749197: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749205: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749213: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749222: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749230: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749238: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749246: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749254: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749262: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749270: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749278: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749286: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749294: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2
2019-07-08 15:20:49.749805: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size
2019-07-08 15:20:49.749895: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:49.750159: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:49.750170: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:49.750179: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-07-08 15:20:50.198559: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4312 operators, 7090 arrays (0 quantized)
2019-07-08 15:20:50.433191: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 475 operators, 732 arrays (0 quantized)
2019-07-08 15:20:50.452701: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 475 operators, 732 arrays (0 quantized)
2019-07-08 15:20:50.469125: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 139 operators, 344 arrays (0 quantized)
2019-07-08 15:20:50.472201: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 139 operators, 344 arrays (0 quantized)
2019-07-08 15:20:50.474837: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 139 operators, 344 arrays (0 quantized)
2019-07-08 15:20:50.481901: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1080128 bytes, theoretical optimal value: 1080128 bytes.
2019-07-08 15:20:50.482406: F tensorflow/lite/toco/tooling_util.cc:2275] Check failed: array.data_type == array.final_data_type Array ""image_tensor"" has mis-matching actual and final data types (data_type=uint8, final_data_type=float).
Fatal Python error: Aborted

Current thread 0x00007fcb303a1740 (most recent call first):
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/davari/virtualenvironments/detection_on_mobile/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped"
30489,core dumped when convert to tflite,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
30488,libtensorflow_framework.so issue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I git cloned it from https://github.com/smallcorgi/Faster-RCNN_TF.git
- OS Platform and Distribution: ubuntu 18.04
- TensorFlow installed from: pip install
- TensorFlow version: used both 1.14.0rc and 1.14.0(gpu)
- Python version: 2.7
- GCC/Compiler version: 7.4
- CUDA/cuDNN version: 10.0 
- GPU model and memory: GV 100

My script file is demo.py. 

Here's the back trace

```
Traceback (most recent call last):
  File ""./tools/demo.py"", line 11, in <module>
    from networks.factory import get_network
  File ""/home/tFaster-RCNN_TF/tools/../lib/networks/__init__.py"", line 8, in <module>
    from .VGGnet_train import VGGnet_train
  File ""/home/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py"", line 2, in <module>
    from networks.network import Network
  File ""/home/Faster-RCNN_TF/tools/../lib/networks/network.py"", line 3, in <module>
    import roi_pooling_layer.roi_pooling_op as roi_pool_op
  File ""/homeFaster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling_op.py"", line 5, in <module>
    _roi_pooling_module = tf.load_op_library(filename)
  File ""/home/.local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 61, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory
```
I checked that libtensorflow_framework.so is in ```python2.7/site-packages/tensorflow/ ```but somehow my program doesn't seem to be able to find it. The path for the tensorflow library is set as ```$TF_LIB = /home/.local/lib/python2.7/site-packages/tensorflow```

I did run into some solutions but coudn't find one with tf version 1.14.0.

Please help. 

Thanks in advance."
30487,How to reuse conv layers with keras,"my data shape is [91,109,91,27] and I unstack the one data to 27 data which shape is [91,109,91,1],

I want to use the convnet to extract the feature. But here is loop my Net 27 times. So how could I reuse the Network for just one time?

Description of the problem in the code comment
```python
x_in = Input(shape=shape) # my data shape is [91,109,91,27]

x = Lambda(self.unstack)(x_in) # there I unstack the data to 27 datas shape is [91,109,91]

t_list = []
for i in range(len(x)): # loop 27 times
   x_expand = Lambda(self.expand)(x[i])  # change the data shape to [91,109,91,1]

   feature = self.cnn_block(x_expand)# here is my Net with 3 convs --->
# I build a convnet here, and I want to reuse the network, but here it is looping 27 times. so is there a way to reuse the cnn_block in keras?

   x_out = Lambda(self.expand)(feture)
   t_list.append(x_out)

x = Concatenate(axis=-1)(t_list)

```"
30486,"Keras TimeDistributed on a Model creates duplicate layers, and is inconsistent with TimeDistributed on a Layer","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu 1.14.0
- Python version: 3.6.7
- CUDA/cuDNN version: 10
- GPU model and memory: GTX1060M

**Describe the current behavior**
Wrapping a model in a TimeDistributed layer creates duplicate nodes in the graph. If we follow the docs [(link)](https://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers) and create a simple Dense Model wrapped in a TD Layer:
```
    inner_input = keras.layers.Input((2,))
    dense = keras.layers.Dense(2, activation='relu')(inner_input)
    inner_model = keras.Model(inputs=inner_input, outputs=dense)

    full_input = keras.layers.Input((2,2))
    td_2 = keras.layers.TimeDistributed(inner_model)(full_input)
    model = keras.models.Model(full_input, td_2)
    model.compile('SGD', 'mse')
```
You end up with this:
![bad_td](https://user-images.githubusercontent.com/24449147/60799985-d4e60c00-a1a6-11e9-84e8-e0b6ddd2a789.png)
Firstly, if you follow the documentation approach you end up with an additional Dense Layer (Bottom Left). This eats up memory, and happens because you build the inner model then rebuild it again when you build the TimeDistributed model. This can be avoided by parameterizing your model [(link)](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#building_models), but it can be a very painful workaround if your model is complex. But for demonstrations sake, here's the model as an object:
```
    class InnerModel(keras.Model):
        def __init__(self):
            super(InnerModel, self).__init__()

            self.dense = keras.layers.Dense(2, activation='relu')

        def call(self, inputs, training=None, mask=None):
            out = self.dense(inputs)
            return out

        def compute_output_shape(self, input_shape):
            return (input_shape[0], 2)

    input = keras.layers.Input((2,2))
    td_model = InnerModel()
    time_dist = keras.layers.TimeDistributed(td_model)(input)
    model = keras.models.Model(input, time_dist)
    model.compile('SGD', 'mse')
```
Here's the improved graph:
![better_td](https://user-images.githubusercontent.com/24449147/60801864-58552c80-a1aa-11e9-9550-24068fffc43c.png)

So the additional Dense layer is gone, but there are still two dense layers inside. They have different contents too.
![better_td_internal](https://user-images.githubusercontent.com/24449147/60801763-25ab3400-a1aa-11e9-9721-00482828ed58.png)


**Describe the expected behavior**
If you compare to what you get if you just wrap the Dense layer itself:
```
    full_input = keras.layers.Input((2,2,2))
    td_2 = keras.layers.TimeDistributed(keras.layers.Dense(2, activation='relu'))(full_input)
    model = keras.models.Model(full_input, td_2)
    model.compile('SGD', 'mse')
```
![good_td](https://user-images.githubusercontent.com/24449147/60800184-5c337f80-a1a7-11e9-8dd3-901054a537d9.png)

I'd expect wrapping a model should result in a very similar looking graph to wrapping a layer

**Code to reproduce the issue**
Full code for creating the graphs:
https://gist.github.com/LukeBolly/0efeca3db275dee97c5f0fbf1f400b5a
"
30485,(Deeplab)(ios)(tflite)using deeplab on ios application ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone 6
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:3.7
- Bazel version (if compiling from source):0.27
- GCC/Compiler version (if compiling from source):Nan
- CUDA/cuDNN version:Nan
- GPU model and memory:Nan
**Describe the current behavior**
deeplab doesn't segment on ios 
**Describe the expected behavior**
deeplab segmentation on ios application
**Code to reproduce the issue**
`tflite_convert ----output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --input_arrays=sub_2 --input_shapes=1,224,224,3 --output_arrays=ResizeBilinear_2 --output_file=/Users/Karizma/Downloads/deeplabv3_mnv2_pascal_trainvall/frozen-224.tflite --graph_def=/Users/Karizma/Downloads/deeplabv3_mnv2_pascal_trainvall/frozen-224.pb --mean_values=128 --std_dev_values=127 --allow_custom_ops --post_training_quantize`
**Other info / logs**
Hi, I have trained deep lab on my custom dataset(200*150) with 224 as crop size and during the test, it detects for crop with crop size 224 .
now what I need is to integrate my model on ios application, i was able to successfully convert the model to tflite .but i does not detecte anything i don't get it whats the problem
because when i tried to convert a deeplab pretrained mobilenet mode [llink](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz) it works for me on mobile and for my model no ,however i have tested my model (.pb model)with python code and it detects
this is my model architecture i hope it will be helpful to indrestoud what's going on
![s](https://user-images.githubusercontent.com/34586193/60801386-c8de5800-a16e-11e9-9a5f-ca319de7b0ee.png)
![ss](https://user-images.githubusercontent.com/34586193/60801396-ced43900-a16e-11e9-8a6d-e9374beea91a.png)
"
30483,Hwloc mirror - download issue,"I am trying to cross-compile TF for Raspberry Pi via Docker container - as described in documentation. Unfortunately. it breaks with the following message:

ERROR: /workspace/tensorflow/core/BUILD:2432:1: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /home/dmitry/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_dmitry/eab0d61a99b6696edb3d2aff87b585e8/external/hwloc/hwloc-2.0.3.tar.gz: Tried to reconnect at offset 6,391,630 but server didn't support it and referenced by '//tensorflow/core:lib_internal_impl'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /home/dmitry/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_dmitry/eab0d61a99b6696edb3d2aff87b585e8/external/hwloc/hwloc-2.0.3.tar.gz: Tried to reconnect at offset 6,391,630 but server didn't support it

Fixes tried with no luck:
- remove a semi-downloaded package from Bazel cache
- download it from the original site and put to a cache
- download from a mirror with Chrome (breaks in two parts)

**System information**
TF_BUILD_INFO = {
container_type: ""pi-python3"", 
command: ""tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE"", 
source_HEAD: ""c407b045b8802f9eded430ef48be18cd85e4788c"", 
source_remote_origin: ""https://github.com/tensorflow/tensorflow.git"", 
OS: ""Linux"", 
kernel: ""4.15.0-54-generic"", 
architecture: ""x86_64"", 
processor: ""Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz"", 
processor_count: ""8"", 
memory_total: ""16305540 kB"", 
swap_total: ""16657404 kB"", 
Bazel_version: ""Build label: 0.24.1"", 
Java_version: ""1.8.0_222-ea"", 
Python_version: ""2.7.6"", 
gpp_version: ""g++ (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4"", 
swig_version: """", 
NVIDIA_driver_version: ""418.56"", 
CUDA_device_count: ""0"", 
CUDA_device_names: """", 
CUDA_toolkit_version: """"
}

**Provide the exact sequence of commands / steps that you executed before running into the problem**
CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4""     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE


"
30482,Can't compile TensorFlow 1.3 on Jetson TX2,"**System information**
- OS Platform and Distribution: Ubuntu 16.04 on Jetson TX2
- TensorFlow installed from: source
- TensorFlow version: 1.3.1
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.4.5 (not working, see below)
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: CUDA 8.0, cuDNN 6.0.
- GPU model and memory: [NVIDIA Pascal GPU, 8GB 128-bit LPDDR4 Memory](https://developer.nvidia.com/embedded/jetson-tx2)



**Describe the problem**

I need to build TensorFlow 1.3 on a Jetson TX2 machine (aarch64). I can't get the build to work, since I don't have Bazel for aarch64 and can't seem to build it. Since TensorFlow can be used on Jetson TX2, I guess that I'm not building it the right way.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Follow [all the steps in the Bazel installation guide](https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu), then

```bash
wget https://github.com/tensorflow/tensorflow/archive/v1.3.1.tar.gz
tar zxf v1.3.1.tar.gz
cd tensorflow-1.3.1
./configure
```

The output:
```
/usr/local/bin/bazel: line 88: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error
/usr/local/bin/bazel: line 88: /usr/local/lib/bazel/bin/bazel-real: Success
```

I realize that this is a Bazel problem, but since I can't figure out how to build Bazel for that machine, I may have gotten the building method wrong."
30481,get the planning path from zed camera,"hello
my question is about how can benefit from planning path topic get from Zed and trace the trajectory of zed motion
"
30480,No module named '_pywrap_tensorflow' error even after installing Visual C++ and MSVCP140.dll.,"I'm trying to run the simple speech recognition network example of Tensorflow:

python tensorflow/examples/speech_commands/train.py

But I keep getting:

> Traceback (most recent call last):
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

>During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

>During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/train.py"", line 79, in <module>
    import tensorflow as tf
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

>During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\DELL 7000\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


>Failed to load the native TensorFlow runtime.

>See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

>for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Most of the solutions I found told me to reinstall visual c++ or to manually add MSVCP140.dll to the path. But neither of these solve my problem.
The github page it tells me to refer returns a 404.
I'm on python 3.7.2 and tensorflow 1.0.0.
Kindly help if possible."
30478,"[TF2.0] Dataset iteration, dynamic TensorArray and reduce operations","**System information**
- Have I written custom code: `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OSX`
- TensorFlow installed from (source or binary): `binary - 2.0.0-beta1`
- TensorFlow version (use command below): `v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1`
- Python version: `3.6`

**Describe the current behaviour**
I'm trying to apply a reduce operation over the result of TensorArray concatenation.
The concatenation happens in a for loop generated by iteration over a dataset.
The resulting value of the reduce operation is malformed tensor:
- the shape is `()`
- the actual value is `[float32]` of shape `(1,)`

This makes the resulting tensor effectively unusable because TF will then fail either because of shape information or because of the actual value of the tensor

Remark:
- If the for loop is generated from the `tf.range` operation, everything works as expected.

**Describe the expected behaviour**
Getting a valid result from the different reduce operations when applied to the result of a TensorArray concatenation operation when this one is filled in a for loop generated by iteration over a dataset.


**Code to reproduce the issue**
```python
import tensorflow as tf


mean = tf.keras.metrics.Mean()

a = tf.random.uniform([10, 2])
d = tf.data.Dataset.from_tensor_slices(a).batch(2)


@tf.function
def compute(mean, dataset):
    # I don't use the dataset at all
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    for i in tf.range(10):  # Simple for loop
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
    all_real_logits = arr.concat()

    score = tf.reduce_mean(all_real_logits)
    tf.print(tf.shape(score), score)  # -> [], 0.0653904751
    mean.update_state(score)
    return mean.result()


@tf.function
def compute_error(mean, dataset):
    # I use the dataset only to get the index
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    for i, _ in dataset.enumerate():  # Dataset for loop with enumerate
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
    all_real_logits = arr.concat()

    score = tf.reduce_mean(all_real_logits)
    tf.print(tf.shape(score), score)  # -> [], [-0.256373167] brackets!
    mean.update_state(score)
    return mean.result()


@tf.function
def compute_error2(mean, dataset):
    # I only use the dataset to simulate a for loop
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    i = tf.constant(0, tf.int32)
    for _ in dataset:  # Dataset for loop
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i = i + 1
    all_real_logits = arr.concat()

    # score = tf.reduce_mean(all_real_logits)
    score = tf.reduce_sum(all_real_logits)
    tf.print(tf.shape(score), score)  # -> [], [-0.256373167] brackets!
    mean.update_state(score)
    return mean.result()


# Works well
print(compute(mean, d))

# Breaks because the shape is wrong for the score var
# We have shape=() and the actual value is [float] of shape (1,)
# Yet we can't do score[0] because the shape is ()
# In the end the score var becomes unusable

# Error: Cannot update variable with shape [] using a Tensor with shape [1], shapes must be equal.
print(compute_error(mean, d))

# it seems that the error still occurs as long as the call to
# arr.write is inside a for loop generated by iteration on a
# dataset
# Switching the reduce operation does not change the behaviour
print(compute_error2(mean, d))

```
"
30477,Run Tflite model on server,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: all android phones
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version:python3
- Bazel version (if compiling from source):4
- GCC/Compiler version (if compiling from source):na
- CUDA/cuDNN version:na
- GPU model and memory:na



I have a custom trained model and exported it on Tensorflow Lite. It run well on android phones. I would like to run the same model on a Ubuntu server. How can I do that? I am choosing tflite for its small file size but I could not find any way to take inference from *.tflite model directly on a Ubuntu server.
"
30476,Op Less<bfloat16> registered twice,"In the 1.14 branch, the CPU kernel op ""Less"" is registered twice for the type bfloat16 (see https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/kernels/cwise_op_less.cc#L19 : first on line 19 and then on line 21).

This has the effect of throwing an error ""InvalidArgumentError: Multiple OpKernel registrations match NodeDef"" whenever a bfloat16 comparison on a CPU is attempted.
"
30475,Tensorflow v1.14.0 with MKL and eigen on osx,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX Mojave 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: on master branch, commit 3e5418c3d6
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): clang-1001.0.46.4
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I tried to install tensorflow along with MKL and eigen

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```shell
# conda 4.7.5
conda create -n idp-6 -c defaults python=3 pip six numpy wheel setuptools mock 'future>=0.17.1' 'keras-preprocessing=1.0.5' 'keras-applications=1.0.6' mkl mklml
conda activate idp-6
cd tensorflow
export TF_MKL_ROOT=$HOME/miniconda3/envs/idp-6
# This is needed since license.txt does not exist in $TF_MKL_ROOT and the build system expects it
touch $TF_MKL_ROOT/license.txt
yes """" | ./configure
bazel build -c opt --verbose_failures --config=mkl --copt=""-DEIGEN_USE_MKL_VML"" --config=opt //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
When I ran it, I got an error at some point in through the build that mkl.h was not found.

As a quick fix, I made the following update and it worked (below). While this is not particularly elegant, but it works for me.

If there is interest, I can provide a pull request that does the right thing for win/osx/linux using third_party/mkl/BUILD as a template.

```diff
diff --git i/third_party/eigen3/BUILD w/third_party/eigen3/BUILD
index bd6cb868a9..fb6970ee36 100644
--- i/third_party/eigen3/BUILD
+++ w/third_party/eigen3/BUILD
@@ -35,6 +35,8 @@ cc_library(
     deps = [
         ""@eigen_archive//:eigen"",
         ""@local_config_sycl//sycl"",
+        ""@mkl_darwin//:mkl_headers"",
+        ""@mkl_darwin//:mkl_libs_darwin"",
     ],
 )
```"
30474,[TF2.0] Bug allowing misuse of the batch dimension of a convolution layer,"`tensorflow-1.14.0` rightfully complains about the following minimal example with `ValueError: could not broadcast input array from shape (20,6,6,32) into shape (10,6,6,32)`.

`tensorflow==2.0.0-beta1` however happily runs it and prints `(20, 6, 6, 32)`.

```python3
import numpy as np
import tensorflow.keras.backend as k
from tensorflow.keras.layers import Input, Conv2D, Lambda
from tensorflow.keras.models import Model

def custom_reshape(inputs):
    return k.reshape(inputs, (-1, 8, 8, 3))

inputs = Input(shape=(8, 8, 6))
x = Lambda(custom_reshape)(inputs)
x = Conv2D(32, (3, 3))(x)
model = Model(inputs=inputs, outputs=x)
model.compile(loss='mean_squared_error', optimizer='nadam')
print(model.summary())
batch_size = 10
result = model.predict(np.ones((batch_size, 8, 8, 6)), batch_size=batch_size)
print(result.shape)
```

As [per discussion](https://groups.google.com/a/tensorflow.org/forum/#!topic/testing/txsgcR3cubQ) this seems to be a bug in TF 2.0."
30472,The flag 'log_dir' is defined twice.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: N/A

**Describe the current behavior**
DuplicateFlagError: The flag 'log_dir' is defined twice.

**Code to reproduce the issue**
import tensorflow as tf
from absl import flags
flags.DEFINE_string('log_dir', './log', 'Log directory')
"
30471,[TF2.0] Bug when export BatchNormalization layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
  binary
- TensorFlow version (use command below):
  2.0.0-beta0
- Python version:
  python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
  cuda 10, 
- GPU model and memory:
  GTX 1080ti

**Describe the current behavior**
```
W0708 11:01:14.168962 140059689457472 saved_model.py:721] Skipping full serialization of 
object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 
0x7f61c004a978>, because an error occurred while tracing layer functions. Error message: 
Expected Operation, Variable, or Tensor, got None
```
**Describe the expected behavior**
There should be no error message.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization

class BNModel(tf.Module):

    def __init__(self):
        super(BNModel, self).__init__()
        self.bn = BatchNormalization()

    @tf.function(input_signature=[tf.TensorSpec([None, 2], tf.float32)])
    def __call__(self, x):
        return self.bn(x)

if __name__ == '__main__':
    module = BNModel()
    import numpy as np
    arr = np.random.randn(2,2)
    module(arr)                                                                                                                                           
    signatures = {""serving_default"": module.__call__}                                                                         
    tf.saved_model.save(module, ""saved_models"", signatures=signatures)                                                      

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30469,TensorflowLite model for On-Device Speech Recognizer,"# Description of issue

There a mention of a tensorflowlite model in the Google AI team [blog](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)

> made publicly available through the model optimization toolkit in the TensorFlow Lite library 

Where can I find this model? Is this the right place: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/g3doc/models"
30468,tensorflow 2.0 variable slice assign_add not supported,"Could anybody tell me in tf 2.0 rc1, why is variable slice assignment (varaible[...].assign function) works well but the assign_add is not supported?
e.g. :

`var = tf.Variable(tf.ones([2,3,3]))`
`var[:, :, 1].assign(tf.zeros([2,3]))  # this will work`

`var[:, :, 1].assign_add(tf.zeros([2,3]))  # this will NOT work`
`AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign_add'`
"
30467,InvalidArgumentError: Retval[0] does not have value when combining tf.case and l2_regularization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')
- Python version:
 2.7.15
- CUDA/cuDNN version:
7
- GPU model and memory:
GTX 1070, 8G

**Describe the current behavior**
Get the following error when using tf.case and slim.l2_regularization.
```
Traceback (most recent call last):
  File ""/home/yfeng23/test/tf/case_test.py"", line 20, in <module>
    print(sess.run(loss))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
```

**Describe the expected behavior**
No error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
import tensorflow.contrib.slim as slim

x = tf.zeros((1, 8))

fn = lambda: slim.fully_connected(x, 4,
                                  weights_regularizer=slim.l2_regularizer(0.1))

pred_fn_pairs = [
  (tf.equal(0, 0), fn),
  (tf.equal(1, 0), fn)
]

y = tf.case(pred_fn_pairs, exclusive=True)

loss = tf.losses.get_regularization_loss()

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  print(sess.run(loss))
```

"
30466,Batched tf.linalg.eigh is much slower on GPU than on CPU for many small matrices,"**Describe the current behavior**
See title.

**Describe the expected behavior**
There shouldn't be such a big discrepancy between the two (see below).

**Code to reproduce the issue**
```python
import tensorflow as tf
sym = lambda a: 0.5 * (a + tf.matrix_transpose(a))
with tf.device('cpu:0'):
    _ = tf.linalg.eigh(sym(tf.random.uniform((100000, 2, 2))))  # fast (~0.02s)
with tf.device('gpu:0'):
    _ = tf.linalg.eigh(sym(tf.random.uniform((100000, 2, 2))))  # slow (~7.3s)
```

**System information**
- os platform: Linux-4.4.0-154-generic-x86_64-with-debian-stretch-sid
- gpu: GeForce GTX TITAN X
- python version: 3.7.3
- tf.version.VERSION = 1.13.1
- tf.version.COMPILER_VERSION = 5.4.0
- CUDA 10, cuDNN 7"
30465,problem about creating my own model,"when i run the code which is write in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model
bazel run -c opt --copt=-mavx2 --copt=-mfma \
tensorflow/examples/speech_commands:train -- \
--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
--wanted_words=""yes,no"" --silence_percentage=25 --unknown_percentage=25 --quantize=1
it also said
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 344
		_create_local_python_repository(repository_ctx)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 56, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
ERROR: /home/wxy/tensorflow-master/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 344
		_create_local_python_repository(repository_ctx)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 56, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/wxy/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
 and referenced by '//third_party/python_runtime:headers'
ERROR: Analysis of target '//tensorflow/examples/speech_commands:train' failed; build aborted: Analysis failed
INFO: Elapsed time: 8.612s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (33 packages loaded, 707 targets c\
FAILED: Build did NOT complete successfully (33 packages loaded, 707 targets c\
onfigured)

i run it in Ubuntu 16.04 and i install the numpy whatever in python 2.x or python 3.n
thank you"
30464,Tensorboard does not display more then 100 bounding boxes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. (I've prepared pipeline.config, classes.bptxt and one tfrecord
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.01 (Linux smok 4.15.0-45-generic #48-Ubuntu SMP Tue Jan 29 16:28:13 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux) 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):
v1.12.0-0-ga6d8ffae09 1.12.0
(I've also tried b'v1.12.0-6120-gdaab2673f2' 1.13.0-dev20190116 and results are the same)
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0
- GPU model and memory: GTX 1080Ti, RTX 2080

**Describe the current behavior**
I run training 
```
python object_detection/model_main.py --pipeline_config_path=/mnt/data/environments/bag/100bboxes/pipeline.config --model_dir=/mnt/data/environments/bag/100bboxes/exp
```
This is only one step training and its goal is to visualize bounding boxes in tensorboard images tam. 
On the ""IMAGES"" tab in tensorboard I don't see all bounding boxes which are in the tfrecord file. 
Options provided in the pipeline.config
```
num_visualizations: 200                                                                                                                   
max_num_boxes_to_visualize: 200
```
have no effect on this. 

**Describe the expected behavior**
I see all bounding boxes (ground thruth) in the tensorboard>images (pictures on the right)

**Code to reproduce the issue**
I've created [repo](https://github.com/wkoziej/100bboxes) where you can find
 * pipeline.config - one step training,  the same tfrecord for training and evaluation
 * classes.pbtxt - two sample classes
 * t.tfrecord - containg two white images: one with 100 bounding boxes and second with 200 bounding boxes
 * notebook-images-view.png - view from notebook when I've tried visualise bboxes 
 * tensorboard-images-view.png - tensorboard images view 


"
30459,Memory leak when using `tf.linalg.expm` in `tf.keras.layers.Layer`,"**System information**
- Have I written custom code: *Yes*
- OS Platform and Distribution: *Arch Linux*
- TensorFlow installed from (source or binary): *Binary*
- TensorFlow version (use command below): *2.0.0 Beta 1*
- Python version: *3.7.3*
- CUDA/cuDNN version: *10.0.130* / *7.6.0*
- GPU model and memory: *Nvidia GTX 1060, 6GB*


**Describe the current behavior**
I'm currently working on a costum Keras Layer and need to use `tf.linalg.expm` several times. 
While training my model I noticed that my system would be out of memory after a few minutes. 
Reducing the Layer definition step by step I noticed that the error should be related to `tf.linalg.expm`. 
Every time the layers `call` method is invoked more and more memory get's allocated.
Replacing `return tf.linalg.expm(X)` in the following code example with e.g. `return tf.matmal(X,X)` does not yield this extreme leak of memory.

**Describe the expected behavior**
The memory allocated by an invocation of a`call` method using `tf.linalg.expm` should be freed.
![Number of `call` invocations plotted against memory allocated in Gigabytes](https://user-images.githubusercontent.com/2351787/60768608-18b51480-a0c6-11e9-9d0f-baad36136cc8.png)


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import numpy as np
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import psutil


def memory():
    pid = os.getpid()
    py = psutil.Process(pid)
    memory_use = py.memory_info()[0] / 2. ** 30
    return memory_use

class TestLayer(tf.keras.layers.Layer):

    def __init__(self, **kwargs):
        
        super(TestLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.s = input_shape

    @tf.function
    def call(self, inputs, training=None):
        
        X = tf.matmul(tf.transpose(inputs),inputs)
        
        return tf.linalg.expm(X)
    


S = tf.keras.Input(shape=(1,), name=""sequence"", dtype=tf.float64)
T = TestLayer()(S)
model = tf.keras.Model(inputs=S,outputs=T)
dataset = tf.data.Dataset.from_tensors(tf.constant(1)).repeat().map(lambda x: tf.ones([tf.random.uniform([1], minval=100, maxval=3000, dtype=tf.int32)[0],1],dtype=tf.float64))

memory_usage = []

i = 0
for data in dataset:
    s = model(data)
    
    memory_usage.append(memory())

    if i == 1000:
        break
        
    i = i+1
    
plt.figure()
plt.plot(memory_usage)
plt.xlabel(""iterations"")
plt.ylabel(""memory usage"")
plt.show()
```

**Other info / logs**
Warning produced when using `tf.linalg.expm`: 
```
    WARNING: Logging before flag parsing goes to stderr.
    W0707 13:40:07.108526 139932625143616 deprecation.py:323] From /home/darvin/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linalg_impl.py:280: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
```"
30458,can not convert tf model to tflite,"Hi.i tried convert my tensorflow model to tensorflow lite with saved model of pretrained model such as:


import tensorflow as tf
converter=tf.lite.TFLiteConverter.from_saved_model(""ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/saved_model"" ,input_arrays=None , input_shapes = (1,300,300,3),output_arrays=None, tag_set=None, signature_key=None)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)





but i got this error:

2019-07-07 16:56:02.926259: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-07 16:56:02.948528: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192640000 Hz
2019-07-07 16:56:02.948956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ab6990 executing computations on platform Host. Devices:
2019-07-07 16:56:02.948982: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
WARNING: Logging before flag parsing goes to stderr.
W0707 16:56:02.949664 140418756732736 deprecation.py:323] From /home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
Traceback (most recent call last):
  File ""TFlite_converter.py"", line 30, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(""ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/saved_model"" ,input_arrays=None , input_shapes = (1,300,300,3),output_arrays=None, tag_set=None, signature_key=None)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 757, in from_saved_model
    output_arrays, tag_set, signature_key)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py"", line 204, in freeze_saved_model
    util.set_tensor_shapes(in_tensors, input_shapes)
  File ""/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py"", line 145, in set_tensor_shapes
    for name, shape in shapes.items():
AttributeError: 'tuple' object has no attribute 'items'

can any one helps me?"
30457,Dataset: Feedable Iterator does not support tf.VarLenFeature (SparseTensor).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): **1.13.1**
- Python version: 3.5.2
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I want to do validation after some training steps, so I choose `Feedable Iterator API`. However Feedable Iterator API does not support tf.VarLenFeature(namely `SparseTensor`), VarLenFeature is useful for Recommend System.

Any suggestions to work around? Below is a code snippet.

**Code to reproduce the issue**
1. Generate the test data
```
import tensorflow as tf                                                         
import numpy as np                                                              
                                                                                
def save_tfrecords(data, label, desfile):                                       
    with tf.python_io.TFRecordWriter(desfile) as writer:                        
        for i in range(len(data)):                                              
            features = tf.train.Features(                                       
                feature = {                                                     
                    ""data"":tf.train.Feature(int64_list = tf.train.Int64List(value=data[i])),
                    ""label"":tf.train.Feature(int64_list = tf.train.Int64List(value=[label[i]]))
                }                                                               
            )                                                                   
            example = tf.train.Example(features = features)                     
            serialized = example.SerializeToString()                            
            writer.write(serialized)                                            
                                                                                
data_size = 32                                                                  
data = []                                                                       
for i in range(data_size):                                                      
    tmp = np.array(range(i + 1)).astype(np.int64)                               
    data.append(tmp)                                                            
label = np.array(range(data_size)).astype(np.int64)                             
save_tfrecords(data, label, 'train.tfrecord')                                   
                                                                                
data_size = 8                                                                   
data = []                                                                       
for i in range(data_size):                                                      
    tmp = np.array(range(i + 1)).astype(np.int64)                               
    data.append(tmp)                                                            
label = np.array(range(data_size)).astype(np.int64)                             
save_tfrecords(data, label, 'test.tfrecord')   
```
2. Run the test code.
```
import tensorflow as tf                                                         
                                                                                
def get_dataset(files):                                                         
    def parser(record):                                                         
      keys_to_features = {                                                      
          ""data"": tf.VarLenFeature(tf.int64),                                   
          ""label"": tf.FixedLenFeature((), tf.int64),                            
      }                                                                         
      parsed = tf.parse_single_example(record, keys_to_features)                
                                                                                
      # parsed['data'] = parsed['data'].values                                  
                                                                                
      return parsed                                                             
                                                                                
    dataset = tf.data.TFRecordDataset(files)                                    
    dataset = dataset.map(parser)                                               
    dataset = dataset.batch(4)                                                  
    dataset = dataset.repeat(1)                                                 
    return dataset                                                              
                                                                                
                                                                                
graph = tf.Graph()                                                              
with graph.as_default():                                                        
    training_ds = get_dataset('train.tfrecord')                                 
    validation_ds = get_dataset('test.tfrecord')                                
                                                                                
    handle = tf.placeholder(tf.string, shape=[])                                
    iterator = tf.data.Iterator.from_string_handle(                             
        handle, training_ds.output_types, training_ds.output_shapes)            
    print(training_ds.output_types, training_ds.output_shapes)                  
    next_element = iterator.get_next()                                          
                                                                                
    training_iterator = training_ds.make_initializable_iterator()               
    validation_iterator = validation_ds.make_initializable_iterator()           
                                                                                
                                                                                
with graph.as_default():                                                        
                                                                                
    with tf.train.MonitoredTrainingSession() as sess:                           
        training_handle = sess.run(training_iterator.string_handle())           
        validation_handle = sess.run(validation_iterator.string_handle())       
        sess.run(training_iterator.initializer)                                 
        count_training = 0                                                      
        while not sess.should_stop():                                           
            x = sess.run(next_element, feed_dict={handle: training_handle})     
            count_training += 1                                                 
            print('{} [training] {}'.format(count_training, x))                 
                                                                                
            if count_training % 4 == 0:                                         
                sess.run(validation_iterator.initializer)                       
                count_validation = 0                                            
                while True:                                                     
                    try:                                                        
                        y = sess.run(next_element, feed_dict={handle: validation_handle})
                        count_validation += 1                                   
                        print('  {} [validation] {}'.format(count_validation, y))
                    except tf.errors.OutOfRangeError:                           
                        break    
```
**Other info / logs**
```
{'data': tf.int64, 'label': tf.int64} {'data': TensorShape([Dimension(None), Dimension(None)]), 'label': TensorShape([Dimension(None)])}

W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at iterator_ops.cc:1225 : Invalid argument: Data type mismatch at component 0: expected int64 but got variant.
Traceback (most recent call last):
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Data type mismatch at component 0: expected int64 but got variant.
	 [[{{node IteratorFromStringHandleV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 46, in <module>
    x = sess.run(next_element, feed_dict={handle: training_handle})
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run
    run_metadata=run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run
    run_metadata=run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run
    raise six.reraise(*original_exc_info)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    return self._sess.run(*args, **kwargs)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run
    run_metadata=run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run
    return self._sess.run(*args, **kwargs)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Data type mismatch at component 0: expected int64 but got variant.
	 [[node IteratorFromStringHandleV2 (defined at test.py:30) ]]

Caused by op 'IteratorFromStringHandleV2', defined at:
  File ""test.py"", line 30, in <module>
    handle, training_ds.output_types, training_ds.output_shapes)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 288, in from_string_handle
    output_shapes=output_structure._flat_shapes)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1596, in iterator_from_string_handle_v2
    output_shapes=output_shapes, name=name)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Data type mismatch at component 0: expected int64 but got variant.
	 [[node IteratorFromStringHandleV2 (defined at test.py:30) ]]
```
"
30456,Unable to use canned RNN Estimator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not really (very close to stock example [here](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNEstimator))
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Version 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip (from binary?)
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

`TypeError: Input must be a SparseTensor.` when using [RNNEstimator](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNEstimator).

**Describe the expected behavior**

No errors when used correctly (maybe that is the issue).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

sequence_feature_colums = [tf.contrib.feature_column.sequence_numeric_column(""test"")]

estimator = tf.contrib.estimator.RNNEstimator(
    head=tf.contrib.estimator.regression_head(),
    sequence_feature_columns=sequence_feature_colums)

def input_fn_train():
  dataset = tf.data.Dataset.from_tensor_slices(({""test"": [0]}, [0]))
  dataset = dataset.batch(1)
  return dataset

estimator.train(input_fn=input_fn_train, steps=1)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30455,loop problem in defining the network,"
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state:**

Each data A has the shape of [height, width, channel] and is saved in TFrecord file.
I use an iterator to read batches from my TFrecord file. The input of my network is a 4-dimensional tensor A_batch with shape [batch, height, width, channel]. 

Let Network() represents the network i define and Output represents the output of the network:
Output = Network(A_batch)

Each data A has the shape of [height, width, channel]. I want to have different trainable ops on each channel like (assume channel=3):
```

    ops_0 on A[ : , : , 0]
    ops_1 on A[ : , : , 1]
    ops_2 on A[ : , : , 2]

```

However, the input A_batch with the shape [batch, height, width, channel]. It means i need to loop on batch (assume channel=3):

```
for m in range(batch)
    ops_0 on A_batch [ m , : , :，0 ]
    ops_1 on A_batch [ m , : , :，1 ]
    ops_2 on A_batch [ m , : , :，2 ]

```
**Note that after trainable ops, original data becomes trainable variable.**

**Question 1:**
In defining the network, is it allowed to separate batch data to perform different trainable ops in loop? 

**Question 2:**
In defining the network, is it allowed to assign trainable variable to a tensor to form another trainable variable? 

for example, let A_batch_operated denotes the trainable variable after performing trainable ops_0, ops_1,and ops_2 on batch data A_batch :

```
for m in range(batch)
    A_batch_operated [ m , : , :，0 ]= ops_0 (A_batch [ m , : , :，0 ] )
    A_batch_operated [ m , : , :，1 ]= ops_1 (A_batch [ m , : , :，1 ] )
    A_batch_operated [ m , : , :，2 ]= ops_2 (A_batch [ m , : , :，2 ] )

```
**Question 3:**
Can i add a trainable ops on the new trainable variable on A_batch_operated?  like:

Output= ops_3 (A_batch_operated)


Thank you



**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Some researchers of image/video processing
Any Other info.

"
30454,curses is not supported on this machine (please install/reinstall curses for an optimal experience),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30453,[TF 2.0] categorical_column_with_vocabulary_list not usable in custom training loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Outside of `fit`, e.g in a custom training loop, `categorical_column_with_vocabulary_list` results in an error. I have provided a modified version of [Classifying Structured data](https://www.tensorflow.org/beta/tutorials/keras/feature_columns) which demonstrates this.

The error is `ValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: <dtype: 'string'>, tensor dtype: <dtype: 'int32'>`

**Describe the expected behavior**

Code runs without causing an error

**Code to reproduce the issue**

It should be directly copy-paste-able 

```
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split


def df_to_dataset(df, shuffle=True, batch_size=32):
    df = df.copy()
    labels = df.pop('target')
    ds = tf.data.Dataset.from_tensor_slices(
        (dict(df), labels)
    )
    if shuffle:
        ds = ds.shuffle(buffer_size=len(df))
    ds = ds.batch(batch_size)
    return ds


def generate_features():
    feature_columns = []
    feature_layer_inputs = {}


    thal = tf.feature_column.categorical_column_with_vocabulary_list(
          'thal', ['fixed', 'normal', 'reversible'])
    thal_one_hot = tf.feature_column.indicator_column(thal)
    feature_columns.append(thal_one_hot)
    feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

    return feature_columns, feature_layer_inputs


def create_model(feature_columns, feature_layer_inputs):
    input_layer = tf.keras.layers.DenseFeatures(feature_columns)
    inputs = input_layer(feature_layer_inputs)

    l1 = tf.keras.layers.Dense(128, activation='relu')(inputs)
    l2 = tf.keras.layers.Dense(128, activation='relu')(l1)

    output = tf.keras.layers.Dense(1, activation='sigmoid')(l2)

    model = tf.keras.Model(
        inputs=[v for v in feature_layer_inputs.values()],
        outputs=[output]
    )
    return model


def make_loss(loss_object):
    def loss(model, x, y):
        y_pred = model(x)
        return loss_object(y_true=y, y_pred=y_pred)
    return loss


def grad(model, inputs, targets, loss):
    with tf.GradientTape() as tape:
        loss_value = loss(model, inputs, targets)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)


def fit(epochs, train_ds, model, optimizer, loss_obj):
    loss = make_loss(loss_obj)
    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_ds):
            loss_values, grad_values = grad(model, x, y, loss)
            optimizer.apply_gradients(zip(grad_values, model.trainable_variables))


if __name__ == '__main__':
    URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
    df = pd.read_csv(URL)
    CUSTOM_TRAINING = True

    train, test = train_test_split(df, test_size=0.2)
    train, val = train_test_split(train, test_size=0.2)

    # hardcoded stuff
    batch_size = 32
    train_ds = df_to_dataset(train, batch_size=batch_size)

    # Create model and features
    feature_columns, feature_layer_inputs = generate_features()
    model = create_model(feature_columns, feature_layer_inputs)

    if CUSTOM_TRAINING:
        print('Trying custom training')
        bce = tf.keras.losses.BinaryCrossentropy()
        adam = tf.keras.optimizers.Adam()
        fit(epochs=5, train_ds=train_ds,
            model=model, optimizer=adam, loss_obj=bce)
    else:
        print('Using pre-defined fit')
        model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy'])
        model.fit(train_ds, epochs=5)
```

If you flip the `CUSTOM_TRAINING` variable between `True` and `False` (line 72) you'll see what I mean. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

1) The complete (relevant) stacktrace is 

```
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1758, in <module>
    main()
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1752, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1147, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py"", line 90, in <module>
    model=model, optimizer=adam, loss_obj=bce)
  File ""/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py"", line 65, in fit
    loss_values, grad_values = grad(model, x, y, loss)
  File ""/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py"", line 57, in grad
    loss_value = loss(model, inputs, targets)
  File ""/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py"", line 50, in loss
    y_pred = model(x)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 753, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 895, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 474, in call
    self._state_manager)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 4299, in get_dense_tensor
    return transformation_cache.get(self, state_manager)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 2562, in get
    transformed = column.transform_feature(self, state_manager)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 4238, in transform_feature
    transformation_cache, state_manager)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 3714, in get_sparse_tensors
    transformation_cache.get(self, state_manager), None)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 2562, in get
    transformed = column.transform_feature(self, state_manager)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 3692, in transform_feature
    return self._transform_input_tensor(input_tensor)
  File ""/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 3668, in _transform_input_tensor
    self.key, self.dtype, input_tensor.dtype))
ValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: <dtype: 'string'>, tensor dtype: <dtype: 'int32'>
```

2)  Placing a debugger on line 50 leads me to `feature_column_v2.py` specifically `_transform_input_tensor`

The `input_tensor` arg to `_transform_input_tensor` is 

```
SparseTensor(indices=tf.Tensor(
[[ 0  0]
 [ 1  0]
 [ 2  0]
 [ 3  0]
 [ 4  0]
 [ 5  0]
 [ 6  0]
 [ 7  0]
 [ 8  0]
 [ 9  0]
 [10  0]
 [11  0]
 [12  0]
 [13  0]
 [14  0]
 [15  0]
 [16  0]
 [17  0]
 [18  0]
 [19  0]
 [20  0]
 [21  0]
 [22  0]
 [23  0]
 [24  0]
 [25  0]
 [26  0]
 [27  0]
 [28  0]
 [29  0]
 [30  0]
 [31  0]], shape=(32, 2), dtype=int64), values=tf.Tensor(
[49 34 58 46 59 47 55 58 41 68 62 51 61 46 39 48 37 41 51 59 51 70 60 57
 54 60 52 44 65 49 44 59], shape=(32,), dtype=int32), dense_shape=tf.Tensor([32  1], shape=(2,), dtype=int64))
```

which seems strange? It's like it forgot that it had transformed those variables? "
30451,Add warning to dropout that it violates Google patent US9406017B2,"
## Description of issue (what needs changing):

Google who to a large extent runs this project has patented dropout.  See https://patents.google.com/patent/US9406017B2/en

This patent is not listed as a pledged patent in Google's list of patents that are thus *not* covered by the [Google Open Patent Non-Assertion Pledge](https://www.google.com/patents/opnpledge/patents/)

Since this could cause serious issues for users, it needs to be documented.
"
30450,tensorboard 1.14.0 has requirement setuptools,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
30449,Error during build on Windows ( SET PATH error),"**System information**
- Windows 10
- Compiled from source
- TensorFlow version 1.14
- Python version 3.6
- Installed using virtualenv? No . pip? Yes:
- Bazel version : 0.27.1
- CUDA/cuDNN version: N/A ( built for CPU) 
- GPU model and memory: N/A ( CPU build) 




**Describe the problem**
So during the installation , everything goes rather well  , but it fails on what seems to be the last step . 

``````
E:\tf\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
``````
 Result : 

``````
WARNING: E:/tf/tensorflow/tensorflow/python/BUILD:3469:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tf/tensorflow/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tf/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: E:/tf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: E:/tf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: E:/tf/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: E:/tf/tensorflow/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 2 targets configured).
INFO: Found 1 target...
ERROR: E:/tf/tensorflow/tensorflow/lite/toco/BUILD:54:1: ProtoCompile tensorflow/lite/toco/model_flags_pb2.py failed (Exit -1073741795): protoc.exe failed: error executing command
  cd C:/users/admin/_bazel_admin/meeqa4gn/execroot/org_tensorflow
  SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\ProgramData\DockerDesktop\version-bin;C:\Program Files\Docker\Docker\Resources\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\nodejs\;C:\Program Files (x86)\GtkSharp\2.12\bin;E:\Program Files\Golem\;C:\Program Files\dotnet\;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\90\Tools\binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\DTS\Binn\;C:\mingw64\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\wamp64\bin\php\php7.2.4;C:\ProgramData\ComposerSetup\bin;C:\Program Files (x86)\Yarn\bin\;C:\Users\admin\AppData\Local\Programs\Python\Python36\Scripts\;C:\Users\admin\AppData\Local\Programs\Python\Python36\;C:\Users\admin\AppData\Local\Microsoft\WindowsApps;C:\Users\admin\AppData\Roaming\npm;C:\Program Files\Heroku\bin;C:\Program Files\Docker Toolbox;F:\opencv\bin;C:\Program Files\MPICH2\bin;C:\Program Files\LAMMPS 64-bit 7Dec2018\bin;C:\Users\admin\AppData\Roaming\LINKS\bin;C:\Users\admin\AppData\Roaming\Composer\vendor\bin;C:\Program Files\Git\bin;C:\Users\admin\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\admin\AppData\Local\Yarn\bin;
    SET PYTHON_BIN_PATH=C:/Users/admin/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/admin/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF_CONFIGURE_IOS=0
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  bazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc.exe --python_out=bazel-out/x64_windows-py2-opt/bin -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python tensorflow/lite/toco/model_flags.proto
Execution platform: @bazel_tools//platforms:host_platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 38.214s, Critical Path: 7.35s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

``````

I'm pretty sure the problem comes from the SET PATH= ... line , here's the output of it alone if i copy-paste it into the terminal : 
 ``````
C:\msys64\bin : The term 'C:\msys64\bin' is not recognized as the name of a cmdlet, function, script file, or operable
program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:30
+   SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\ProgramData\DockerDeskt ...
+                              ~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (C:\msys64\bin:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException

C:\ProgramData\DockerDesktop\version-bin : The term 'C:\ProgramData\DockerDesktop\version-bin' is not recognized as
the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was
included, verify that the path is correct and try again.
At line:1 char:44
+ ... sr\bin;C:\msys64\bin;C:\ProgramData\DockerDesktop\version-bin;C:\Prog ...
+                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (C:\ProgramData\DockerDesktop\version-bin:String) [], CommandNotFoundExc
   eption
    + FullyQualifiedErrorId : CommandNotFoundException
``````
and a whole lot of those. 
I'm using Powershell , too , i don't know if it's important

"
30448,Keras has memory leak when passing in dataset object to `predict(...)` function,"**Summary**
Performance degrades quickly and memory increases consistently when calling the Keras `predict` function in a loop with a dataset object. This does not happen when passing `predict` a numpy array, or when passing in a tensor from a dataset iterator.

**System information**
- Have I written custom code: Minimally reproducible example below uses only stock 1.14.0 code.
- OS Platform and Distribution: Ubuntu 18.04 / Linux Mint 19.1
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu` (example not using GPU, `CUDA_VISIBLE_DEVICES=-1`)
- TensorFlow version (use command below): `v1.14.0-rc1-22-gaf24dc9 1.14.0`
- Python version: `3.7.3`

**Describe the current behavior**
Looping over `model.predict(x=mydataset)` in a continuous loop degrades in performance after a few hundred iterations. The minimally reproducible example below starts at ~0.04s per loop iteration and within about a minute of running is near 0.5s per loop iteration. Memory continues to climb.

This does not happen when passing in a numpy array to `model.predict(x=myndarray)`. The problem is also less severe when passing in `tf.data.Iterator` rather than a `tf.data.Dataset`. If you pass an iterator the performance will continue to degrade at a fifth to a tenth the rate. 

The cause of the difference between the dataset performance and the iterator performance is likely at `training_utils.py:1314` where Keras creates a new iterator for each `predict` loop. 

The issue is completely ameliorated when passing predict the tensor produced from `tf.data.make_one_shot_iterator(mydataset).get_next()`. In this case no additional dataset operations appear to be created by keras in the `predict` loop.

**Describe the expected behavior**
Multiple calls to `predict` should not degrade in performance over time when passing in a dataset. 

**Code to reproduce the issue**
This code reproduces the issue and is copy/paste runnable, performance will degrade significantly within ~30 seconds running this example.
```
import tensorflow as tf
import numpy as np
import time

SIZE = 5000

inp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')
x = tf.keras.layers.Dense(units=SIZE)(inp)

model = tf.keras.Model(inputs=inp, outputs=x)

np_data = np.random.rand(1, SIZE)
ds = tf.data.Dataset.from_tensor_slices(np_data).batch(1).repeat()

debug_time = time.time()
while True:
    model.predict(x=ds, steps=1)
    print('Processing time {:.2f}'.format(time.time() - debug_time))
    debug_time = time.time()
```

This example demonstrates passing a numpy array does not have the same issue.
```
import tensorflow as tf
import numpy as np
import time

SIZE = 5000

inp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')
x = tf.keras.layers.Dense(units=SIZE)(inp)

model = tf.keras.Model(inputs=inp, outputs=x)

np_data = np.random.rand(1, SIZE)

debug_time = time.time()
while True:
    model.predict(x=np_data)  # using numpy array directly
    print('Processing time {:.2f}'.format(time.time() - debug_time))
    debug_time = time.time()
```

This issue started at SO at: https://stackoverflow.com/questions/56910950/keras-predict-loop-memory-leak-using-tf-data-dataset-but-not-with-a-numpy-array

I decided to post it here when I realized that `predict` is creating a new iterator each predict loop iteration, and works when the get_next tensor is passed in directly."
30447,kissfft compile errors due to old archived version of kissfft,"external ""kissfft"" fails to compile on Windows 10

**System Information**
Windows 10 v1903 on desktop computer 
Building Latest git pull of Tensorflow (7/6/2019)
MSys64
Python 3.6.8 :: Anaconda, Inc.
CUDA 10.1
cuDNN 7
Bazel 0.24.1
MS Visual Studio 2019 Community
NVIDIA GeForce GTX 850M
TF_CUDA_COMPUTE_CAPABILITIES=""5.0""

Build Command:
bazel build --config=opt --config=v2 --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

Problem:
When building Tensorflow from source, I get the following errors:

c:\users\lawrence\_bazel_lawrence\jc3cdzjy\execroot\org_tensorflow\external\kissfft\kiss_fft.h(60): error C2061: syntax error: identifier 'int16_t'
...

The problem is line 46 of kiss_fft.h:
""#include <sys/types.h>""
which is an attempt to define int16_t. This doesn't work in Windows, and is depricated in unix. The correct approach is:
""#include <stdint.h>""

NOTE: The author of kissfft has fixed this in recent distributions (see https://github.com/mborgerding/kissfft/blob/master/kiss_fft.h) so updating the tensorflow external archive with a more recent version of kissfft would resolve this issue.
"
30445,[TF 2.0 API Docs] tf.image.image_gradients,"## URL(s) with the issue:

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/image_gradients
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py

## Description of issue (what needs changing):

### Usage example

No usage example provided

### Submit a pull request?

Yes
https://github.com/tensorflow/tensorflow/pull/30446"
30444,ctc_* api support custom blank_index,"since `ctc_loss_v2(labels, logits, label_length, logit_length, logits_time_major=True, unique=None, blank_index=None, name=None) ` has supporting `blank_index`,  I think the `ctc_greed_decode_v2` and `ctc_beam_search_decode_v2` also should support it."
30443,Densnet169 produces memory leak in TF2.0 but works well with latest TF1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
- Python version:3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: colab version
- GPU model and memory: colab version

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Densnet169 crashes with memory error even with datagenerator and a batchsize of 8.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30442,License used in tensorflow repos,"Hello Tensorflow Community, 

I am not sure this is the right place to ask this question, but if I were to create my own repository that uses tensorflow and does not modify or add to tensorflow, which license am I required to use for the code that uses tensorflow?  If this is not the correct place to ask this, please direct me to the correct place. 

Thanks
"
30441,EntityTooLarge when saving estimator checkpoint too big to s3.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.0 (Final)
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): ('v1.11.0-0-gc19e29306c', '1.11.0')
- Python version: Python 2.7.15

**Describe the current behavior**
I build a tf.estimator.LinearClassifier with 5 categorical_column_with_hash_bucket(hash_bucket_size=100\*1024\*1024).
When passing model_dir with a local file system, it saved successfully.
When passing model_dir with a s3 path, it failed with exception EntityTooLarge.



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import logging
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
from tensorflow.python.lib.io import file_io


def test_tf_estimator(bucket='tensorflow-testing'):
  data_file = ['s3://%s/input_data.txt'%(bucket)]
  label_name = 'click_label'
  feas = 'fea1,fea2,fea3,fea4,fea5'.split(',')
  input_data_schema = [label_name] + feas
  input_data_dtypes = [ 
    [0.0 if col==label_name else '']
      for col in input_data_schema 
  ]
  input_data_sep = '\t'
  def gen_input_data(line_per_file=2000):
    for path in data_file :
      with file_io.FileIO(path, 'w') as fout:
        for i in range(line_per_file):
          label = int(i % 10 == 0)
          cols = [label] + [ '%s_%s'%(col, i%107) for col in feas ] 
          fout.write('%s\n'%(input_data_sep.join(map(str, cols))))
  gen_input_data()
  def parse_csv(value):
    columns = tf.decode_csv(value, record_defaults=input_data_dtypes, field_delim=input_data_sep, use_quote_delim=False)
    features = dict(zip(input_data_schema, columns))
    label = features[label_name]
    return features, label
  def input_fn(batch_size=1000):
    dataset = tf.data.TextLineDataset(data_file)
    dataset = dataset.prefetch(buffer_size=batch_size*10)
    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_csv, num_parallel_batches=4, batch_size=batch_size))
    iterator = dataset.make_one_shot_iterator()
    features, labels = iterator.get_next()
    return features, labels
  fids = [ tf.feature_column.categorical_column_with_hash_bucket(fname, 100*1024*1024) for fname in feas ]

  feature_columns = fids
  partitioner = None
  partitioner = tf.fixed_size_partitioner(16, axis=0) 
  estimator = tf.estimator.LinearClassifier(feature_columns, model_dir='s3://%s/model_dir'%(bucket), partitioner=partitioner)
  estimator.train(input_fn=input_fn, steps=1)
  estimator.evaluate(input_fn=input_fn, steps=1)

if __name__ == ""__main__"":
  test_tf_estimator()
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

When passing model_dir with a local file system, it saved successfully.
```
tensorflow-testing/model_dir/
|-- [ 124]  checkpoint
|-- [  64]  eval
|-- [7.6M]  events.out.tfevents.1562343007.9-21-165-119
|-- [4.4M]  graph.pbtxt
|-- [   8]  model.ckpt-0.data-00000-of-00002
|-- [5.9G]  model.ckpt-0.data-00001-of-00002
|-- [ 22K]  model.ckpt-0.index
|-- [2.2M]  model.ckpt-0.meta
|-- [   8]  model.ckpt-1.data-00000-of-00002
|-- [5.9G]  model.ckpt-1.data-00001-of-00002
|-- [ 22K]  model.ckpt-1.index
`-- [2.2M]  model.ckpt-1.meta

1 directory, 11 files
```

When passing model_dir with a s3 path, it failed with exception EntityTooLarge.
```
2019-07-05 23:56:40.450188: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2019-07-05 23:57:22.245940: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
Traceback (most recent call last):
  File ""test_s3.py"", line 82, in <module>
    main()
  File ""test_s3.py"", line 77, in main
    test_tf_estimator()
  File ""test_s3.py"", line 67, in test_tf_estimator
    estimator.train(input_fn=input_fn, steps=1)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1406, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 807, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 567, in after_create_session
    self._save(session, global_step)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 598, in _save
    self._get_saver().save(session, self._save_path, global_step=step)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1433, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: EntityTooLarge: Unable to parse ExceptionName: EntityTooLarge Message: 
     [[{{node save/SaveV2_1}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, save/Identity_487, save/Identity_489, save/Identity_491, save/Identity_493, save/Identity_495, save/Identity_497, save/Identity_499, save/Identity_501, save/Identity_503, save/Identity_505, save/Identity_507, save/Identity_509, save/Identity_511, save/Identity_513, save/Identity_515, save/Identity_517, save/Identity_519, save/Identity_521, save/Identity_523, save/Identity_525, save/Identity_527, save/Identity_529, save/Identity_531, save/Identity_533, save/Identity_535, save/Identity_537, save/Identity_539, save/Identity_541, save/Identity_543, save/Identity_545, save/Identity_547, save/Identity_549, save/Identity_551, save/Identity_553, save/Identity_555, save/Identity_557, save/Identity_559, save/Identity_561, save/Identity_563, save/Identity_565, save/Identity_567, save/Identity_569, save/Identity_571, save/Identity_573, save/Identity_575, save/Identity_577, save/Identity_579, save/Identity_581, save/Identity_583, save/Identity_585, save/Identity_587, save/Identity_589, save/Identity_591, save/Identity_593, save/Identity_595, save/Identity_597, save/Identity_599, save/Identity_601, save/Identity_603, save/Identity_605, save/Identity_607, save/Identity_609, save/Identity_611, save/Identity_613, save/Identity_615, save/Identity_617, save/Identity_619, save/Identity_621, save/Identity_623, save/Identity_625, save/Identity_627, save/Identity_629, save/Identity_631, save/Identity_633, save/Identity_635, save/Identity_637, save/Identity_639, save/Identity_641, save/Identity_643, save/Identity_645, save/Identity_647, save/Identity_649, save/Identity_651, save/Identity_653, save/Identity_655, save/Identity_657, save/Identity_659, save/Identity_661, save/Identity_663, save/Identity_665, save/Identity_667, save/Identity_669, save/Identity_671, save/Identity_673, save/Identity_675, save/Identity_677, save/Identity_679, save/Identity_681, save/Identity_683, save/Identity_685, save/Identity_687, save/Identity_689, save/Identity_691, save/Identity_693, save/Identity_695, save/Identity_697, save/Identity_699, save/Identity_701, save/Identity_703, save/Identity_705, save/Identity_707, save/Identity_709, save/Identity_711, save/Identity_713, save/Identity_715, save/Identity_717, save/Identity_719, save/Identity_721, save/Identity_723, save/Identity_725, save/Identity_727, save/Identity_729, save/Identity_731, save/Identity_733, save/Identity_735, save/Identity_737, save/Identity_739, save/Identity_741, save/Identity_743, save/Identity_745, save/Identity_747, save/Identity_749, save/Identity_751, save/Identity_753, save/Identity_755, save/Identity_757, save/Identity_759, save/Identity_761, save/Identity_763, save/Identity_765, save/Identity_767, save/Identity_769, save/Identity_771, save/Identity_773, save/Identity_775, save/Identity_777, save/Identity_779, save/Identity_781, save/Identity_783, save/Identity_785, save/Identity_787, save/Identity_789, save/Identity_791, save/Identity_793, save/Identity_795, save/Identity_797, save/Identity_799, save/Identity_801, save/Identity_803, save/Identity_805, save/Identity_807, save/Identity_809, save/Identity_811, save/Identity_813, save/Identity_815, save/Identity_817, save/Identity_819, save/Identity_821, save/Identity_823, save/Identity_825, save/Identity_827, save/Identity_829, save/Identity_831, save/Identity_833, save/Identity_835, save/Identity_837, save/Identity_839, save/Identity_841, save/Identity_843, save/Identity_845, save/Identity_847, save/Identity_849, save/Identity_851, save/Identity_853, save/Identity_855, save/Identity_857, save/Identity_859, save/Identity_861, save/Identity_863, save/Identity_865, save/Identity_867, save/Identity_869, save/Identity_871, save/Identity_873, save/Identity_875, save/Identity_877, save/Identity_879, save/Identity_881, save/Identity_883, save/Identity_885, save/Identity_887, save/Identity_889, save/Identity_891, save/Identity_893, save/Identity_895, save/Identity_897, save/Identity_899, save/Identity_901, save/Identity_903, save/Identity_905, save/Identity_907, save/Identity_909, save/Identity_911, save/Identity_913, save/Identity_915, save/Identity_917, save/Identity_919, save/Identity_921, save/Identity_923, save/Identity_925, save/Identity_927, save/Identity_929, save/Identity_931, save/Identity_933, save/Identity_935, save/Identity_937, save/Identity_939, save/Identity_941, save/Identity_943, save/Identity_945, save/Identity_947, save/Identity_949, save/Identity_951, save/Identity_953, save/Identity_955, save/Identity_957, save/Identity_959, save/Identity_961, save/Identity_963, save/Identity_965, save/Identity_967, save/Identity_969, save/Identity_971)]]

Caused by op u'save/SaveV2_1', defined at:
  File ""test_s3.py"", line 82, in <module>
    main()
  File ""test_s3.py"", line 77, in main
    test_tf_estimator()
  File ""test_s3.py"", line 67, in test_tf_estimator
    estimator.train(input_fn=input_fn, steps=1)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1181, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1215, in _train_model_default
    saving_listeners)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1406, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session
    self._scaffold.finalize()
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 215, in finalize
    self._saver.build()
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 778, in _build_internal
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 369, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 343, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 284, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 202, in save_op
    tensors)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1690, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): EntityTooLarge: Unable to parse ExceptionName: EntityTooLarge Message: 
     [[{{node save/SaveV2_1}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, save/Identity_487, save/Identity_489, save/Identity_491, save/Identity_493, save/Identity_495, save/Identity_497, save/Identity_499, save/Identity_501, save/Identity_503, save/Identity_505, save/Identity_507, save/Identity_509, save/Identity_511, save/Identity_513, save/Identity_515, save/Identity_517, save/Identity_519, save/Identity_521, save/Identity_523, save/Identity_525, save/Identity_527, save/Identity_529, save/Identity_531, save/Identity_533, save/Identity_535, save/Identity_537, save/Identity_539, save/Identity_541, save/Identity_543, save/Identity_545, save/Identity_547, save/Identity_549, save/Identity_551, save/Identity_553, save/Identity_555, save/Identity_557, save/Identity_559, save/Identity_561, save/Identity_563, save/Identity_565, save/Identity_567, save/Identity_569, save/Identity_571, save/Identity_573, save/Identity_575, save/Identity_577, save/Identity_579, save/Identity_581, save/Identity_583, save/Identity_585, save/Identity_587, save/Identity_589, save/Identity_591, save/Identity_593, save/Identity_595, save/Identity_597, save/Identity_599, save/Identity_601, save/Identity_603, save/Identity_605, save/Identity_607, save/Identity_609, save/Identity_611, save/Identity_613, save/Identity_615, save/Identity_617, save/Identity_619, save/Identity_621, save/Identity_623, save/Identity_625, save/Identity_627, save/Identity_629, save/Identity_631, save/Identity_633, save/Identity_635, save/Identity_637, save/Identity_639, save/Identity_641, save/Identity_643, save/Identity_645, save/Identity_647, save/Identity_649, save/Identity_651, save/Identity_653, save/Identity_655, save/Identity_657, save/Identity_659, save/Identity_661, save/Identity_663, save/Identity_665, save/Identity_667, save/Identity_669, save/Identity_671, save/Identity_673, save/Identity_675, save/Identity_677, save/Identity_679, save/Identity_681, save/Identity_683, save/Identity_685, save/Identity_687, save/Identity_689, save/Identity_691, save/Identity_693, save/Identity_695, save/Identity_697, save/Identity_699, save/Identity_701, save/Identity_703, save/Identity_705, save/Identity_707, save/Identity_709, save/Identity_711, save/Identity_713, save/Identity_715, save/Identity_717, save/Identity_719, save/Identity_721, save/Identity_723, save/Identity_725, save/Identity_727, save/Identity_729, save/Identity_731, save/Identity_733, save/Identity_735, save/Identity_737, save/Identity_739, save/Identity_741, save/Identity_743, save/Identity_745, save/Identity_747, save/Identity_749, save/Identity_751, save/Identity_753, save/Identity_755, save/Identity_757, save/Identity_759, save/Identity_761, save/Identity_763, save/Identity_765, save/Identity_767, save/Identity_769, save/Identity_771, save/Identity_773, save/Identity_775, save/Identity_777, save/Identity_779, save/Identity_781, save/Identity_783, save/Identity_785, save/Identity_787, save/Identity_789, save/Identity_791, save/Identity_793, save/Identity_795, save/Identity_797, save/Identity_799, save/Identity_801, save/Identity_803, save/Identity_805, save/Identity_807, save/Identity_809, save/Identity_811, save/Identity_813, save/Identity_815, save/Identity_817, save/Identity_819, save/Identity_821, save/Identity_823, save/Identity_825, save/Identity_827, save/Identity_829, save/Identity_831, save/Identity_833, save/Identity_835, save/Identity_837, save/Identity_839, save/Identity_841, save/Identity_843, save/Identity_845, save/Identity_847, save/Identity_849, save/Identity_851, save/Identity_853, save/Identity_855, save/Identity_857, save/Identity_859, save/Identity_861, save/Identity_863, save/Identity_865, save/Identity_867, save/Identity_869, save/Identity_871, save/Identity_873, save/Identity_875, save/Identity_877, save/Identity_879, save/Identity_881, save/Identity_883, save/Identity_885, save/Identity_887, save/Identity_889, save/Identity_891, save/Identity_893, save/Identity_895, save/Identity_897, save/Identity_899, save/Identity_901, save/Identity_903, save/Identity_905, save/Identity_907, save/Identity_909, save/Identity_911, save/Identity_913, save/Identity_915, save/Identity_917, save/Identity_919, save/Identity_921, save/Identity_923, save/Identity_925, save/Identity_927, save/Identity_929, save/Identity_931, save/Identity_933, save/Identity_935, save/Identity_937, save/Identity_939, save/Identity_941, save/Identity_943, save/Identity_945, save/Identity_947, save/Identity_949, save/Identity_951, save/Identity_953, save/Identity_955, save/Identity_957, save/Identity_959, save/Identity_961, save/Identity_963, save/Identity_965, save/Identity_967, save/Identity_969, save/Identity_971)]]
```
"
30440,mpi_portable_platform.h not found because MPI_LIB_IS_OPENMPI variable is always set to True,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Linux 7.6 (Maipo) 
- TensorFlow installed from (source or binary): Issue when trying to build from source.
- TensorFlow version: Master Branch (as of 7/4/2019)
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: Nope.
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): 5.5

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```./configure``` with ```mpi``` enabled.

```bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.

Sorry, the log is somehow lost. But the error is about ```mpi_portable_platform.h``` not found. My system has ```mpich``` installed but not ```open_mpi```. The source file in ```tensorflow/third_party/mpi/mpi.bzl``` contains:

```
def mpi_hdr():
    MPI_LIB_IS_OPENMPI = True
    hdrs = []
    if MPI_LIB_IS_OPENMPI:
        hdrs = [""mpi.h"", ""mpi_portable_platform.h""]  #When using OpenMPI
    else:
        hdrs = [""mpi.h"", ""mpio.h"", ""mpicxx.h""]  #When using MVAPICH
    return hdrs
```

It seems that the variable ```MPI_LIB_IS_OPENMPI``` is always set to ```True``` which won't work if one does not use ```open mpi```. I set the variable to ```False``` and the error goes away.


"
30438,Calibration with Post-training quantization fails on networks that have multiple placeholders,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying to use post-training quantisation with a calibration data-set to capture the activation ranges. The network is an unrolled lstm-based model. This model has multiple placeholders (3 ph) and the post-training quantisation process breaks because of that.

**Describe the expected behavior**

It is expected the post-training quantisation method to accept networks with multiple placeholders. Trying the same code as below in other networks that has only one placeholder works as expected (although the reference implementations of quantise/de-quantise which are located at the beginning and at the end of the network is extremely slow, but this is another issue).

**Code to reproduce the issue**
```
def representative_dataset_gen():
    for _ in range(1):
        input_node = np.array(np.random.rand(1,16,19,26), dtype=np.float32)
        new_state_c = np.array(np.random.rand(1,2048), dtype=np.float32)
        new_state_h = np.array(np.random.rand(1,2048), dtype=np.float32)
        yield [input_node,new_state_c,new_state_h]
```

```
def load_graph_and_post_train():

    converter = tf.lite.TFLiteConverter.from_frozen_graph(args.graph, 
        input_arrays=[ ""input_node"",""previous_state_c"",""previous_state_h"" ], 
        output_arrays=[""logits""], 
        input_shapes={""input_node"": [1,16,19,26], ""previous_state_c"":[1,2048], ""previous_state_h"":[1,2048]} )

    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset_gen

    tflite_quant_model = converter.convert()
    open(""./models/post_train_calibration_model.tflite"", ""wb"").write(tflite_quant_model)

if __name__ == '__main__':
    parser = ArgumentParser(description='post-training quantisation')
    parser.add_argument('-graph', required=True, type=str, help='The pb model')
    args = parser.parse_args()
    load_graph_and_post_train()
```


**Other info / logs**
2019-07-05 14:03:13.996584: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-05 14:03:14.003163: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192860000 Hz
2019-07-05 14:03:14.007918: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b0b650 executing computations on platform Host. Devices:
2019-07-05 14:03:14.007953: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
WARNING: Logging before flag parsing goes to stderr.
W0705 14:03:15.397149 140670832994048 deprecation_wrapper.py:119] From post-trainin-quantisation-calib.py:96: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0705 14:03:15.397516 140670832994048 deprecation_wrapper.py:119] From post-trainin-quantisation-calib.py:96: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

2019-07-05 14:03:15.401800: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-07-05 14:03:15.401952: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-07-05 14:03:31.757150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-07-05 14:03:31.757222: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 388 nodes (-14), 510 edges (-14), time = 10271.6318ms.
2019-07-05 14:03:31.757235: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 388 nodes (0), 510 edges (0), time = 5687.64893ms.
INFO: Initialized TensorFlow Lite runtime.
terminate called after throwing an instance of 'std::out_of_range'
  what():  _Map_base::at
Abort (core dumped)
"
30437,"Remove ""global_step"" in usage example of CosineDecay and other related classes","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py

## Description of issue (what needs changing):
Usage example of CosineDecay, CosineDecayRestarts, LinearCosineDecay, NoisyLinearCosineDecay takes a parameter ""global_step"" which does not correspond to the definition. 

### Clear description

The script defines the various learning rate decay classes used for network training.

### Usage example

There is a usage example. However, it does not correspond to the definition. 

### Submit a pull request?

I can if required
"
30436,tf.data.Dataset.list_files return is deterministic order when shuffle=False?,"
## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files

## Description of issue (what needs changing):
In the doc above, it says 
```
NOTE: The default behavior of this method is to return filenames in 
a non-deterministic random shuffled order. 
Pass a seed or shuffle=False to get results in a deterministic order.
```

So if pass `shuffle=False`, it will return a deterministic order.

But if check source code of the function, it calls following function to get matching files.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L769

```
  @staticmethod
  def list_files(file_pattern, shuffle=None, seed=None):
      ...
      matching_files = gen_io_ops.matching_files(file_pattern)
```

If we check description of `gen_io_ops.matching_files`,  it says `Note also that the order of filenames returned can be non-deterministic.`

```
@tf_export('matching_files')
def matching_files(pattern, name=None):
  r""""""Returns the set of files matching one or more glob patterns.

  Note that this routine only supports wildcard characters in the

  basename portion of the pattern, not in the directory portion.

  Note also that the order of filenames returned can be non-deterministic.

  Args:
    pattern: A `Tensor` of type `string`.
      Shell wildcard pattern(s). Scalar or vector of type string.
    name: A name for the operation (optional).

  Returns:
    A `Tensor` of type `string`.
  """"""
```

And also the document in https://www.tensorflow.org/api_docs/python/tf/io/matching_files.

```
Defined in generated file: python/ops/gen_io_ops.py.

Note that this routine only supports wildcard characters in the basename portion of the pattern,
not in the directory portion. 
Note also that the order of filenames returned can be non-deterministic.
```

And also description in the function https://www.tensorflow.org/api_docs/python/tf/io/match_filenames_once 

```
Defined in python/training/input.py.

NOTE: The order of the files returned can be non-deterministic.
```

Check source code of the fucntion
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/input.py#L63

Both `tf.io.matching_files` and  tf.io.match_filenames_once` call `gen_io_ops.matching_files`.

I think it is quite confuse here."
30435,Operation marked as not fetchable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04, Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13 and 1.14
- Python version: 3
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: Nvidia GTX 960M 4GB

**Describe the current behavior**
I wanted to reference a list item inside tf.while_loop using a loop variable. Instead of printing the list items one after another it says the following:
```
ValueError: Operation 'while/Identity' has been marked as not fetchable.
```

**Describe the expected behavior**
Expected behaviour is to print the list items.

**Code to reproduce the issue**
```
import tensorflow as tf

class A():
    def __init__(self):
        self.lst = [1, 2, 3]
        self.sess = tf.Session()
        self.total_length = tf.constant(len(self.lst))

    def loop(self, i):
        pr = tf.print(i)
        current_value = self.lst[i.eval(session=self.sess)]
        with tf.control_dependencies([pr]):
            i = tf.add(i, 1)
        return [i]

    def cond(self, i):
        return tf.less(i, self.total_length) 

    def run(self):
        i = tf.constant(0)
        while_op = tf.while_loop(self.cond, self.loop, [i])
        final_i = self.sess.run(while_op)


if __name__ == ""__main__"":
    obj = A()
    obj.run()
```
I have tried eager execution as well but the same error pops up.

**Other info / logs**
This issue was originally a comment by me in [this thread](https://github.com/tensorflow/tensorflow/issues/4094#issuecomment-502346438). It was confirmed by @mrry that this indeed was a bug. 

Thanks for all the help!"
30433,Unable to build micro_speech for bluepill target (region `FLASH' overflowed by 9656 bytes),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow version: r1.14
- Python version:
- GCC/Compiler version (if compiling from source):
arm-none-eabi-gcc (15:4.9.3+svn231177-1) 4.9.3 20150529 (prerelease)
arm-none-eabi-g++ (15:4.9.3+svn231177-1) 4.9.3 20150529 (prerelease)

**Describe the problem**
I am unable to generate the binary for micro_speech_bin. The error log says that the FLASH memory is not big enough. I have had a similar issue with different build tests. If I increase the size of the FLASH memory (say to 90K), the executables and .bin can be generated and tested with renode. However, these can now not be ported to the Blue Pill development board (I assume, our dev board has not arrived yet).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=bluepill micro_speech_bin```

**Any other info / logs**
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/main.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/main.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,
                 from tensorflow/lite/experimental/micro/examples/micro_speech/main.cc:22:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,
                 from tensorflow/lite/experimental/micro/examples/micro_speech/main.cc:22:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.o
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_error_reporter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_error_reporter.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/simple_tensor_allocator.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/simple_tensor_allocator.o
In file included from ./tensorflow/lite/experimental/micro/simple_tensor_allocator.h:21:0,
                 from tensorflow/lite/experimental/micro/simple_tensor_allocator.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/experimental/micro/simple_tensor_allocator.h:21:0,
                 from tensorflow/lite/experimental/micro/simple_tensor_allocator.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/bluepill/debug_log.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/bluepill/debug_log.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/debug_log_numbers.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/debug_log_numbers.o
tensorflow/lite/experimental/micro/debug_log_numbers.cc: In function 'char* {anonymous}::FastFloatToBufferLeft(float, char*)':
tensorflow/lite/experimental/micro/debug_log_numbers.cc:117:53: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
   const uint32_t u = *reinterpret_cast<uint32_t*>(&f);
                                                     ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_interpreter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_interpreter.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_interpreter.h:20,
                 from tensorflow/lite/experimental/micro/micro_interpreter.cc:15:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_interpreter.h:20,
                 from tensorflow/lite/experimental/micro/micro_interpreter.cc:15:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/pooling.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/pooling.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/softmax.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/softmax.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/conv.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/elementwise.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/elementwise.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,
                 from tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc:13:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,
                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,
                 from tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc:13:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/fully_connected.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/fully_connected.o
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/c/c_api_internal.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/c/c_api_internal.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/error_reporter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/error_reporter.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/flatbuffer_conversions.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/core/api/flatbuffer_conversions.h:24,
                 from tensorflow/lite/core/api/flatbuffer_conversions.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from ./tensorflow/lite/core/api/flatbuffer_conversions.h:24,
                 from tensorflow/lite/core/api/flatbuffer_conversions.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/op_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/op_resolver.o
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from tensorflow/lite/core/api/op_resolver.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
         ^
In file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,
                 from tensorflow/lite/core/api/op_resolver.cc:16:
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < Padding_SAME || e > Padding_VALID) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
         ^
./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
         ^
tensorflow/lite/core/api/op_resolver.cc: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&, tflite::ErrorReporter*, const TfLiteRegistration**)':
tensorflow/lite/core/api/op_resolver.cc:29:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]
       builtin_code < BuiltinOperator_MIN) {
                    ^
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/kernel_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/kernel_util.o
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/internal/quantization_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/internal/quantization_util.o
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'FastFloatToBufferLeft':
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:72:3: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
   const uint32_t u = *(uint32_t*)(&i);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'StrCatInt32':
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:135:3: warning: variable length array 'number_string' is used [-Wvla]
   char number_string[kFastToBufferSize];
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'StrCatUInt32':
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:142:3: warning: variable length array 'number_string' is used [-Wvla]
   char number_string[kFastToBufferSize];
   ^
arm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.o
cc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C
cc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c: In function 'HardFaultHandler':
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:62:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(SCB->HFSR);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:64:5: warning: variable length array 'log' is used [-Wvla]
     LOG_HEX32(SCB->CFSR);
     ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:65:5: warning: variable length array 'log' is used [-Wvla]
     LOG_HEX32(SCB_CFSR_IBUSERR_Msk);
     ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:92:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(r0);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:93:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(r1);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:94:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(r2);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:95:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(r3);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:96:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(r12);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:97:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(lr);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:98:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(pc);
   ^
tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:99:3: warning: variable length array 'log' is used [-Wvla]
   LOG_HEX32(psr);
   ^
arm-none-eabi-ar -r tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_error_reporter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/simple_tensor_allocator.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/bluepill/debug_log.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/debug_log_numbers.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_interpreter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/pooling.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/softmax.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/conv.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/elementwise.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/fully_connected.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/c/c_api_internal.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/error_reporter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/flatbuffer_conversions.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/op_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/kernel_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/internal/quantization_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.o 
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/main.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.o  tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a -T tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/stm32_linker_layout.lds -Wl,-Map=tensorflow/lite/experimental/micro/tools/make/gen/bluepill.map,--cref -Wl,--gc-sections -lm
/usr/lib/gcc/arm-none-eabi/4.9.3/../../../arm-none-eabi/bin/ld: tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech section `.rodata' will not fit in region `FLASH'
/usr/lib/gcc/arm-none-eabi/4.9.3/../../../arm-none-eabi/bin/ld: region `FLASH' overflowed by 9656 bytes
collect2: error: ld returned 1 exit status
tensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:370: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech' failed
make: *** [tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech] Error 1"
30432,pasta.base.annotate.AnnotationError: Indent detection failed (line 300),"Hello,
I use google colab and I have this error:
Traceback (most recent call last):
  File ""/usr/local/bin/tf_upgrade_v2"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py"", line 139, in main
    args.input_file, output_file, upgrade)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py"", line 40, in process_file
    upgrader.process_file(in_filename, out_filename)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 900, in process_file
    temp_file)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 960, in process_opened_file
    self.update_string_pasta("""".join(lines), in_filename))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 916, in update_string_pasta
    t = pasta.parse(text)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/__init__.py"", line 25, in parse
    annotator.visit(t)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 1194, in visit
    super(AstAnnotator, self).visit(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 132, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 220, in visit_Module
    self.generic_visit(node)
  File ""/usr/lib/python3.6/ast.py"", line 261, in generic_visit
    self.visit(item)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 1194, in visit
    super(AstAnnotator, self).visit(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 132, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 95, in wrapped
    f(self, node, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 411, in visit_FunctionDef
    self.visit(stmt)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 1194, in visit
    super(AstAnnotator, self).visit(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 132, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 95, in wrapped
    f(self, node, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 291, in visit_For
    self.visit(stmt)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 1194, in visit
    super(AstAnnotator, self).visit(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 132, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 95, in wrapped
    f(self, node, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 290, in visit_For
    for stmt in self.indented(node, 'body'):
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py"", line 1224, in indented
    'more than the outer indentation.' % cur_loc[0])
pasta.base.annotate.AnnotationError: Indent detection failed (line 300); inner indentation level is not more than the outer indentation.

thanks for help me"
30431,Significantly reduced validation accuracy when switching from alpha0 to beta0/1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Code is strongly based on: [Stock example](https://www.tensorflow.org/beta/tutorials/images/transfer_learning ), but uses kaggle aerial-cactus-identification dataset. 
- OS Platform and Distribution: Google Colab
- TensorFlow installed from: binary via `pip install tensorflow-gpu==2.0.0-beta0`
- TensorFlow version: 2.0.0-beta0
- Python version: 3.6.7
- CUDA/cuDNN version: N/A (Google Colab as of 05.07.2019)
- GPU model and memory:  N/A (Google Colab as of 05.07.2019)

**Describe the current behavior**
With beta 0 and 1 the training/validation history is as follows:
![Beta 0/1](https://puu.sh/DOY9H/8ec1a1a7f5.png)
This is a very low validation accuracy. This can of course happen, even with code based on an example, the issue is that this only appears with beta0 and beta1 builds, not with alpha0 (see below) when using the exactly same code and training data.

**Describe the expected behavior**
With alpha0 and the same code the history looks as follows:
![Alpha 0](https://puu.sh/DOYss/b14cc7f979.png)

An upgrade of the tensorflow version should not affect the resulting accuracy in such a manner.

**Code to reproduce the issue**
1. Open this [Google Colab](https://colab.research.google.com/drive/14oaC63n1n3yymRyB90uMx3GNvzt-rGMT)
2. Run the code with Alpha0
![Choose Version](https://puu.sh/DOYdz/3c3efe344c.png)
3. Make note of the training history plot.
4. Restart the runtime.
5. Run the code with Beta0 or Beta1
![Choose Version](https://puu.sh/DOYeP/6a24c7c681.png)
6. Make note of the training history plot.
7. Observe that with no change to the code but using the beta0 instead of alpha0 the validation accuracy goes down from > 95% to < 90%, with beta1 even to < 80%

**Other info / logs**
My best guess is that changes have been made to the pretrained MobileNetV2 or to the Adam optimizer, otherwise the drastic loss in accuracy is hard to explin.

"
30430,"Batch size affects prediction output in RNN layers (LSTM, GRU)","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): docker
- TensorFlow version (use command below): 1.12.3 and 2.0.0-beta1
- Python version: 2.7.12 and 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The output during prediction from RNN layers, i.e. `LSTM` and `GRU` is dependent on the batch size when `stateful=False`. 

**Describe the expected behavior**
The predicted output tensor for a given input tensor should always produce the same result regardless of the size of the batch the sample is found in.

**Code to reproduce the issue**
```
from __future__ import print_function
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import LSTM, GRU

if __name__ == ""__main__"":
    with tf.device('/CPU:0'):
        batches = [1] + list(range(1, 10))
        shape = (1000, 512)
        inputs = tf.keras.Input(shape=shape)
        rnn = GRU(shape[-1]//2, return_sequences=True)(inputs)
        model = tf.keras.Model(inputs=inputs, outputs=rnn)
        results = []
        for i, batch in enumerate(batches):
            x = tf.ones((batch,) + shape)
            y = model.predict_on_batch(x)
            results.append(y[0])

        for b, x in list(zip(batches, results))[1:]:
            print(b, np.max(np.abs(results[0] - x)))
        if not all(np.allclose(x, results[0]) for x in results[1:]):
            raise ValueError(""Varying batch size produces different results"")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30429,TPU Distribution not working for r1.13,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No, I have used XLNet training
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I tried to use TPUDistribution Strategy to use TPUv2 pods for training XLNet, but it gave me error(mentioned in logs below)
**Describe the expected behavior**
TPU Distribution Strategy should be able to implement this
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

NUM_TPUS = 4
tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver([os.environ['TPU_NAME']], zone=os.environ['TPU_ZONE'], project=os.environ['GCE_PROJECT_NAME'])
tpu_strategy = tf.contrib.tpu.TPUDistributionStrategy(tpu_cluster_resolver)
run_config = tf.contrib.tpu.RunConfig(
    cluster=tpu_cluster_resolver,
    model_dir=OUTPUT_DIR,
    save_checkpoints_steps=1000,
    tpu_config=tf.contrib.tpu.TPUConfig(
        iterations_per_loop=1000,
        num_shards=8 * NUM_TPUS,
        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2),
    keep_checkpoint_max=5,
    train_distribute=tpu_strategy
)

xlnet_config = xlnet.XLNetConfig(json_path=os.path.join(XLNET_PRETRAINED_DIR, 'xlnet_config.json'))
xlnet_model = run_classifier.get_model_fn(num_classes)
estimator = tf.contrib.tpu.TPUEstimator(
        use_tpu=True,
        model_fn=xlnet_model,
        config=run_config,
        params=None,
        train_batch_size=TRAIN_BATCH_SIZE,
        predict_batch_size=PREDICT_BATCH_SIZE,
        eval_batch_size=EVAL_BATCH_SIZE)

train_input_fn = run_classifier.file_based_input_fn_builder(
    input_file=""gs://input_file.tfrecord"",
    seq_length=MAX_SEQ_LENGTH,
    is_training=True,
    drop_remainder=True)

estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I0705 07:33:40.742511 139772774643520 tpu_system_metadata.py:59] Querying Tensorflow master (grpc://a.b.c.d:8470) for TPU system metadata.

INFO:tensorflow:Found TPU system:

I0705 07:33:40.888471 139772774643520 tpu_system_metadata.py:120] Found TPU system:

INFO:tensorflow:*** Num TPU Cores: 32

I0705 07:33:40.890206 139772774643520 tpu_system_metadata.py:121] *** Num TPU Cores: 32

INFO:tensorflow:*** Num TPU Workers: 4

I0705 07:33:40.895273 139772774643520 tpu_system_metadata.py:122] *** Num TPU Workers: 4

INFO:tensorflow:*** Num TPU Cores Per Worker: 8

I0705 07:33:40.898422 139772774643520 tpu_system_metadata.py:124] *** Num TPU Cores Per Worker: 8

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4144468757051904050)

I0705 07:33:40.905382 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4144468757051904050)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8307488279136102617)

I0705 07:33:40.907173 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8307488279136102617)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15853259856546976898)

I0705 07:33:40.908568 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15853259856546976898)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15507240626127054220)

I0705 07:33:40.909704 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15507240626127054220)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9584144648126289353)

I0705 07:33:40.910813 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9584144648126289353)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1596766444393261122)

I0705 07:33:40.911922 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1596766444393261122)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 107997388316727553)

I0705 07:33:40.913051 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 107997388316727553)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5933300785999976036)

I0705 07:33:40.914163 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5933300785999976036)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17288917258161835028)

I0705 07:33:40.915283 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17288917258161835028)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10754988339652322123)

I0705 07:33:40.916407 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10754988339652322123)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 15827474055072985247)

I0705 07:33:40.917562 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 15827474055072985247)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:CPU:0, CPU, -1, 17772760716915846644)

I0705 07:33:40.918668 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:CPU:0, CPU, -1, 17772760716915846644)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:XLA_CPU:0, XLA_CPU, 17179869184, 1079066005188293108)

I0705 07:33:40.919769 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:XLA_CPU:0, XLA_CPU, 17179869184, 1079066005188293108)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:0, TPU, 17179869184, 8833670584752429122)

I0705 07:33:40.920902 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:0, TPU, 17179869184, 8833670584752429122)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:1, TPU, 17179869184, 9749399796990995666)

I0705 07:33:40.922011 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:1, TPU, 17179869184, 9749399796990995666)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:2, TPU, 17179869184, 4972353230220870757)

I0705 07:33:40.923123 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:2, TPU, 17179869184, 4972353230220870757)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:3, TPU, 17179869184, 513286311176106128)

I0705 07:33:40.924228 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:3, TPU, 17179869184, 513286311176106128)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:4, TPU, 17179869184, 14096714381964779427)

I0705 07:33:40.925353 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:4, TPU, 17179869184, 14096714381964779427)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:5, TPU, 17179869184, 11601778884065416558)

I0705 07:33:40.926468 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:5, TPU, 17179869184, 11601778884065416558)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:6, TPU, 17179869184, 5715109243185029245)

I0705 07:33:40.927602 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:6, TPU, 17179869184, 5715109243185029245)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:7, TPU, 17179869184, 293318216316046063)

I0705 07:33:40.928760 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:7, TPU, 17179869184, 293318216316046063)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5955036069992402374)

I0705 07:33:40.929885 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5955036069992402374)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, -1, 6513862482907012835)

I0705 07:33:40.930993 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, -1, 6513862482907012835)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 17179869184, 7175494438094722165)

I0705 07:33:40.932108 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 17179869184, 7175494438094722165)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 17179869184, 14689466335568240094)

I0705 07:33:40.933240 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 17179869184, 14689466335568240094)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 17179869184, 2413636980211343743)

I0705 07:33:40.934341 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 17179869184, 2413636980211343743)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 17179869184, 8959053062706485002)

I0705 07:33:40.935440 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 17179869184, 8959053062706485002)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 17179869184, 13668243215792653265)

I0705 07:33:40.936621 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 17179869184, 13668243215792653265)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:4, TPU, 17179869184, 916241737357705146)

I0705 07:33:40.937787 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:4, TPU, 17179869184, 916241737357705146)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:5, TPU, 17179869184, 9081936198773042571)

I0705 07:33:40.938918 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:5, TPU, 17179869184, 9081936198773042571)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:6, TPU, 17179869184, 17137058361537129430)

I0705 07:33:40.940040 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:6, TPU, 17179869184, 17137058361537129430)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:7, TPU, 17179869184, 9871713588844769915)

I0705 07:33:40.941166 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:7, TPU, 17179869184, 9871713588844769915)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13242834065860745167)

I0705 07:33:40.942301 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13242834065860745167)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:CPU:0, CPU, -1, 14166663181551736188)

I0705 07:33:40.943438 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:CPU:0, CPU, -1, 14166663181551736188)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:XLA_CPU:0, XLA_CPU, 17179869184, 2214898881771697800)

I0705 07:33:40.944600 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:XLA_CPU:0, XLA_CPU, 17179869184, 2214898881771697800)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:0, TPU, 17179869184, 17428311216696528109)

I0705 07:33:40.945710 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:0, TPU, 17179869184, 17428311216696528109)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:1, TPU, 17179869184, 5022423960481121783)

I0705 07:33:40.946825 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:1, TPU, 17179869184, 5022423960481121783)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:2, TPU, 17179869184, 3367013141681705422)

I0705 07:33:40.947974 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:2, TPU, 17179869184, 3367013141681705422)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:3, TPU, 17179869184, 11677911886123920679)

I0705 07:33:40.949125 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:3, TPU, 17179869184, 11677911886123920679)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:4, TPU, 17179869184, 8941627580085686113)

I0705 07:33:40.950233 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:4, TPU, 17179869184, 8941627580085686113)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:5, TPU, 17179869184, 3622957282534488438)

I0705 07:33:40.951513 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:5, TPU, 17179869184, 3622957282534488438)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:6, TPU, 17179869184, 11366405219786765949)

I0705 07:33:40.952693 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:6, TPU, 17179869184, 11366405219786765949)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:7, TPU, 17179869184, 4659665062672917141)

I0705 07:33:40.953825 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:7, TPU, 17179869184, 4659665062672917141)

INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16417307345580175404)

I0705 07:33:40.955019 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16417307345580175404)

INFO:tensorflow:Error recorded from training_loop: 'TPUDistributionStrategy' object has no attribute 'configure'

I0705 07:33:40.956579 139772774643520 error_handling.py:70] Error recorded from training_loop: 'TPUDistributionStrategy' object has no attribute 'configure'

INFO:tensorflow:training_loop marked as finished

I0705 07:33:40.957767 139772774643520 error_handling.py:93] training_loop marked as finished

WARNING:tensorflow:Reraising captured error

W0705 07:33:40.958972 139772774643520 tf_logging.py:161] Reraising captured error

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<timed exec> in <module>

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
   2455     finally:
   2456       rendezvous.record_done('training_loop')
-> 2457       rendezvous.raise_errors()
   2458 
   2459   def evaluate(self,

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in raise_errors(self, timeout_sec)
    126       else:
    127         logging.warn('Reraising captured error')
--> 128         six.reraise(typ, value, traceback)
    129 
    130     for k, (typ, value, traceback) in kept_errors:

/home/jinamshah/venv/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
   2450           steps=steps,
   2451           max_steps=max_steps,
-> 2452           saving_listeners=saving_listeners)
   2453     except Exception:  # pylint: disable=broad-except
   2454       rendezvous.record_error('training_loop', sys.exc_info())

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    356 
    357       saving_listeners = _check_listeners_type(saving_listeners)
--> 358       loss = self._train_model(input_fn, hooks, saving_listeners)
    359       logging.info('Loss for final step: %s.', loss)
    360       return self

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1120   def _train_model(self, input_fn, hooks, saving_listeners):
   1121     if self._train_distribution:
-> 1122       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1123     else:
   1124       return self._train_model_default(input_fn, hooks, saving_listeners)

/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1181       return self
   1182     else:
-> 1183       self._config._train_distribute.configure(self._config.session_config)
   1184       return self._actual_train_model_distributed(
   1185           self._config._train_distribute, input_fn, hooks, saving_listeners)

AttributeError: 'TPUDistributionStrategy' object has no attribute 'configure'"
30427,FutureWarning: Deprecated numpy API calls in tf.python.framework.dtypes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not really
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.5 (18F132)
- TensorFlow installed from (source or binary): tensorflow==2.0.0b1 from https://pypi.org/
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

A plenty of FutureWarning errors:

```
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
```

**Describe the expected behavior**

No warnings.

**Code to reproduce the issue**

```
import tensorflow.python.framework.dtypes

print(""Hello world"")
```"
30426,How to use CMake to add tf which builded by bazel into project?,"Hello, I am confused by importing the tf into my project with CMake,  I compiled the tensorflow with bazel,without GPU support.
I dont know how to write the CMakeLists. I am not so clear about the compiling result of the bazel, there are severl folders, how should I do to use these folders in my CMakeLists ?
Hope your reply, Thx :)"
30425,Failed to load the native TensorFlow runtime,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): 
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 8
- CUDA/cuDNN version:
- GPU model and memory:

I got this error

```Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/lib/python3.7/site-packages/tensorflow/__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```




**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30423,"2.0 beta's ""zero_output_for_mask"" produces unexpected zero-filling behaviors for GRU, LSTM.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS in GOOGLE COLAB
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version: 10.0
- GPU model and memory: Tesla T4 (16G)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The call() methods of tf.keras.layers.GRU and others (e.g. LSTM) does not produce output as expected for ""zero_output_for_mask""  when the layer is created with ""return_sequences=True"".
Regardless of the value ""zero_output_for_mask"", the result of call() fills zeros for masked timestamps.

**Describe the expected behavior**
It should produce outputs as the masked timestamps should be filled zeros if zero_output_for_mask is True. If false,  the masked timestamps should be filled with previous outputs. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import numpy as np

for zero_output_for_mask in [False, True]:
  #tf.keras.Sequential Model
  model = tf.keras.Sequential([
      tf.keras.layers.Embedding(16, 5, mask_zero=True),
      tf.keras.layers.GRU(5, return_sequences=True, zero_output_for_mask=zero_output_for_mask)
  ]
  )

  model.compile(
      optimizer='rmsprop',
      loss='mse'
      )

  np_x = np.ones((2, 5), dtype=np.float32)

  # masking timestamp 3,4 of sample index 1.
  np_x[1, 3:] = 0
  #print(np_x)

  # model's call()
  result_mask_call = model(np_x)

  print(""---- zero_output_for_mask is {} ------"".format(zero_output_for_mask))
  print(result_mask_call[1,:].numpy())

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I suspect that tf2.0 alpha's GRU's call() is changed in tf2.0 beta version.
PLEASE refer FINAL line of both snippets.

beta version snippet of GRU's call() in recurrent_v2.py 
```
  def call(self, inputs, mask=None, training=None, initial_state=None):
    # GRU does not support constants. Ignore it during process.
    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)

    if isinstance(mask, list):
      mask = mask[0]

    input_shape = K.int_shape(inputs)
    timesteps = input_shape[0] if self.time_major else input_shape[1]

    if not self.could_use_cudnn:
```

alpha version snippet of GRU's call() in recurrent.py (UnifiedGRU)
```
  def call(self, inputs, mask=None, training=None, initial_state=None):
    # GRU does not support constants. Ignore it during process.
    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)

    if isinstance(mask, list):
      mask = mask[0]

    input_shape = K.int_shape(inputs)
    timesteps = input_shape[0] if self.time_major else input_shape[1]

    if mask is not None or not self.could_use_cudnn:
```
"
30421,TF2.0beta1+cudnn: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): virtualenv and pip
- TensorFlow version (use command below): tensorflow-gpu 2.0.0-beta1
- Python version: 3.6.7
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 7.6.0
- GPU model and memory:  GTX1660Ti, 6 GB

**Describe the current behavior**
UnknownError                              Traceback (most recent call last)
<ipython-input-1-df32bfd3ff54> in <module>
      1 import tensorflow as tf
----> 2 tf.keras.layers.Conv2D(10, 3)(tf.ones((1, 5, 5, 3)))

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    710           with base_layer_utils.autocast_context_manager(
    711               input_list, self._mixed_precision_policy.should_cast_variables):
--> 712             outputs = self.call(inputs, *args, **kwargs)
    713           self._handle_activity_regularization(inputs, outputs)
    714           self._set_mask_metadata(inputs, outputs, input_masks)

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
    194 
    195   def call(self, inputs):
--> 196     outputs = self._convolution_op(inputs, self.kernel)
    197 
    198     if self.use_bias:

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)
   1076 
   1077   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
-> 1078     return self.conv_op(inp, filter)
   1079 
   1080 

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)
    632 
    633   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
--> 634     return self.call(inp, filter)
    635 
    636 

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)
    231         padding=self.padding,
    232         data_format=self.data_format,
--> 233         name=self.name)
    234 
    235 

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)
   1950                            data_format=data_format,
   1951                            dilations=dilations,
-> 1952                            name=name)
   1953 
   1954 

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)
   1029             input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,
   1030             padding=padding, explicit_paddings=explicit_paddings,
-> 1031             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)
   1032       except _core._SymbolicException:
   1033         pass  # Add nodes to the TensorFlow graph.

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)
   1128   explicit_paddings, ""data_format"", data_format, ""dilations"", dilations)
   1129   _result = _execute.execute(b""Conv2D"", 1, inputs=_inputs_flat, attrs=_attrs,
-> 1130                              ctx=_ctx, name=name)
   1131   _execute.record_gradient(
   1132       ""Conv2D"", _inputs_flat, _attrs, _result, name)

~/venv_tf2.0/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/lib/python3/dist-packages/six.py in raise_from(value, from_value)

UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]

**Describe the expected behavior**
Output the inference results

**Code to reproduce the issue**
import tensorflow as tf
tf.keras.layers.Conv2D(10, 3)(tf.ones((1, 5, 5, 3)))


How should resolve it? Thanks"
30420,"tensorflow java api, branch r1.14,the version in pom file is still 1.13","build jni from branch r1.14,the jar name is still 1.13.0-rc2
which is correct version?"
30419,类型问题,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: https://www.tensorflow.org/tutorials/keras/basic_classification

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing): plt.xlabel(class_names[train_labels[i]])中train_labels返回的是numpy.float64 ，但是class_names需要Integer。

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
30418,Memory Saving Gradients for TF2,"**System information**
- TensorFlow 2.0 beta

There are libraries for TF1 that are able to calculate more memory efficient gradients such as [gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing). They edit the graph with the [tf.contrib.graph_editor](https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor) to save memory. However, tf.contrib does not exist anymore which is why I cannot run large models such as GPT2 in TF2 (converted with `tf_upgrade_v2`) as it overflows my GPU memory in Google Colab. Is there a TF2 feature that can help me? 
"
30417,Mixed-precision mode in keras: 'AutoCastVariable' object is not subscriptable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): official Tensorflow docker = tensorflow/tensorflow:1.14.0-gpu-py3
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8

**Describe the current behavior**
The following error occurs in a given code below when trying to enable mixed-precision mode with `keras.mixed_precision.experimental.set_policy('infer_float32_vars')`:
```
    auto_cast_variable_slice_repr.py:16 call
        return keras.backend.dot(inputs, w[:16, :16])

    TypeError: 'AutoCastVariable' object is not subscriptable
```

It looks like _ResourceVariable_ (which is returned by self.add_weight with mixed-precision is OFF) supports slice operations, while _AutoCastVariable_ (returned by self.add_weight when mixed-precision is ON) doesn't.

It's possible to workaround this issue by converting this variable into tensor (as shown on line 15), but it's not clear if it's a straightforward way to perform slice op on Variable.

**Describe the expected behavior**
Should work without any error.

**Code to reproduce the issue**
```
import numpy as np

from tensorflow import keras

# Comment this line out to make code complete successfully
keras.mixed_precision.experimental.set_policy('infer_float32_vars')

class MyLayer(keras.layers.Layer):
    def build(self, input_shape):
        self.w = self.add_weight(shape=(16, 16))

    def call(self, inputs, **kwargs):
        w = self.w
        # Uncomment this workaround line below to make it work with mixed-precision ON
        # w = keras.backend.cast(w, dtype=w.dtype)
        return keras.backend.dot(inputs, w[:16, :16])

input = keras.layers.Input(shape=(16, ))
output = MyLayer()(input)
model = keras.models.Model(input, output)

model.predict(np.zeros(shape=(16, 16)))
```"
30416,Build doesn't find files that exist,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: git tag v2.0.0_beta1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): bazel release 0.24.1
- GCC/Compiler version (if compiling from source): ""C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC//bin/amd64/cl.exe""
Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64
- CUDA/cuDNN version: 10.1/7.6.1
- GPU model and memory: GeForce GTX 1060 (6GB)



**Describe the problem**
Build fails, each time with other .h file, that it can't find.
All files are available.

Run 1:
ERROR: D:/data/users/andrey/projects/tensorflow_gpu_v2.0_3/tensorflow/core/kernels/data/BUILD:1012:1: C++ compilation of rule '//tensorflow/core/kernels/data:optional_ops_gpu' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/andrey/_bazel_andrey/ntapso4i/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.o /c tensorflow/core/kernels/data/optional_ops.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
c1xx: fatal error C1083: Cannot open source file: 'tensorflow/core/kernels/data/optional_ops.cu.cc': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build

Run 2:
ERROR: D:/data/users/andrey/projects/tensorflow_gpu_v2.0_3/tensorflow/contrib/framework/BUILD:111:1: C++ compilation of rule '//tensorflow/contrib/framework:python/ops/_variable_ops_gpu' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/andrey/_bazel_andrey/ntapso4i/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings /DEIGEN_STRONG_INLINE=inline -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/zero_initializer_op_gpu.cu.o /c tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
c1xx: fatal error C1083: Cannot open source file: 'tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.cc': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build


Run 3:
ERROR: D:/data/users/andrey/projects/tensorflow_gpu_v2.0_3/tensorflow/contrib/rnn/BUILD:199:1: C++ compilation of rule '//tensorflow/contrib/rnn:python/ops/_gru_ops_gpu' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/andrey/_bazel_andrey/ntapso4i/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings /DEIGEN_STRONG_INLINE=inline -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/rnn/_objs/python/ops/_gru_ops_gpu/gru_ops_gpu.cu.o /c tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
c1xx: fatal error C1083: Cannot open source file: 'tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build


Run 4:
ERROR: D:/data/users/andrey/projects/tensorflow_gpu_v2.0_3/tensorflow/core/kernels/BUILD:321:1: C++ compilation of rule '//tensorflow/core/kernels:extract_volume_patches_op_gpu' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/andrey/_bazel_andrey/ntapso4i/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.o /c tensorflow/core/kernels/extract_volume_patches_op_gpu.cu.cc
Execution platform: @bazel_tools//platforms:host_platform
c1xx: fatal error C1083: Cannot open source file: 'tensorflow/core/kernels/extract_volume_patches_op_gpu.cu.cc': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build



**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
.tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe""
build --action_env PYTHON_LIB_PATH=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages""
build --python_path=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe""
build:xla --define with_xla_support=false
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7.6.1""
build --action_env TF_CUDA_PATHS=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --config=cuda
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI
build --verbose_failures
build --distinct_host_configuration=false
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-no_windows,-gpu
test --build_tag_filters=-no_windows,-gpu
build --action_env TF_CONFIGURE_IOS=""0"""
30415,Failed to build Windows +TF2 + XLA + python,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: git tag v2.0.0_beta1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel release 0.24.1
- GCC/Compiler version (if compiling from source): ""C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC//bin/amd64/cl.exe""
Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24234.1 for x64
- CUDA/cuDNN version: 10.1/7.6.1
- GPU model and memory: GeForce GTX 1060 (6GB)



**Describe the problem**
Build fails with error:
ERROR: D:/data/users/andrey/projects/tensorflow_gpu_v2.0_2/tensorflow/compiler/xla/service/BUILD:2338:1: C++ compilation of rule '//tensorflow/compiler/xla/service:hlo_execution_profile' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/andrey/_bazel_andrey/xstmf7fr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\andrey\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\andrey\AppData\Local\Temp
  D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/_objs/hlo_execution_profile/hlo_execution_profile.o /c tensorflow/compiler/xla/service/hlo_execution_profile.cc
Execution platform: @bazel_tools//platforms:host_platform
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: expression did not evaluate to a constant
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): note: failure was caused by unevaluable pointer value
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): error C2131: expression did not evaluate to a constant
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): note: failure was caused by unevaluable pointer value
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): error C2131: expression did not evaluate to a constant
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): note: failure was caused by unevaluable pointer value
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): error C2131: expression did not evaluate to a constant
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): note: failure was caused by unevaluable pointer value
Target //tensorflow/tools/pip_package:build_pip_package failed to build

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
.tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe""
build --action_env PYTHON_LIB_PATH=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/lib/site-packages""
build --python_path=""D:/Data/Users/andrey/AppData/Local/Programs/Python/Python36/python.exe""
build:xla --define with_xla_support=true
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7.6.1""
build --action_env TF_CUDA_PATHS=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/cudnn-10.1-windows10-x64-v7.6.1.34""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --config=cuda
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI
build --verbose_failures
build --distinct_host_configuration=false
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-no_windows,-gpu
test --build_tag_filters=-no_windows,-gpu
build --action_env TF_CONFIGURE_IOS=""0"""
30413,gradient back propagation in tf.contrib.image.transform,"**System information** Ubuntu 16.04
TensorFlow version 1.10.1

tf.contrib.image.transform applies the given transform(s) to the image(s). But gradients are not backpropagated into transformation parameters https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform. 
Affine transformations can be constructed using a series of translations, scales, rotations and shears. It could calculate the gradient and back propagation, at least in rotation,scale and translations. I noticed that tf.contrib.image.rotate and tf.contrib.image.translate do not emphasize that gradients cannot propagate backwards.
I'd like to know how to back propagation into the transformation parameters especially in affine transform. Do I need to explicitly define the gradient using gradient_map_override?（#17442）

Any suggestions would be helpful. Thanks

This may change the current api."
30412," I noticed that MonitoredSession is a very inefficient method.How to save the model according to the set conditions, not periodically.","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**win10
- TensorFlow version (you are using):1.12
- Are you willing to contribute it (Yes/No):no



**Describe the feature and the current behavior/state.**
 I noticed that MonitoredSession is a very inefficient method.Saving the model periodically.
**Will this change the current api? How?**
Setting conditions to save the model, not periodically.
**Who will benefit with this feature?**
everyone
**Any Other info.**
"
30411,NCHW or NHWC when convert .pb into .tflite,"**System information**
- OS Platform and Distribution :Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):1.13.1


**Provide the text output from tflite_convert**

Check failed: status.ok() Unexpected value for attribute 'data_format'. Expected 'NHWC'

**Any other info / logs**

The model I trained uses 'NCHW', such as my TF point - conv2d, which is in NCHW format. Now I want to convert frozen.pb to frozen. tflite. The above error occurred in the program. I want to know how I can successfully convert NCHW-formatted models to tflite when I only have frozen.pb. Thanks
"
30410,Get prediction from tflite model Android,"I worked on eye detection in real time project.
I have generated a tflite model graph from trained frozen inference graph .pb and I want to use the tflite model in Android Studio.
I have integrated OpenCV library and I have added TensorFlow tflite library in the application.
The front cam of my android device works fine.
+ I have this code: 
```.java

package com.example.opencvtest;

import org.opencv.android.BaseLoaderCallback;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewFrame;
import org.opencv.android.LoaderCallbackInterface;
import org.opencv.android.OpenCVLoader;
import org.opencv.core.Mat;
import org.opencv.android.CameraBridgeViewBase;
import org.opencv.android.CameraBridgeViewBase.CvCameraViewListener2;

import android.app.Activity;
import android.content.pm.ActivityInfo;
import android.content.res.AssetFileDescriptor;
import android.hardware.Camera;
import android.os.Bundle;
import android.util.Log;
import android.view.Menu;
import android.view.MenuItem;
import android.view.SurfaceView;
import android.view.WindowManager;
import android.widget.Toast;

import java.io.FileInputStream;
import java.io.IOException;
import java.lang.reflect.Method;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.Tensor;


public class MainActivity extends Activity implements CvCameraViewListener2 {
    private static final String TAG = ""OCVSample::Activity"";

    private CameraBridgeViewBase mOpenCvCameraView;
    private boolean              mIsJavaCamera = true;
    private MenuItem             mItemSwitchCamera = null;
    String modelFile=""iristf.tflite"";
    Interpreter tflite;
    Tensor ts;


    //Load model tflite
    private MappedByteBuffer loadModelFile(Activity activity, String MODEL_FILE) throws IOException {
        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);
        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }
    private BaseLoaderCallback mLoaderCallback = new BaseLoaderCallback(this) {
        @Override
        public void onManagerConnected(int status) {
            switch (status) {
                case LoaderCallbackInterface.SUCCESS:
                {
                    Log.i(TAG, ""OpenCV loaded successfully"");
                    mOpenCvCameraView.setCameraIndex(1);
                    mOpenCvCameraView.enableView();
                } break;
                default:
                {
                    super.onManagerConnected(status);
                } break;
            }
        }
    };

    public MainActivity() {
        Log.i(TAG, ""Instantiated new "" + this.getClass());
    }

    /** Called when the activity is first created. */
    @Override
    public void onCreate(Bundle savedInstanceState) {
        Log.i(TAG, ""called onCreate"");
        super.onCreate(savedInstanceState);
        setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);

        getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);

        setContentView(R.layout.activity_main);
        try {
            tflite = new Interpreter(loadModelFile(this, modelFile));
             ts=tflite.getInputTensor(0);

        }
        catch (IOException e) {
            e.printStackTrace();
        }

        mOpenCvCameraView = (CameraBridgeViewBase) findViewById(R.id.tutorial1_activity_java_surface_view);

        mOpenCvCameraView.setVisibility(SurfaceView.VISIBLE);

        mOpenCvCameraView.setCvCameraViewListener(this);







    }

    @Override
    public void onPause()
    {
        super.onPause();
        if (mOpenCvCameraView != null)
            mOpenCvCameraView.disableView();
    }

    @Override
    public void onResume()
    {
        super.onResume();
        if (!OpenCVLoader.initDebug()) {
            Log.d(TAG, ""Internal OpenCV library not found. Using OpenCV Manager for initialization"");
            OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_3_0_0, this, mLoaderCallback);
        } else {
            Log.d(TAG, ""OpenCV library found inside package. Using it!"");
            mLoaderCallback.onManagerConnected(LoaderCallbackInterface.SUCCESS);
        }
    }

    public void onDestroy() {
        super.onDestroy();
        if (mOpenCvCameraView != null)
            mOpenCvCameraView.disableView();
    }

    public void onCameraViewStarted(int width, int height) {
    }

    public void onCameraViewStopped() {
    }

    public Mat onCameraFrame(CvCameraViewFrame inputFrame) {
        return inputFrame.rgba();
    }
    protected void setDisplayOrientation(Camera camera, int angle){
        Method downPolymorphic;
        try
        {
            downPolymorphic = camera.getClass().getMethod(""setDisplayOrientation"", new Class[] { int.class });
            if (downPolymorphic != null)
                downPolymorphic.invoke(camera, new Object[] { angle });
        }
        catch (Exception e1)
        {
            e1.printStackTrace();
        }
    }
}


```

My goals are : 

+ Decomposed the input streaming from cam into frames.
+ Get predictions from frames using tflite model.

I have followed many tutorials ( Medium StackOverflow, Github, Tensorflow Documentation...).
But I didn't get an answer to those questions below :

I want to know how can I get predictions from video stream?
How can I get frames from input streaming?
How to use the tflite model to get predictions from the video stream ( or images )?

Thanks."
30409,[TF2.0] TensorArray and tf.function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OSX`
- TensorFlow installed from (source or binary): `binary - 2.0.0-beta1`
- TensorFlow version (use command below): `v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1`
- Python version: `3.6`

**Describe the current behaviour**

The code works in eager mode but does not work when applying the decorator `@tf.function`.
2 errors happen sequentially:
- The first one is about types, `Dataset.enumerate()` outputs an index of type `int64` which is different from a `TensorArray` inner type (`int32`). 

- The second one breaks because of uninitialized tensors.

**Describe the expected behaviour**
For the first one, It should break in eager mode too.
For the second one, it should work as in eager mode

**Code to reproduce the issue**
```python
import tensorflow as tf

a = tf.random.uniform([10, 2])
d = tf.data.Dataset.from_tensor_slices(a).batch(2)


def accumulate(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        arr.write(i, t)

    return arr.concat()


@tf.function
def accumulate_f_error1(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        arr.write(i, t)

    return arr.concat()


@tf.function
def accumulate_f_error2(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        j = tf.cast(i, tf.int32)
        arr.write(j, t)

    return arr.concat()


# Works well
data = accumulate(d)
print(data)

# # Breaks because of  a wrong type
# data = accumulate_f_error1(d)
# print(data)

# Breaks due to uninitialized tensors?
data = accumulate_f_error2(d)
print(data)
```"
30408,Poor documentation of tf.saved_model.Builder,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/saved_model/Builder

## Description of issue (what needs changing):

The documentation is unclear on several points and especially hard to understand for people who are new to tensorflow.

### Clear description

What is the difference of using saved_model.Builder to other ways of exporting models and graphs? For someone just wanting to use a pre-trained tensorflow model it is very hard to get an overview over all the different types of formats etc. 

For example what is the difference to tf.io.write_graph? As far as I could find out I need to use the saved_model stuff, because it is able to add tags, which tf.io is not able to do and which is needed for serving. The docs are very unclear on the whole topic of how to use pre trained models in a custom application. (for example in https://github.com/tensorflow/models/blob/master/research/slim/export_inference_graph.py tf.io.write_graph is used)

There is also no data types given for the parameters so as someone new to TF I am absolutely unable to guess what should go there. The only description is foo_signatures and foo_assets, but there is no example showing how these parameters are properly used.

I managed to use saved_model.simple_save, but it is deprecated and from the documentation of the saved_model.Builder, I have no idea how I could replicate the same functionality as with simple_save.

### Usage example

There is example code, but there is no complete example on how to create a frozen graph or on how to export (and import again) a pre-trained model.

### Submit a pull request?

I am an absolute beginner to TF so I cannot correct the docs in a meaningful way. sorry
"
30407,Error in post-training quantization with TFLiteConverter,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly (1.15.0-dev20190704)
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 2080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
RuntimeError when convert frozen graph to TFLite with `converter.optimizations = [tf.lite.Optimize.DEFAULT]` and `converter.representative_dataset = representative_data_gen`

`RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.No calibrator found for context.Node number 0 (PAD) failed to invoke.`

I double checked the shape of the input array which is [1, 512, 512, 3] and I can convert it to a float tflite model without any dynamic-sized tensor. I'm not sure where and what raised this error... 
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
My data generator:
```
    def representative_data_gen():
        for im_info in image_list:
            path = im_info.split(' ')[0]
            img = cv2.imread(path)
            img = letterbox_transform(img)
            img = normalize_image(img)
            img = np.expand_dims(img, 0)
            yield [img.astype('float32')]
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30406,how to use tensorflow::tensor in c++,"
python code:
 input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name='input_tensor')
 phase_tensor = tf.constant(False, tf.bool)

c++ code:
tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 256, 512, 3}));
    tensorflow::Tensor phase(tensorflow::DT_BOOL, tensorflow::TensorShape());
    phase.scalar<bool>()() = false;

this convertion is right?
who can help me?


"
30405,Resolving Empty Clusters in tf.contrib.factorization.KMeans ,"**System information**
- TensorFlow version : 1.14.0
- Are you willing to contribute it : If my time and my skills permit

**Describe the feature and the current behavior/state.**

Hi,
I am working on implementing a state-of-the-art deep learning architecture. The architecture requires applying KMeans algorithm on every epoch. However, the current graph generated by the ""__init__"" function in **tf.contrib.factorization.KMeans** doesn't support resolving empty clusters. I.e. it doesn't prevent the formation of empty clusters. 

The approach I want to implement is to assign a new cluster center for the empty cluster by perturbing a non-empty cluster center and assigning some points in the non_empty cluster to the empty one.

**Will this change the current API? How?**
Yup, by adding a new parameter to the KMeans ""__init__"" function.
The added parameter corresponds to whether the returned graph should support the prevention of empty clusters formation or no. Something like  a Boolean **prevent_empty_clusters**.

**Who will benefit with this feature?**
The feature will give the KMeans API users more options. Those who are interested in a KMeans algorithm that prevents empty clusters are the ones who will benefit. 

**Any Other info.**
No
"
30404,TypeError: Input 'y' of 'Sub' Op has type int64 that does not match type int32 of argument 'x'.,"System Info:
OS:  Mac OS 10.14.5
Python Version: 2.7.13
Tensorflow Version: 1.10.0 (CPU)
Tensorflow Installation: conda + pip install

---------------------------------------------------------

Issue Description:

I have a slice operation ( tf.slice ) in model graph.  the 'dtype'  of 'begin' and 'size' in tf.slice() is defined 'tf.int64', which looks like:
```python
begin_tensor = tf.convert_to_tensor([0, 0, 0, 0], dtype=tf.int64)
size_tensor = tf.convert_to_tensor([-1, -1, 1, -1], dtype=tf.int64)
# shape of A_t is [batch_size, max_sequence_len, hidden_dim, hidden_dim]
transition_A_t = tf.squeeze(tf.slice(A_t, begin=begin_tensor, size=size_tensor), axis=2)
```
When I start to train, it shows the error:
**TypeError: Input 'y' of 'Sub' Op has type int64 that does not match type int32 of argument 'x'.**
 From the Traceback, I guess there's something wrong in the backpropagation and gradient computation.

And everything goes on well when  I change the 'dtype'  of 'begin' and 'size' from 'tf.int64' to 'tf.int 32'  which likes:
```python
begin_tensor = tf.convert_to_tensor([0, 0, 0, 0], dtype=tf.int32)
size_tensor = tf.convert_to_tensor([-1, -1, 1, -1], dtype=tf.int32)
```
I don't know why there will be a tensor with dtype=tf.int32 in gradient computation. I'm sure the dtype of variables I defined is either tf.float32 or tf.int64.
I'm not sure if this is a bug. But it really confuses me for a long time. Please have look !
The whole Traceback is as follows:

> Traceback (most recent call last):
  File ""/Users/xxx/working/xxx_project/classifiers/train_seasonal_content_classifier.py"", line 230, in <module>
    tf.app.run()
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/xxx/working/xxx_project/classifiers/train_seasonal_content_classifier.py"", line 224, in main
    eval_result, _ = tf.estimator.train_and_evaluate(classifier, train_spec=train_spec, eval_spec=eval_spec)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1170, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/xxx/working/xxx_project/classifiers/models/universal_transformer_multi_tags_classifier.py"", line 50, in model_fn
    mode=mode)
  File ""/Users/xxx/working/xxx_project/classifiers/losses/multi_tags_loss.py"", line 43, in process_multi_tag_loss_fn
    grads = tf.gradients(total_loss, tvars)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 779, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 398, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 779, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py"", line 250, in _SliceGrad
    array_ops.shape(input_vec) - slice_size - begin_vec, shape)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 8188, in sub
    ""Sub"", x=x, y=y, name=name)
  File ""/Users/xxx/anaconda3/envs/tensorflow_1_10_py2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 546, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Sub' Op has type int64 that does not match type int32 of argument 'x'.
"
30397,"ValueError: Cannot feed value of shape (60000,) for Tensor 'Placeholder_1:0', which has shape '(?, 10)'","```python
import tensorflow as tf
import numpy as np


mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data(r'C:\Users\Ati\Downloads\mnist.npz')


Reshaping the array to 2-dims so that it can work with the Keras API
x_train = x_train.reshape(x_train.shape[0], 784)
x_test = x_test.reshape(x_test.shape[0], 784)
#input_shape = (28, 28, 1)

 Making sure that the values are float so that we can get decimal points after division
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

Normalizing the RGB codes by dividing it to the max RGB value.
x_train /= 255
x_test /= 255
#y_train = tf.one_hot(y_train, 1)

#b=len(y_train)
#print('number of rows in y_train=',b) # it's 1*60000

print('y_train shape:', y_train.shape)
print('x_train shape:', x_train.shape)
z=len(x_train)
print('number of rows in x_train=',z) # it's 1*60000
class NeuralNetwork:
    def add_layer(inputs,in_size,out_size,activation_function=None):
        Weights= tf.Variable(tf.random_normal([in_size,out_size]))
        biases= tf.Variable(tf.zeros([out_size])+0.1)
        Wx_plus_b = tf.matmul(inputs,Weights)+biases
        if activation_function is None:
            outputs = Wx_plus_b
        else:
            outputs = activation_function(Wx_plus_b)
        return outputs

#y_train = y_train.reshape((60000, 1))
xs = tf.placeholder(tf.float32,[None, 784]) #same with x_train=60000*784
ys = tf.placeholder(tf.float32,[None,10 ])

l1=NeuralNetwork.add_layer(xs ,784,10,activation_function=None)

prediction= NeuralNetwork.add_layer(l1,10,1,activation_function=None)

loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                  reduction_indices=[1]))
train_step=tf.train.GradientDescentOptimizer(0.1).minimize(loss)

init=tf.global_variables_initializer()

sess=tf.Session()
sess.run(init)

for i in range(1000):
    sess.run(train_step, feed_dict={xs:x_train, ys:y_train})
    sess.run(train_step, feed_dict={xs:x_test, ys:y_test})
    if i % 50==0: #print loss every 50 step
        print(""loss after training ="",sess.run(loss,feed_dict={xs:x_train,ys:y_train}))

"
30396,Tensorflow 1.12 - ERROR: missing input file '@kissfft//:LICENSE',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source) :0.26.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: disabled
- GPU model and memory: no GPU, 16GB of RAM

**Describe the problem**
Fails to build.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
export PYTHON_BIN_PATH=""$VIRTUAL_ENV""'/bin/python'
export PYTHON_LIB_PATH=""$VIRTUAL_ENV""
export TF_DOWNLOAD_MKL='1'
export TF_NEED_MKL='1'
export CC_OPT_FLAGS='-march=native'
export TF_NEED_JEMALLOC='1'
export TF_NEED_GCP='0'
export TF_NEED_HDFS='0'
export TF_ENABLE_XLA='0'  # JIT
export TF_NEED_VERBS='0'
export TF_NEED_OPENCL='0'
export TF_NEED_OPENCL_SYCL='0'
export TF_NEED_COMPUTECPP='0'
export TF_NEED_CUDA='0'
export TF_NEED_MPI='0'
export TF_NEED_S3='0'
export TF_NEED_GDR='0'
export TF_SET_ANDROID_WORKSPACE='0'
export TF_NEED_KAFKA='0'
export TF_CUDA_CLANG='0'
export TF_DOWNLOAD_CLANG='0'
export TF_NEED_ROCM='0'
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package ""$HOME""'/repos/tensorflow_pkg'
```

**Any other info / logs**
Last bit of the build log:
```
[6,438 / 6,444] Compiling .../core/ops/summary_ops.cc [for host]; 2s local

[6,450 / 6,454] 2 actions, 1 running
    Compiling tensorflow/core/ops/summary_ops.cc [for host]; 2s local
    [Prepa] ProtoCompile .../core/example/example_parser_configuration_pb2.py



ERROR: missing input file '@kissfft//:LICENSE'
[6,452 / 6,456] Compiling .../core/ops/summary_ops.cc [for host]; 2s local

ERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/tools/pip_package/BUILD:263:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@kissfft//:LICENSE'
[6,453 / 6,456] checking cached actions

Target //tensorflow/tools/pip_package:build_pip_package failed to build
[6,453 / 6,456] checking cached actions

Use --verbose_failures to see the command lines of failed build steps.
[6,453 / 6,456] checking cached actions

ERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/tools/pip_package/BUILD:263:1 1 input file(s) do not exist
[6,453 / 6,456] checking cached actions

INFO: Elapsed time: 1405.912s, Critical Path: 107.29s
[6,453 / 6,456] checking cached actions

INFO: 529 processes: 529 local.
[6,453 / 6,456] checking cached actions

FAILED: Build did NOT complete successfully

FAILED: Build did NOT complete successfully
```

EDIT: Wait, this might be the solution https://github.com/tensorflow/tensorflow/issues/28129"
30395,"ERROR: Invalid command prefix ""invoke_stepper""","When I use tfdbg and I input the command line: invoke_stepper. It appears ERROR: Invalid command prefix ""invoke_stepper"""
30393,TFLite on Android (JNI) gives inconsistent results when the input ByteBuffer is reused,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Android 7.1 (compileSdkVersion 28)
- Mobile device: Galaxy S7 (custom ROM Android 7.1)
- TensorFlow version: TFLite 1.14.0

**Describe the current behavior**
I create a TFLite Interpreter and allocate a direct ByteBuffer in Java to process a sequence of images. I then copy image data into the buffer for inference, which runs in an AsyncTask with a serial executor, so the Interpreter and buffer are only used by one forward pass at a time.

Doing this the predictions sometimes (always?) relate to an earlier (possibly always the first) image in the sequence, not the current image. So things work reliably for the first image, but not for the following images.

I can circumvent this problem by allocating a new ByteBuffer for every frame. I'd like to avoid these frequent allocations, but more importantly I am having a hard time figuring out why I am getting this behaviour in the first place. It's almost like TFLite doesn't use the updated data in the buffer when it sees that the buffer reference hasn't changed.

**Describe the expected behavior**

I expect to be able to reuse the input ByteBuffer with updated contents across multiple inference calls."
30390,run bazel failed:FATAL: ExecuteProgram(C:\Users\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/java.exe) failed: 6,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):source
- TensorFlow version: 2.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source): 0.27
- GCC/Compiler version (if compiling from source):no
- CUDA/cuDNN version:no
- GPU model and memory:no



**Describe the problem**
After I installed bazel 0.27.1, I try to run the command ""bazel"", it show the error
""FATAL: ExecuteProgram(C:\Users\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/java.exe) failed: 6""
but when move into the path ""C:\Users\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/"", it is ok to run ""bazel"", do you have idea on it? and how to solve it.
Thanks,


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
30389,The GPU is not used after installed from source. ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
build from source
- TensorFlow version:
1.14
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
0.24
- GCC/Compiler version (if compiling from source):
gcc 5.4
- CUDA/cuDNN version:
CUDA 9.0 cuDNN 7.0
- GPU model and memory:
Tesla K80



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel build -c opt --config=cuda --config=mkl --config=nogcp --config=nohdfs --config=nokafka --config=noignite --config=nonccl //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
pip install /tmp/tensorflow_pkg/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl
`

After installing, I am trying to use the GPU to traning, get such error.

`2019-07-04 03:30:19.903538: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
30388,"tensorflow2.0 detected 'xla_gpu' , but 'gpu' expected","**System information**
-  Linux Ubuntu 16.04 :
- TensorFlow installed from binary, tensorflow2.0-gpu-beta1:
- Python version 3.6.4:
- CUDA 9.1, cuDNN 7.0:
- GPU: Titan Xp.
 
tensorflow detected 'xla_gpu' , but 'gpu' expected

`train_model = keras.utils.multi_gpu_model(train_model, gpus=2)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py"", line 182, in multi_gpu_model
    available_devices))
ValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/xla_gpu:0', '/xla_gpu:1', '/xla_cpu:0']. Try reducing `gpus`.
`  "
30387,TensorFlow Lite Metadata samples,Are there any sample files or documentation how `metadata` and `metadata_buffer` in TensorFlow Lite models is used? @wangtz 494ed4d2100d94b3cca782d7134eac306d79d59b
30384,Problem with keras saving when using custom loss in compile (problem in custom_objects parameter passing),"**System information**

System: windows 10, wsl with ubuntu 18 LTS
Tensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)
Python version: 3.6.8

It also happens in real linux environments (actually, it's easy to simulate each these errors)

**Describe the current behavior**

The code can't handle custom losses (added with `.compile`) when loading a model

**Describe the expected behavior**

When I use a custom loss and compile it, tensorflow returns the following error:

Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 27, in <module>
    tf.keras.models.load_model('model.keras.tf', custom_objects={'null_fn': null_fn})
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 141, in load_model
    return saved_model.load_from_saved_model_v2(filepath, compile)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 1225, in load_from_saved_model_v2
    model._training_config))  # pylint: disable=protected-access
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 300, in compile
    self.loss, self.output_names)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 1240, in prepare_loss_functions
    loss_functions = [get_loss_function(loss) for _ in output_names]
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 1240, in <listcomp>
    loss_functions = [get_loss_function(loss) for _ in output_names]
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 1029, in get_loss_function
    loss_fn = losses.get(loss)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py"", line 1122, in get
    return deserialize(identifier)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py"", line 1113, in deserialize
    printable_module_name='loss function')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 211, in deserialize_keras_object
    raise ValueError('Unknown ' + printable_module_name + ':' + object_name)
ValueError: Unknown loss function:null_fn

**Code to reproduce the issue**

```python
inp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))
tensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)
model = tf.keras.Model(inputs=inp, outputs=tensor)
def null_fn(y_true, y_pred):
    return tf.constant(0.)

model.compile('adam',loss=null_fn)
model.save('model.keras.tf', save_format='tf')
tf.keras.models.load_model('model.keras.tf', custom_objects={'null_fn': null_fn})
```

**Other info / logs**

In   File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 1240, function `prepare_loss_functions`, the custom_objects are not passed as parameter (actually the compile does not accept it currently).

Consisdering the stack, in the two latest files, there are generic keras functions which receive `custom_objects` params. But as they were not passed in the other functions, they are not treated correctly there. So it seems a real bug concerning custom losses.

The keras `.compile` uses the same stack of code, but in that situation, when tryining to get the corresponding function we have a callable. When loading, we have dict.

 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py"", line 1117

``` python
@keras_export('keras.losses.get')
def get(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, six.string_types):
    identifier = str(identifier)
    return deserialize(identifier)
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif callable(identifier):
    return identifier
  else:
    raise ValueError('Could not interpret '
                     'loss function identifier:', identifier)
```

So, there's a need of passing the `custom_objects` param to the `deserialize` method somehow."
30383,"intel-tensorflow (MKL) throws ""could not initialize a memory descriptor"" (CPU, GPU work fine)","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/a

**Describe the current behavior**

Tensorflow with MKL-DNN (intel-tensorflow) throws exception ""could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:380""

**Describe the expected behavior**

No exception thrown.

**Code to reproduce the issue**

pip install intel-tensorflow
tar xvzf testcase_2367.tar.gz # attached
cd testcase_2367
python testcase_2367.py

**Other info / logs**

This inference network runs fine on tensorflow CPU and tensorflow GPU. Only MKL-DNN tensorflow fails.

Yes this is similar to issue #23145, but it is definitely not fixed in r1.13.1.

It is also not fixed in r1.14, which I confirmed by compiling from source (although the line number changes).

[testcase_2367.tar.gz](https://github.com/tensorflow/tensorflow/files/3357086/testcase_2367.tar.gz)
"
30378,Problems with keras model saving when there's a loss added with add_loss,"**System information**

System: windows 10, wsl with ubuntu 18 LTS
Tensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)
Python version: 3.6.8

It also happens in real linux environments (actually, it's easy to simulate each these errors)

**Describe the current behavior**

I'm having many problems when saving/loading keras models with custom loss (added with add_loss). I'll describe each one of the scenarios below (I think they're all related)

**Code to reproduce the issue**

``` python
inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))
tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)
model = tf.keras.Model(inputs=inp, outputs=tensor)
model.add_loss(tf.keras.losses.mean_absolute_error(tensor, tensor + 1))
model.compile('adam')
tf.keras.experimental.export_saved_model(model, 'model.tf')
```

> **When not using a keras layer as loss, it produces a non-valid JSON**:

Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 8, in <module>
    tf.keras.experimental.export_saved_model(model, 'model.tf')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 169, in export_saved_model
    _export_model_json(model, saved_model_path)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 177, in _export_model_json
    model_json = model.to_json()
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1449, in to_json
    model_config, default=serialization.get_json_type, **kwargs)
  File ""/usr/lib/python3.7/json/__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""/usr/lib/python3.7/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.7/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/util/serialization.py"", line 69, in get_json_type
    raise TypeError('Not JSON Serializable:', obj)
TypeError: ('Not JSON Serializable:', b'\n\x03add\x12\x03Add\x1a\x0fconv2d/Identity\x1a\x05add/y*\x07\n\x01T\x12\x020\x01')

Now, a code that uses keras layers

**Code to reproduce the issue**

``` python
inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))
tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)
model = tf.keras.Model(inputs=inp, outputs=tensor)
lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))
model.add_loss(lbd([tensor, tensor + 1]))
model.compile('adam')
tf.keras.experimental.export_saved_model(model, 'model.tf')
```
Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 16, in <module>
    tf.keras.experimental.export_saved_model(model, 'model.tf')
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 166, in export_saved_model
    input_signature)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 236, in _save_v1_format
    _export_mode(mode_keys.ModeKeys.TRAIN, has_saved_vars, **export_args)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 299, in _export_mode
    compile_clone=compile_clone)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 538, in clone_and_build_model
    clone = clone_model(model, input_tensors=input_tensors)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 326, in clone_model
    model, input_tensors=input_tensors, layer_fn=clone_function)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 202, in _clone_functional_model
    model._insert_layers(ancillary_layers, relevant_nodes=relevant_nodes)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1633, in _insert_layers
    for node in layer.inbound_nodes
ValueError: min() arg is an empty sequence

> **When using a keras layer, it loses their inbound_nodes (I've debugged it). It puts them apart from other layers, but loses information of the objects (it's not a question of passing or not `custom_objects` as param).**

Now, trying to use non-experimental saves/loads.

**Code to reproduce the issue**

``` python
inp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))
tensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)
model = tf.keras.Model(inputs=inp, outputs=tensor)
lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))
model.add_loss(lbd([tensor, tensor + 1]))
model.compile('adam')
model.save('model.keras.tf', save_format='tf')
tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})
```

> **A first annoying thing is that it's based on .ext. Even though it only saves in tf2 (put any extension), in load it verifies these extensions. It not a clear way of working in my opinion. Maybe some additional information on the files saved could make it not use the extensions.**

Traceback (most recent call last):
  File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 26, in <module>
    tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 141, in load_model
    return saved_model.load_from_saved_model_v2(filepath, compile)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 1225, in load_from_saved_model_v2
    model._training_config))  # pylint: disable=protected-access
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 337, in compile
    self._compile_weights_loss_and_weighted_metrics()
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1494, in _compile_weights_loss_and_weighted_metrics
    self.total_loss = self._prepare_total_loss(masks)
  File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1595, in _prepare_total_loss
    raise ValueError('The model cannot be compiled '
ValueError: The model cannot be compiled because it has no loss to optimize.


> **The second is related to the loading, is that it does not accept no loss to compile. Keras accepts it (you can see in the examples). It accepts because it accounts for the non-default losses added. This is another saving/loading bug.**

Both experimental and non-experimental functions seem to have some problems in saving / loading. I think if you simulate some custom scenarios with keras, you'll find a bunch of other errors. So, it's worth to take a whole look at them (specially considering that Keras is the default prototyping tool in tf2).

In the current situation, i've not thought of a simple and easy way to save keras models with custom components (those which are not in the list of arguments of compile)"
30376,"Tflite for microcontrollers : micro speech example with more keywords (yes, no, up, down ..) fails","**System information**
- Have I written custom code : No
- OS Platform and Distribution : Linux Ubuntu 16.04 host machine with TensorFlow docker image
- Mobile device : SparkFun Edge Development Board
- TensorFlow installed from (source or binary): Source
- TensorFlow version : 1.14.0
- Python version: Python 2.7.15+
- Bazel version : 0.27 (apt repository)
- GCC/Compiler version : 7.4.0
- GPU model and memory: Nvidia GTX 1070

**Describe the current behavior**

I am trying to extend the [Micro Speech example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech) by adding more keywords such as 'yes, no, stop, go, up, down' etc. I followed the instructions for  training and converting the model to tflite format from the [Micro Speech GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model). However, when I add my newly trained and converted 'tiny_conv_micro_features_model_data.cc' file to my project, compiling and running it on the Sparkfun Microcontroller gives the following error :

 '_Bad input tensor parameters in model_' . 

**More detail and analysis on the above problem**

I successfully compiled and ran the Micro Speech example on SparkFun Edge Development Board using [AI on a microcontroller with TensorFlow Lite and SparkFun Edge](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#0) tutorial.

Next, I trained the model for recognizing more (""yes,no,up,down"")  keywords giving the following commands:

> bazel run -c opt --copt=-mavx2 --copt=-mfma --verbose_failures --host_force_python=PY2 tensorflow/examples/speech_commands:train -- \
--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
--wanted_words=""yes,no,up,down"" --silence_percentage=25 --unknown_percentage=25 --quantize=1

Then I added the newly created 'tiny_conv_micro_features_model_data.cc' file to the [micro_speech_test project](https://drive.google.com/file/d/1cawEQAkqquK_SO4crReDYqf_v7yAwOY8/view) in order to verify that the newly trained model works correctly. Compiling the project and running the binary gives the following error: 

![image](https://user-images.githubusercontent.com/36483321/60624860-310df200-9d9b-11e9-8fef-83cab23fc777.png)

This shows that interpreter requires an input tensor of shape: (1, 49, 43, 1), but the interpreter receives tensor of shape (1, 49, 40, 1) from the newly trained model. 

If the model is trained on input tensor of shape  (1, 49, 43, 1) as per the the [ spectrogram ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#calculating-the-input-to-the-neural-network) of (49x43) pixels, how does the model interpreter of the micro speech project receives wrong shaped input tensor? 

Is it because we select input shape as (1, 49, 40, 1), while converting '.pb' file to '.tfilte' file by running the bazel command:

> bazel run tensorflow/lite/toco:toco -- \
--input_file=/tmp/tiny_conv.pb --output_file=/tmp/tiny_conv.tflite \
--input_shapes=1,49,40,1 --input_arrays=Reshape_1 --output_arrays='labels_softmax' \
--inference_type=QUANTIZED_UINT8 --mean_values=0 --std_values=9.8077

as per the instruction on [GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model) If this is the reason why is the input shape been selected as (1,49,40,1) and not (1,49,43,1).

I also tried the above command by changing the input_shapes argument as --input_shapes=1,49,43,1. But this gives the following error:

![image](https://user-images.githubusercontent.com/36483321/60626163-c8c10f80-9d9e-11e9-9bff-b49f9a92cedb.png)


To verify that whether this error occurs due to adding more keywords to the wanted words argument (such as 'yes, no, up, down'), I trained the model only for 'yes,no' keywords. Still the same error and problem persisted as above.

I have not made any changes to the TensorFlow or micro speech example source files for the above experimentation. I just replaced the pre-existing 'tiny_conv_micro_features_model_data.cc' file by the new 'tiny_conv_micro_features_model_data.cc' file of the newly trained model for verifying my work.

Kindly guide me to solve my above problem.

Thanks."
30372,Poor Feature/Example serialization performance,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): unknown 2.0.0-beta1
- Python version: 3.7.2
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: GeForce GTX Titan X, 12GB

**Describe the current behavior**
The rate of serializing examples is unreasonably slow. In particular, performance is slow for `Feature`s that are a sequence of values. I have a dataset with numerous fields, including two that are represented by short, fixed length, 1-dimensional `Tensor`s. When excluding those two features, serialization happens in a slow but manageable amount of time. Adding either of those two features causes it to take many times as long.

Notably, when mapping a `Dataset` to a function that performs serialization, increasing the number of threads does not significantly impact performance and the CPU remains mostly idle. I do see marked performance improvements and higher CPU usage when parallelizing other mapped functions, so it could be that there is some kind of global bottleneck for this operation.

**Describe the expected behavior**
Records should be serialized in a reasonable amount of time.

**Code to reproduce the issue**
The following code serializes 10000 records in about 6s on a particular machine. Note that if I replace the mapped function with one that simply returns a constant, it takes less than 1s to complete, so the serialization is the problem.

    def make_example_1(*data_list):
        feature_dict = {
            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[0]])),
            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[1]])),
            'c':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[2]])),
            'd':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[3]])),
            'e':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[4]])),
            'f':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[5]])),
            'g':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[6]])),
            'h':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[7]])),
            'i':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[8]])),
            'j':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[9]]))}
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        return example.SerializeToString()
    
    def make_example_1_wrap(*input):
        data = input[0]
        return tf.py_function(
            make_example_1,
            (
                data['a'],data['b'],data['c'],data['d'],data['e'],
                data['f'],data['g'],data['h'],data['i'],data['j']),
            Tout=tf.string)
        
    def feature_test_1(num_threads=None):
        source = {
            'a':tf.constant(0),
            'b':tf.constant(1),
            'c':tf.constant(2),
            'd':tf.constant(3),
            'e':tf.constant(4),
            'f':tf.constant(5),
            'g':tf.constant(6),
            'h':tf.constant(7),
            'i':tf.constant(8),
            'j':tf.constant(9)}
        ds = tf.data.Dataset.from_tensors(source).repeat(10000)
        ds = ds.map(make_example_1_wrap, num_threads)
        it = iter(ds)
        for x in it:
            pass

The following example uses about the same input data size as the previous one, but uses 2 features of length 5 instead of 10 features of length 1. The execution time increases to 17s, highlighting the problem with sequences as `Int64List`s.

    def make_example_2(*data_list):
        feature_dict = {
            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[0])),
            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[1]))}
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        return example.SerializeToString()
    
    def make_example_2_wrap(*input):
        data = input[0]
        return tf.py_function(
            make_example_2,
            (data['a'], data['b']),
            Tout=tf.string)
    
    def feature_test_2(num_threads=None):
        x = {
            'a':tf.constant([0,1,2,3,4]),
            'b':tf.constant([5,6,7,8,9])}
        ds = tf.data.Dataset.from_tensors(x).repeat(10000)
        ds = ds.map(make_example_2_wrap, num_threads)
        it = iter(ds)
        for x in it:
            pass

**Other info / logs**
A possibly related performance issue is mentioned by many users in #16933, although that issue was closed over a year ago due to inactivity."
