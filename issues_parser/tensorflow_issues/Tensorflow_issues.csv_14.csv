Issue Number,Issue Title,Issue Body
48598,Failed to build `xla_client:libxla_computation_client` undeclared inclusion,"
**System information**
- OS Platform and Distribution: OS: Ubuntu 20.04.2 LTS (x86_64)
- TensorFlow installed from (source or binary): 00d31f1d50c
- TensorFlow version: 00d31f1d50c
- Python version: 3.8.5
- Installed using: conda
- Bazel version (if compiling from source): 3.7.2 (1608224243)
- GCC/Compiler version (if compiling from source):  c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: cuda-11.2
- GPU model and memory: GeForce RTX 3090 / 24267MiB



**Describe the problem**

Failed to build `xla_client:libxla_computation_client.so ` at the middle with `this rule is missing dependency declarations for the following files included by 'buffer_reuse_pass.cc': 'bazel-out/.../Dialect/mhlo/IR/lhlo_ops_structs.h.inc'`


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```

(xla) ✘-8 ~/Documents/github/pytorch/xla/third_party/tensorflow [:00d31f1d50c|…3] 
19:54 $ bazel build -s --define framework_shared_object=false -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --cxxopt=-std=c++14 --cxxopt=-Wno-c++11-narrowing --cxxopt=-DXLA_CUDA=1 --config=cuda //tensorflow/compiler/xla/xla_client:libxla_computation_client.so
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Found applicable config definition build:short_logs in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:linux in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/compiler/xla/xla_client:libxla_computation_client.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
SUBCOMMAND: # //tensorflow/cc:client_session [action 'Compiling tensorflow/cc/client/client_session.cc', configuration: 56c9a992f14816a9d59321cd4c3b0fb4f53a5114659e8a55ae567caceaaa3e8d, execution platform: @local_execution_config_platform//:platform]
(cd /home/tyoc213/.cache/bazel/_bazel_tyoc213/19011bf2b17a0b6da5215ad1a05b9611/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64: \
    PATH=/home/tyoc213/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/cuda-11.2/bin:/home/tyoc213/miniconda3/envs/xla/lib:/home/tyoc213/miniconda3/envs/xla/include:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/home/tyoc213/go/bin:/home/tyoc213/.nvm/versions/node/v15.6.0/bin:/home/tyoc213/.deta/bin:/home/tyoc213/miniconda3/envs/xla/bin:/home/tyoc213/miniconda3/condabin:/home/tyoc213/.rvm/gems/ruby-2.7.0/bin:/home/tyoc213/.rvm/gems/ruby-2.7.0@global/bin:/home/tyoc213/.rvm/rubies/ruby-2.7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/tyoc213/.rvm/bin:/home/tyoc213/.rvm/bin:/usr/local/go/bin:/home/tyoc213/go/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.o' -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/k8-opt/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/k8-opt/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/k8-opt/bin/external/aws-checksums -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cufft/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cufft/include -isystem external/local_config_cuda/cuda/curand/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/curand/include -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/k8-opt/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/k8-opt/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/k8-opt/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' '-std=c++14' -Wno-c++11-narrowing '-DXLA_CUDA=1' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -msse3 -DTENSORFLOW_MONOLITHIC_BUILD -pthread -c tensorflow/cc/client/client_session.cc -o bazel-out/k8-opt/bin/tensorflow/cc/_objs/client_session/client_session.pic.o)

...................

ERROR: /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/transforms/BUILD:75:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/tools/kernel_gen/transforms:passes':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/tools/kernel_gen/transforms/buffer_reuse_pass.cc':
  'bazel-out/host/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'

Target //tensorflow/compiler/xla/xla_client:libxla_computation_client.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1719.611s, Critical Path: 267.51s
INFO: 799 processes: 5 internal, 794 local.
FAILED: Build did NOT complete successfully
```


or if I don't use -s
```
ERROR: /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/compiler/mlir/hlo/BUILD:556:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:hlo_dialect_registration':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/init.cc':
  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'
Target //tensorflow/compiler/xla/xla_client:libxla_computation_client.so failed to build

```
**Any other info / logs**

[full.log](https://github.com/tensorflow/tensorflow/files/6330345/full.log)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6330346/tf_env.txt)

"
48593,No rule to make target 'install' [cmake],"Hello!
While compiling the minimal.cpp example of TensorFlow Lite with cmake on Yocto platform, we are having error as follows:

```
Log data follows:
| DEBUG: Executing shell function do_install
| NOTE: make -j 4 DESTDIR=/home/student/fsl-release-bsp/build-analytics-tflite/tmp/work/cortexa9hf-neon-poky-linux-gnueabi/minimal/1.0-r0/image install
| ERROR: oe_runmake failed
| make: *** No rule to make target 'install'.  Stop.
| ERROR: Function failed: do_install (log file is located at /home/student/fsl-release-bsp/build-analytics-tflite/tmp/work/cortexa9hf-neon-poky-linux-gnueabi/minimal/1.0-r0/temp/log.do_install.2654)
ERROR: Task (/home/student/fsl-release-bsp/sources/meta-analytics-tflite/recipes-hello/patch/minimal.bb:do_install) failed with exit code '1'
NOTE: Tasks Summary: Attempted 1787 tasks of which 1786 didn't need to be rerun and 1 failed.
```
On the install line of our CMakeLists.txt file the idea is not clear in our mind about ""TARGETS"". If you brighten up our mind it would be perfect.

- The CMakeLists.txt file is as follows:

```
cmake_minimum_required(VERSION 3.6)

project(patch)

add_executable(minimal minimal.cc)

target_include_directories(minimal PUBLIC /home/student/fsl-release-bsp/build-analytics-tflite/tmp/sysroots/analytics/usr/include/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include)

target_link_libraries(minimal tensorflow-lite pthread ${CMAKE_DL_LIBS})

install(TARGETS patch DESTINATION git)
```
Your guidance and clarifications are important for us.

Thank you in advance !"
48592,TensorFlow Reddit Dataset to DataFrame Throws Error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Python version: 3.6.9
- TensorFlow version (use command below):

```
>>> tf.version.GIT_VERSION
'v2.4.0-49-g85c8b2a817f'
>>> tf.version.VERSION
'2.4.1'
```


**Describe the current behavior**

I want to load the [tfds reddit dataset](https://www.tensorflow.org/datasets/catalog/reddit) and convert it to a dataframe. The below standalone code throws the following error:
```
  File ""/home/rylan/Documents/FieteLab-RCRP/exp_03_language_modeling/main.py"", line 36, in load_dataset
    reddit_dataframe = tfds.as_dataframe(reddit_dataset)
  File ""/home/rylan/Documents/FieteLab-RCRP/rcrp/lib/python3.6/site-packages/tensorflow_datasets/core/as_dataframe.py"", line 218, in as_dataframe
    df = StyledDataFrame(rows)
  File ""/home/rylan/Documents/FieteLab-RCRP/rcrp/lib/python3.6/site-packages/tensorflow_datasets/core/as_dataframe.py"", line 144, in __init__
    super().__init__(*args, **kwargs)
TypeError: object.__init__() takes no parameters
python-BaseException
```



**Standalone code to reproduce the issue**


```
import tensorflow as tf
import tensorflow_datasets as tfds


reddit_dataset = tfds.load(
    'reddit',
    split='train',
    shuffle_files=False,
    download=True,
    data_dir=data_dir)
assert isinstance(reddit_dataset, tf.data.Dataset)
reddit_dataframe = tfds.as_dataframe(reddit_dataset)

```
"
48590,"error: 'tf.TensorScatterUpdate', 'tf.Size' op is neither a custom op nor a flex op","### 1. System information

- OS Platform and Distribution: Ubuntu 18.04.4 LTS
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0

### 2. Code
I'm providing the saved model in the [google-drive](https://drive.google.com/file/d/16tkI7FWiQ8QTqNpBUAceLbS8jNljLKsL/view?usp=sharing).
```
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_6.2090354') # path to the SavedModel directory
tflite_model = converter.convert()
```

### 3. Failure after conversion
I'm getting following error:
```
loc(callsite(""while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
loc(callsite(""while/Size@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): error: 'tf.Size' op is neither a custom op nor a flex op
loc(callsite(""while/NotEqual@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): error: 'tf.NotEqual' op is neither a custom op nor a flex op
loc(""while/cond@__inference_while_body_32776_19417""): error: failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.NotEqual {device = """", incompatible_shape_error = false}
	tf.Size {device = """"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.TensorScatterUpdate {device = """"}
Traceback (most recent call last):
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
    enable_mlir_converter)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %37 = ""tf.TensorScatterUpdate""(%arg5, %1, %36) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %39 = ""tf.TensorScatterUpdate""(%arg6, %1, %38) {device = """"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %66 = ""tf.TensorScatterUpdate""(%arg7, %1, %65) {device = """"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %75 = ""tf.TensorScatterUpdate""(%arg8, %1, %74) {device = """"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %80 = ""tf.TensorScatterUpdate""(%arg9, %1, %79) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %12 = ""tf.TensorScatterUpdate""(%arg5, %11, %6) {device = """"} : (tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> tensor<?x4xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %14 = ""tf.TensorScatterUpdate""(%arg2, %11, %13) {device = """"} : (tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> tensor<?x?xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %19 = ""tf.TensorScatterUpdate""(%arg4, %11, %18) {device = """"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %20 = ""tf.TensorScatterUpdate""(%arg3, %11, %15) {device = """"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>
<unknown>:0: error: loc(callsite(""while/Size@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""]): called from
<unknown>:0: note: loc(callsite(""while/Size@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): see current operation: %28 = ""tf.Size""(%27) {device = """"} : (tensor<?x?xf32>) -> tensor<i32>
<unknown>:0: error: loc(callsite(""while/NotEqual@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): 'tf.NotEqual' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""]): called from
<unknown>:0: note: loc(callsite(""while/NotEqual@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): see current operation: %29 = ""tf.NotEqual""(%28, %cst) {device = """", incompatible_shape_error = false} : (tensor<i32>, tensor<i32>) -> tensor<i1>
<unknown>:0: error: loc(""while/cond@__inference_while_body_32776_19417""): failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.NotEqual {device = """", incompatible_shape_error = false}
	tf.Size {device = """"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.TensorScatterUpdate {device = """"}
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): see current operation: ""func""() ( {
^bb0(%arg0: tensor<i32>, %arg1: tensor<?x?xf32>, %arg2: tensor<?x?xf32>, %arg3: tensor<?x?x?x64xf32>, %arg4: tensor<?x4xf32>, %arg5: tensor<?x300xf32>, %arg6: tensor<?xi32>, %arg7: tensor<?x300x4xf32>, %arg8: tensor<?x300x?x?xf32>, %arg9: tensor<?x300xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<[2, 0, 1]> : tensor<3xi32>} : () -> tensor<3xi32>
  %cst_0 = ""std.constant""() {value = dense<[1, 0]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_1 = ""std.constant""() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_2 = ""std.constant""() {value = dense<300> : tensor<i32>} : () -> tensor<i32>
  %cst_3 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_4 = ""std.constant""() {value = dense<4> : tensor<i32>} : () -> tensor<i32>
  %cst_5 = ""std.constant""() {value = dense<100> : tensor<i32>} : () -> tensor<i32>
  %cst_6 = ""std.constant""() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
  %cst_7 = ""std.constant""() {value = dense<1> : tensor<i32>} : () -> tensor<i32>
  %cst_8 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_9 = ""std.constant""() {value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %cst_10 = ""std.constant""() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_11 = ""std.constant""() {value = dense<[300, 4]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_12 = ""std.constant""() {value = dense<[0, 4]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_13 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_14 = ""std.constant""() {value = dense<[0, 2]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_15 = ""std.constant""() {value = dense<[0, 3]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_16 = ""std.constant""() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_17 = ""std.constant""() {value = dense<[300, -1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_18 = ""std.constant""() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_19 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_20 = ""std.constant""() {value = dense<300> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %0 = ""tfl.pack""(%arg0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %1 = ""tfl.pack""(%0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>
  %2 = ""tfl.add""(%arg0, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %3 = ""tfl.pack""(%2) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %4 = ""tfl.shape""(%arg1) : (tensor<?x?xf32>) -> tensor<2xi32>
  %5 = ""tfl.strided_slice""(%4, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %6 = ""tfl.shape""(%arg2) : (tensor<?x?xf32>) -> tensor<2xi32>
  %7 = ""tfl.strided_slice""(%6, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %8 = ""tfl.maximum""(%7, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %9 = ""tfl.floor_div""(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %10 = ""tfl.floor_mod""(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %11 = ""tfl.not_equal""(%10, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %12 = ""tfl.cast""(%11) : (tensor<i1>) -> tensor<i32>
  %13 = ""tfl.add""(%9, %12) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %14 = ""tfl.maximum""(%13, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %15 = ""tfl.mul""(%7, %cst_5) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %16 = ""tfl.pack""(%15, %cst_4) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %17 = ""tfl.fill""(%16, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x4xf32>
  %18 = ""tfl.pack""(%15, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %19 = ""tfl.fill""(%18, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x?xf32>
  %20 = ""tfl.reshape""(%15, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %21 = ""tfl.fill""(%20, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %22 = ""tfl.strided_slice""(%arg3, %0, %3, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<?x?x?x64xf32>, tensor<1xi32>, tensor<1xi32>, tensor<4xi32>) -> tensor<180x604x64xf32>
  %23:11 = ""tfl.while""(%cst_6, %cst_6, %19, %21, %21, %17, %8, %14, %arg2, %arg4, %arg1) ( {
  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors
    %81 = ""std.call""(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @""while/cond/while_cond""} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> tensor<i1>
    ""tfl.yield""(%81) : (tensor<i1>) -> ()
  },  {
  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors
    %81:11 = ""std.call""(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @""while/cond/while_body""} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)
    ""tfl.yield""(%81#0, %81#1, %81#2, %81#3, %81#4, %81#5, %81#6, %81#7, %81#8, %81#9, %81#10) : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> ()
  }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)
  %24 = ""tfl.shape""(%23#3) : (tensor<?xf32>) -> tensor<1xi32>
  %25 = ""tfl.strided_slice""(%24, %cst_10, %cst_19, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %values, %indices = ""tfl.topk_v2""(%23#3, %25) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %26 = ""tfl.gather""(%23#3, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %27 = ""tfl.strided_slice""(%26, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %28 = ""tfl.gather""(%23#5, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %29 = ""tfl.strided_slice""(%28, %cst_16, %cst_11, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x4xf32>
  %30 = ""tfl.shape""(%29) : (tensor<?x4xf32>) -> tensor<2xi32>
  %31 = ""tfl.strided_slice""(%30, %cst_19, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %32 = ""tfl.sub""(%cst_2, %31) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %33 = ""tfl.reshape""(%32, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %34 = ""tfl.fill""(%33, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %35 = ""tfl.concatenation""(%27, %34) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %36 = ""tfl.expand_dims""(%35, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>
  %37 = ""tf.TensorScatterUpdate""(%arg5, %1, %36) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
  %38 = ""tfl.pack""(%31) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %39 = ""tf.TensorScatterUpdate""(%arg6, %1, %38) {device = """"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>
  %40 = ""tfl.pack""(%cst_6, %32) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %41 = ""tfl.pack""(%40, %cst_16, %cst_16) {axis = 0 : i32, values_count = 3 : i32} : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<3x2xi32>
  %42 = ""tfl.pack""(%40, %cst_16) {axis = 0 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %43 = ""tfl.strided_slice""(%29, %cst_13, %cst_14, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %44 = ""tfl.strided_slice""(%29, %cst_15, %cst_12, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %45 = ""tfl.maximum""(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %46 = ""tfl.add""(%45, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %47 = ""tfl.minimum""(%46, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %48 = ""tfl.maximum""(%47, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %49 = ""tfl.minimum""(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %50 = ""tfl.sub""(%49, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %51 = ""tfl.minimum""(%50, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %52 = ""tfl.maximum""(%51, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %53 = ""tfl.strided_slice""(%29, %cst_16, %cst_13, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %54 = ""tfl.strided_slice""(%29, %cst_14, %cst_15, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %55 = ""tfl.maximum""(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %56 = ""tfl.add""(%55, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %57 = ""tfl.minimum""(%56, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %58 = ""tfl.maximum""(%57, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %59 = ""tfl.minimum""(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %60 = ""tfl.sub""(%59, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %61 = ""tfl.minimum""(%60, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %62 = ""tfl.maximum""(%61, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %63 = ""tfl.pack""(%62, %52, %58, %48) {axis = 1 : i32, values_count = 4 : i32} : (tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) -> tensor<?x4xf32>
  %64 = ""tfl.pad""(%63, %42) : (tensor<?x4xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %65 = ""tfl.expand_dims""(%64, %cst_6) : (tensor<?x?xf32>, tensor<i32>) -> tensor<1x?x?xf32>
  %66 = ""tf.TensorScatterUpdate""(%arg7, %1, %65) {device = """"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>
  %67 = ""tfl.gather""(%23#2, %indices) {axis = 0 : i32} : (tensor<?x?xf32>, tensor<?xi32>) -> tensor<?x?xf32>
  %68 = ""tfl.strided_slice""(%67, %cst_16, %cst_17, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x?xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %69 = ""tfl.transpose""(%68, %cst_0) : (tensor<?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %70 = ""tfl.batch_matmul""(%22, %69) {adj_x = false, adj_y = false} : (tensor<180x604x64xf32>, tensor<?x?xf32>) -> tensor<180x604x?xf32>
  %71 = ""tfl.logistic""(%70) : (tensor<180x604x?xf32>) -> tensor<180x604x?xf32>
  %72 = ""tfl.transpose""(%71, %cst) : (tensor<180x604x?xf32>, tensor<3xi32>) -> tensor<?x180x604xf32>
  %73 = ""tfl.pad""(%72, %41) : (tensor<?x180x604xf32>, tensor<3x2xi32>) -> tensor<?x?x?xf32>
  %74 = ""tfl.expand_dims""(%73, %cst_6) : (tensor<?x?x?xf32>, tensor<i32>) -> tensor<1x?x?x?xf32>
  %75 = ""tf.TensorScatterUpdate""(%arg8, %1, %74) {device = """"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>
  %76 = ""tfl.gather""(%23#4, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %77 = ""tfl.strided_slice""(%76, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %78 = ""tfl.concatenation""(%77, %34) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %79 = ""tfl.expand_dims""(%78, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>
  %80 = ""tf.TensorScatterUpdate""(%arg9, %1, %79) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
  ""std.return""(%39, %75, %37, %66, %80) : (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>) -> ()
}) {sym_name = ""while/cond_then"", sym_visibility = ""private"", type = (tensor<i32>, tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?x?x64xf32>, tensor<?x4xf32>, tensor<?x300xf32>, tensor<?xi32>, tensor<?x300x4xf32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>) -> (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>)} : () -> ()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 633, in convert
    **converter_kwargs)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 574, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %37 = ""tf.TensorScatterUpdate""(%arg5, %1, %36) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %39 = ""tf.TensorScatterUpdate""(%arg6, %1, %38) {device = """"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %66 = ""tf.TensorScatterUpdate""(%arg7, %1, %65) {device = """"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %75 = ""tf.TensorScatterUpdate""(%arg8, %1, %74) {device = """"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>
<unknown>:0: error: loc(callsite(""while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): see current operation: %80 = ""tf.TensorScatterUpdate""(%arg9, %1, %79) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %12 = ""tf.TensorScatterUpdate""(%arg5, %11, %6) {device = """"} : (tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> tensor<?x4xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %14 = ""tf.TensorScatterUpdate""(%arg2, %11, %13) {device = """"} : (tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> tensor<?x?xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %19 = ""tf.TensorScatterUpdate""(%arg4, %11, %18) {device = """"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>
<unknown>:0: error: loc(callsite(""while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
<unknown>:0: note: loc(callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417"")): called from
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): called from
<unknown>:0: note: loc(callsite(""while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586"" at callsite(""while/cond/while@__inference_while_cond_true_32942_19253"" at ""while/cond@__inference_while_body_32776_19417""))): see current operation: %20 = ""tf.TensorScatterUpdate""(%arg3, %11, %15) {device = """"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>
<unknown>:0: error: loc(callsite(""while/Size@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""]): called from
<unknown>:0: note: loc(callsite(""while/Size@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): see current operation: %28 = ""tf.Size""(%27) {device = """"} : (tensor<?x?xf32>) -> tensor<i32>
<unknown>:0: error: loc(callsite(""while/NotEqual@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): 'tf.NotEqual' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""]): called from
<unknown>:0: note: loc(callsite(""while/NotEqual@__inference_while_body_32776_19417"" at fused[""while@__inference_call_40958"", ""StatefulPartitionedCall/yolact/StatefulPartitionedCall/while""])): see current operation: %29 = ""tf.NotEqual""(%28, %cst) {device = """", incompatible_shape_error = false} : (tensor<i32>, tensor<i32>) -> tensor<i1>
<unknown>:0: error: loc(""while/cond@__inference_while_body_32776_19417""): failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.NotEqual {device = """", incompatible_shape_error = false}
	tf.Size {device = """"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.TensorScatterUpdate {device = """"}
<unknown>:0: note: loc(""while/cond@__inference_while_body_32776_19417""): see current operation: ""func""() ( {
^bb0(%arg0: tensor<i32>, %arg1: tensor<?x?xf32>, %arg2: tensor<?x?xf32>, %arg3: tensor<?x?x?x64xf32>, %arg4: tensor<?x4xf32>, %arg5: tensor<?x300xf32>, %arg6: tensor<?xi32>, %arg7: tensor<?x300x4xf32>, %arg8: tensor<?x300x?x?xf32>, %arg9: tensor<?x300xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<[2, 0, 1]> : tensor<3xi32>} : () -> tensor<3xi32>
  %cst_0 = ""std.constant""() {value = dense<[1, 0]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_1 = ""std.constant""() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_2 = ""std.constant""() {value = dense<300> : tensor<i32>} : () -> tensor<i32>
  %cst_3 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_4 = ""std.constant""() {value = dense<4> : tensor<i32>} : () -> tensor<i32>
  %cst_5 = ""std.constant""() {value = dense<100> : tensor<i32>} : () -> tensor<i32>
  %cst_6 = ""std.constant""() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
  %cst_7 = ""std.constant""() {value = dense<1> : tensor<i32>} : () -> tensor<i32>
  %cst_8 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_9 = ""std.constant""() {value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %cst_10 = ""std.constant""() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_11 = ""std.constant""() {value = dense<[300, 4]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_12 = ""std.constant""() {value = dense<[0, 4]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_13 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_14 = ""std.constant""() {value = dense<[0, 2]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_15 = ""std.constant""() {value = dense<[0, 3]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_16 = ""std.constant""() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_17 = ""std.constant""() {value = dense<[300, -1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_18 = ""std.constant""() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_19 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_20 = ""std.constant""() {value = dense<300> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %0 = ""tfl.pack""(%arg0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %1 = ""tfl.pack""(%0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>
  %2 = ""tfl.add""(%arg0, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %3 = ""tfl.pack""(%2) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %4 = ""tfl.shape""(%arg1) : (tensor<?x?xf32>) -> tensor<2xi32>
  %5 = ""tfl.strided_slice""(%4, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %6 = ""tfl.shape""(%arg2) : (tensor<?x?xf32>) -> tensor<2xi32>
  %7 = ""tfl.strided_slice""(%6, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %8 = ""tfl.maximum""(%7, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %9 = ""tfl.floor_div""(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %10 = ""tfl.floor_mod""(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %11 = ""tfl.not_equal""(%10, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %12 = ""tfl.cast""(%11) : (tensor<i1>) -> tensor<i32>
  %13 = ""tfl.add""(%9, %12) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %14 = ""tfl.maximum""(%13, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %15 = ""tfl.mul""(%7, %cst_5) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %16 = ""tfl.pack""(%15, %cst_4) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %17 = ""tfl.fill""(%16, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x4xf32>
  %18 = ""tfl.pack""(%15, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %19 = ""tfl.fill""(%18, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x?xf32>
  %20 = ""tfl.reshape""(%15, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %21 = ""tfl.fill""(%20, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %22 = ""tfl.strided_slice""(%arg3, %0, %3, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<?x?x?x64xf32>, tensor<1xi32>, tensor<1xi32>, tensor<4xi32>) -> tensor<180x604x64xf32>
  %23:11 = ""tfl.while""(%cst_6, %cst_6, %19, %21, %21, %17, %8, %14, %arg2, %arg4, %arg1) ( {
  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors
    %81 = ""std.call""(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @""while/cond/while_cond""} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> tensor<i1>
    ""tfl.yield""(%81) : (tensor<i1>) -> ()
  },  {
  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors
    %81:11 = ""std.call""(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @""while/cond/while_body""} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)
    ""tfl.yield""(%81#0, %81#1, %81#2, %81#3, %81#4, %81#5, %81#6, %81#7, %81#8, %81#9, %81#10) : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> ()
  }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)
  %24 = ""tfl.shape""(%23#3) : (tensor<?xf32>) -> tensor<1xi32>
  %25 = ""tfl.strided_slice""(%24, %cst_10, %cst_19, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %values, %indices = ""tfl.topk_v2""(%23#3, %25) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %26 = ""tfl.gather""(%23#3, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %27 = ""tfl.strided_slice""(%26, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %28 = ""tfl.gather""(%23#5, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %29 = ""tfl.strided_slice""(%28, %cst_16, %cst_11, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x4xf32>
  %30 = ""tfl.shape""(%29) : (tensor<?x4xf32>) -> tensor<2xi32>
  %31 = ""tfl.strided_slice""(%30, %cst_19, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %32 = ""tfl.sub""(%cst_2, %31) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %33 = ""tfl.reshape""(%32, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %34 = ""tfl.fill""(%33, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %35 = ""tfl.concatenation""(%27, %34) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %36 = ""tfl.expand_dims""(%35, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>
  %37 = ""tf.TensorScatterUpdate""(%arg5, %1, %36) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
  %38 = ""tfl.pack""(%31) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %39 = ""tf.TensorScatterUpdate""(%arg6, %1, %38) {device = """"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>
  %40 = ""tfl.pack""(%cst_6, %32) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %41 = ""tfl.pack""(%40, %cst_16, %cst_16) {axis = 0 : i32, values_count = 3 : i32} : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<3x2xi32>
  %42 = ""tfl.pack""(%40, %cst_16) {axis = 0 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %43 = ""tfl.strided_slice""(%29, %cst_13, %cst_14, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %44 = ""tfl.strided_slice""(%29, %cst_15, %cst_12, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %45 = ""tfl.maximum""(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %46 = ""tfl.add""(%45, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %47 = ""tfl.minimum""(%46, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %48 = ""tfl.maximum""(%47, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %49 = ""tfl.minimum""(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %50 = ""tfl.sub""(%49, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %51 = ""tfl.minimum""(%50, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %52 = ""tfl.maximum""(%51, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %53 = ""tfl.strided_slice""(%29, %cst_16, %cst_13, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %54 = ""tfl.strided_slice""(%29, %cst_14, %cst_15, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>
  %55 = ""tfl.maximum""(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %56 = ""tfl.add""(%55, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %57 = ""tfl.minimum""(%56, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %58 = ""tfl.maximum""(%57, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %59 = ""tfl.minimum""(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %60 = ""tfl.sub""(%59, %cst_3) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %61 = ""tfl.minimum""(%60, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %62 = ""tfl.maximum""(%61, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %63 = ""tfl.pack""(%62, %52, %58, %48) {axis = 1 : i32, values_count = 4 : i32} : (tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) -> tensor<?x4xf32>
  %64 = ""tfl.pad""(%63, %42) : (tensor<?x4xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %65 = ""tfl.expand_dims""(%64, %cst_6) : (tensor<?x?xf32>, tensor<i32>) -> tensor<1x?x?xf32>
  %66 = ""tf.TensorScatterUpdate""(%arg7, %1, %65) {device = """"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>
  %67 = ""tfl.gather""(%23#2, %indices) {axis = 0 : i32} : (tensor<?x?xf32>, tensor<?xi32>) -> tensor<?x?xf32>
  %68 = ""tfl.strided_slice""(%67, %cst_16, %cst_17, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x?xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %69 = ""tfl.transpose""(%68, %cst_0) : (tensor<?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %70 = ""tfl.batch_matmul""(%22, %69) {adj_x = false, adj_y = false} : (tensor<180x604x64xf32>, tensor<?x?xf32>) -> tensor<180x604x?xf32>
  %71 = ""tfl.logistic""(%70) : (tensor<180x604x?xf32>) -> tensor<180x604x?xf32>
  %72 = ""tfl.transpose""(%71, %cst) : (tensor<180x604x?xf32>, tensor<3xi32>) -> tensor<?x180x604xf32>
  %73 = ""tfl.pad""(%72, %41) : (tensor<?x180x604xf32>, tensor<3x2xi32>) -> tensor<?x?x?xf32>
  %74 = ""tfl.expand_dims""(%73, %cst_6) : (tensor<?x?x?xf32>, tensor<i32>) -> tensor<1x?x?x?xf32>
  %75 = ""tf.TensorScatterUpdate""(%arg8, %1, %74) {device = """"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>
  %76 = ""tfl.gather""(%23#4, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %77 = ""tfl.strided_slice""(%76, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %78 = ""tfl.concatenation""(%77, %34) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %79 = ""tfl.expand_dims""(%78, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>
  %80 = ""tf.TensorScatterUpdate""(%arg9, %1, %79) {device = """"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>
  ""std.return""(%39, %75, %37, %66, %80) : (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>) -> ()
}) {sym_name = ""while/cond_then"", sym_visibility = ""private"", type = (tensor<i32>, tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?x?x64xf32>, tensor<?x4xf32>, tensor<?x300xf32>, tensor<?xi32>, tensor<?x300x4xf32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>) -> (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>)} : () -> ()
```

The function where I'm using `tensor_scatter_nd_update` is as follows:
```
def _traditional_nms(self, boxes, mask_coef, scores, iou_threshold=0.5, score_threshold=0.3, max_class_output_size=100, max_output_size=300, soft_nms_sigma=0.5):
        num_classes = tf.shape(scores)[1]

        _num_coef = tf.shape(mask_coef)[1]
        _boxes = tf.zeros((max_class_output_size*num_classes, 4), tf.float32)
        _coefs = tf.zeros((max_class_output_size*num_classes, _num_coef), tf.float32)
        _classes = tf.zeros((max_class_output_size*num_classes), tf.float32)
        _scores = tf.zeros((max_class_output_size*num_classes), tf.float32)

        for _cls in range(num_classes):
            cls_scores = scores[:, _cls]
            selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(
                boxes, 
                cls_scores, 
                max_output_size=max_class_output_size, 
                iou_threshold=iou_threshold, 
                score_threshold=score_threshold,
                soft_nms_sigma=soft_nms_sigma)

            _update_boxes = tf.gather(boxes, selected_indices)
            _num_boxes = tf.shape(_update_boxes)[0]
            _ind_boxes = tf.range(_cls*max_class_output_size, _cls*max_class_output_size+_num_boxes)

            _boxes = tf.tensor_scatter_nd_update(_boxes, tf.expand_dims(_ind_boxes, axis=-1), _update_boxes)
            _coefs = tf.tensor_scatter_nd_update(_coefs, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(mask_coef, selected_indices))
            _classes = tf.tensor_scatter_nd_update(_classes, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(cls_scores, selected_indices) * 0.0 + tf.cast(_cls, dtype=tf.float32) + 1.0)
            _scores = tf.tensor_scatter_nd_update(_scores, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(cls_scores, selected_indices))

        _ids = tf.argsort(_scores, direction='DESCENDING')
        scores = tf.gather(_scores, _ids)[:max_output_size]
        boxes = tf.gather(_boxes, _ids)[:max_output_size]
        mask_coef = tf.gather(_coefs, _ids)[:max_output_size]
        classes = tf.gather(_classes, _ids)[:max_output_size]

        return boxes, mask_coef, classes, scores
```
also in the following `__call__` function:
```
def __call__(self, net_outs, trad_nms=True):
        """"""
        Args:
             pred_offset: (tensor) Loc preds from loc layers
                Shape: [batch, num_priors, 4]
            pred_cls: (tensor) Shape: Conf preds from conf layers
                Shape: [batch, num_priors, num_classes]
            pred_mask_coef: (tensor) Mask preds from mask layers
                Shape: [batch, num_priors, mask_dim]
            priors: (tensor) Prior boxes and variances from priorbox layers
                Shape: [num_priors, 4]
            proto_out: (tensor) If using mask_type.lincomb, the prototype masks
                Shape: [batch, mask_h, mask_w, mask_dim]
        
        Returns:
            output of shape (batch_size, top_k, 1 + 1 + 4 + mask_dim)
            These outputs are in the order: class idx, confidence, bbox coords, and mask.
            Note that the outputs are sorted only if cross_class_nms is False
        """"""

        box_p = net_outs['pred_offset']  # [1, 27429, 4]
        class_p = net_outs['pred_cls']  # [1, 27429, 2]
        coef_p = net_outs['pred_mask_coef']  # [1, 27429, 32]
        anchors = net_outs['priors']  # [27429, 4]
        proto_p = net_outs['proto_out']  # [1, 90, 302, 32]
        
        proto_h = tf.shape(proto_p)[1]
        proto_w = tf.shape(proto_p)[2]

        box_decode = self._decode(box_p, anchors)  # [1, 27429, 4]
        
        num_class = tf.shape(class_p)[2] - 1

        # Apply softmax to the prediction class
        class_p = tf.nn.softmax(class_p, axis=-1)
        # exclude the background class
        class_p = class_p[:, :, 1:]
        # get the max score class of 27429 predicted boxes
        class_p_max = tf.reduce_max(class_p, axis=-1)  # [1, 27429]
        batch_size = tf.shape(class_p_max)[0]

        detection_boxes = tf.zeros((batch_size, self.max_output_size, 4), tf.float32)
        detection_classes = tf.zeros((batch_size, self.max_output_size), tf.float32)
        detection_scores = tf.zeros((batch_size, self.max_output_size), tf.float32)
        detection_masks = tf.zeros((batch_size, self.max_output_size, proto_h, proto_w), tf.float32)
        num_detections = tf.zeros((batch_size), tf.int32)

        for b in range(batch_size):
            # filter predicted boxes according the class score
            class_thre = tf.boolean_mask(class_p[b], class_p_max[b] > 0.3)
            box_thre = tf.boolean_mask(box_decode[b], class_p_max[b] > 0.3) 
            coef_thre = tf.boolean_mask(coef_p[b], class_p_max[b] > 0.3)

            if tf.size(class_thre) != 0:
                if not trad_nms:
                    box_thre, coef_thre, class_ids, class_thre = _fast_nms(box_thre, coef_thre, class_thre)
                else:
                    box_thre, coef_thre, class_ids, class_thre = self._traditional_nms(box_thre, coef_thre, class_thre)

                # Padding with zeroes to reach max_output_size
                class_ids = tf.concat([class_ids, tf.zeros(self.max_output_size - tf.shape(box_thre)[0])], 0)
                class_thre = tf.concat([class_thre, tf.zeros(self.max_output_size - tf.shape(box_thre)[0])], 0)
                num_detection = [tf.shape(box_thre)[0]]
                pad_num_detection = self.max_output_size - num_detection[0]

                _masks_coef = tf.matmul(proto_p[b], tf.transpose(coef_thre))
                _masks_coef = tf.sigmoid(_masks_coef) # [138, 138, NUM_BOX]

                boxes, masks = self._sanitize(_masks_coef, box_thre)
                masks = tf.transpose(masks, (2,0,1))
                paddings = tf.convert_to_tensor( [[0, pad_num_detection], [0,0], [0, 0]])
                masks = tf.pad(masks, paddings, ""CONSTANT"")
                
                paddings = tf.convert_to_tensor( [[0, pad_num_detection], [0, 0]])
                boxes = tf.pad(boxes, paddings, ""CONSTANT"")

                detection_boxes = tf.tensor_scatter_nd_update(detection_boxes, [[b]], tf.expand_dims(boxes, 0))
                detection_classes = tf.tensor_scatter_nd_update(detection_classes, [[b]], tf.expand_dims(class_ids, 0))
                detection_scores = tf.tensor_scatter_nd_update(detection_scores, [[b]], tf.expand_dims(class_thre, 0))
                detection_masks = tf.tensor_scatter_nd_update(detection_masks, [[b]], tf.expand_dims(masks, 0))
                num_detections = tf.tensor_scatter_nd_update(num_detections, [[b]], num_detection)
        
        result = {'detection_boxes': detection_boxes,'detection_classes': detection_classes, 'detection_scores': detection_scores, 'detection_masks': detection_masks, 'num_detections': num_detections}
        return result
```
"
48589,Conv2d didn't raise exception for invalid input argument.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): bianry
- TensorFlow version (use command below): 2 .4.1
- Python version: 3.7


**Standalone code to reproduce the issue**
When `Conv2D` with `kernel_size`=`2` and padding=`valid` receives an invalid input, it does not raise any exception. Instead it outputs a tensor with zero-dimension. This can lead to future crash for other APIs with 0-dim tensor as input.

```
import tensorflow as tf
import numpy as np

filters, kernel_size, strides, padding = 3, [2, 2], 2, 'valid'
data = np.random.rand(1, 1, 1, 1)
layer = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)
print(layer(data).shape)
```

Outputs
```
(1, 0, 0, 3)
```



**Describe the current behavior**
No exception is raised for invalid input argument.

**Describe the expected behavior**
Expect `ValueError` to be raised.
"
48587,TFlite Converter - EfficientDet conversion fails on int quantization (IndexError: _Map_base::at),"Hello, 

I am trying to convert EfficientDet-D4 model to tflite, with integer quantization with CPU fallback for unsupported operations. I am encountering error same as [this issue](https://github.com/tensorflow/tensorflow/issues/43239). I have tried using different converter settings but none of them work. Interestingly, though, EfficientDet-D0 converts with this type of quantization without any issue. What I've noticed is that all EfficientDet versions from D1-D7 have this error, only D0 converts successfully.

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

I am using slightly modified code from [EfficientDet repo](https://github.com/google/automl/tree/master/efficientdet), file ""inference.py"". I had to enable TF Select Ops in order to convert the model to tflite.

I am converting the model using guidelines on EfficientDet github repo (model_inspect.py)

def representative_dataset():
        files = [f for f in os.listdir('./representative_dataset') if os.path.isfile(os.path.join(""representative_dataset"", f))]
        for file in files:
           file = ""representative_dataset/"" + file
           img = cv2.imread(file, cv2.IMREAD_COLOR)
           resized = cv2.resize(img, (1024, 1024), interpolation = cv2.INTER_CUBIC)
           rgb_resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
           np_resized = np.array([rgb_resized])
           yield [np_resized.astype(np.uint8)]

if tflite_path:
      height, width = utils.parse_image_size(self.params['image_size'])
      input_name = signitures['image_arrays'].op.name
      input_shapes = {input_name: [None, height, width, 3]}
      converter = tf.lite.TFLiteConverter.from_saved_model(
          output_dir,
          input_arrays=[input_name],
          input_shapes=input_shapes,
          output_arrays=[signitures['prediction'].op.name])
      converter.optimizations = [tf.lite.Optimize.DEFAULT]
      converter.representative_dataset = representative_dataset
      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
      tflite_model = converter.convert()

This code results in the following error

Traceback (most recent call last):
  File ""../automl/efficientdet/model_inspect.py"", line 520, in <module>
    app.run(main)
  File ""/home/fapannen/.local/lib/python3.7/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/fapannen/.local/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""../automl/efficientdet/model_inspect.py"", line 513, in main
    trace_filename=FLAGS.trace_filename)
  File ""../automl/efficientdet/model_inspect.py"", line 462, in run_model
    self.export_saved_model(**config_dict)
  File ""../automl/efficientdet/model_inspect.py"", line 151, in export_saved_model
    driver.export(self.saved_model_dir, self.tflite_path, self.tensorrt)
  File ""/mnt/c/Users/Owner/Desktop/git/Thesis-EfficientDet/automl/efficientdet/inference.py"", line 621, in export
    tflite_model = converter.convert()
  File ""/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1947, in convert
    return super(TFLiteConverter, self).convert()
  File ""/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1313, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 461, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 100, in calibrate_and_quantize
    self._calibrator.FeedTensor(sample)
[tflitelog.txt](https://github.com/tensorflow/tensorflow/files/6328939/tflitelog.txt)

### 5. (optional) Any other info / logs
Attached full traceback log.

Any help is highly appreciated.
Cheers,
Tomas
"
48586,Why do we need to divide the loss in case of multi-dimensional labels?,"I am following TensorFlow's tutorial on [custom training][1].
There are two things that I do not understand:

First, when defining the losses, the authors point to dividing by the shape of one example, in the case of a multi-dimensional labels:

""If `labels` is multi-dimensional, then average the `per_example_loss` across the number of elements in each sample. For example, if the shape of predictions is `(batch_size, H, W, n_classes)` and labels is `(batch_size, H, W)`, you will need to update per_example_loss like: `per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)`""

Why do we need to rescale the loss this way? And does this also hold for Cycle-GANs?

Secondly, a general question: I noticed that several functions use the `with strategy.scope()` annotation. If this object is available globally, then we can define a function on the global level like this
```
with strategy.scope():
  def example_function():
    #do stuff
```
If the object is not available globally, we have to structure the code like this:
```
def train():
  strategy = ...

  with strategy.scope():
    def sub_function1():
      #do stuff
```

This leads to one large function, `train()`, with many short sub functions. I'd like to define the sub functions on the same level as `train()`, without using a global `strategy` object. 
Is there a workaround to achieve this?



  [1]: https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function"
48585,how use session in function for TF1.14?,"I build a seesion,but I use it in functoin,it will call me that No default TensorFlow session found.
how I reslove the issue?"
48584,Models having locally connected layers with implementation = 2 or 3 cannot be saved in SavedModel format.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab, so binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7




**Describe the current behavior**
This issue is a continuation of #47689 . Whenever the user opts for `implementation=2` or `implementation=3`, the model cannot be saved as a SavedModel format.  

**Describe the expected behavior**
One should be able to save it as SavedModel format.

**Standalone code to reproduce the issue**
Please see the gist [here](https://colab.research.google.com/gist/AdityaKane2001/c74692d463f14517d84ee9be9443941d/tfpr_maker.ipynb).

**Other info**
I have played around a bit with the source code in the colab environment. I have inserted many `logging_ops.print_v2(...)` statements to make it easier to debug. Also, I have cloned my fork (it's a clean fork, no personal commits) and ran the `local_test.py` file. I have added `model_2.save('model2')` line, too check for the issue.

**Some observations:**
1. The `local.py`  file has no errors (none that I found). But many function calls can be updated as they are deprecated.
2. The `K.reshape` or `array_ops.reshape` calls are the ones that are causing the issue. There's some bug in  `tensorflow/python/framework/op_def_library.py  : _apply_op_helper(op_type_name, name=None, **keywords)` function. 
3. Note that `local.py: 790` calls `reshape`, which calls `gen_array_ops.reshape`, which calls `op_def_library._apply_op_helper`. But the arguments that go to the latter **were not recorded in `**keywords` argument.**  I checked that by printing the keywords dict in the error message. 

Please take a look at the error log under local_test.py execution cell in the colab notebook.
Thanks. 
"
48579, ai-explanations-image.ipynb - Saved model error ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
https://colab.research.google.com/github/GoogleCloudPlatform/ml-on-gcp/blob/master/tutorials/explanations/ai-explanations-image.ipynb#scrollTo=Yn8hEAHiXaE7

Performance issues
Error encountered during saved model of iris Dataset Flower - using sample colab tutorial - ai-explanations-image.ipynb

![Screen Shot 2021-04-16 at 4 47 57 PM](https://user-images.githubusercontent.com/62075076/115094774-97fa8d80-9ed3-11eb-8f60-cc93d26dad98.png)

"
48575,tensortflow-lite installation and packaging,"Hello,

I'm trying to create a standalone artifact of TFLite SDK. Running

`cmake -DCMAKE_INSTALL_PREFIX=~/tmp/foo ../tensorflow/tensorflow/lite
make
make install
`

The build completes, but the result of the build isn't installed. How can I assemble a standalone package?"
48572,tf.nn.compute_average_loss documentation/code misaligned,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss

## Description of issue (what needs changing):
The name of the method itself and the documentation at the URL above implies that the calculation will be an average (mean) reduction.
But in the source code (https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/ops/nn_impl.py#L391-L446), line 446 calculates a sum reduction.
Either this is a mistake, and the code should be changed to do a mean reduction, or the documentation should be made more clear as to what the method actually does and why."
48571,No allocator statistics when starting a network,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10, debain 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):  2.5.0
- Python version: 3.9.2
- CUDA/cuDNN version: 11.3, tried with 10.0, 11.0, 11.1, 11.2 aswell
- GPU model and memory: Rtx 2060 Super 8G



**Describe the current behavior**
If i try to use tensorflow in any way i could it throws a 
`RuntimeError: No allocator statistics` Error 
Even if i type 
`import tensorflow as tf; 
tf.test.is_gpu_available() `
Or if i try to Initialise a Sequential  model 

**Describe the expected behavior**
Normaly it worked fine until today when it just stopped working


**Other info / logs** 
If i try to initialise a Sequential() model it gives me the following 
` 2021-04-16 21:27:46.833849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll

2021-04-16 21:27:49.167053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-04-16 21:27:49.193965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-04-16 21:27:49.194117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-16 21:27:49.199998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-16 21:27:49.200139: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-04-16 21:27:49.203405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-04-16 21:27:49.204197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-04-16 21:27:49.206899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll
2021-04-16 21:27:49.208804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-04-16 21:27:49.209441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-04-16 21:27:49.209593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2021-04-16 21:27:49.214120: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-16 21:27:49.217270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-04-16 21:27:49.217438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-04-16 21:27:49.713688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-16 21:27:49.713844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
2021-04-16 21:27:49.715934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N
2021-04-16 21:27:49.717181: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.
Traceback (most recent call last):
  File ""f:\Github\test\tensorflow\word.py"", line 107, in <module>
    model = Sequential()
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\tracking\base.py"", line 522, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\keras\engine\sequential.py"", line 114, in __init__
    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\tracking\base.py"", line 522, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\keras\engine\training.py"", line 318, in __init__
    self._init_batch_counters()
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\training\tracking\base.py"", line 522, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\keras\engine\training.py"", line 326, in _init_batch_counters
    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\variables.py"", line 262, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\variables.py"", line 244, in _variable_v2_call
    return previous_getter(
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\variables.py"", line 237, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\variable_scope.py"", line 2662, in default_variable_creator_v2
    return resource_variable_ops.ResourceVariable(
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1584, in __init__
    self._init_from_args(
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1727, in _init_from_args
    initial_value = ops.convert_to_tensor(initial_value,
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\profiler\trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\ops.py"", line 1566, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\tensor_conversion_registry.py"", line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py"", line 264, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py"", line 276, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\context.py"", line 525, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: No allocator statistics `

As well as this if i run 
`--> tf.test.is_gpu_available()`

` >>> import tensorflow as tf
2021-04-16 21:23:14.876381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
INFO:tensorflow:Enabling eager execution
INFO:tensorflow:Enabling v2 tensorshape
INFO:tensorflow:Enabling resource variables
INFO:tensorflow:Enabling tensor equality
INFO:tensorflow:Enabling control flow v2
--> tf.test.is_gpu_available()
WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-04-16 21:23:25.481405: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-16 21:23:25.484960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-04-16 21:23:25.520614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-04-16 21:23:25.520759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-16 21:23:25.529842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-16 21:23:25.529971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-04-16 21:23:25.534167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-04-16 21:23:25.535573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-04-16 21:23:25.540947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll
2021-04-16 21:23:25.544673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-04-16 21:23:25.545533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-04-16 21:23:25.545662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-04-16 21:23:26.045993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-16 21:23:26.046109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
2021-04-16 21:23:26.047907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N
2021-04-16 21:23:26.048739: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\util\deprecation.py"", line 337, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\test_util.py"", line 1600, in is_gpu_available
    for local_device in device_lib.list_local_devices():
  File ""C:\Users\Weise\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\client\device_lib.py"", line 43, in list_local_devices
    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)
RuntimeError: No allocator statistics` "
48570,Fails building example projects for ESP micro,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 2.5.0-rc1
- TensorFlow version:
- Python version:3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
1. Got tensorflow-master
2. make -f tensorflow/lite/micro/tools/make/Makefile PARSE_THIRD_PARTY=true TARGET=esp generate_projects
3. Results in the following
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
Traceback (most recent call last):
  File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 122, in <module>
    parse_args()
  File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 118, in parse_args
    main(unparsed, flags)
  File ""tensorflow/lite/micro/tools/make/generate_keil_project.py"", line 40, in main
    six.ensure_str(flags.executable),
AttributeError: 'module' object has no attribute 'ensure_str'
make: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:36: tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32_default/prj/hello_world_test/keil/keil_project.uvprojx] Error 1



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48568,Model datatype,"Hi,

I am trying print the summary of give model. I am more specifically interested in the datatype of the model.

I have tried the print_model_analysis() but this doesn't print the datatype. I have also tried summarize_graph utility but had no luck.

Is there something I am missing ? Are there any apis to do so ?

Cheers,
Prabhakar"
48567,Dilated and causal convolutions on microcontrollers,"@tensorflow/micro

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.10
- TensorFlow version: 2.4.1
- Python version : 3.8

**Describe the problem**

I would like to run inference on a microcontroller above by using a model characterized by Conv1D layers which implement causal convolutions 1D. In particular, in main.cpp I thought I would use something like this:

`// Pull in only needed operations (should match NN layers).`
`// Template parameter <n> is number of ops to be added. Available ops:`
`//` `tensorflow/lite/micro/kernels/micro_ops.h`

`static tflite::MicroMutableOpResolver <1> micro_op_resolver;`
`tflite_status = micro_op_resolver.Conv1D(); `

`if (tflite_status != kTfLiteOk) {`
`error_reporter->Report(""Could not add Conv1D op"");`
`while(1);`
`}`

However, the Conv1D operation shouldn't be supported by TensorFlow Lite: how can I solve this problem? Have I create a custom operator, in order to implement the op, or there is another way to fix it?
In order to create the op, I wrote this simple code:

`import tensorflow as tf`
`tf.config.run_functions_eagerly(True)`

`input_shape = (1, 7, 1)`
`x = tf.random.normal(input_shape)`

`@tf.function`
`def convol1d():`
`y=tf.keras.layers.Conv1D(1, 3, input_shape=input_shape[1:], name=""Conv1D"")(x)`
`return y`

`data = convol1d()`
`print(""\n\n data is:"", data)`

`tflite_model_name = 'convol1d'`
`converter= tf.lite.TFLiteConverter.from_concrete_functions([convol1d.get_concrete_function()])`
`converter.allow_custom_ops = True`
`tflite_model = converter.convert()`
`open(tflite_model_name + '.tflite', 'wb').write(tflite_model)`

If I run it, it appears the following error:

`ValueError: tf.function-decorated function tried to create variables on non-first call.`

Instead, I would expect it:

`Error: Didn't find custom operator for name 'Conv1D'`
`Registration failed.`

If this latter error appeared, I would try to define the functions Prepare and Eval and construct a TfLiteRegistration in a file .cpp, then I would add an AddCustom call to register.cpp, am I right? At this point, if all went well, I would try to use the op. 

Thanks in advance.
"
48564,while creating VGGSEGNET giving following error,"<em>Below is VGGSEGNET code</em>

This function call- **model=VGGSegnet(n_classes=1, input_height=224, input_width=224)** is giving same error on following both versions of TF and KERAS and Python

**System information**
- tf version=2.4.1 & 2.2.0
- keras version=2.4.3 & 2.4.3
- python=3.7 and python 3.8

**Following is code defining VGGSEGNET:-**

`def VGGSegnet(n_classes, input_height, input_width, vgg_level=3, pretrained_weights = None):

    img_input = Input(shape=(input_height, input_width,3 ))

    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last')(img_input)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format='channels_last')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool1', data_format='channels_last')(x)
    f1 = x

    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format='channels_last')(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format='channels_last')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last')(x)
    f2 = x

    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format='channels_last')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format='channels_last')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format='channels_last')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool1', data_format='channels_last')(x)
    f3 = x

    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format='channels_last')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format='channels_last')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format='channels_last')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool1', data_format='channels_last')(x)
    f4 = x

    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format='channels_last')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format='channels_last')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format='channels_last')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool1', data_format='channels_last')(x)
    f5 = x

    x = Flatten(name='flatten')(x)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dense(1000, activation='softmax', name='predictions')(x)

    vgg = Model(img_input, x)
    vgg.load_weights(""image-segmentation-keras-py3-master/Models/vgg16_weights_th_dim_ordering_th_kernels.hdf5"")

    levels = [f1, f2, f3, f4, f5]

    o = levels[vgg_level]

    o = ZeroPadding2D((1,1),data_format='channels_last')(o)
    o = Conv2D(512,(3,3),padding='valid',data_format='channels_last')(o)
    o = BatchNormalization()(o)

    o = UpSampling2D((2,2),data_format='channels_last')(o)
    o = ZeroPadding2D((1,1),data_format='channels_last')(o)
    o = Conv2D(256,(3,3),padding='valid',data_format='channels_last')(o)
    o = BatchNormalization()(o)

    o = UpSampling2D((2,2),data_format='channels_last')(o)
    o = ZeroPadding2D((1,1),data_format='channels_last')(o)
    o = Conv2D(128,(3,3),padding='valid',data_format='channels_last')(o)
    o = BatchNormalization()(o)

    o = UpSampling2D((2, 2), data_format='channels_last')(o)
    o = ZeroPadding2D((1, 1), data_format='channels_last')(o)
    o = Conv2D(64, (3, 3), padding='valid', data_format='channels_last')(o)
    o = BatchNormalization()(o)
    
    o = UpSampling2D((2, 2), data_format='channels_last')(o)
    o = ZeroPadding2D((1, 1), data_format='channels_last')(o)
    o = Conv2D(32, (3, 3), padding='valid', data_format='channels_last')(o)
    o = BatchNormalization()(o)

    #o = UpSampling2D((2, 2), data_format='channels_last')(o)
    #o = ZeroPadding2D((1, 1), data_format='channels_last')(o)
    o = Conv2D(n_classes,(3,3),padding='same',data_format='channels_last')(o)
    #o = BatchNormalization()(o)
    o_shape = Model(img_input,o).output_shape
    #outputHeight = o_shape[2]
    #outputWidth = o_shape[3]
    outputHeight = o_shape[2]
    outputWidth = o_shape[1]


    #o = (Reshape((outputHeight*outputWidth, -1)))(o)
    #o = (Permute((1,2)))(o)
    o = (Activation('sigmoid'))(o)
    model = Model(img_input,o)
    model.outputWidth = outputWidth
    model.outputHeight = outputHeight
    if(pretrained_weights):
        model.load_weights(pretrained_weights)

    return  model`

**Error is as below**

`Traceback (most recent call last):
  File ""/scratch/pkasar.dbatu/training/VGGSEGNET_224_224_working_on_20_03_21_on_augmented_images_of_size_256_by_256.py"", line 248, in <module>
    model=VGGSegnet(n_classes=1, input_height=224, input_width=224)
  File ""/scratch/pkasar.dbatu/training/VGGSEGNET_224_224_working_on_20_03_21_on_augmented_images_of_size_256_by_256.py"", line 56, in VGGSegnet
    vgg.load_weights(""image-segmentation-keras-py3-master/Models/vgg16_weights_th_dim_ordering_th_kernels.h5"")
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 2234, in load_weights
    hdf5_format.load_weights_from_hdf5_group(f, self.layers)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 710, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 3706, in batch_set_value
    x.assign(np.asarray(value, dtype=dtype(x)))
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py"", line 781, in assign
    return values_util.on_write_assign(
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py"", line 140, in on_write_assign
    return var._update(  # pylint: disable=protected-access
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py"", line 940, in _update
    return self._update_cross_replica(update_fn, value, **kwargs)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py"", line 893, in _update_cross_replica
    return self.distribute_strategy.extended.update(
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2494, in update
    return self._update(var, fn, args, kwargs, group)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 710, in _update
    fn(v, *distribute_utils.select_replica_mirrored(i, args),
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 572, in wrapper
    return func(*args, **kwargs)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py"", line 139, in <lambda>
    assign_fn = lambda var, *a, **kw: var.assign(*a, **kw)
  File ""/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 888, in assign
    raise ValueError(
ValueError: Cannot assign to variable block1_conv1/kernel:0 due to variable shape (3, 3, 3, 64) and value shape (3, 3, 64, 3) are incompatible`

I am doing segmentation task using iou as performance metric.
The link for vgg16_weights_th_dim_ordering_th_kernels.h5 is this [https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5](url)
Help me out. Thank you in advance"
48563,Could not load dynamic library amazon ec2,"
**System information**
- OS Platform and Distribution (aws Linux 2):

- TensorFlow installed from (source or binary): via pip
- TensorFlow version: 2.4.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtual env, pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla t4 (g4dn.xlarge)



**Describe the problem**

I installed GPU drivers on fresh EC2 instance using commands mentioned [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#nvidia-GRID-driver)
But it is not picking gpu. 

2021-04-16 13:19:22.605167: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not  creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-16 13:19:22.622026: I tensorflow/stream_executor/platform/default/dso_lo  ader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-16 13:19:23.590887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.   cc:941] successful NUMA node read from SysFS had negative value (-1), but there                         must be at least one NUMA node, so returning NUMA node zero
2021-04-16 13:19:23.591452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-16 13:19:23.591551: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591612: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591657: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591698: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591745: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591798: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591838: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591883: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.s.8: cannot open shared object file: No such file or directory
2021-04-16 13:19:23.591894: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1                        757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the                         required libraries for your platform."
48562,Building the latest 'tflite_runtime' from source for Windows,"**System information**
- Windows 10
- tf-nightly
- Python version: 3.7
- Bazel version:3.7.2


**Describe the problem**
I am trying to build from source the latest `tflite_runtime-2.6.0` for windows. using the following command:

`$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows`
and it fails, the output is below. I took this command from the guide https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package 


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
$ git clone https://github.com/tensorflow/tensorflow.git
$ cd tensorflow
```

```
$ bash tensorflow/lite/tools/make/download_dependencies.sh
download_dependencies.sh completed successfully.
```

Here are a list of extra commands that I took just to make sure that everything is in place:

```
$ bazel --version
bazel 3.7.2
```

```
$ mintty --version
mintty 3.0.1 (x86_64-pc-msys)
```

```
$ which python
/e/ProgramData/Anaconda3/envs/python37_april2021_tf24/python
```

```
$ git --version
git version 2.22.0.windows.1
```

```
$ which pip
/e/ProgramData/Anaconda3/envs/python37_april2021_tf24/Scripts/pip
```

```
$ bash --version
GNU bash, version 4.4.23(1)-release (x86_64-pc-msys)
```


**Any other info / logs**
So when i run the command `$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows`
This is the output:

```
+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh
++ cd tensorflow/lite/tools/pip_package
++ pwd
+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package
+ PYTHON=python3
+ VERSION_SUFFIX=
+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite
++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py
++ cut -d= -f2
++ sed 's/[ '\''-]//g'
+ TENSORFLOW_VERSION=2.6.0
+ export PACKAGE_VERSION=2.6.0
+ PACKAGE_VERSION=2.6.0
+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ TENSORFLOW_TARGET=windows
+ '[' '!' -z '' ']'
+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ echo '__version__ = '\''2.6.0'\'''
++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe
+ echo '__git_version__ = '\''0.6.0-108808-g1fe211938d1'\'''
+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ BAZEL_FLAGS=--copt=-O3
+ export CROSSTOOL_PYTHON_INCLUDE_PATH
+ case ""${TENSORFLOW_TARGET}"" in
+ LIBRARY_EXTENSION=.pyd
+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper
WARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from c:\users\radus\downloads\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe
INFO: Reading rc options for 'build' from c:\users\radus\downloads\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\users\radus\downloads\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --action_env PYTHON_LIB_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/lib/site-packages --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions
INFO: Found applicable config definition build:short_logs in file c:\users\radus\downloads\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\radus\downloads\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file c:\users\radus\downloads\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:noaws in file c:\users\radus\downloads\tensorflow\.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file c:\users\radus\downloads\tensorflow\.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file c:\users\radus\downloads\tensorflow\.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file c:\users\radus\downloads\tensorflow\.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:windows in file c:\users\radus\downloads\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\radus\downloads\tensorflow\.bazelrc: --define framework_shared_object=false
Loading:
Loading: 0 packages loaded
DEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Build options --action_env, --copt, and --host_copt have changed, discarding analysis cache.
Analyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (14 packages loaded, 15 targets configured)
INFO: Repository local_config_python instantiated at:
  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace
  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains
Repository rule python_configure defined at:
  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>
INFO: Repository local_execution_config_python instantiated at:
  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace
  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains
  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl"", line 214, column 51, in _create_local_python_repository
                python_include_rule = _symlink_genrule_for_dir(
        File ""C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl"", line 66, column 35, in _symlink_genrule_for_dir
                files = ""\n"".join(read_dir(repository_ctx, src_dir))
        File ""C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl"", line 113, column 30, in read_dir
                find_result = execute(
        File ""C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
The system cannot find the path specified.
INFO: Repository go_sdk instantiated at:
  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>
  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace
  C:/users/radus/_bazel_radus/tfciaawb/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps
  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains
  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Repository command failed
The system cannot find the path specified.
INFO: Elapsed time: 1.534s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (14 packages loaded, 15 targets configured)
FAILED: Build did NOT complete successfully (14 packages loaded, 15 targets configured)

```

"
48561,"What does ”Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them“ mean?","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Tensorflow-2.3.0
Every time when I saved model as :
```
model_train.save(FLAG.save_path,include_optimizer=False)
```
The warming log showed that :
```
W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
```
It actually from tensorflow/python/util/util.cc:
```
int IsSequenceHelper(PyObject* o) {
  // We treat dicts and other mappings as special cases of sequences.
  if (IsMappingHelper(o)) return true;
  if (IsMappingViewHelper(o)) return true;
  if (IsAttrsHelper(o)) return true;
  if (PySet_Check(o) && !WarnedThatSetIsNotSequence) {
    LOG(WARNING) << ""Sets are not currently considered sequences, ""
                    ""but this may change in the future, ""
                    ""so consider avoiding using them."";
    WarnedThatSetIsNotSequence = true;
  }
  static auto* const check_cache = new CachedTypeCheck([](PyObject* to_check) {
    int is_instance = IsInstanceOfRegisteredType(to_check, ""Sequence"");

    // Don't cache a failed is_instance check.
    if (is_instance == -1) return -1;

    return static_cast<int>(is_instance != 0 && !IsString(to_check));
  });
  return check_cache->CachedLookup(o);
}
```
Although it does not influence the process of traing, I really want to know what does this warming mean?"
48560,RTX 3070 gpu usage is low in CNN model training using tensorflow.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): tensorflow 2.5.0rc1
- Python version: python 3.6.9
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory: RTX 3070

**Describe the current behavior**

I trained the CNN model below using Docker.
It took 10 minutes per epoch on the GTX 1060, but over 20 minutes per epoch on the RTX 3070. When I checked the gpu usage through the nvidia-smi command, it was mostly 0%.
Why does the RTX 3070 have a slower training time than the GTX 1060? How do I fix it?

tensorflow recognizes RTX 3070 and uses vram as well.
docker used tensorflow/tensorflow:2.5.0rc1-gpu-jupyter
The cpu is using the intel i7-10700k

**Standalone code to reproduce the issue**
```
effnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)`
def create_model():
    model = keras.Sequential()
    model.add(effnet)
    model.add(GlobalAveragePooling2D())
    
    model.add(Dense(512))
    model.add(BatchNormalization())
    model.add(ReLU())
    
    model.add(Dense(128))
    model.add(BatchNormalization())
    model.add(ReLU())
    
    model.add(Dense(32))
    model.add(BatchNormalization())
    model.add(ReLU())
    model.add(Dense(num_classes, activation=""sigmoid""))
    return model
```
```
def scheduler(epoch, lr):
    if epoch == 0:
        lr = init_learning_rate
    else:
        lr = lr * 0.9
    
    print('learning rate: {:.10f}'.format(lr))
    tf.summary.scalar('learning_rate', data=lr, step=epoch)
    return lr

learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)
```
```
model = create_model()

f1_score = tfa.metrics.F1Score(
    num_classes=num_classes,
    average='macro')

optimizer = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)
model.compile(loss='binary_crossentropy', optimizer=optimizer,
              metrics=['accuracy',f1_score])
```
```
logdir = ""/tf/notebooks/compete/Plant_Pathology_2021/logs/scalars/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
file_writer = tf.summary.create_file_writer(logdir + ""/metrics"")
file_writer.set_as_default()
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

history = model.fit(
        train_generator,
        steps_per_epoch=len(train_generator),
        epochs=num_epochs,
        validation_data=validation_generator,
        validation_steps=len(validation_generator),
        callbacks=[tensorboard_callback,learning_rate_scheduler],
        verbose=1)
```"
48559,Unexpected behavior of soft device placement,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Soft device placement by default is set to True. Despite this running the code below causes it to fail with error:
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:dummy:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0]. [Op:Fill]

Even though tf.config.get_soft_device_placement() returns True in the error we can see ""enable_soft_placement=0""
WA for this issue is to explicitly call  tf.config.set_soft_device_placement(True)

**Describe the expected behavior**
Soft device placement should work as indicated by tf.config.get_soft_device_placement()
Also tf.device inside tf.function should not have such side effects

**Standalone code to reproduce the issue**
```
    # tf.config.set_soft_device_placement(True) # Uncomment this for WA
    @tf.function
    def func():
        with tf.device(""/device:CPU:0""): # this tf.device within tf.function breaks soft placement in eager mode later
            return tf.constant([1])    
    func()

    print(""tf.config.get_soft_device_placement() = "", tf.config.get_soft_device_placement()) # Always prints True

    # Commenting code above this line makes code below execute correctly
    with tf.device(""/device:dummy:0""):
        tf.fill([1, 1], 1) # Error
```
"
48558,tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: cloud server
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2..0.0
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: CUDA 10.1 & cuDNN 7.6.5
- GPU model and memory: 
00:07.0 VGA compatible controller: NVIDIA Corporation Device 1eb8 (rev a1) (prog-if 00 [VGA controller])
	Subsystem: NVIDIA Corporation Device 130e
	Physical Slot: 7
	Flags: bus master, fast devsel, latency 0, IRQ 37
	Memory at fc000000 (32-bit, non-prefetchable) [size=16M]
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at fa000000 (64-bit, non-prefetchable) [size=32M]
	I/O ports at c500 [size=128]
	Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+
	Kernel driver in use: nvidia
	Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia

![Screen Shot 2021-04-16 at 6 59 05 PM](https://user-images.githubusercontent.com/11975415/115015147-d26d2600-9ee5-11eb-91db-2fb3c863fa4f.png)



**Describe the problem**

Here is how i got the error

``Python 3.7.7 (default, May  7 2020, 21:25:33) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
2021-04-16 16:46:25.884063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-04-16 16:46:25.888707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.889366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: GRID T4-4Q computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 3.97GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-16 16:46:25.889562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-04-16 16:46:25.891366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-04-16 16:46:25.893293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-04-16 16:46:25.893593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-04-16 16:46:25.895611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-04-16 16:46:25.896780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-04-16 16:46:25.901308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-04-16 16:46:25.901433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.902118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.902701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-04-16 16:46:25.903031: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-04-16 16:46:25.910663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2500000000 Hz
2021-04-16 16:46:25.910940: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa728000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-04-16 16:46:25.910965: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-04-16 16:46:25.967935: W tensorflow/compiler/xla/service/platform_util.cc:210] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error
2021-04-16 16:46:25.968035: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA
2021-04-16 16:46:25.968258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.968912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: GRID T4-4Q computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 3.97GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-16 16:46:25.968993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-04-16 16:46:25.969017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-04-16 16:46:25.969042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-04-16 16:46:25.969066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-04-16 16:46:25.969089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-04-16 16:46:25.969113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-04-16 16:46:25.969140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-04-16 16:46:25.969237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.969898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-04-16 16:46:25.970481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2021-04-16 16:46:25.970546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 262, in constant
    allow_broadcast=True)
  File ""/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 270, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 95, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 515, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Indeed, before this error, I had another error:

Could not load dynamic library 'libcublas.so.10'; dlerror: libcublasLt.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:

I didn't know why, but libcublas.so.10 and libcublasLt.so.10 were in /usr/local/cuda-10.2/targets/x86_64-linux/lib instead of /usr/local/cuda-10.1/lib64, so I copied them both to /usr/local/cuda-10.1/lib64 and it solved the problem. Would there be any connections between these two errors? If no, any help would be appreciated !
"
48557,Why there is no DepthwiseConv1D function in TensorFlow?,"The function DepthwiseConv2D、SeparableConv1D and SeparableConv2D are all supported in TensorFlow, however, the function DepthwiseConv1D is still not supported yet. But in fact, the function of SeparableConv1D is conduct by a sequence of a DepthwiseConv and a PointWiseConv, so it seems that it's very convient to add DepthwiseConv1D function to the TensorFlow."
48556,"Create image_classifier in colab have an error. ""unknown image file format. one of jpeg png gif bmp required""","![image](https://user-images.githubusercontent.com/44739285/115004251-d04c9c80-9ed0-11eb-924c-7dcf0060dae8.png)
The images datasets [here](https://babycuatoi.vn/tree.zip)
Help me please!"
48555,InvalidArgumentError when using class_weight in Model.fit with labels having extra dimension for time step,"We used the `class_weight` parameter to assign different weights for samples of different classes in `keras.Model.fit()`.
Our data (input) and labels (output) have an extra time-step dimension besides the batch dimension.
The labels are indices of classes (i.e. not one-hot encoded), used along with the **sparse** CE loss function.
We've written a minimal reproduction script (as presented below) to simplify the situation.

It looks like that `class_weight` is only designed for outputs of shape (batch_dim, n_classes).

Possible workarounds:
 1. convert `class_weight` to `sample_weight`
 2. collapse time-step axis into batch axis using `tf.reshape()` in a `Lambda` layer, which would mess the code up

I've noticed there are relevant issues but they are left there and closed as the authors did not reply in time.

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Anaconda, binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.5
- CUDA/cuDNN version: cudnn 7.6.5 cuda10.1_0.conda (irrelevant)
- GPU model and memory: Tesla K40c, 11441MiB (irrelevant)


**Describe the current behavior**
Crashed on `model.fit` with the following exception (traceback appended at the end as it is too long):

> InvalidArgumentError: indices[0] = 3 is not in [0, 2)
>	 [[{{node GatherV2}}]] [Op:IteratorGetNext]

The training progress bar did not appear.

**Describe the expected behavior**
Complete the training, though the minimal reproduction script has no actual trainable parameter.

**Standalone code to reproduce the issue**
```python
import numpy
import tensorflow as tf
from tensorflow import keras

def test_class_weight_error():
    # the model simply return the input time_step*n_class=20x2 data as-is
    model = keras.Sequential([keras.layers.Reshape((20, 2), input_shape=(20, 2))])
    # run_eagerly improves the readability of the traceback a bit
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', run_eagerly=True)
    model.summary()
    # X (inputs, as well as y_pred): samples*time_step*n_classes=15x20x2
    xs = tf.reshape(tf.one_hot(tf.ones(300, dtype=tf.int32), 2), [-1, 20, 2]).numpy()
    # Y (labels i.e. y_true): samples*time_step=15x20, class labels of 0 or 1
    ys = np.ones([15, 20], dtype=np.int32)
    # without the line below (a.k.a. all labels being 1) there's no exception
    ys[:,:3] = 0
    # here's the crash
    model.fit(xs, ys, batch_size=3, class_weight={0:1.,1:1.})
    
test_class_weight_error()
```

**Other info / logs** 

Traceback:
[traceback.txt](https://github.com/tensorflow/tensorflow/files/6323917/traceback.txt)

Model Structure (in minimal repro script):
 > Model: ""sequential_64""
 > Layer \(type\)                 Output Shape              Param
 > reshape_64 \(Reshape\)         \(None, 20, 2\)             0         
 > Total params: 0
 > Trainable params: 0
 > Non-trainable params: 0
 > _________________________________________________________________"
48554,RuntimeError: MetaGraphDef associated with tags {''serve'} could not be found in SavedModel.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source):anaconda installation
- TensorFlow library (version, if pip package or github SHA, if built from source):2.3.0

### 2. Code
 I am using, the official code from the tensorflow website:
REFERENCE : https://www.tensorflow.org/lite/convert

`import tensorflow as tf `                                                                            
                                                                                                                 
`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)`
`tflite_model = converter.convert()                                                            `
`open(""converted_model.tflite"", ""wb"").write(tflite_model)                         `


### 3. ERROR  

According above code,here is an error.

**RuntimeError :  MetaGraphDef associated with tags {'serve'} could not be found in SavedModel. 
To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`
available_tags: [{'train', 'serve'}]**
 

### 4. modify code
I modeify my code:

`import tensorflow as tf `                                                                            
                                                                                                                
`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,tags=[{'serve', 'train'}])`
`tflite_model = converter.convert()                                                            `
`open(""converted_model.tflite"", ""wb"").write(tflite_model)                         `


Have an other error:

 **if set(meta_graph_def.meta_info_def.tags) == set(tags):
TypeError: unhashable type: 'set'**


### Any other info / logs

I don't know this modify is true.

When I try to convert this is the error I get, so I cannot successfully convert to Lite.




"
48553,Best methods to debug and identify dynamic-sized tensors to work around: TensorFlow Lite Error: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 12 pro
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): v1.12.1-53831-ga8b6d5ff93a 2.5.0-rc0
- Python version: 3.8


**Describe the current behavior**
Converted Google's Repnet Model to a tflite model to run on ios. I appear to be using too much ram to be able to run with the cpu, so I am trying to run on gpu. Except `TensorFlow Lite Error: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.` is being thrown at runtime on ios.

**Describe the expected behavior**
I understand dynamic-sized tensors cannot be used with tflite (edit: Tflite GPU), but how can I debug to identity the use of dynamic-sized tensors?


**Standalone code to reproduce the issue**
https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb

```
# converting to tflite with
model = get_repnet_model(PATH_TO_CKPT)

tf.keras.models.save_model( model,
    ""repnet_savedmodel""
)

# Convert the model using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_saved_model(""repnet_savedmodel"")
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] # .OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
with open(""repnet.tflite"", 'wb') as f:
  f.write(tflite_model)

```

"
48552,How to create a representative dataset using TFrecords for tensorflow lite post training quantize conversion?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.6 dev


I am trying to create a representative data set for conversion to tflite and perform a int8 quantize operations. I was wondering, is it possible to use TFrecords as a input data to be used to create a `representative_dataset` instead of looping through each images to generate a `representative_dataset`? This is due to no examples of using TFrecords in the API but the API just says you can load TFrecord and create a dataset for manipulation. "
48551,tf.strings.lower() does not lower-case non-ASCII characters,"**System information**
This bug happens in Colab notebook. It happens on other platforms as well, but that's beside the point.

**Describe the current behavior**

tf.strings.lower() lower cases A-Z to a-z but it does not lower case other unicode characters, e.g. LATIN CAPITAL LETTER E WITH ACUTE, LATIN CAPITAL LETTER Z WITH CARON, and many others, etc. See the code below. 

It seems that tf.strings.lower() keeps all upper-case non-ASCII characters as is. I was unable to find an example of any character outside ASCII that is lower cased correctly. But I did only spot checks.

**Describe the expected behavior**

tf.strings.lower() should lower case all accented characters. E.g. É should become é, Ž should become ž, same as in Python. My understanding of Unicode standard is that there is a language-independent mapping from upper case to lower case for every unicode character. This Unicode web page explains the details: https://unicode.org/faq/casemap_charprop.html 
Tensorflow's tf.strings.lower() should implement this mapping.

**Standalone code to reproduce the issue**

Here is a link to a Colab notebook: https://colab.research.google.com/drive/1LyF0FZp9uMsqsyUGaWEdalhLql5B69k5

Here is the copy of the code from the Colab botebook:

```
# This piece of code demonstrates a bug in tf.strings.lower().

import tensorflow as tf

# Text in Slovak language. (It is first half of a panagram.)
# The text is in 3 different casings: Upper case, lower case, and
# mixed case. In the mixed case, all letters with diacritics 
# are upper case and standard English ASCII letters are lower case.
UPPER_CASE = u""STARÝ KÔŇ NA HŔBE KNÍH ŽUJE TÍŠKO POVÄDNUTÉ RUŽE""
LOWER_CASE = u""starý kôň na hŕbe kníh žuje tíško povädnuté ruže""
MIXED_CASE = u""starÝ kÔŇ na hŔbe knÍh Žuje tÍŠko povÄdnutÉ ruŽe""


class TestLowerCase(tf.test.TestCase):

    # This test case passes.
    # It verifies that, in Python, all three texts
    # are the same after lower casing.
    def test_python(self):
        self.assertEqual(UPPER_CASE.lower(), LOWER_CASE)
        self.assertEqual(MIXED_CASE.lower(), LOWER_CASE)
        self.assertEqual(LOWER_CASE.lower(), LOWER_CASE)

    # This test case passes.
    # It demonstrates the current tensorflow behaviour,
    # which is arguably incorrect.
    def test_tensorflow_current(self):
        tf_upper_case = tf.constant(UPPER_CASE, dtype=tf.string)
        tf_mixed_case = tf.constant(MIXED_CASE, dtype=tf.string)
        self.assertAllEqual(tf.strings.lower(tf_upper_case), tf_mixed_case)

    # This test case fails!
    # It demonstrates the desired/expected behavior.
    def test_tensorflow_desired(self):
        tf_upper_case = tf.constant(UPPER_CASE, dtype=tf.string)
        tf_lower_case = tf.constant(LOWER_CASE, dtype=tf.string)
        self.assertAllEqual(tf.strings.lower(tf_upper_case), tf_lower_case)


# Run all three test cases. The first two pass. The third one fails.
TestLowerCase().test_python()
TestLowerCase().test_tensorflow_current()
TestLowerCase().test_tensorflow_desired()
```"
48550,Missing libtensorflow builds for 2.5.0-rc0 and 2.5.0-rc1,"There don't appear to be prebuilt binaries for libtensorflow (https://www.tensorflow.org/install/lang_c) for `2.5.0-rc0` and `2.5.0-rc1`.

These binaries exist for 2.3.0 and 2.4.0 (and the corresponding RCs) under https://storage.googleapis.com/tensorflow/libtensorflow/, but none exist for the 2.5.0 RCs.

For example, https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0-rc0.tar.gz is one of the missing files.

Would it be possible to push those binaries/trigger CI builds to generate them?

Thanks!
"
48549,TF Squeezing dimension off of sequential model inputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab notebook provided below
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f

**Describe the current behavior**

Creating a simple model where the final axis dimension is 1 causes data called by the model to fail with the following error: `ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (5, 160)`
For the output above, the input was defined as `(160, 1)`, but running an array with the shape `(5, 160, 1)` caused the error. Running in TF 2.2 everything works as expected.

**Describe the expected behavior**

The model should successfully be called on data where the final axis is 1.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1owEhfGDRt3lrRfpu5PzRE9v1I6SWVzxc

**Other info / logs**

The problem seems to be due to [this statement](https://github.com/tensorflow/tensorflow/blob/5f808811084219533fd58238b6e97577218808e6/tensorflow/python/keras/engine/functional.py#L615). There's a function, `_conform_to_reference_input` which will squeeze the final axis ""if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).""

This behaviour seems quite counter intuitive to me. The data I'm working with is a time series that can have variable numbers of input channels. Removing the axis with a single input channel breaks many of my models when I have a single input channel. Even a basic model, such as a 2D CNN on MNIST, this would seem to make the inputs incompatible unless that axis is added again in the model before the CNN layer.

Is there a different way we're supposed to handle inputs with a single channel?
"
48548,TFlite INT8 conversion error- Quantization not supported for op: 'DIV',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source): 2.4.1
- TensorFlow library (version, if pip package or github SHA, if built from source): pip

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
(just the part where I get the error, of the whole script)
```python
# tflite experimentation
optimize_lite_model = True
num_calibration_examples = 20 
representative_dataset = None
if optimize_lite_model and num_calibration_examples:
    # Use a bounded number of training examples without labels for calibration.
    # TFLiteConverter expects a list of input tensors, each with batch size 1.
    representative_dataset = lambda: itertools.islice(
      ([image[None, ...]] for batch, _ in train_dataset for image in batch),
      num_calibration_examples)

if optimize_lite_model:
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    if representative_dataset:  # This is optional, see above.
        converter.representative_dataset = representative_dataset

tflite_model = converter.convert()
tflite_model_name = 'TFlites/m0/int8_optimize.tflite'
open(tflite_model_name, ""wb"").write(tflite_model)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:
```
RuntimeError                              Traceback (most recent call last)
<ipython-input-34-5b618f18cb5b> in <module>
     43         converter.representative_dataset = representative_dataset
     44 
---> 45 tflite_model = converter.convert()
     46 tflite_model_name = 'TFlites/m0/int8_optimize.tflite'
     47 open(tflite_model_name, ""wb"").write(tflite_model)

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    872 
    873     return super(TFLiteKerasModelConverterV2,
--> 874                  self).convert(graph_def, input_tensors, output_tensors)
    875 
    876 

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)
    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()
    631     if calibrate_and_quantize:
--> 632       result = self._calibrate_quantize_model(result, **flags)
    633 
    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)
    459       return calibrate_quantize.calibrate_and_quantize(
    460           self.representative_dataset.input_gen, inference_input_type,
--> 461           inference_output_type, allow_float, activations_type)
    462 
    463   def _is_unknown_shapes_allowed(self):

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input)
    102         np.dtype(input_type.as_numpy_dtype()).num,
    103         np.dtype(output_type.as_numpy_dtype()).num, allow_float,
--> 104         np.dtype(activations_type.as_numpy_dtype()).num)
    105 
    106   def calibrate_and_quantize_single(self,

RuntimeError: Quantization not yet supported for op: 'DIV'.
```"
48547,"WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.3 and Ubuntu 18 (google colab)
- TensorFlow installed from (source or binary): pip (binary)
- TensorFlow versions (use command below):

  - colab    --> v2.4.1-0-g85c8b2a817f 2.4.1
  - macOS --> v2.4.0-49-g85c8b2a817f 2.4.1

- Python version: 3.8.8 (macOS) and 3.7 (colab)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I'm using the following actor critic model:

    def create_a2c(input_shape, actor_units):
        initializer = Orthogonal(tf.math.sqrt(2.0))
        x0 = Input(input_shape)
        x = Dense(64, activation='tanh', kernel_initializer=initializer)(x0)
        x = Dense(64, activation='tanh', kernel_initializer=initializer)(x)
        actor_out = Dense(actor_units, kernel_initializer=Orthogonal(0.01))(x)
        critic_out = Dense(1, kernel_initializer=Orthogonal(1.0))(x)
        return Model(x0, [actor_out, critic_out])

I keep getting:

    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.

**Describe the expected behavior**
I expect the gradients to exist.

**Standalone code to reproduce the issue**

Here's a colab [notebook](https://colab.research.google.com/drive/1gqkXUBiulwZ4XSiWg4eVmfcKFQ3Ce1G_?usp=sharing) 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48545,Introduce ability to clear GPU memory in Tensorflow 2,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1, 2.4.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Currently there is no way to completely free the (once) allocated GPU RAM. 
For example, i want to use tensorflow in the context of 3d visualization which is made next to impossible by this behavior. Standard solutions like `tf.config.experimental.set_memory_growth(gpus[0], True)` are unfortunately not sufficient, because the once allocated RAM cannot be released again.

In #36465 (https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-818742876), it is mentioned that by using `GPUProcessState::TestOnlyReset` and `ProcessState::TestOnlyReset` the option to release GPU memory exists, but is just not exposed or for testing purposes only.

It would be very nice for applications using tensorflow to have proper access to gpu ram release functions.

**Will this change the current api? How?**
Introduce a new (experimental) function to reset the current session/graph/device/... - state and thus free the GPU RAM completely.

**Who will benefit with this feature?**
People who use Tensorflow in their application in conjunction with other GPU-RAM critical operations such as 3D rendering."
48544,Tf lite interpreter.allocate_tensors() Runtime error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (binary):
- TensorFlow version (2.4.1):
- Also tried tf-nightly


Ive used the following code to convert keras model to tflite. Conversion went successfull without any errors. Note that i used tf-nightly for the conversion as stable version of tf 2.4.1 was unable to convert
```
import tensorflow as tf
from tensorflow.keras.models import load_model

modelPath = ""u2net_keras.h5""

tflite_model = load_model(modelPath)
converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)

# converter = tf.lite.TFLiteConverter.from_keras_model_file( 'u2netp_keras.h5')
tfmodel = converter.convert()
open (""model.tflite"" , ""wb"") .write(tfmodel)
print(""Sucess!"")
```
Now I am using the following code to get inference from the converted model but code seems to get stuck on 
`'interpreter.allocate_tensors()'` line

```
import tensorflow as tf
import PIL.Image as Image
import numpy as np

import cv2


def preprocess(img_path, dim):
    img = Image.open(img_path)
    img = img.resize(dim, Image.BILINEAR)
    img_data = np.array(img)
    img_data = np.transpose(img_data, [2, 0, 1])
    img_data = np.expand_dims(img_data, 0)
    mean_vec = np.array([0.485, 0.456, 0.406])
    stddev_vec = np.array([0.229, 0.224, 0.225])
    norm_img_data = np.zeros(img_data.shape).astype('float32')
    for i in range(img_data.shape[1]):
        norm_img_data[:,i,:,:] = (img_data[:,i,:,:]/255 - mean_vec[i]) / stddev_vec[i]
    return norm_img_data


w = 320
h = 320
dim = (w,h)
model = ""U2net-tflite-models/u2net.tflite""
img_path = 'boat.jpg'
img = preprocess(img_path, dim)


# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model)

interpreter.resize_tensor_input(0, [1,3, 320, 320], strict=False)

interpreter.allocate_tensors()



#get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()


# Read the image and decode to a tensor

#Preprocess the image to required size and cast
input_shape = input_details[0]['shape']
print(""input shape = "",np.expand_dims(img,0))
input_tensor= np.array(np.expand_dims(img,0))

#set the tensor to point to the input data to be inferred
input_index = interpreter.get_input_details()[0][""index""]
interpreter.set_tensor(input_index, input_tensor)
#Run the inference
interpreter.invoke()
output_details = interpreter.get_output_details()


output_data = interpreter.get_tensor(output_details[0]['index'])
results = np.squeeze(output_data)
top_k = results.argsort()
for label, idx in train_data_gen.class_indices.items():
    if top_k[idx]==1:
        print(""Prediction: "" ,label)
```

ERROR log

```
2021-04-15 18:34:01.240901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-15 18:34:01.240937: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""/home/faraz/Desktop/Esper_Solutions/upwrok/tfLite/U-2-Net-Keras/tflite_inference.py"", line 34, in <module>
    interpreter.allocate_tensors()
  File ""/home/faraz/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 420, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: tensorflow/lite/kernels/space_to_batch_nd.cc:67 op_context->block_shape->dims->data[0] != spatial_dims_num (3 != 2)Node number 49 (SPACE_TO_BATCH_ND) failed to prepare.


Process finished with exit code 1
```

"
48541,ValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Variable 'UnreadVariable' ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I am using Python 3.6 with Tensorflow 2.3.0 and Keras 2.3.0 on Windows 10.  NVIDIA-SMI 452.56       Driver Version: 452.56       CUDA Version: 11.0

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I run training of a Color-CNN model (https://github.com/beerboaa/Color-Classification-CNN/blob/master/color_net_training.py) I get 

Found 1260 images belonging to 3 classes.
Found 420 images belonging to 3 classes.
Traceback (most recent call last):
  File ""C:/workspace/color_model/color_classification_cnn.py"", line 162, in <module>
    model.fit_generator(
  File ""C:\Python3\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""C:\Python38\lib\site-packages\keras\engine\training.py"", line 1718, in fit_generator
    return training_generator.fit_generator(
  File ""C:\Python3\lib\site-packages\keras\engine\training_generator.py"", line 42, in fit_generator
    model._make_train_function()
  File ""C:\Python3\lib\site-packages\keras\engine\training.py"", line 328, in _make_train_function
    self.train_function = K.function(
  File ""C:\Python3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3007, in function
    return tf_keras_backend.function(inputs, outputs,
  File ""C:\Python3\lib\site-packages\tensorflow\python\keras\backend.py"", line 3932, in function
    raise ValueError('`updates` argument is not supported during '
ValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Variable 'UnreadVariable' shape=(48,) dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
      dtype=float32)>, <tf.Variable 'UnreadVariable' shape=(48,) dtype=float32, numpy=


**Describe the expected behavior**

This script should train the color-CNN model

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://github.com/beerboaa/Color-Classification-CNN/blob/master/color_net_training.py

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48540,How can I change dtype of TFLITE model Weights ?,"**System information**
- OS Platform and Distribution (Windows 10):
- TensorFlow installed from binary
- TensorFlow version 2.4.1


I have a TFLITE model which was made from KERAS Functional API. Unfortunately I have deleted the all files related to that model like weights.h5 and model structure file. Now All I have is my TFLITE model. Its weights are in float 32 but I have to run it on mobile. So I need to convert weights to int8 (Quantization mode) or float16. Is there anyway So I can cast my TFLITE model weights ?  
[Model.zip](https://github.com/tensorflow/tensorflow/files/6318011/Model.zip)

I have Attached the Model below
"
48538,build_all_linux.sh failed: threadpool.cc:94:29: error: no matching function for call to ‘Eigen::NonBlockingThreadPoolTempl,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):  https://github.com/tensorflow/tensorflow.git
- TensorFlow version: 1.13
- Python version: 3.6.13
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc version 7.5.0
- CUDA/cuDNN version: No CUDA
- GPU model and memory: No GPU



**Describe the problem**
When I checkout tensorflow source code r1.13 and run the build_all_linux.sh, the downloading of eigen is failed as the eigen resource URL https://bitbucket.org/eigen/eigen/get/9f48e814419e.tar.gz writen in download_dependencies.sh did not exist. It seems that the version is eigen 3.3.7. Then I downloaded this eigen version from github.
Then I run build_all_linux.sh again. There were some errors during compiling: Please find the compiling log below. 
It seems that the eigen version 3.3.7 did not match the tensorflow r1.13 source code: the tensorflow/core/lib/core/threadpool.cc requires a different input parameter of NonBlockingThreadPoolTempl format that Eigen::NonBlockingThreadPoolTempl provided. However after I downloaded all the eigen 3.3.x and checked their NonBlockingThreadPoolTempl definition there were none of them match the tensorflow's requirements. What's the matter?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
tensorflow/tensorflow/contrib/makefile/build_all_linux.sh

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
tensorflow/core/lib/core/threadpool.cc: In constructor ‘tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int, bool, Eigen::Allocator*)’:
tensorflow/core/lib/core/threadpool.cc:94:29: error: no matching function for call to ‘Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int&, bool&, tensorflow::thread::EigenEnvironment)’
         allocator_(allocator) {}
                             ^
In file included from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:58:0,
                 from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:74,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from tensorflow/core/lib/core/threadpool.cc:19:
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note: candidate: Eigen::NonBlockingThreadPoolTempl<Environment>::NonBlockingThreadPoolTempl(int, Environment) [with Environment = tensorflow::thread::EigenEnvironment]
   NonBlockingThreadPoolTempl(int num_threads, Environment env = Environment())
   ^~~~~~~~~~~~~~~~~~~~~~~~~~
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note:   candidate expects 2 arguments, 3 provided
tensorflow/core/lib/core/threadpool.cc: In member function ‘void tensorflow::thread::ThreadPool::Impl::ParallelFor(tensorflow::int64, tensorflow::int64, std::function<void(long long int, long long int)>)’:
tensorflow/core/lib/core/threadpool.cc:100:72: error: no matching function for call to ‘Eigen::ThreadPoolDevice::ThreadPoolDevice(tensorflow::thread::ThreadPool::Impl*, int, Eigen::Allocator*&)’
     Eigen::ThreadPoolDevice device(this, this->NumThreads(), allocator_);
                                                                        ^
In file included from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:92:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from tensorflow/core/lib/core/threadpool.cc:19:
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:109:3: note: candidate: Eigen::ThreadPoolDevice::ThreadPoolDevice(Eigen::ThreadPoolInterface*, int)
   ThreadPoolDevice(ThreadPoolInterface* pool, int num_cores) : pool_(pool), num_threads_(num_cores) { }
   ^~~~~~~~~~~~~~~~
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:109:3: note:   candidate expects 2 arguments, 3 provided
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note: candidate: constexpr Eigen::ThreadPoolDevice::ThreadPoolDevice(const Eigen::ThreadPoolDevice&)
 struct ThreadPoolDevice {
        ^~~~~~~~~~~~~~~~
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note:   candidate expects 1 argument, 3 provided
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note: candidate: constexpr Eigen::ThreadPoolDevice::ThreadPoolDevice(Eigen::ThreadPoolDevice&&)
/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note:   candidate expects 1 argument, 3 provided
tensorflow/core/lib/core/threadpool.cc: In member function ‘void tensorflow::thread::ThreadPool::ScheduleWithHint(std::function<void()>, int, int)’:
tensorflow/core/lib/core/threadpool.cc:204:10: error: ‘struct tensorflow::thread::ThreadPool::Impl’ has no member named ‘ScheduleWithHint’; did you mean ‘Schedule’?
   impl_->ScheduleWithHint(std::move(fn), start, limit);
          ^~~~~~~~~~~~~~~~
          Schedule
tensorflow/core/lib/core/threadpool.cc: In member function ‘void tensorflow::thread::ThreadPool::SetStealPartitions(const std::vector<std::pair<unsigned int, unsigned int> >&)’:
tensorflow/core/lib/core/threadpool.cc:209:10: error: ‘struct tensorflow::thread::ThreadPool::Impl’ has no member named ‘SetStealPartitions’
   impl_->SetStealPartitions(partitions);
          ^~~~~~~~~~~~~~~~~~
gcc --std=c++11 -march=native -I. -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/../../../ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/double_conversion -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/absl -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/lib/io/inputstream_interface.cc -o /home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/io/inputstream_interface.o
tensorflow/contrib/makefile/Makefile:885: recipe for target '/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/core/threadpool.o' failed
make: *** [/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/core/threadpool.o] Error 1
"
48537,micro: Port RESIZE_BILINEAR from lite to micro,"@tensorflow/micro

@advaitjain This issue tracks my work on porting the RESIZE_BILINEAR kernel for int8 and float to TFL micro. It will be delivered in four PRs.
* PR1 (merged): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function. #43427
* PR2 (merged): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header. #48616
* PR3 (merged): Port the RESIZE_BILINEAR kernel, port the tests and add it to the micro build as three separate commits. #48617




"
48535,"Wrong warning message about ""Variables were used a Lambda layer's call""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF 2.4.1 and TF nightly
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I want to create a trainable variable. I use a subclassed layer and make this variable a weight of the layer, instead of creating a variable directly and use it in a Lambda layer, as recommended by https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda.

However, there is a warning message says
```
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.linalg.matmul_7), but
are not present in its tracked objects:
  <tf.Variable '...'>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
```

The model builds, compiles, and trains successfully, and the variable is recognized as a trainable weight. 


**Describe the expected behavior**

Expect no warning.

**Standalone code to reproduce the issue**

I have tested the code on Colab in both tensorflow 2.4.1 and tf-nightly, and received the same warning.

```
import tensorflow as tf
import numpy as np


class VariableLayer(tf.keras.layers.Layer):
    def __init__(self, shape, initializer, trainable, v_name):
        super(VariableLayer, self).__init__()
        self.shape = shape
        self.initializer = initializer
        self.trainable = trainable
        self.v_name = v_name

    def build(self, input_shape):
        self.v = self.add_weight(shape=self.shape,
                                 initializer=self.initializer,
                                 trainable=self.trainable,
                                 name=self.v_name)

    def call(self, inputs):
        return self.v


def build_subclass_functional():
    x = tf.keras.Input(shape=(8,))
    layer = VariableLayer(shape=(8, 3), initializer='glorot_normal', trainable=True, v_name='v')
    z = layer(x)
    o = tf.matmul(x, z)
    model = tf.keras.Model(inputs=x, outputs=o)
    return model


if __name__=='__main__':
    data = np.random.rand(2, 8)
    model = build_subclass_functional()
    print(model(data))
    x = np.random.rand(5, 8)
    y = np.random.rand(5, 3)
    model.compile(optimizer='Adam', loss=""mse"")
    model.fit(x, y)
    print(model.trainable_weights)
```

**Output**
```
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.linalg.matmul_7), but
are not present in its tracked objects:
  <tf.Variable 'variable_layer/v:0' shape=(8, 3) dtype=float32, numpy=
array([[...]], dtype=float32)>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
```

"
48534,Training a keras model on pretrained weights using load_weights(),"
I am using a custom keras model in Databricks environment. For a custom keras model, `model.save(model.h5)` does not work, because custom model is not serializable. Instead it is recommended to use` model.save_weights(path)` as an alternate.

model.save_weights(pathDirectory) works. This yields 3 files c`heckpoint`,`.data-00000-of-00001,``.index` in the pathDirectory

For loading weights, Following mechanism is working fine.

```
model = Model()

model.load_weights(path)
```

But I want to train my model on pretrained weights I just saved. Like I saved model weights, and continue training on these saved weights afterwards. 

So, when I load model weights and apply training loop, I get this error, TypeError: 'CheckpointLoadStatus' object is not callable

Code is [here](https://gist.github.com/irfanumar1994/f38c3f80ac399717af5bce9904daa042)"
48533,'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'FastGFile',"I'm using TensorRT in Docker 20.12 with Tensorflow-gpu 2.4 in Ubuntu 18
"
48532,Select-tf-ops Is Not Working in Android Kitkat 4.4 (API 19),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Tab 3 (Android KitKat with API 19)
- TensorFlow installed from (source or binary): Installed using pip
- TensorFlow version (use command below): 2.4.3
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.4
- GPU model and memory: Tesla T4 16GB

**Describe the problem**
I'm trying to implement YoloV3 on android application based on this [implementation](https://github.com/zzh8829/yolov3-tf2). To convert the model I have to enable the custom operation by adding this snippet code while converting the model
```
converter.target_spec.supported_ops = [
   tf.lite.OpsSet.TFLITE_BUILTINS,
   tf.lite.OpsSet.SELECT_TF_OPS ]
```
the model conversion was successful, and to implement the model on android application, I added the select-tf-ops dependency 
```
implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
```
The application working correctly on my Android 5.0 (API 21) and Android 10 (API 29), but it gives an error when I run the application on Android 4.4.2 (API 19)
```
04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.
04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.
04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:204)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:378)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.yolo.Yolo.predict(Yolo.java:71)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.POSM.predict(POSM.java:45)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.run_posm(MainActivity.java:110)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.lambda$onCreate$1$MainActivity(MainActivity.java:75)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.-$$Lambda$MainActivity$w2a9VvlIKLLPHcyHZBUHuFlJhQA.onClick(lambda)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at android.view.View.performClick(View.java:4640)
04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.view.View$PerformClick.run(View.java:19431)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.handleCallback(Handler.java:733)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:95)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Looper.loop(Looper.java:146)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.app.ActivityThread.main(ActivityThread.java:5598)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invokeNative(Native Method)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invoke(Method.java:515)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)
04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at dalvik.system.NativeStart.main(Native Method)
04-14 09:18:33.952 8903-8903/com.example.posm E/ERROR: Exception java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
    Node number 65 (FlexSize) failed to prepare.
    
    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
    Node number 65 (FlexSize) failed to prepare.
    
    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer
```
Based on my experience, this happened when I run the Android project without select-tf-ops dependency, then after adding the select-tf-ops dependency in my project, the application working correctly in Android 5.0 and Android 10. Is it a compatibility issue with older android APIs?

I have tested by adding the ```FlexDelegate.initTensorFlowForTesting();``` before running the model, and it gives another error
```
04-15 10:53:58.399 358-358/com.example.posm E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.posm, PID: 358
    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""wcsnrtombs"" referenced by ""libtensorflowlite_flex_jni.so""...
        at java.lang.Runtime.loadLibrary(Runtime.java:364)
        at java.lang.System.loadLibrary(System.java:526)
        at org.tensorflow.lite.flex.FlexDelegate.<clinit>(FlexDelegate.java:61)
        at com.example.posm.MainActivity.onCreate(MainActivity.java:71)
        at android.app.Activity.performCreate(Activity.java:5459)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1093)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2364)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2458)
        at android.app.ActivityThread.access$900(ActivityThread.java:172)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1305)
        at android.os.Handler.dispatchMessage(Handler.java:102)
        at android.os.Looper.loop(Looper.java:146)
        at android.app.ActivityThread.main(ActivityThread.java:5598)
        at java.lang.reflect.Method.invokeNative(Native Method)
        at java.lang.reflect.Method.invoke(Method.java:515)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)
        at dalvik.system.NativeStart.main(Native Method)
04-15 10:54:01.952 358-358/com.example.posm I/Process: Sending signal. PID: 358 SIG: 9
```

**Describe the current behavior**
The select-tf-ops dependency is not working on Android 4.4.2 

**Describe the expected behavior**
The select-tf-ops dependency working correctly on any devices and the model running well

**Standalone code to reproduce the issue**
Here's the link to the [model](https://drive.google.com/file/d/1yW1KxUZuNIbWXhOzEh3zFBAC5Cb7Ztpa/view?usp=sharing) that cannot be run in Android 4.4.2 even tough I've added the select-tf-ops dependency on it."
48531,Computing gradients with Tensorflow 2.4.1: 'KerasTensor' object has no attribute '_id',"I am trying to manipulate gradients within a layer in Tensorflow 2.4.1.  There seems to be a problem leading to the error 'KerasTensor' object has no attribute '_id'.  I am using a custom loss function.

I have linked standalone code below that reproduces the issue.  This is the bare minimum necessary to generate the problem.  My actual working code is part of a large RL pipeline so I am looking for a solution without modifying the general structure of the linked example.  I suspect it is a bug.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10

**Describe the current behavior**

AttributeError: 'KerasTensor' object has no attribute '_id' when computing gradients using custom loss function in a lambda layer.

**Describe the expected behavior**

The model to compile without error.

**Standalone code to reproduce the issue**

https://drive.google.com/file/d/11jAwZM2cq-4ng9Spo03dWxg_H5iaMbVN/view?usp=sharing

Thank you in advance for the assistance."
48528,AttributeError: 'ConvolutionalBoxPredictor' object has no attribute 'inputs',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
Colab
- Python version:3.7
_ TPU
_ Tensorflow version 2.4.1


**Describe the current behavior**

getting error below during running model_tpu_main.py:
`!python model_tpu_main.py --model_dir=model --pipeline_config_path=model/pipeline_tpu.config  --job-dir=model`


**Standalone code to reproduce the issue**
Refer to Colab [notebook](https://colab.research.google.com/drive/1XojAvhpsM4BaauXLKI1ObJw4-Sn8rfZ8?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
**error Msg:**
 _File ""/usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 2947, in updates
    self._first_stage_box_predictor.inputs))
AttributeError: 'ConvolutionalBoxPredictor' object has no attribute 'inputs'_
"
48526,Shard data across TFRecords,"**System information**
- Code straight from the documentation at https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter 
- Google Colab with TPU
- TensorFlow version 2.4

**Describe the current behavior**

This code works and writes a file to Google Cloud Storage

```
dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(""/path/to/file.tfrecord"")
writer.write(dataset)
```

But this code does not write any file. It also does not throw any error. Also, I can't for the life of me figure out how to print filename to debug that part.

```
dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)

def reduce_func(key, dataset):
  filename = tf.strings.join(['/path/to/', tf.strings.as_string(key)])
  writer = tf.data.experimental.TFRecordWriter(filename)
  writer.write(dataset.map(lambda _, x: x))
  return tf.data.Dataset.from_tensors(filename)

dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.group_by_window(
  lambda i, _: i % 2, reduce_func, tf.int64.max
))
```


**Describe the expected behavior**
I would expect the second bit of code to write a TFRecord file."
48524,ImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- N/A
- TensorFlow installed from (source or binary):
by pip install tensorflow-cpu
- TensorFlow version (use command below):
- Python version:
-3.9.2
- Bazel version (if compiling from source):
-N/A
- GCC/Compiler version (if compiling from source):
not compile
- CUDA/cuDNN version:
not use GPU
- GPU model and memory:
not use GPU



You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.5.0-rc0-36-g0d1805aede0 2.5.0-rc1

**Describe the current behavior**
WARNING:tensorflow:From /home/azuryl/anaconda3/envs/nmtpy=3.8/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Traceback (most recent call last):
  File ""t/cnn/Neural-Machine-Translation-NMT-/NMT.py"", line 24, in <module>
    from tensorflow.python.keras.optimizers import RMSprop
ImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers' (/home/azuryl/anaconda3/envs/nmtpy=3.8/lib/python3.9/site-packages/tensorflow/python/keras/optimizers.py)
**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48523,[TensorflowLite GPUDelegate] Outputs NaN Tensors,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Android 10 mobile / Windows 10 Desktop
- Mobile device: OnePlus 7T
- Python version: 3.6.3
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.1
- CUDA / cuDNN: CUDA 11.0 / cuDNN 8.0
- TensorFlow Lite version: 2.4.0


**Describe the current behavior**
A tensorflow lite model outsputs NaN tensors if GPUDelegate is used. Only on GPUDelegate, not on CPU, not on NnApiDelegate, not on XNNPack. Specific the tensors demo_class01, demo_class02, demo_class03, demo_class04. If I round the input of the tf.keras.layers.Multiply() layers to 4 digits, NaN output is gone, but performance is then the same on GPU as on CPU. Please look at the standalone code to see the tensor naming. The interessting thing is, that I have to use 4 different output classifications (demo_class01, demo_class02, demo_class03, demo_class04) to produce the NaN tensor output in Android Studio console, if less, constant tensors get printed. It seems like GPUDelegate + Multiply() is numerical instabil if the inputs have a too hight precision. Because if I round the inputs, the NaNs are gone (bit this seems more like a workaround).

Edit:
Another model with 210x210x3 input outputs NaN tensors even with the rounding.

The output printed in Android Studio is the following:
```
demo_class01: [NaN, NaN, NaN, NaN]
demo_class02: [NaN, NaN, NaN, NaN]
demo_class03: [NaN, NaN, NaN, NaN]
demo_class04: [NaN, NaN, NaN, NaN]
```

For rounding I used the following in the ""working"" model:
```
    def tf_round(x, decimals=0):
        multiplier = tf.constant(10**decimals, dtype=x.dtype)
        return tf.round(x * multiplier) / multiplier
    
    out_relu  = tf_round(out_relu, 4)
    demo_slice01 = tf_round(demo_slice01, 4)
    demo_slice02 = tf_round(demo_slice02, 4)
    demo_slice03 = tf_round(demo_slice03, 4)
    demo_slice04 = tf_round(demo_slice04, 4)
```



**Describe the expected behavior**
Non NaN tensors without extra steps.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Following demo model code:
```
def create_demo():

    demo_ins = tf.keras.layers.Input(shape=(300, 300, 3), name=""demo_ins"", batch_size=1)
    
    model = tf.keras.applications.MobileNetV2(include_top=False, weights=""imagenet"", input_shape=(300, 300, 3), 
                                              pooling=None, input_tensor=demo_ins)
    out_relu = model.get_layer(name=""out_relu"").output
    
    demo_conv01 = tf.keras.layers.Conv2D(4, (3, 3), padding=""same"", strides=(1, 1), 
                                         kernel_initializer=""he_uniform"", name=""demo_conv01"")(out_relu)
    demo_bn01   = tf.keras.layers.BatchNormalization(name=""demo_bn01"")(demo_conv01)
    demo_softmax01 = tf.keras.layers.Softmax(axis=2, name=""a_demo_softmax01"")(demo_bn01)
    
    
    demo_slice01 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 0], name=""demo_slice01"")(demo_softmax01)
    demo_slice02 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 1], name=""demo_slice02"")(demo_softmax01)
    demo_slice03 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 2], name=""demo_slice03"")(demo_softmax01)
    demo_slice04 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 3], name=""demo_slice04"")(demo_softmax01)
    
    demo_slice01_reshape = tf.keras.layers.Reshape([10, 10, 1], name=""demo_slice01_reshape"")(demo_slice01)
    demo_slice02_reshape = tf.keras.layers.Reshape([10, 10, 1], name=""demo_slice02_reshape"")(demo_slice02)
    demo_slice03_reshape = tf.keras.layers.Reshape([10, 10, 1], name=""demo_slice03_reshape"")(demo_slice03)
    demo_slice04_reshape = tf.keras.layers.Reshape([10, 10, 1], name=""demo_slice04_reshape"")(demo_slice04)
    
    demo_mul01 = tf.keras.layers.Multiply(name=""w_demo_mul01"")([out_relu, demo_slice01_reshape])
    demo_mul02 = tf.keras.layers.Multiply(name=""x_demo_mul02"")([out_relu, demo_slice02_reshape])
    demo_mul03 = tf.keras.layers.Multiply(name=""y_demo_mul03"")([out_relu, demo_slice03_reshape])
    demo_mul04 = tf.keras.layers.Multiply(name=""z_demo_mul04"")([out_relu, demo_slice04_reshape])
    
    demo_block_conv01 = tf.keras.layers.Conv2D(3, (3, 3), padding=""same"", strides=(1, 1), 
                                                   kernel_initializer=""he_uniform"", name=""demo_block_conv01"")
    demo_block_bn01   = tf.keras.layers.BatchNormalization(name=""demo_block_bn01"")
    demo_block_relu01 = tf.keras.layers.ReLU(name=""demo_block_relu01"")
    demo_block_flatten = tf.keras.layers.Flatten(name=""demo_block_flatten"")
    demo_block_softmax01 = tf.keras.layers.Dense(4, activation=""softmax"", name=""demo_block_softmax01"")
    
    def block(inputs):
        
        conv01  = demo_block_conv01(inputs)
        bn01    = demo_block_bn01(conv01)
        relu01  = demo_block_relu01(bn01)
        flatten = demo_block_flatten(relu01)
        softmax01 = demo_block_softmax01(flatten)
        
        return softmax01
        
    demo_class01 = tf.keras.layers.Layer(name=""b_demo_class01"")(block(demo_mul01))
    demo_class02 = tf.keras.layers.Layer(name=""c_demo_class02"")(block(demo_mul02))
    demo_class03 = tf.keras.layers.Layer(name=""d_demo_class03"")(block(demo_mul03))
    demo_class04 = tf.keras.layers.Layer(name=""e_demo_class04"")(block(demo_mul04))
    
    model = tf.keras.models.Model(
        inputs=[model.input], 
        outputs=[demo_softmax01, 
                 demo_class01, demo_class02, demo_class03, demo_class04, 
                 demo_mul01, demo_mul02, demo_mul03, demo_mul04]
    )
    
    return model 
```

Following tfLite convert code:
```
model = create_demo()
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open(""demo.tflite"", 'wb') as f:
    f.write(tflite_model)
```

Following tfLite Interpreter code:
```
Interpreter.Options opt = new Interpreter.Options();
CompatibilityList compatList = new CompatibilityList();

GpuDelegate.Options gpuOptions = compatList.getBestOptionsForThisDevice();
GpuDelegate delegate = new GpuDelegate(gpuOptions);

opt.addDelegate(delegate);
interpreter = new Interpreter(modelByteBuffer, opt);
```

Edit02:

Maybe it helps, the tensorflow lite output map should be the following:
```
Map<Integer, Object> outputMap = new HashMap<>();

float[][][][] aMap = new float[1][10][10][4];

float[][][][] demo_mul01 = new float[1][10][10][1280];
float[][][][] demo_mul02 = new float[1][10][10][1280];
float[][][][] demo_mul03 = new float[1][10][10][1280];
float[][][][] demo_mul04 = new float[1][10][10][1280];

float[][] demo_class01 = new float[1][4];
float[][] demo_class02 = new float[1][4];
float[][] demo_class03 = new float[1][4];
float[][] demo_class04 = new float[1][4];

outputMap.put(0, aMap);

outputMap.put(1, demo_class01);
outputMap.put(2, demo_class02);
outputMap.put(3, demo_class03);
outputMap.put(4, demo_class04);

outputMap.put(5, demo_mul01);
outputMap.put(6, demo_mul02);
outputMap.put(7, demo_mul03);
outputMap.put(8, demo_mul04);
```

**Other info / logs** 
I tryed differnet Tensorflow versions different Converter options like:

```
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
```

With this options the NaNs are gone but the performance impact is so strong, I could just use the old converter and use the mobile CPU.

I attached a converted TensorflowLite NaN model created with the code above.
[demo_issue.zip](https://github.com/tensorflow/tensorflow/files/6311102/demo_issue.zip)"
48521,[TFLite] Potential out-of-bound array access in LOG_SOFTMAX optimized kernel,"Hello,

The optimized LOG_SOFTMAX operator seems to have a bug that may lead to out-of-bound memory accesses.

The following [access](https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L4143) is equivalent to `params.table[input_data[j] + max_q8 - max_val]` which can be be negative with int8 input_data.

Example:
```
input_data = [-109, -9, 0, 12, 78]
max_q8 = 127
max_val = 78
```

Access for `j = 0` is `params.table[-109 + 127 - 78]` which result in a `params.table[-60]` access.

The tests currently don't highlight the bug by luck as we end-up accessing other fields in `LogSoftmaxOpData` but replacing [`LogSoftmaxOpData`](https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/kernels/activations.cc#L87) to remove all the unused parameters (or even just putting SoftmaxParams as fist member):
```c++
struct LogSoftmaxOpData {
    struct SoftmaxParams params = {};
    float f_table[256];
};
```

makes the tests fail when running the tests in `bazel test -c opt` mode (though it may vary as it's undefined). 

Valgrind output on the test binary:
```
[ RUN      ] LogSoftmaxOpTest/LogSoftmaxOpTest.LogSoftmaxInt8/0
==8559== Invalid read of size 4
==8559==    at 0x697D124: void tflite::optimized_ops::LogSoftmax<signed char>(tflite::SoftmaxParams const&, float, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) (optimized_ops.h:4143)
==8559==    by 0x6976EF9: TfLiteStatus tflite::ops::builtin::activations::LogSoftmaxEval<(tflite::ops::builtin::activations::KernelType)1>(TfLiteContext*, TfLiteNode*) (activations.cc:1250)
==8559==    by 0x13626C77: tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*) (subgraph.h:430)
==8559==    by 0x13623ED3: tflite::Subgraph::Invoke() (subgraph.cc:1062)
...
```

Thibaut
"
48520,Customize continuous string tensor type for feature hashing on GPU.,"**System information**
- TensorFlow version (you are using): 1.15 & 2.4
- Are you willing to contribute it (Yes/No): Yes

**Background**
In a recommender system, there are large number of features are expressed in the form of text, like the name of a product, or behavior of user, in advertisement.

**Describe the feature and the current behavior/state.**
Currently, Tensorflow use discontinuous host memory to store the text. When hashing the text features, it will lead to lack of performance, by some reasons:
1. Extract string tensor from TFRecord will introduce lots of memcpy for moving string elements one by one.
2. When using GPU to hash the text features, is hard to do memcpy from discontinuous memory to continuous memory, both [H2D] or [H2H + single H2D], since we cannot use discontinuous data on GPU.

**Who will benefit with this feature?**
I implemented a Murmur hashing with CUDA, wrapped by Tensorflow OP. I found that string copying one by one consumes 90% of the time. So it will make a huge improvement if a continuous string Tensor is enabled.


I'm new to Tensorflow framework type system. How could I create a new tensor type? And what kinds of problems should I consider?

"
48518,The repository 'file:/var/nccl-repo-2.2.13-ga-cuda9.2  Release' no longer has a Release file. Ubuntu-18.04,"I am getting this error while enabling the GPU support. 
I am following the steps as mentioned here.
https://www.tensorflow.org/install/gpu
![err](https://user-images.githubusercontent.com/79437844/114683080-3e416a00-9cff-11eb-9994-035dd9f21d57.png)
"
48516,PR to port resize_bilinear to TFLM broke the Xtensa build,"https://github.com/tensorflow/tensorflow/pull/43426 broke the Xtensa build:

```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=vision_p6 XTENSA_CORE=P6_200528 micro_allocator_test -j8
```

passes if we revert the PR and fails on tip of tree with:
```
xt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=P6_200528 -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DVISION_P6 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/bin/micro_allocator_test tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/obj/tensorflow/lite/micro/micro_allocator_test.o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/obj/tensorflow/lite/micro/testing/test_conv_model.o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm
tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(memory_helpers.o): In function `tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*)':
memory_helpers.cc:(.text._ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x13f): dangerous relocation: call8: call target out of range: _Assert
tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(micro_allocator.o): In function `tflite::internal::InitializeTfLiteTensorFromFlatbuffer(tflite::SimpleMemoryAllocator*, bool, tflite::Tensor const&, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*)':
micro_allocator.cc:(.text._ZN6tflite8internal36InitializeTfLiteTensorFromFlatbufferEPNS_21SimpleMemoryAllocatorEbRKNS_6TensorEPKN11flatbuffers6VectorINS6_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensor+0x47d): dangerous relocation: call8: call target out of range: _Assert
tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(micro_allocator.o): In function `tflite::internal::InitializeTfLiteEvalTensorFromFlatbuffer(tflite::SimpleMemoryAllocator*, tflite::Tensor const&, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteEvalTensor*)':
micro_allocator.cc:(.text._ZN6tflite8internal40InitializeTfLiteEvalTensorFromFlatbufferEPNS_21SimpleMemoryAllocatorERKNS_6TensorEPKN11flatbuffers6VectorINS6_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP16TfLiteEvalTensor+0x140): dangerous relocation: call8: call target out of range: _Assert
/home/advaitjain/xtensa/XtDevTools/install/tools/RI-2020.4-linux/XtensaTools/bin/xt-ld: error: 3 warnings, treating warnings as errors
clang-6.0: error: Xtensa-ld command failed with exit code 1 (use -v to see invocation)
make: *** [tensorflow/lite/micro/tools/make/Makefile:788: tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/bin/micro_allocator_test] Error 1
```

Couple of things to note:
 * The PR author (@patriklaurell) does not have any way to reproduce the errors, and we do not currently have any CI for the Xtensa toolchain.
 * Only one of 3 xtensa builds fails so there is a distinct possibility that the issue here is some subtlety with the Xtensa toolchain rather than the PR itself. However, the code that the Xtensa linker is complaining about it untouched by the PR that causes the error, so something odd is happening.
 * The micro_allocator_test is currently disabled for bluepill and stm32f4 (the two platforms that test for dynamic memory allocation etc.)
 * PR #43426 was created right around the time where TFLM had new guidelines for sending smaller PRs as part of OP porting and it was treated as an exception to the rule (which meant the PR is quite large and touches a lot of files).

Next steps:
 * https://github.com/tensorflow/tensorflow/pull/48515 reverts #43426 so that the Xtensa build is green again.
 * We should break up #43426 into smaller PRs following the newer guidelines for OP porting.
 * We should enable the micro_allocator_test on bluepill sooner rather than later

Tagging some of the folks that have a stake in this: @nyadla-sys @patriklaurell @freddan80 @petewarden "
48514,Mapping an OpDef into a TF op (Python API),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF2
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

I'm currently for a way to map an OpDef into a TF op. Currently, there is no way to retrieve the Python bindings for a certain TF op given the OpDef (there isn't a way to access the op registry from the Python API). It's possible to export the OpList proto and load that into Python easily, but looking a way to access the actual python bindings for any registered TF op.

**Will this change the current api? How?**

Unclear, also made this post asking if such a feature was already available.

**Who will benefit with this feature?**

Any end users who wish to work at the TF op level and want to access more detailed information about each op (number of arguments attributes, etc.)

**Any Other info.**
"
48513,TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType',"I am trying to have a model that can get different input_shapes. I use a **(128,128,1)** input_shape for for training data as all my training images have the same size.

However, after training the image size will be variable (e.g.; **312x256, 550x2300**, etc). How to create a model that can get variable input shapes. The model below raises error in **`get_crop_shape`**, if I use input_shape as **[None, None,1]**. I have to use **`get_crop_shape`** to prevent problems during concatenations as tensor shape will be different if the shape is not multiple of 128.

if I use any shape excepts None (e.g.; **[128, 128, 1]** or **[129, 239, 1]**), the model works!

I need the model accepts different input_shapes, so I can use it in tensorflow/java.

In Python, I create model, perform training and then save the model as **`model.save(""model"")`**. Then, I create a new model based on the shape of new images and only load weights and it works! For instance, if I save my model after training as **`model.save(""model"")`**. I can use it for different image shapes as below.

```
import tensorflow as tf
model1 = Unet(input_shape = (129,239,1))
model1.load_weights(""model/variables/variables"")

model2= Unet(input_shape = (2300,3450,1))
model2.load_weights(""model/variables/variables"")

prediction1 = model1(input_tensor1) # input_tensor1 shape is (None, 129,239,1), None is the batch size.
prediction1 = model2(input_tensor2) # input_tensor2 shape is (None, 2300,3450,1), None is the batch size.

```

I ask the same question on tensorflow/java as well. [#288](https://github.com/tensorflow/java/issues/288) 




```
import os
import skimage.io as io
import cv2
import numpy as np
import tensorflow as tf
import time
import imageio
import matplotlib.pyplot as plt
import copy as copy
import matplotlib

def get_crop_shape(target, query):
	# the height
	channelHeight = target.get_shape()[1] - query.get_shape()[1]
	assert (channelHeight >= 0)
	channelHeight1 = int(channelHeight/2)
	if channelHeight % 2 != 0:
		channelHeight2 = channelHeight1 + 1
	else:
		channelHeight2 = channelHeight1
	# the width
	channelWidth = target.get_shape()[2] - query.get_shape()[2]
	assert (channelWidth >= 0)
	channelWidth1 = int(channelWidth/2)
	if channelWidth % 2 != 0:
		channelWidth2 = channelWidth1 + 1
	else:
		channelWidth2 = channelWidth1
	return (channelHeight1, channelHeight2), (channelWidth1, channelWidth2)


def getAct(x):
	return tf.keras.layers.ReLU()(x)


def Unet(input_shape = (None,None,1), kernelSize = 3, drop_level = 0.5, nChannels = 1):
	inputs = tf.keras.layers.Input(shape = [input_shape[0], input_shape[1], input_shape[2]])
	conv1 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(inputs)
	conv1 =  getAct(conv1)
	conv1 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv1)
	conv1 =  getAct(conv1)
	#drop1 = tf.keras.layers.Dropout(drop_level)(conv1)
	pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1) # drop1 --> conv1
	#
	conv2 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool1)
	conv2 =  getAct(conv2)
	conv2 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv2)
	conv2 =  getAct(conv2)
	#drop2 = tf.keras.layers.Dropout(drop_level)(conv2)
	pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2) #drop2 --> conv2
	#
	conv3 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool2)
	conv3 =  getAct(conv3)
	conv3 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv3)
	conv3 =  getAct(conv3)
	#drop3 = tf.keras.layers.Dropout(drop_level)(conv3)
	pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)  #drop3 --> conv3
	#
	conv4 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool3)
	conv4 =  getAct(conv4)
	conv4 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv4)
	conv4 =  getAct(conv4)
	drop4 = tf.keras.layers.Dropout(drop_level)(conv4)
	pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)
	#
	conv5 = tf.keras.layers.Conv2D(1024, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool4)
	conv5 =  getAct(conv5)
	conv5 = tf.keras.layers.Conv2D(1024, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv5)
	conv5 =  getAct(conv5)
	drop5 = tf.keras.layers.Dropout(drop_level)(conv5)
	up6 = tf.keras.layers.Conv2DTranspose(512, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(drop5)
	ch, cw = get_crop_shape(drop4, up6)
	up6 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up6) # add zeropadding.
	merge6 = tf.keras.layers.concatenate([drop4,up6], axis = 3)
	#
	conv6 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge6)
	conv6 =  getAct(conv6)
	conv6 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv6)
	conv6 =  getAct(conv6)
	up7 = tf.keras.layers.Conv2DTranspose(256, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv6)
	ch, cw = get_crop_shape(conv3, up7)   #drop3 --> conv3
	up7 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up7) # add zeropadding.
	merge7 = tf.keras.layers.concatenate([conv3,up7], axis = 3)   #drop3 --> conv3
	#
	conv7 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge7)
	conv7 =  getAct(conv7)
	conv7 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv7)
	conv7 =  getAct(conv7)
	up8 = tf.keras.layers.Conv2DTranspose(128, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv7)
	ch, cw = get_crop_shape(conv2, up8) #drop2 --> conv2
	up8 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up8) # add zeropadding.
	merge8 = tf.keras.layers.concatenate([conv2,up8], axis = 3) #drop2 --> conv2
	#
	conv8 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge8)
	conv8 =  getAct(conv8)
	conv8 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv8)
	conv8 =  getAct(conv8)
	up9 = tf.keras.layers.Conv2DTranspose(64, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv8)
	ch, cw = get_crop_shape(conv1, up9) #drop1 --> conv1
	up9 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up9) # add zeropadding.
	merge9 = tf.keras.layers.concatenate([conv1,up9], axis = 3) #drop1 --> conv1
	#
	conv9 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge9)
	conv9 =  getAct(conv9)
	conv9 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv9)
	conv9 =  getAct(conv9)
	conv9 = tf.keras.layers.Conv2D(2, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv9)
	conv9 =  getAct(conv9)
	conv10 = tf.keras.layers.Conv2D(nChannels, 1, activation = 'sigmoid')(conv9)
	model = tf.keras.Model(inputs = inputs, outputs = conv10)
	model.compile(optimizer = tf.keras.optimizers.Adam(1e-4, beta_1 = 0.9, beta_2 = 0.999), loss = 'binary_crossentropy', metrics = ['accuracy'])
	print(""Using UnetLeakyPReLU ..."")
	print(model.summary())
	return model

```






"
48512,CMAKE_DL_LIBS refers to which libraries？I can't find the libs in CMakelists.txt,
48511,"Output names for models defined via subclassing tf.kerasModel are ignored, fit with multiple losses raises error","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8
- CUDA/cuDNN version: cuda 11.1


**Describe the current behavior**
When using a custom Model defined via inheritance from tf.keras.Model, if the model has multiple output I could not find a way to specify output names, but keras seems to assign a name automatically. Could these names be specified? (I will attach code down below to better illustrate the situation). The output names specified in a custom keras model defined by subclassing seems to be ignored

I could not find how to specify output names in this setting. If specifying output names is not implemented for keras Model subclassing, then this would become a feature request.
Maybe a custom model could specify the output names. I saw there is the attribute output_names in Model, but I am not sure who is setting it

**Describe the expected behavior**
I would expect the output names to be the one which are specified in the custom class, as it happens when using the functional API

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

X = np.random.rand(100, 30)
Y_1 = np.random.rand(100, 1)
Y_2 = np.random.rand(100, 5)


# FUNCTIONAL API is ok

inputs = tf.keras.layers.Input((30,))

output_1 = tf.keras.layers.Dense(1, name=""myname1"")(inputs)
output_2 = tf.keras.layers.Dense(5, name=""myname2"")(inputs)


model = tf.keras.Model(inputs=inputs, outputs=[output_1, output_2])

losses = {
    ""myname1"": 'mse',
    ""myname2"": 'mse',
}

model.compile(optimizer='adam', loss=losses)

model.fit(x=X, y=(Y_1, Y_2), epochs=2)

# SUBCLASSING raises error

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(1, name=""myname1"")
        self.dense2 = tf.keras.layers.Dense(5, name=""myname2"")
        
    def call(self, x):
        return self.dense1(x), self.dense2(x) 


model = MyModel()

losses = {
    ""myname1"": 'mse',
    ""myname2"": 'mse',
}

model.compile(optimizer='adam', loss=losses)

model.fit(x=X, y=(Y_1, Y_2), epochs=2)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Epoch 1/2
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-b791eb51ceb6> in <module>
     18 model.compile(optimizer='adam', loss=losses)
     19 
---> 20 model.fit(x=X, y=(Y_1, Y_2), epochs=2)

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--> 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    723     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    724     self._concrete_stateful_fn = (
--> 725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    726             *args, **kwds))
    727 

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-> 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3194     arg_names = base_arg_names + missing_arg_names
   3195     graph_function = ConcreteFunction(
-> 3196         func_graph_module.func_graph_from_py_func(
   3197             self._name,
   3198             self._python_function,

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--> 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

ValueError: in user code:

    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step
        loss = self.compiled_loss(
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:186 __call__
        self.build(y_pred)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:138 build
        self._losses = self._conform_to_outputs(y_pred, self._losses)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:62 _conform_to_outputs
        struct = map_to_output_names(outputs, self._output_names, struct)
    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:585 map_to_output_names
        raise ValueError('Found unexpected keys that do not correspond '

    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['myname1', 'myname2']). Expected: ['output_1', 'output_2']
​```"
48509,Documentation on hosted models,"
## URL(s) with the issue:

https://www.tensorflow.org/lite/guide/hosted_models#image_classification

## Description of issue (what needs changing):
Additional information is required for the available models

### Clear description

Models available for image classification: only models are provided, there is no information with respect to preprocessing required, dataset where it was trained or labels file. This should be given, or at least, said somewhere in the docs.

"
48505,How to append -lineinfo to nvcc flags?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.1
- Python version: 3.9.2
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.2
- GPU model and memory: RTX2080Ti 11GB



**Describe the problem**

I'd like to add lineinfo to nvcc flags without set the whole GPU kernels to debug mode. 
Could anybody tell me where should I modify? Or what argument I can pass to bazel?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48504,MobileNetV3 keras models have bug with GlobalAveragePooling2D,"**System information**
TensorFlow version:
```python
print(tf.version.GIT_VERSION, tf.version.VERSION)
v2.4.0-rc4-71-g582c8d236cb 2.4.0
```

**Describe the current behavior**
According to [MobileNetV3Small documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Small)

> The weights for all 6 models are obtained and translated from the Tensorflow checkpoints from TensorFlow checkpoints found here.

But keras models cannot reproduce performance on ImageNet #48066.

Tensorflow pb model contains GlobalAvgPool **before** Conv2D with filter <1x1x576x1024>:
![image](https://user-images.githubusercontent.com/22346860/114558652-93379e80-9c73-11eb-8165-5d5230c0494c.png)

But MobileNetV3Small from keras.application contains GlobalAvgPool **after** Conv2D with filter <1x1x576x1024>:
![image](https://user-images.githubusercontent.com/22346860/114558795-b82c1180-9c73-11eb-95e4-3f1eb05dc374.png)

https://github.com/tensorflow/tensorflow/blob/6a278b9cf2fbed781661079b13d88b4106622a09/tensorflow/python/keras/applications/mobilenet_v3.py#L295-L308

Such version of MobileNetV3Small achieves top1: 56.79% instead of 68.1%.

**Describe the expected behavior**
To achieve reported accuracy, change [model code](https://github.com/tensorflow/tensorflow/blob/6a278b9cf2fbed781661079b13d88b4106622a09/tensorflow/python/keras/applications/mobilenet_v3.py#L295-L315) to:

```python
  x = activation(x)

  x = layers.GlobalAveragePooling2D()(x)
  if channel_axis == 1:
    x = layers.Reshape((last_conv_ch, 1, 1))(x)
  else:
    x = layers.Reshape((1, 1, last_conv_ch))(x)

  x = layers.Conv2D(
      last_point_ch,
      kernel_size=1,
      padding='same',
      use_bias=True,
      name='Conv_2')(x)
  x = activation(x)

  if include_top:
    if dropout_rate > 0:
      x = layers.Dropout(dropout_rate)(x)
    x = layers.Conv2D(classes, kernel_size=1, padding='same', name='Logits')(x)
    x = layers.Flatten()(x)
    imagenet_utils.validate_activation(classifier_activation, weights)
    x = layers.Activation(activation=classifier_activation,
                          name='Predictions')(x)
```
"
48502,error when using XLA with multiple GPUs,"The TF version is 2.4.1. When I enable XLA with mulit-gpu training by using MirroredStrategy, I got the following errors. 

```
...
while/NcclAllReduce_271: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_271}}
        Stacktrace:
                Node: __inference_train_multiple_steps_260276, function: 
                Node: while, function: __inference_train_multiple_steps_260276
                Node: while/NcclAllReduce_271, function: while_body_135159

while/NcclAllReduce_272: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_272}}
        Stacktrace:
                Node: __inference_train_multiple_steps_260276, function: 
                Node: while, function: __inference_train_multiple_steps_260276
                Node: while/NcclAllReduce_272, function: while_body_135159

while/NcclAllReduce_273: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_273}}
        Stacktrace:
                Node: __inference_train_multiple_steps_260276, function: 
                Node: while, function: __inference_train_multiple_steps_260276
                Node: while/NcclAllReduce_273, function: while_body_135159

while/NcclAllReduce_274: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_274}}
        Stacktrace:
                Node: __inference_train_multiple_steps_260276, function: 
                Node: while, function: __inference_train_multiple_steps_260276
                Node: while/NcclAllReduce_274, function: while_body_135159
....

```

How to fix it?"
48501,LookupError: No gradient defined for operation 'CudnnRNN' (op type: CudnnRNN),"model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)

This is the code i am using"
48500,How to use StatsAggregator (esp. to collect prefetch size),"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/data/experimental/StatsAggregator

## Description of issue (what needs changing):

The documentation does not fully describe how to use `StatsAggregator`, i.e. it is missing what the output should be, where it is and what to do with it.

### Clear description

Backstory: I had a setup, where  speeding up the training step did not yield a speedup of the training which turns out to be dataset input pipeline performance and could be solved by `prefetch(large_number)`
However using the autotune parameter did not help here and I wanted to know why.
In the source code I've seen, that there is a possibility to collect the current prefetch buffer size over training steps so I wanted to look what the chosen values are and how they change, i.e. check if autotune is working as expected
I seems it is not increasing the buffer size fast enough or at all.

However I was not able to figure out how to get this information. And when using `StatsAggregator` it shows a deprecation warning and refers to TF Profiler but the information I need is not there at all. 

### Usage example

Is there a usage example?

No
"
48499,[Performance Boost Request] SLIDE algorithm,"I have recently read two new article discussing about accelerating training algorithm using hashtables. And since the library implemetation using TensorFlow, do TF devs would use it for application in TF-CPU to boost up training performance, or even by TF-GPU if possible?
Link: https://arxiv.org/abs/1903.03129 
Link: https://arxiv.org/abs/2103.10891"
48498,Why tensorflow 2.4.1 requires libcusolver.so.10 other than libcusolver.so.11?????? It is so weird since other dynamic files are 11 ended,As the question said.
48497,Activation layer fails to be merged in TFLite conversion after quantization-aware training,"### 1. System information

- OS Platform and Distribution: Windows 10.0.19042 Build 19042
- TensorFlow installation: pip package 
- TensorFlow library: 2.6.0-dev20210412

### 2. Code

**Model training (with QA training)**: https://colab.research.google.com/gist/Mjonir/c2f281c417b3846887a242c19caa2c06/tensorflow-datasets.ipynb
**Trained model (with QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMqPggjpHJ4azvwgg?e=l4II0F
**Model training (no QA training)**: https://colab.research.google.com/gist/Mjonir/f83a50157a722e7b93d99ef6c5a3de9e/tensorflow-datasets.ipynb
**Trained model (no QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMtDEYyVWq-dGOGsA?e=HUU30i
**TFLite conversion**: https://colab.research.google.com/gist/Mjonir/956546ac2c73277ab71d7b6a4f536646/tensorflow-lite-debugger-colab.ipynb
**Converted model (with QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMpmYF1Y17JNt2Wlw?e=cHk0YD
**Converted model (no QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMsaOLa1RKE03beQQ?e=zRHFvN

### 3. Failure after conversion

When converting a QA-trained model, the Relu activation fails to be merged with the preceding Conv2D/Dense layer. When porting to target hardware, in my case to microcontroller, this results in the activation layers being processed separately on the target instead of during the evaluation of the Conv2D kernel, inducing a 10% performance loss in my overall network.

For reference, an identical non-QA trained model has its activation indeed merged with the preceding Conv2D/Dense layer.

### 5. (optional) Any other info / logs

Netron visualisations:
**With QA training:**
![with_qa](https://user-images.githubusercontent.com/5678238/114494711-1b3e8980-9c1d-11eb-88ec-a0248d9fc6ef.png)
**No QA training:**
![no_qa](https://user-images.githubusercontent.com/5678238/114494704-18dc2f80-9c1d-11eb-8f54-c0ef7a30c35e.png)

"
48496,Build Failed with  Missing Dependencies Error  with Clang-11 and get 404 Not Found Warning ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.8.8
- Installed using virtualenv? pip? conda?: cinda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): clang-11
- CUDA/cuDNN version:  CUDA 11.2, CuDNN 8
- GPU model and memory: GTX 1080 Ti



**Describe the problem**
Building latest master with Clang-11 failed with 404 Not Found Warning and  missing dependency declarations. I tried to access the file through chrome to download it, but the file does not exist.

Tried to use ` bazel clean --expunge` and ` rm -rf ~/.cache/bazel` for several times, still not working.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

``` Bash
➜ git pull
➜ tensorflow git:(master) ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/user/anaconda3/envs/tensorflow/bin/python3]:


Found possible Python library paths:
  /home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1,6.1,6.1]:


Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]:
Clang will not be downloaded.

Please specify which clang should be used as device and host compiler. [Default is ]: /usr/bin/clang-11


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

...
Configuration finished

➜ tensorflow git:(master) bazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/user/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/user/anaconda3/envs/tensorflow/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages --python_path=/home/user/anaconda3/envs/tensorflow/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,6.1,6.1,6.1 --action_env LD_LIBRARY_PATH=/usr/lib:/usr/lib/clang/11.0.0/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda/targets/x86_64-linux/lib: --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang-11 --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/user/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/user/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/user/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/user/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/5a5a94ed34b07079046ac81e7e97d980ce2c834f.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/zlib/BUILD.bazel:5:11: undeclared inclusion(s) in rule '@zlib//:zlib':
this rule is missing dependency declarations for the following files included by 'zlib/adler32.c':
  '/usr/lib/clang/11.0.0/include/stddef.h'
  '/usr/lib/clang/11.0.0/include/__stddef_max_align_t.h'
  '/usr/lib/clang/11.0.0/include/limits.h'
  '/usr/lib/clang/11.0.0/include/stdarg.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.532s, Critical Path: 0.22s
INFO: 18 processes: 18 internal.
FAILED: Build did NOT complete successfully
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```Bash
➜  tensorflow git:(master) env
USER=user
LOGNAME=user
HOME=/home/user
PATH=/usr/local/cuda-11.2/bin:/home/user/anaconda3/envs/tensorflow/bin:/home/user/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
SHELL=/usr/bin/zsh
TERM=xterm-256color
XDG_SESSION_ID=373
XDG_RUNTIME_DIR=/run/user/1000
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
XDG_SESSION_TYPE=tty
XDG_SESSION_CLASS=user
MOTD_SHOWN=pam
LANG=en_US.UTF-8
LC_NUMERIC=zh_CN.UTF-8
LC_TIME=zh_CN.UTF-8
LC_MONETARY=zh_CN.UTF-8
LC_PAPER=zh_CN.UTF-8
LC_NAME=zh_CN.UTF-8
LC_ADDRESS=zh_CN.UTF-8
LC_TELEPHONE=zh_CN.UTF-8
LC_MEASUREMENT=zh_CN.UTF-8
LC_IDENTIFICATION=zh_CN.UTF-8
SSH_TTY=/dev/pts/0
SHLVL=1
PWD=/home/user/tensorflow
OLDPWD=/home/user
ZSH=/home/user/.oh-my-zsh
PAGER=less
LESS=-R
LSCOLORS=Gxfxcxdxbxegedabagacad
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
LD_PRELOAD=/home/user/anaconda3/envs/dlrm/lib/libiomp5.so
LD_LIBRARY_PATH=/usr/lib:/usr/lib/clang/11.0.0/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda/targets/x86_64-linux/lib:
CONDA_EXE=/home/user/anaconda3/bin/conda
_CE_M=
_CE_CONDA=
CONDA_PYTHON_EXE=/home/user/anaconda3/bin/python
CONDA_SHLVL=3
CONDA_PREFIX=/home/user/anaconda3/envs/tensorflow
CONDA_DEFAULT_ENV=tensorflow
CONDA_PROMPT_MODIFIER=(tensorflow)
CONDA_PREFIX_1=/home/user/anaconda3
CXX=/usr/bin/clang++-11
CC=/usr/bin/clang-11
LDFLAGS=-L/usr/lib/clang/11.0.0/lib -lm -lrt
CXXFLAGS=-stdlib=libc++ -L/usr/lib/clang/11.0.0/lib
_=/usr/bin/env
```
Accessing the address :
https://storage.googleapis.com/mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip

``` XML
<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>No such object: mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip</Details>
</Error>
```
"
48494,Invalid (404) link in the the tf.data guide,"## URL(s) with the issue:

https://www.tensorflow.org/guide/data

## Description of issue (what needs changing):

In the sentence ""See Loading TFRecords for an end-to-end example."" the link points to 
https://www.tensorflow.org/tutorials/load_data/tf_records which returns a 404

It needs to point to the correct url https://www.tensorflow.org/tutorials/load_data/tfrecord 

"
48492,tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0,"When i try to run training using this command:

python model_main_tf2.py --pipeline_config_path=training/ssd_mobilenet_v2_320x320_coco17_tpu-8.config --model_dir=training --alsologtostderr

I get this error: 

2021-04-12 19:15:52.041217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-12 19:15:54.632151: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-12 19:15:54.632623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-04-12 19:15:54.652563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-04-12 19:15:54.652663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-12 19:15:54.657606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-12 19:15:54.657684: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-04-12 19:15:54.660457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-04-12 19:15:54.661423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-04-12 19:15:54.667705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-04-12 19:15:54.669994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-04-12 19:15:54.670519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-04-12 19:15:54.670663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-12 19:15:54.670947: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-12 19:15:54.671897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-04-12 19:15:54.671961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-04-12 19:15:54.672085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-04-12 19:15:54.672161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-04-12 19:15:54.672229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-04-12 19:15:54.672256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-04-12 19:15:54.672371: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-04-12 19:15:54.672436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-04-12 19:15:54.672501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-04-12 19:15:54.672644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-12 19:15:55.109057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-12 19:15:55.109155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-04-12 19:15:55.109181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-04-12 19:15:55.109375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6637 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2021-04-12 19:15:55.110339: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0412 19:15:55.111349 17276 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: None
I0412 19:15:55.119328 17276 config_util.py:552] Maybe overwriting train_steps: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0412 19:15:55.119328 17276 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\model_lib_v2.py:546: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0412 19:15:55.387193 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\model_lib_v2.py:546: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['C:/tensorflow1/models/research/object_detection/train.record']
I0412 19:15:55.439054 17276 dataset_builder.py:163] Reading unweighted datasets: ['C:/tensorflow1/models/research/object_detection/train.record']
INFO:tensorflow:Reading record datasets for input file: ['C:/tensorflow1/models/research/object_detection/train.record']
I0412 19:15:55.440239 17276 dataset_builder.py:80] Reading record datasets for input file: ['C:/tensorflow1/models/research/object_detection/train.record']
INFO:tensorflow:Number of filenames to read: 1
I0412 19:15:55.441213 17276 dataset_builder.py:81] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0412 19:15:55.442210 17276 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\builders\dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
W0412 19:15:55.443208 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\builders\dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\builders\dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0412 19:15:55.458168 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\builders\dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\util\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0412 19:16:00.092059 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\util\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\util\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0412 19:16:02.495897 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\util\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0412 19:16:04.121161 17276 deprecation.py:339] From C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
Traceback (most recent call last):
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py"", line 95, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\capta\AppData\Roaming\Python\Python37\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""C:\Users\capta\AppData\Roaming\Python\Python37\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""model_main_tf2.py"", line 110, in main
    record_summaries=FLAGS.record_summaries)
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\model_lib_v2.py"", line 597, in train_loop
    train_input, unpad_groundtruth_tensors)
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\model_lib_v2.py"", line 386, in load_fine_tune_checkpoint
    if not is_object_based_checkpoint(checkpoint_path):
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\object_detection\model_lib_v2.py"", line 346, in is_object_based_checkpoint
    var_names = [var[0] for var in tf.train.list_variables(checkpoint_path)]
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\training\checkpoint_utils.py"", line 112, in list_variables
    reader = load_checkpoint(ckpt_dir_or_file)
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\training\checkpoint_utils.py"", line 67, in load_checkpoint
    return py_checkpoint_reader.NewCheckpointReader(filename)
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py"", line 99, in NewCheckpointReader
    error_translator(e)
  File ""C:\Users\capta\.conda\envs\tf24\lib\site-packages\tensorflow\python\training\py_checkpoint_reader.py"", line 35, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0"
48490,Cholesky decomposition was not successful. The input might not be valid.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.5.0rc0**
- Python version: **3.8.8**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **tried with 11.1/11.2 same issue (cudnn 8.1.1.33)**
- GPU model and memory: **RTX3080 10GB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
**v1.12.1-53831-ga8b6d5ff93a 2.5.0-rc0**

**Describe the current behavior**
I was trying to run the sample code from TFP homepage (https://blog.tensorflow.org/2019/03/structural-time-series-modeling-in.html)
I ran this in google colab its fully working.
But when I ran in my own Jupyter it gives error ""**_Cholesky decomposition was not successful. The input might not be valid._**"" when I reached the code `component_dists = sts.decompose_by_component(
    co2_model,
    observed_time_series=co2_by_month,
    parameter_samples=q_samples_co2_)`

When I run the forecast `tfp.sts.forecast` the result is different with the colab result significantly too.

**Describe the expected behavior**
The result from colab is same pretty much the same with the result shown on the webpage. I expect the result from the code running in my pc should be the same too.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

[https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Structural_Time_Series_Modeling_Case_Studies_Atmospheric_CO2_and_Electricity_Demand.ipynb](url)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48487,[RNN] TFLite integer 8 quantization yielding worse accuracy than CoreML integer 8,"### 1. System information

- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installation: pip package
- TensorFlow library: tf-nightly==2.6.0.dev20210407

### 2. Issue and code

I'm implementing a Keras GRU model (tensorflow.keras.layers.GRU) on mobile devices for character level language modelling. In order to have a smaller on-device checkpoint I'm experimenting both with TFLite and CoreML integer quantization at 8 bits. The problem is that the TFLite compressed model yields up to 3% worse accuracy results on the test set when compared to the CoreML compressed model. The test set is big enough and the CoreML model results are very close to the full 32 bits model (or the 16 bit model).

This is how I'm executing the conversion for TFLite (where model is of type `tf.keras.Model`):

```
converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]

converted = converter.convert()
```

I've tried with various versions of TensorFlow, including the 2.4.1, 2.5.0 and the nightly.

This is how I'm executing the conversion for CoreML (where model is of type `tf.keras.Model`):

```
import coremltools as ct

coreml_model = ct.convert(model, use_float_arraytype=True)
coreml_model = ct.models.neural_network.quantization_utils.quantize_weights(coreml_model, 8, 
                                                                     quantization_mode=""linear"")
```
TensorFlow Lite doesn't allow you to specify a quantization mode, while CoreML lets you choose among: linear (default), linear_lut, kmeans_lut, custom_lut, linear_symmetric. I tried all of them but custom_lut. The ones I've tried all yield results very similar to the un-quantized model, except for linear_lut.

Now the question is: why is the model coming out of the TFLite quantization process so much worse than almost all the approaches coming from CoreML? is there a way to reproduce the CoreML results in TFLite?

Thanks a lot!
"
48486,Deprecation Warning in LocallyConnected1D,"Tensorflow version 2.4.1

https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/layers/local.py#L39-L338

And also (perhaps more specifically)
https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/layers/local.py#L780

In a LocallyConnected1D layer, if you use `implementation==2` you get a deprecation warning for using `math_ops.sparse_matmul`.

Possible solution:
Change that line to use `tf.linalg.matmul` as the deprecation warning recommends.

To reproduce:

```python
import tensorflow as tf

# Create a tensor to run the layer on.
# The specifics of the shape don't matter as long as it's at least 3 long so that the
# layer will process properly

tensor = tf.random.normal((1, 32, 5))

# Again, the specifics don't really matter as long as the input shape is compatible.
# The important part is the `implementation` parameter.
lc_layer = tf.keras.layers.LocallyConnected1D(10, 1, implementation=2)

print(lc_layer(tensor))
```
output:
```
WARNING:tensorflow:From c:\project_dir\venv\lib\site-packages\tensorflow\python\keras\layers\local.py:780: sparse_mat_mul (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.linalg.matmul` instead
```"
48485,Tensorflow Lite - Flatbuffers.h error [cmake],"Hello !

Our aim is to install TensorFlow Lite in our embedded board (iMX6). To reach this aim, firstly we made a recipe to build TFLite in our Yocto framework. After this step our static tensorflow-lite library was generated.

Then, we wanted to try the functionality of the library with a basic helloworld program. For this step, we chose to start with minimal.cpp which is from official github repo of tensorflow.

For this helloworld (minimal.cpp) program we created a recipe which includes our static tensorflow-lite library inside DEPENDS part. When we bitbake with cmake recipe we get this error about flatbuffers.h ;

`/home/student/fsl-release-bsp/build-analytics-tflite/tmp/sysroots/analytics/usr/include/tensorflow/contrib/lite/schema/schema_generated.h:21:37: fatal error: flatbuffers/flatbuffers.h: No such file or directory
|  #include ""flatbuffers/flatbuffers.h""`

**-Please check below for files-**

- #include part of minimal.cpp :

`#include <cstdio>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""`

- Our recipe (helloworld.bb) is as follows:

`# Recipe for the Hello World program

SUMMARY = ""Recipe for the Hello World program""
LICENSE = ""CLOSED""
LIC_FILES_CHKSUM = """"
DEPENDS += ""opencv""
DEPENDS += ""libconfig""
DEPENDS += ""tensorflow-lite""
SRC_URI = ""git://github.com/aniladar/patch.git;protocol=https;branch=main""
SRCREV = ""${AUTOREV}""
S = ""${WORKDIR}/git""
B = ""${S}""
inherit cmake`

- The CMakeLists.txt file is as follows:

`cmake_minimum_required(VERSION 3.14)
project(patch)
add_executable(minimal minimal.cc)`

Maybe we need some additional things inside CMakeLists.txt file or maybe we have missing DEPENDS inside our recipe. If you brighten up our path it would be perfect. Your guidance and clarifications are important for us.

Thank you in advance !"
48484,einsum with ellipsis is slow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16-20
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:N/A
-   **TensorFlow installed from (source or binary)**: N/A
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.5-3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**: see https://colab.research.google.com/drive/14pGREWEo2zd4j5jDJIxEERfNcpLBGuvv?usp=sharing

### Describe the problem
Runtime of einsum heavily depends on the usage of an ellipsis (Both on GPU and CPU)
```python
a = tf.random.normal([100,32,32,32,3])
e = tf.random.normal([100,3,3])

x=timer()
R = tf.einsum('b...l,blr->b...r',a,e)
print(timer()-x)
# output: 0.28

x=timer()
R = tf.einsum('bijkl,blr->bijkr',a,e)
print(timer()-x)
# output: 0.0008
```

### Source code / logs
https://colab.research.google.com/drive/14pGREWEo2zd4j5jDJIxEERfNcpLBGuvv?usp=sharing


"
48483,GPU is not working even after fine installation,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version:2.4.1
- Python version:3.8.8
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11
- GPU model and memory:GTX1660TI

![Problem](https://user-images.githubusercontent.com/56079922/114399450-e4944f00-9bdb-11eb-9d28-c0c2829b2d3f.PNG)

Good Morning- Or Night- My friends.
I'm an student learning TF and ML stuffs with plenty of interests.
I installed the CUDA, CDNN, Tensorflow-gpu properly, and python console also answered me that I have proper graphic card for 
the tensorflow. However, whenever I try to run my tf codes, only cpu usage makes tons of works, while my GPU is chillin' like a summer vacation. I think the reason might be the fact that I'm using laptop right now, FX705-DU, which made several issues about gpu while gaming. 

Does anyone suffering from same problem like me? 
TF Device check answers me that I got right GPU and even TF does not claim about absence of GPU while running the code,
smiling happily with lines of 'successfully loaded .....' 
How can I make this GPU work? I'll happily wait for the answer:)





"
48482,Model converts to TFlite but invocation fails,"**System information**
- OS Platform: Windows, Linux:
- TensorFlow 2.4.1:

**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1f22ow0a4p1WQLdlm7AIZ0V3DHbxI8EjG

**or if you like. here is the same code as in Google Colab to reproduce the problem** 
```
from tensorflow.keras.layers import concatenate, Input, LSTM, Bidirectional, Embedding, Dense, TimeDistributed,SpatialDropout1D
from tensorflow.keras.models import Model
import tensorflow as tf
print(tf.__version__)

# Create Tensorflow model 
word_in = Input(shape=(300,), name=""input_wor"")
emb_wor = Embedding(input_dim=1834, output_dim=16, input_length=300, mask_zero=True, name=""emb_wor"")(word_in)
char_in = Input(shape=(300, 20 ,), name=""input_char"")
emb_char = TimeDistributed(Embedding(input_dim=132, output_dim=32, input_length=20, mask_zero=True, name=""emb_char""))(char_in)
char_enc = TimeDistributed(LSTM(units=32, return_sequences=False, recurrent_dropout=0.15, name=""char_enc""))(emb_char)
input_pos = Input(shape=(300, 4, ), name=""input_pos"")
input_par = Input(shape=(300, 3, ), name=""input_par"")

x = concatenate([emb_wor, char_enc, input_pos, input_par])
x = SpatialDropout1D(0.1)(x)
main_lstm = Bidirectional(LSTM(units=64, return_sequences=True, dropout=0., recurrent_dropout=0.1, name=""main_lstm""))(x)
inputs=[word_in,char_in, input_pos, input_par]
outputs = TimeDistributed(Dense(4, activation=""softmax"", name=""out""))(main_lstm)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"")
print(model.summary())


# Convert Model to Tensorflow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.experimental_new_converter = True
tflite_model = converter.convert()
with open(""model.tflite"", 'wb') as f:
  f.write(tflite_model)


# # Install tflite_runtime
# !pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime

use_tflite_runtime = False # If True then you need to first restart runtime before running this code
import numpy as np

if(use_tflite_runtime):
  import tflite_runtime.interpreter as tflite
  interpreter =  tflite.Interpreter(model_path=""model.tflite"")
else:
  import tensorflow as tf
  interpreter = tf.lite.Interpreter(model_path=""model.tflite"")

interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(""input_details"", input_details)
print(""output_details"", output_details)

# Set random values
for i in range(len(input_details)):
    x = np.random.random(input_details[i][""shape""])
    interpreter.set_tensor(i, x.astype(input_details[i][""dtype""]))
    print(i, input_details[i][""name""], input_details[i][""shape""], input_details[i][""dtype""], ""/"", x.shape)

# Invoke
interpreter.invoke()
```


**Provide the text output from tflite_convert**

```
2021-04-12 13:24:02.507539: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-04-12 13:24:07.691525: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-04-12 13:24:07.691860: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-04-12 13:24:07.730306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize
  function_optimizer: Graph size after: 447 nodes (0), 564 edges (0), time = 6.973ms.
  function_optimizer: Graph size after: 447 nodes (0), 564 edges (0), time = 6.713ms.
Optimization results for grappler item: model_bidirectional_forward_main_lstm_while_body_19625
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: model_bidirectional_backward_main_lstm_while_cond_19891
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: model_time_distributed_1_char_enc_while_body_19340
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: model_bidirectional_forward_main_lstm_while_cond_19624
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: model_time_distributed_1_char_enc_while_cond_19339
  function_optimizer: function_optimizer did nothing. time = 0ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: model_bidirectional_backward_main_lstm_while_body_19892
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.

2021-04-12 13:24:08.058077: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.
2021-04-12 13:24:08.058217: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.
2021-04-12 13:24:08.086587: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-04-12 13:24:08.172281: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1782] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):
Flex ops: FlexAll
Details:
	tf.All {device = """", keep_dims = false}
```


When I run the invocation using `tflite_runtime` i get this error:

```
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 16 (FlexAll) failed to prepare.

```

When I run the invocation using tensorflow i get this following error

```
RuntimeError: external/org_tensorflow/tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (300 != 1)Node number 37 (CONCATENATION) failed to prepare.
Node number 49 (WHILE) failed to invoke.

```

Either way, the invocation fails and it seems that is has something to do with the `concatenate`  layer. 
I would highly appreciate an answer or eventually a solution. As you can see the model does convert, but the invocation doesn't run. I tested it on both Windows and Linux,. same problem and same error.

   
"
48481,Matmul with int32 datatype is not supported on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 2.4
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory: N/A


**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

tf.debugging.set_log_device_placement(True)

a = np.random.rand(8,8)
b = np.random.rand(8,8)

with tf.device('/GPU:0'):
  x = tf.Variable(a, dtype=""int32"")
  y = tf.Variable(b, dtype=""int32"")
  c_GPU = tf.matmul(x, y)
```

Output:
```
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0
```

See the [gist](https://colab.research.google.com/gist/lugalUrim/07e68fbce476c25bdd41daea6a5b24d1/matmul-for-int32.ipynb#scrollTo=EguARwuQxuIH) here.

**Describe the current behavior**

The op `MatMul` (with datatype `int32`) is actually executed on CPU (as a fallback, I suppose), instead of GPU as is specified by the user.

**Describe the expected behavior**

Currently `MatMul` for `float32` is well-supported on GPU. It would be great if the kernel of `MatMul` for `int32` on GPU can also be implemented.
There should be significant speedup on GPU compared with CPU implementation. Pytorch invokes `CuBLAS` to benefit from the special hardware instructions like `Tensor Core`."
48480,Lua Wrapper,"I wrapped it with Swig. But, I can not wrap TF_NewWhile because of the const pointer. I do not know why it says that it is read-only, because I tested it with another C program and it compiled. If anyone can explain why it says it is read-only I would like to learn the reason it refuses to compile it."
48478,"Functions attributes error ""'Functional' object has no attribute '_make_train_function'"" and eager mode and non-eager returns different logits","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/a
- TensorFlow installed from (source or binary):pip3
- TensorFlow version (use command below):2.4.1
- Python version:3.7.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:N/a
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
`
def adversarial_training(model, embedding_name, epsilon=1):
	
	if model.train_function is None:  # 如果还没有训练函数
		model._make_train_function()  # 手动make
	old_train_function = model.train_function  # 备份旧的训练函数

	# 查找Embedding层
	for output in model.outputs:
		embedding_layer = search_layer(output, embedding_name)
		if embedding_layer is not None:
			break
	if embedding_layer is None:
		raise Exception('Embedding layer not found')

	# 求Embedding梯度
	embeddings = embedding_layer.embeddings  # Embedding矩阵
	gradients = K.gradients(model.total_loss, [embeddings])  # Embedding梯度
	gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor

	# 封装为函数
	inputs = (
		model._feed_inputs + model._feed_targets + model._feed_sample_weights
	)  # 所有输入层
	embedding_gradients = K.function(
		inputs=inputs,
		outputs=[gradients],
		name='embedding_gradients',
	)  # 封装为函数

	def train_function(inputs):  # 重新定义训练函数
		grads = embedding_gradients(inputs)[0]  # Embedding梯度
		delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动
		K.set_value(embeddings, K.eval(embeddings) + delta)  # 注入扰动
		outputs = old_train_function(inputs)  # 梯度下降
		K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动
		return outputs

	model.train_function = train_function  # 覆盖原训练函数
pops
'Functional' object has no attribute '_make_train_function'
and when I turned on disable_eager_execution(), no errors pops.

When in prediction, eager call  model(inputs) returns wrong logits when I finished training and save the model to h5 format then reloaded it through tf.kears.models.load_model. This process is done in non-eager execution mode

But in the meanwhile I transform the h5 model to tf saved model(*.pb) file and run it in non-eager mode. the correct logits appears.

**Describe the expected behavior**
shouldn't these two calls return the same logits?

**Standalone code to reproduce the issue**
https://github.com/bojone/bert4keras/blob/master/examples/task_sentiment_virtual_adversarial_training.py

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
`Traceback` (most recent call last):
  File ""/home/pycharmProjs/bert4keras/examples/task_proxy_recognization.py"", line 217, in <module>
    adversarial_training(test_model,'Embedding-Token', 0.5)
  File ""/home/pycharmProjs/bert4keras/examples/task_proxy_recognization.py"", line 174, in adversarial_training
    model._make_train_function()  # 手动make
AttributeError: 'Functional' object has no attribute '_make_train_function'"
48477,Failed to designate device placement on GPU with tf.device and executed without warning,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA Version: 11.2   
- GPU model and memory: N/A

**Describe the current behavior**
The operation within a `with tf.device('/GPU:0'):` context runs on CPU, with no warnings received.

**Describe the expected behavior**

The operations within a `with tf.device` context should run in that device. 

If the device is not available or the operations are not supported on the particular device, warnings or exceptions are expected.


**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
tf.debugging.set_log_device_placement(True)

a = np.random.rand(2,2)
b = np.random.rand(2,2)

with tf.device('/GPU:0'):

  x = tf.Variable(a, dtype=tf.bfloat16)
  y = tf.Variable(b, dtype=tf.bfloat16)
  c_GPU = tf.matmul(x, y)
  
```

Outputs:
```
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0
```

If I change the datatype from `bfloat16` to `float32`, the output is expected:
```
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
```

"
48476,Conv2D for GPU is not currently supported without cudnn,"**System information**

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
    TensorFlow installed from (source or binary): binary
    TensorFlow version (use command below): 2.4.1
    Python version: 3.7
    Bazel version (if compiling from source): N/A
    GCC/Compiler version (if compiling from source): N/A
    CUDA/cuDNN version: N/A
    GPU model and memory: N/A


**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

a = np.random.rand(1, 4, 4, 1)
b = np.random.rand(4, 4, 1, 1)

tf.raw_ops.Conv2D(input=a, filter=b, strides=[1, 1, 1, 1], padding='SAME', use_cudnn_on_gpu=False)
```


**Describe the current behavior**
Raises UnimplementedError: Conv2D for GPU is not currently supported without cudnn [Op:Conv2D].


**Describe the expected behavior**
Expect Conv2D to run on GPU without cudnn, like pytorch.
"
48474,Possible race condition in nccl_manager_test,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/nccl/nccl_manager_test.cc#L157

``in_cpu`` is not referenced and goes out of scope (line 159) right after the ``stream->ThenMemcpy`` is called

The source cpu buffer may become invalid before the actual H2D memory transfer start.

This test failed in our in-house platform. I'm not sure why this test seems fine on tensorflow's CI.  That piece of code looks buggy to me though.

Thanks
Kevin"
48473,Documentation for tf.tensor_scatter_nd_add mentions non existing method,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add

## Description of issue (what needs changing):

The documentation for `tensor_scatter_nd_add` mentions `tf.scatter_nd_add`, but that method no longer exists."
48471,app crash when load mlmodel with modelWithContentsOfURL:configuration:error,"## ❓Question
App probabilistic crash when load mlmodel with modelWithContentsOfURL:configuration:error；

crash stack：
0  Espresso                       0x1b3d38c10 Espresso::cpu_context_transfer_algo_t::cpu_context_transfer_algo_t(Espresso::cpu_context_transfer_algo_options const&) + 228
1  Espresso                       0x1b3d9e13c std::__1::__shared_ptr_emplace<Espresso::cpu_context_transfer_algo_t, std::__1::allocator<Espresso::cpu_context_transfer_algo_t> >::__shared_ptr_emplace<Espresso::cpu_context_transfer_algo_options&>(std::__1::allocator<Espresso::cpu_context_transfer_algo_t>, Espresso::cpu_context_transfer_algo_options&) + 48
2  Espresso                       0x1b3d9b054 Espresso::get_net_info_ir(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, Espresso::platform const&, Espresso::compute_path const&, std::__1::shared_ptr<Espresso::cpu_context_transfer_algo_t>&, std::__1::shared_ptr<Espresso::net_info_ir_t>&, std::__1::shared_ptr<Espresso::kernels_validation_status_t>&) + 440
3  Espresso                       0x1b3d9b450 try_dispatch(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*, Espresso::platform const&, Espresso::compute_path const&) + 468
4  Espresso                       0x1b3d9c56c Espresso::run_dispatch_v2(std::__1::shared_ptr<Espresso::abstract_context>, std::__1::shared_ptr<Espresso::net>, std::__1::vector<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::allocator<std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> > > const&, Espresso::network_shape const&, Espresso::compute_path const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*) + 1680
5  Espresso                       0x1b3dac470 Espresso::load_network_layers_internal(std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::network_shape const&, std::__1::basic_istream<char, std::__1::char_traits<char> >*, Espresso::compute_path, bool, std::__1::shared_ptr<Espresso::blob_storage_abstract> const&) + 244
6  Espresso                       0x1b3da8850 Espresso::load_and_shape_network(std::__1::shared_ptr<Espresso::SerDes::generic_serdes_object> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::network_shape const&, Espresso::compute_path, std::__1::shared_ptr<Espresso::blob_storage_abstract> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 508
7  Espresso                       0x1b3dab31c Espresso::load_network(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<Espresso::abstract_context> const&, Espresso::compute_path, bool) + 1892
8  Espresso                       0x1b3ccce60 EspressoLight::espresso_plan::add_network(char const*, espresso_storage_type_t) + 304
9  Espresso                       0x1b3cdb2d0 espresso_plan_add_network + 340
10 CoreML                         0x1b389c36c -[MLNeuralNetworkEngine _setupContextAndPlanWithConfiguration:usingCPU:error:] + 752
11 CoreML                         0x1b389d754 -[MLNeuralNetworkEngine initWithContainer:configuration:error:] + 384
12 CoreML                         0x1b38a28e8 +[MLNeuralNetworkEngine loadModelFromCompiledArchive:modelVersionInfo:compilerVersionInfo:configuration:error:] + 208
13 CoreML                         0x1b38f4cb0 +[MLLoader loadModelFromArchive:configuration:loaderEvent:error:] + 624
14 CoreML                         0x1b38f61ec +[MLLoader loadModelFromAssetAtURL:configuration:loaderEvent:error:] + 280
15 CoreML                         0x1b38f648c +[MLLoader loadModelFromAssetAtURL:configuration:error:] + 120
16 CoreML                         0x1b38dcacc -[MLModelAsset load:] + 220
17 CoreML                         0x1b38dc8d0 -[MLModelAsset modelWithError:] + 68
18 CoreML                         0x1b391f28c +[MLModel modelWithContentsOfURL:configuration:error:] + 156
19 appName                      0x105156438 XYAIBridge::CoremlInit(char const*, XYAIBridge::tag_Config const&, void**) + 4385399864

Load the mlmodel code：
`  if (@available(iOS 12.0, *)) {
    MLModelConfiguration* config = [MLModelConfiguration alloc];
    config.computeUnits = MLComputeUnitsAll;
    _model = [MLModel modelWithContentsOfURL:compileUrl configuration:config error:&error];
  } else {
    _model = [MLModel modelWithContentsOfURL:compileUrl error:&error];
  }`

## System Information
iPhone 11
iOS 14
"
48470,Conv2DTranspose crashes with filters=0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Standalone code to reproduce the issue**
Sample code:
```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

input = np.random.rand(2, 8, 8, 8)
x = tf.keras.Input([None, None, 8])
y = tf.keras.layers.Conv2DTranspose(filters=0,kernel_size=3, padding='same', dilation_rate=(1,1))(x)
model = tf.keras.Model(x, y)
z = model(input).numpy()
print(z.mean())
```


**Describe the current behavior**
The process dies after calling `model(input)`.


**Describe the expected behavior**
Expect a `ValueError` raised if `filters`=`0` is not supported. It seems that conv2d supports this, for example:

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

input = np.random.rand(2, 8, 8, 8)
x = tf.keras.Input([None, None, 8])

y = tf.keras.layers.Conv2D(0, kernel_size=3)(x)
model = tf.keras.Model(x, y)
z = model(input).numpy()
print(z.shape)
```

outputs `(2, 6, 6, 0)`."
48469,tf.keras.layers.Conv2DTranspose raises error when dilation rates are different in two dimensions,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Conv2DTranspose raises AssertionError with diifferent dilation_rate for each dimension.

**Describe the expected behavior**
Expect no error.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
x = tf.keras.Input([None, None, 16])
tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=32, dilation_rate=(1,2))(x)
```

Output:


Traceback (most recent call last):
  File ""...\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-32-469167c62d25>"", line 1, in <module>
    tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=32, dilation_rate=(1,2))(x)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 952, in __call__
    input_list)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1091, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1315, in call
    dilation_rate=self.dilation_rate)
  File ""...\venv\lib\site-packages\tensorflow\python\util\dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""...\venv\lib\site-packages\tensorflow\python\keras\backend.py"", line 5355, in conv2d_transpose
    assert dilation_rate[0] == dilation_rate[1]
AssertionError
"
48468,Build bazel error,"**System information**
- OS Platform Linux Ubuntu 18.04:
- TensorFlow installing from source or binary
- Python version: 3.6.9
- Bazel version: 3.7.2
- GCC/Compiler version: 7.5.0
- CUDA/cuDNN version: 10.2 / 7
- GPU model and memory: GeForce GTX 1060 3GB/PCIe/SSE2

After run ./configure, i call `bazel build --local_ram_resources=HOST_RAM*.5 --local_cpu_resources=HOST_CPUS-1 --config=cuda  --spawn_strategy=standalone --config=v2 --verbose_failures //tensorflow/tools/pip_package:build_pip_package`
and after several minutes thowing error:
`ERROR: /home/dinar/tensorflow/tensorflow/core/kernels/BUILD:1221:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1)`

Full output:

```ERROR: /home/dinar/tensorflow/tensorflow/core/kernels/BUILD:1221:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/dinar/.cache/bazel/_bazel_dinar/573fdf00de65bc899e7e19da07d6af8e/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/dinar/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dinar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/host/bin/external/mkl_dnn_v1 -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/host/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/host/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
external/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with '...':
   template <typename F, typename = EnableIfCompatible<const F&>>
                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                
external/com_google_absl/absl/functional/function_ref.h:100:29: note:         'Args'
external/com_google_absl/absl/functional/function_ref.h:114:13: error: parameter packs not expanded with '...':
       typename F, typename = EnableIfCompatible<F*>,
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                
external/com_google_absl/absl/functional/function_ref.h:114:13: note:         'Args'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/dinar/tensorflow/tensorflow/python/tools/BUILD:225:10 C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/dinar/.cache/bazel/_bazel_dinar/573fdf00de65bc899e7e19da07d6af8e/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/dinar/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dinar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/host/bin/external/mkl_dnn_v1 -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/host/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/host/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/host/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -g0 '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 2006.565s, Critical Path: 81.30s
INFO: 519 processes: 53 internal, 466 local.
FAILED: Build did NOT complete successfully```
"
48467,tf.math.acos should accept only float/double inputs,"OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version: 2.4.1
Python version: 3.7
Installed using virtualenv? pip? conda?: pip
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A


## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/math/acos

## Description of issue (what needs changing):

### Parameters defined

The input tensor `x` can not be bfloat16, half, uint8, int8, int16, int32, int64, complex64, complex128, string. In fact, only float32 and float64 is supported now.

### Usage example

```
import tensorflow as tf
a = tf.constant([1.0], dtype = tf.half)
tf.math.acos(a)
```

Output:
NotFoundError: Could not find device for node: {{node Acos}} = Acos[T=DT_INT8]
All kernels registered for op Acos:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
 [Op:Acos]"
48466,keras.layers.Dense should not have float units,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
tf.keras.layers.Dense accepts a floating `units` and successfully inits.

**Describe the expected behavior**
Expect 'ValueError' because 'units' should be a positive integer.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
layer = tf.keras.layers.Dense(units=3.3)
layer(tf.ones([1, 3]))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48465,The `.adapt()` method should be called `.fit()` for TextVectorization,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, to train or fit a text vectorizer in `tf.keras.layers.experimental.preprocessing.TextVectorization`, we have to call a weird `.adapt()` method which no one use this name in standard machine learning. It is more commonly to use the name `.fit()` like in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit). I suggest to name the method as `.fit()` so it will be more familiar to people who just came to tensorflow.

**Will this change the current api? How?** Yes, there will be a new `.fit()` method which is exactly the `.adapt()` method.

**Who will benefit with this feature?**
New beginner to tensorflow who are familiar with other ML framework but just came to tensorflow to use this awesome framework.

**Any Other info.**
"
48464, Int incompatibility error on building TensorFlowLiteC framework,"# System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): github repository
- TensorFlow version: github repository
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: directly from standard terminal
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.29)
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

# Problem description

I tried to build TensorFlow Lite as suggested here: 
Build TensorFlowLiteC dynamic framework (recommended)
https://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended
and ended up with int vs int32 incompatibility error

# Steps

1. Install Bazel
2. Clone TensoFlow repository from github
3. Run `.configure`
4. Start `bazel build`

The build is ending up with `error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'`

I tried on two different MacBooks with M1 and Intel chips.

The issues seem to come from here:
https://github.com/google/XNNPACK/issues?q=is%3Aissue+int
but I can't find any appropriate one.

Could you suggest why it is happened and how the TensorFlowLiteC dynamic framework can be built?

# Logs

Log part:

external/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-neondot.c:63:18: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'
      vacc0x0123 = vdotq_lane_s32(vacc0x0123, vb0123x0123, va0x01234567, 0);
                 ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Full log:
[tflite_install.log](https://github.com/tensorflow/tensorflow/files/6292923/tflite_install.log)

"
48463,Error: Invalid argument: Quantized tensors must have non-zero scales,"Hi, 

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): Nvidia docker image (TF1.15)


### 2. Code

[model.zip](https://github.com/tensorflow/tensorflow/files/6292072/model.zip)


**Transformation from Checkpoints (used scripts from the Object detection API, see model.zip)**

```
python export_tflite_ssd_graph.py --pipeline_config_path=/Studienarbeit/tensorflow_od/models/Variation79/mobilnetv2_variation79.config --trained_checkpoint_prefix=/Studienarbeit/tensorflow_od/models/Variation79/model_checkpoints/model.ckpt-176720 --output_directory=exported-models/Variation79    --add_postprocessing_op=true
```

```
tflite_convert --graph_def_file=/Studienarbeit/tensorflow_od/exported-models/Variation79/tflite_graph.pb  --output_file=/Studienarbeit/tensorflow_od/exported-models/Variation79/detect.tflite --input_shapes=1,360,640,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8  --mean_values=128 --std_dev_values=127 --allow_custom_ops
```

### 3. Failure after conversion
I'm trying to convert a QAT trained Model from the TF1 Object Detection API (MobileNetV2-SSDLite) into a working model for the USB Edge TPU. It is working on some models fine but on others I get the following error when I try to convert the .tflite model into a .tflite model for the TPU.  I used the Command Line tool.

```
Edge TPU Compiler version 15.0.340273435
loc(""BoxPredictor_4/BoxEncodingPredictor/Conv2D_bias""): error: Invalid argument: Quantized tensors must have non-zero scales
error: could not translate function : Quantized tensors must have non-zero scales
```
I can see in Netron that the Graph contains for the Bias no quantization term. 
In this issue #https://github.com/tensorflow/tensorflow/issues/46334  they found a solution or a work around with the option _(converter._experimental_new_quantizer=True)_. 
Unfortunatly it is just working with newer tf2 versions but i use tf1.15. 

**Question:**

- Is there a other solution to solve the error?
- Can I use models trained with tf1.15 and convert them in e.g. tf2.5.0 with the option _(converter._experimental_new_quantizer=True)_?

### 5. Extra Information

Here you can see that the quantization conversion term is missing for the Bias. For alle the other Conv2D there are quantization terms for the BIAS values.
![error_netron](https://user-images.githubusercontent.com/74291692/114309180-a6693380-9ae6-11eb-937f-3a3a62287b14.png)

Here how it should look like.
![correct_bias](https://user-images.githubusercontent.com/74291692/114309669-43789c00-9ae8-11eb-885d-66e1e7389d70.png)
"
48462,"Tensorflow error messages are verbose, frustrating and hard to understand","I run into weird and verbose errors that are hard to understand all the time, and it always takes me tons of time to debug into it, however actually, even debugging is also frustrating. 

I don't know if other people feel the same.

I'm not actually talking about some specific type of error messages and it is a common issue which might be blamed to the computation graph tensorflow builds and causes that `model.fit` could lead to tons of types of errors.

I hesitated for a long time, thinking about whether I should submit it here as an issue.

### Expectation

- The error stacks should not always from `model.fit`
- `ValueError: in user code:` should list real **user code** instead of verbose and detailed implementation of keras. There might be some mechanism to track the execution of graph flow and show users the root cause."
48461,pipenv shell not working,"I have been trying to call ""pipenv shell"" but it gives this error:|
                                                                                                 \/


Traceback (most recent call last):
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Scripts\pipenv.exe\__main__.py"", line 7, in <module>
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\decorators.py"", line 73, in new_func
    return ctx.invoke(f, obj, *args, **kwargs)
  File ""C:\Users\yaseen\AppData\Local\Programs\Python\Python38-32\Lib\site-packages\pipenv\vendor\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\cli\command.py"", line 428, in shell
    do_shell(
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\core.py"", line 2356, in do_shell
    ensure_project(
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\core.py"", line 576, in ensure_project
    ensure_virtualenv(
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\core.py"", line 498, in ensure_virtualenv
    python = ensure_python(three=three, python=python)
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\core.py"", line 388, in ensure_python
    path_to_python = find_a_system_python(python)
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\core.py"", line 350, in find_a_system_python
    return next(iter(finder.find_all_python_versions()), None)
  File ""c:\users\yaseen\appdata\local\programs\python\python38-32\lib\site-packages\pipenv\vendor\pythonfinder\pythonfinder.py"", line 328, in find_all_python_versions
    path_list = sorted(versions, key=version_sort, reverse=True)
AttributeError: 'NoneType' object has no attribute 'version_sort'"
48460,Keras models latency grows too fast after converting to TFLite,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: custom code.
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: IOS 14.4.2, Android 11.
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: iPad 5, Pixel 5.
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.7.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**: 

### Describe the problem
In my code, I use the default keras.applications.MobileNetV2 model and then add some layers on top. The latency of MobileNetV2 in Google Colab (GPU) is about 0.046 s, with my layers it increases to 0.048 s (about 4%). Then I convert both models to TFLite and check on my smartphone's GPU. The latency of MobileNetV2 is about 0.065 s, but when layers are added, it increases to 0.11 s (about 40% !!!). What could be the reason for this behavior?

### Source code / logs
Colab Notebook to reproduce: https://colab.research.google.com/drive/18qryA4hPu-uCEyFggoyWRkW_ouT8024A?usp=sharing
"
48459,fatal error: tensorflow/core/framework/types.pb.h: No such file or directory,"
**System information**
- OS Platform and Distribution (Linux Ubuntu 20.04 LTS):
- TensorFlow installed from (source):
- TensorFlow version: r2.4
- Python version: 3.8, but I did not build for it
- Bazel version (if compiling from source): bazel release 3.1.0
- GCC/Compiler version (if compiling from source): gcc-7
- CUDA/cuDNN version: CUDA Compute: 7.5, CUDA Toolkit 11.1, cuDNN: 8.1.0.77
- GPU model and memory: RTX 2060, 6GB



**Describe the problem**
tensorflow/core/framework/types.pb.h is not found in bazel-bin directory.
Quoting this issue: #44602
CMake build tthrows this following error:
build] In file included from /home/aswath/Downloads/tensorflow/tensorflow/core/framework/tensor.h:24,
[build]                  from /home/aswath/Downloads/tensorflow/tensorflow/cc/framework/ops.h:21,
[build]                  from /home/aswath/Downloads/tensorflow/tensorflow/cc/client/client_session.h:24,
[build]                  from /home/aswath/Downloads/tfcc/tfcc.cpp:1:
[build] /home/aswath/Downloads/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory
**[build]    22 | #include ""tensorflow/core/framework/types.pb.h""**

CMakeLists.txt:
```
include_directories(
~/Downloads/tensorflow/
~/Downloads/tensorflow/tensorflow
~/Downloads/tensorflow/third_party
~/Downloads/tensorflow/third_party/**abseil-cpp-master** 
${OpenCV_INCLUDE_DIRS}
${EIGEN3_INCLUDE_DIR}
)

set(LIBS
~/Downloads/tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so.2
~/Downloads/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so.2.4.1
${OpenCV_LIBS}
)

add_executable(tfcc tfcc.cpp)

target_link_libraries(tfcc
${LIBS}
)
```
**abseil-cpp-master:**
I had this error previously: cannot open source file ""absl/strings/string_view.h"" (dependency of ""tensorflow/cc/client/client_session.h"")C/C++(1696)
I solved this by cloning this repo into ""third-party"" folder: https://github.com/abseil/abseil-cpp


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
~/Downloads/tensorflow$ git clone https://github.com/tensorflow/tensorflow.git
~/Downloads/tensorflow$ git checkout r2.4
~/Downloads/tensorflow$ ./configure 
~/Downloads/tensorflow$ bazel build -c opt --config=monolithic --jobs=1 //tensorflow:libtensorflow_cc.so
```

**Any other info / logs**
**1)Configuring with bazel**
```
~/Downloads/tensorflow$ ./configure 

You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.8/dist-packages
  /home/aswath/slam_ws/devel/lib/python3/dist-packages
  /home/aswath/Capstone_Project/catkin_ws/devel/lib/python3/dist-packages
  /usr/lib/python3/dist-packages
  /opt/ros/noetic/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]
/usr/lib/python3/dist-packages
Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Found CUDA 11.1 in:
    /usr/local/cuda-11.1/targets/x86_64-linux/lib
    /usr/local/cuda-11.1/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.1/targets/x86_64-linux/lib
    /usr/local/cuda-11.1/targets/x86_64-linux/include
Found TensorRT 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]: 7.5


Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-7


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN support for Aarch64.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

**2)Building tensorflow_cc with Bazel**

```
aswath@aswath:~/Downloads/tensorflow$ bazel build -c opt --config=monolithic --jobs=1 //tensorflow:libtensorflow_cc.so

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=166
INFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/aswath/Downloads/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:/home/aswath/ORB_SLAM2_CUDA/Examples/ROS/ORB_SLAM2_CUDA/build/devel/lib:/home/aswath/slam_ws/devel/lib:/home/aswath/Capstone_Project/catkin_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/aswath/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:tensorrt in file /home/aswath/Downloads/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /home/aswath/Downloads/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:monolithic in file /home/aswath/Downloads/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:linux in file /home/aswath/Downloads/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/aswath/Downloads/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (224 packages loaded, 22536 targets configured).
INFO: Found 1 target...
Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
**
INFO: Elapsed time: 29923.294s, Critical Path: 161.20s
INFO: 16233 processes: 16233 local.
INFO: Build completed successfully, 24186 total actions
**
```

**3) I listed out the locations of types.pb.h:**
```
aswath@aswath~$ locate tensorflow/core/framework/types.pb.h

/home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/framework/types.pb.h
/home/aswath/.cache/bazel/_bazel_aswath/32d28df2ab135fee90e5882bd2c56192/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/framework/types.pb.h
/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/framework/types.pb.h
/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/framework/types.pb.h
/home/aswath/.cache/bazel/_bazel_aswath/39c920a04757f746432c7b023a8d9b9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/include/tensorflow/core/framework/types.pb.h
/home/aswath/anaconda3/envs/rosconda/lib/python3.8/site-packages/tensorflow/include/tensorflow/core/framework/types.pb.h
```

**4) As you can see above, types.pb.h is not in bazel-bin directory, please tell me where I have gone wrong. Thank you**
Bazel-bin contains the .so files:
```
bazel-bin/tensorflow$ ls
c  cc  compiler  core  libtensorflow_cc.so  libtensorflow_cc.so.2  libtensorflow_cc.so.2.4.1  libtensorflow_cc.so.2.4.1-2.params
```
[bazel_tensorflowcc_build_logs.txt](https://github.com/tensorflow/tensorflow/files/6291200/bazel_tensorflowcc_build_logs.txt)

"
48458,Broken link at tf.keras.callbacks.TensorBoard in docs page,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

URL(s) with the issue: 
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard

## Description of issue (what needs changing): 
The link needs to be changed or removed

### Submit a pull request?
Yes, here it is #48457 

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
48456,"""OP_REQUIRES failed at conv_grad_input_ops.cc:1103 : Resource exhausted..."" after training for a while","**System information**
- Windows10
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.12
- CUDA/cuDNN version: cudatoolkit=10.1, cudnn=7.6
- GPU model and memory: 6GB

**Describe the current behavior**
- I want to train a model using `GradientTape`. The training went well at the beginning, but **after 27 batches** (`batch_size=4 `in this case) the error report pops out. It's like the memory usage is not released and accumulated, otherwise it's supposed to report this error at the beginning, right? (but I don't know for sure...)
- Reducing `batch_size` fro 4 to 2 doesn't help, only making the progress proceed to 70 batches, then the error appeared again. Therefore I don't think it's about batch_size.

**Standalone code to reproduce the issue**
Since there are 4 models loaded and the code is massive (many custom loss functions) it's difficult to reproduce the error with minimal implementation. So sorry that I don't know how to reproduce this error with minimal implementation.
But I used to run the training code bug-free, until I rewrote a part of my **custom loss function**. So it must be something wrong with that part. I will just show that part

```

def project_3d(points, K, T, shape, scale):
    """"""Layer which projects 3D points into a camera with intrinsics K and at position T
    """"""
    batch_size, height, width, _ = shape
    height, width = height // (2 ** scale), width // (2 ** scale)
    eps = 1e-7

    P = tf.matmul(K, T)[:, :3, :]

    cam_points = tf.matmul(P, points)
    pix_coords = cam_points[:, :2, :] / (tf.expand_dims(cam_points[:, 2, :], axis=1) + eps)
    pix_coords = tf.reshape(pix_coords, (batch_size, 2, height, width))
    pix_coords = tf.transpose(pix_coords, [0, 2, 3, 1])
    pix_coords = pix_coords.numpy()
    pix_coords[..., 0] /= width - 1
    pix_coords[..., 1] /= height - 1
    pix_coords = (pix_coords - 0.5) * 2
    return pix_coords


def back_proj_depth(depth, inv_K, shape, scale):
    """"""Layer to transform a depth image into a point cloud
    shape_s: scaled shapes, corresponds to scales = [0,1,2,3]
    """"""

    batch_size, height, width, _ = shape
    height, width = height // (2**scale), width // (2**scale)

    meshgrid = tf.meshgrid(range(width), range(height), indexing='xy')
    id_coords = tf.stack(meshgrid, axis=0)

    ones = tf.ones((batch_size, 1, height * width), dtype=tf.int32)

    pix_coords = tf.expand_dims(
        tf.stack([tf.reshape(id_coords[0], [-1]),
                  tf.reshape(id_coords[1], [-1])], 0), 0)

    multiples = tf.constant([batch_size, 1, 1])
    pix_coords = tf.tile(pix_coords, multiples)

    pix_coords = tf.concat([pix_coords, ones], 1)
    pix_coords = tf.cast(pix_coords, tf.float32)

    ones = tf.cast(ones, tf.float32)

    cam_points = tf.matmul(inv_K[:,:3, :3], pix_coords)
    cam_points = tf.reshape(depth, (batch_size, 1, -1)) * cam_points
    cam_points = tf.concat([cam_points, ones], 1)
    return cam_points


def bilinear_sampler(img, coords):
    """""" TF-version Bilinear Sampler
    Performs bilinear sampling of the input images according to the
    normalized coordinates provided by the sampling grid. Note that
    the sampling is done identically for each channel of the input.
    To test if the function works properly, output image should be
    identical to input image when theta is initialized to identity transform.
    Input
    -----
    - img: batch of images in (B, H, W, C) layout.
    - grid: x, y which is the output of affine_grid_generator.
    Returns
    -------
    - out: interpolated images according to grids. Same size as grid.
    """"""

    def get_pixel_value(img, x, y):
        """"""
        Utility function to get pixel value for coordinate
        vectors x and y from a  4D tensor image.
        Input
        -----
        - img: tensor of shape (B, H, W, C)
        - x: flattened tensor of shape (B*H*W,)
        - y: flattened tensor of shape (B*H*W,)
        Returns
        -------
        - output: tensor of shape (B, H, W, C)
        """"""
        shape = tf.shape(x)
        batch_size = shape[0]
        height = shape[1]
        width = shape[2]

        batch_idx = tf.range(0, batch_size)
        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))
        b = tf.tile(batch_idx, (1, height, width))

        indices = tf.stack([b, y, x], 3)
        res = tf.gather_nd(img, indices)
        return res

    H = tf.shape(img)[1]
    W = tf.shape(img)[2]
    max_y = tf.cast(H - 1, 'int32')
    max_x = tf.cast(W - 1, 'int32')
    zero = tf.zeros([], dtype='int32')

    x, y = coords[:, ..., 0], coords[:, ..., 1]
    x = tf.cast(x, 'float32')
    y = tf.cast(y, 'float32')
    x = 0.5 * ((x + 1.0) * tf.cast(max_x - 1, 'float32'))
    y = 0.5 * ((y + 1.0) * tf.cast(max_y - 1, 'float32'))

    # grab 4 nearest corner points for each (x_i, y_i)
    x0 = tf.cast(tf.floor(x), 'int32')
    x1 = x0 + 1
    y0 = tf.cast(tf.floor(y), 'int32')
    y1 = y0 + 1

    # clip to range [0, H-1/W-1] to not violate img boundaries
    x0 = tf.clip_by_value(x0, zero, max_x)
    x1 = tf.clip_by_value(x1, zero, max_x)
    y0 = tf.clip_by_value(y0, zero, max_y)
    y1 = tf.clip_by_value(y1, zero, max_y)

    # get pixel value at corner coords
    Ia = get_pixel_value(img, x0, y0)
    Ib = get_pixel_value(img, x0, y1)
    Ic = get_pixel_value(img, x1, y0)
    Id = get_pixel_value(img, x1, y1)

    # recast as float for delta calculation
    x0 = tf.cast(x0, 'float32')
    x1 = tf.cast(x1, 'float32')
    y0 = tf.cast(y0, 'float32')
    y1 = tf.cast(y1, 'float32')

    # calculate deltas
    wa = (x1 - x) * (y1 - y)
    wb = (x1 - x) * (y - y0)
    wc = (x - x0) * (y1 - y)
    wd = (x - x0) * (y - y0)

    # add dimension for addition
    wa = tf.expand_dims(wa, axis=3)
    wb = tf.expand_dims(wb, axis=3)
    wc = tf.expand_dims(wc, axis=3)
    wd = tf.expand_dims(wd, axis=3)

    # compute output
    out = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])
    return out


def transformation_from_parameters(axisangle, translation, invert=False):
    """"""Convert the network's (axisangle, translation) output into a 4x4 matrix
    """"""

    R = rot_from_axisangle(axisangle)
    t = tf.identity(translation)

    if invert:
        R = tf.transpose(R, (0, 2, 1))
        t *= -1

    T = get_translation_matrix(t)

    if invert:
        M = tf.matmul(R, T)
    else:
        M = tf.matmul(T, R)
    return M


def get_translation_matrix(trans_vec):
    """"""Convert a translation vector into a 4x4 transformation matrix
    """"""
    batch_size = trans_vec.shape[0]
    one = tf.ones([batch_size, 1, 1], dtype=tf.float32)
    zero = tf.zeros([batch_size, 1, 1], dtype=tf.float32)

    T = tf.concat([
        one, zero, zero, trans_vec[:, :, :1],
        zero, one, zero, trans_vec[:, :, 1:2],
        zero, zero, one, trans_vec[:, :, 2:3],
        zero, zero, zero, one

    ], axis=2)
    T = tf.reshape(T, [batch_size, 4, 4])
    return T


def rot_from_axisangle(vec):
    """"""Convert an axisangle rotation into a 4x4 transformation matrix
    (adapted from https://github.com/Wallacoloo/printipi)
    Input 'vec' has to be Bx1x3
    """"""
    angle = tf.norm(vec, 2, 2, keepdims=True)
    axis = vec / (angle + 1e-7)

    ca = tf.math.cos(angle)
    sa = tf.math.sin(angle)
    C = 1-ca

    x = tf.expand_dims(axis[..., 0], 1)
    y = tf.expand_dims(axis[..., 1], 1)
    z = tf.expand_dims(axis[..., 2], 1)

    xs = x * sa
    ys = y * sa
    zs = z * sa
    xC = x * C
    yC = y * C
    zC = z * C
    xyC = x * yC
    yzC = y * zC
    zxC = z * xC

    one = tf.ones_like(zxC, dtype=tf.float32)
    zero = tf.zeros_like(zxC, dtype=tf.float32)
    rot_matrix = tf.concat([
        x * xC + ca,
        xyC - zs,
        zxC + ys,
        zero,
        xyC + zs,
        y * yC + ca,
        yzC - xs,
        zero,
        zxC - ys,
        yzC + xs,
        z * zC + ca,
        zero, zero, zero, zero, one
    ], axis=2)

    rot_matrix = tf.reshape(rot_matrix, [-1, 4, 4])

    return rot_matrix
```


**Other info / logs** Include any logs or source code that would be helpful to
```
D:\ProgramData\Anaconda3\envs\movis_sort\python.exe D:/MA/Recources/monodepth2_tf2/new_trainer.py
2021-04-10 17:39:04.209000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2021-04-10 17:39:07.330472: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2021-04-10 17:39:07.363266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 with Max-Q Design computeCapability: 6.1
coreClock: 1.3415GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-04-10 17:39:07.363687: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2021-04-10 17:39:07.371125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2021-04-10 17:39:07.376128: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2021-04-10 17:39:07.378017: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2021-04-10 17:39:07.383580: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2021-04-10 17:39:07.387137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2021-04-10 17:39:07.402036: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2021-04-10 17:39:07.402410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-04-10 17:39:07.409179: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-10 17:39:07.420304: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2161a5093e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-04-10 17:39:07.420676: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-04-10 17:39:07.421217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 with Max-Q Design computeCapability: 6.1
coreClock: 1.3415GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-04-10 17:39:07.421735: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2021-04-10 17:39:07.421937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2021-04-10 17:39:07.422110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2021-04-10 17:39:07.422283: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2021-04-10 17:39:07.422491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2021-04-10 17:39:07.422869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2021-04-10 17:39:07.423133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2021-04-10 17:39:07.423431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-04-10 17:39:08.101095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-10 17:39:08.101318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2021-04-10 17:39:08.101444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2021-04-10 17:39:08.101845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4826 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-04-10 17:39:08.105369: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21640434780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-04-10 17:39:08.105633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 with Max-Q Design, Compute Capability 6.1
2021-04-10 17:39:08.847134: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2021-04-10 17:39:09.634403: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-04-10 17:39:09.767162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
depth_enc
depth_dec
pose_enc
pose_dec
Epoch1/2:   0%|          | 0/52 [00:00<?, ?it/s]Start training...
WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:   2%|▏         | 1/52 [00:06<05:31,  6.50s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:   4%|▍         | 2/52 [00:07<02:38,  3.16s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:   6%|▌         | 3/52 [00:08<01:44,  2.13s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:   8%|▊         | 4/52 [00:09<01:18,  1.64s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  10%|▉         | 5/52 [00:10<01:04,  1.38s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  12%|█▏        | 6/52 [00:10<00:56,  1.23s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  13%|█▎        | 7/52 [00:11<00:50,  1.12s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  15%|█▌        | 8/52 [00:12<00:46,  1.05s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  17%|█▋        | 9/52 [00:13<00:43,  1.01s/it]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  19%|█▉        | 10/52 [00:14<00:40,  1.03it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  21%|██        | 11/52 [00:15<00:38,  1.06it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  23%|██▎       | 12/52 [00:16<00:37,  1.08it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  25%|██▌       | 13/52 [00:17<00:35,  1.11it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  27%|██▋       | 14/52 [00:18<00:34,  1.12it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  29%|██▉       | 15/52 [00:19<00:33,  1.10it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  31%|███       | 16/52 [00:19<00:33,  1.09it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  33%|███▎      | 17/52 [00:20<00:32,  1.09it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  35%|███▍      | 18/52 [00:21<00:31,  1.07it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  37%|███▋      | 19/52 [00:22<00:30,  1.07it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  38%|███▊      | 20/52 [00:23<00:29,  1.10it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  40%|████      | 21/52 [00:24<00:27,  1.11it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  42%|████▏     | 22/52 [00:25<00:26,  1.13it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  44%|████▍     | 23/52 [00:26<00:25,  1.12it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  46%|████▌     | 24/52 [00:27<00:24,  1.13it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  48%|████▊     | 25/52 [00:28<00:24,  1.12it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  50%|█████     | 26/52 [00:28<00:23,  1.12it/s]WARNING:tensorflow:Gradients do not exist for variables ['res_net18_new_1/conv0/kernel:0', 'res_net18_new_1/conv0/BatchNorm/gamma:0', 'res_net18_new_1/conv0/BatchNorm/beta:0', 'basic_block_pad_4/conv1_1/conv_1/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_1/beta:0', 'basic_block_pad_4/conv2d_8/kernel:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/gamma:0', 'basic_block_pad_4/conv1_1/BatchNorm_2/beta:0', 'basic_block_nopad_4/conv1_2/conv_1/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_1/beta:0', 'basic_block_nopad_4/conv2d_9/kernel:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/gamma:0', 'basic_block_nopad_4/conv1_2/BatchNorm_2/beta:0', 'basic_block_pad_5/conv2_1/conv_1/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_1/beta:0', 'basic_block_pad_5/conv2d_10/kernel:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/gamma:0', 'basic_block_pad_5/conv2_1/BatchNorm_2/beta:0', 'basic_block_pad_5/sequential_3/conv2_1/downsample/kernel:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_5/sequential_3/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_5/conv2_2/conv_1/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_1/beta:0', 'basic_block_nopad_5/conv2d_11/kernel:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/gamma:0', 'basic_block_nopad_5/conv2_2/BatchNorm_2/beta:0', 'basic_block_pad_6/conv3_1/conv_1/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_1/beta:0', 'basic_block_pad_6/conv2d_12/kernel:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/gamma:0', 'basic_block_pad_6/conv3_1/BatchNorm_2/beta:0', 'basic_block_pad_6/sequential_4/conv3_1/downsample/kernel:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_6/sequential_4/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_6/conv3_2/conv_1/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_1/beta:0', 'basic_block_nopad_6/conv2d_13/kernel:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/gamma:0', 'basic_block_nopad_6/conv3_2/BatchNorm_2/beta:0', 'basic_block_pad_7/conv4_1/conv_1/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_1/beta:0', 'basic_block_pad_7/conv2d_14/kernel:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/gamma:0', 'basic_block_pad_7/conv4_1/BatchNorm_2/beta:0', 'basic_block_pad_7/sequential_5/conv4_1/downsample/kernel:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/gamma:0', 'basic_block_pad_7/sequential_5/downsample/BatchNorm_3/beta:0', 'basic_block_nopad_7/conv4_2/conv_1/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_1/beta:0', 'basic_block_nopad_7/conv2d_15/kernel:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/gamma:0', 'basic_block_nopad_7/conv4_2/BatchNorm_2/beta:0', 'pose_decoder/Conv_squeeze/kernel:0', 'pose_decoder/Conv_squeeze/bias:0', 'pose_decoder/Conv_pose_0/kernel:0', 'pose_decoder/Conv_pose_0/bias:0', 'pose_decoder/Conv_pose_1/kernel:0', 'pose_decoder/Conv_pose_1/bias:0', 'pose_decoder/Conv_pose_2/kernel:0', 'pose_decoder/Conv_pose_2/bias:0'] when minimizing the loss.
Epoch1/2:  52%|█████▏    | 27/52 [00:29<00:22,  1.12it/s]2021-04-10 17:39:53.874112: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 46.22MiB (rounded to 48470016)requested by op Conv2DBackpropInput
Current allocation summary follows.
2021-04-10 17:39:53.874460: I tensorflow/core/common_runtime/bfc_allocator.cc:970] BFCAllocator dump for GPU_0_bfc
2021-04-10 17:39:53.874609: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (256): 	Total Chunks: 207, Chunks in use: 206. 51.8KiB allocated for chunks. 51.5KiB in use in bin. 29.7KiB client-requested in use in bin.
2021-04-10 17:39:53.874893: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (512): 	Total Chunks: 104, Chunks in use: 104. 55.2KiB allocated for chunks. 55.2KiB in use in bin. 52.3KiB client-requested in use in bin.
2021-04-10 17:39:53.875191: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (1024): 	Total Chunks: 109, Chunks in use: 108. 120.0KiB allocated for chunks. 118.2KiB in use in bin. 108.6KiB client-requested in use in bin.
2021-04-10 17:39:53.875481: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (2048): 	Total Chunks: 98, Chunks in use: 96. 212.2KiB allocated for chunks. 207.2KiB in use in bin. 193.2KiB client-requested in use in bin.
2021-04-10 17:39:53.875767: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (4096): 	Total Chunks: 6, Chunks in use: 5. 28.8KiB allocated for chunks. 24.5KiB in use in bin. 22.5KiB client-requested in use in bin.
2021-04-10 17:39:53.876050: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (8192): 	Total Chunks: 8, Chunks in use: 6. 83.2KiB allocated for chunks. 60.2KiB in use in bin. 57.0KiB client-requested in use in bin.
2021-04-10 17:39:53.876339: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (16384): 	Total Chunks: 7, Chunks in use: 5. 160.0KiB allocated for chunks. 116.0KiB in use in bin. 90.0KiB client-requested in use in bin.
2021-04-10 17:39:53.876616: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (32768): 	Total Chunks: 10, Chunks in use: 10. 365.2KiB allocated for chunks. 365.2KiB in use in bin. 337.0KiB client-requested in use in bin.
2021-04-10 17:39:53.876899: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (65536): 	Total Chunks: 13, Chunks in use: 10. 1.17MiB allocated for chunks. 930.0KiB in use in bin. 883.5KiB client-requested in use in bin.
2021-04-10 17:39:53.877182: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (131072): 	Total Chunks: 27, Chunks in use: 27. 3.84MiB allocated for chunks. 3.84MiB in use in bin. 3.66MiB client-requested in use in bin.
2021-04-10 17:39:53.877457: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (262144): 	Total Chunks: 19, Chunks in use: 17. 6.35MiB allocated for chunks. 5.62MiB in use in bin. 5.18MiB client-requested in use in bin.
2021-04-10 17:39:53.877732: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (524288): 	Total Chunks: 31, Chunks in use: 30. 22.16MiB allocated for chunks. 21.50MiB in use in bin. 19.78MiB client-requested in use in bin.
2021-04-10 17:39:53.878019: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (1048576): 	Total Chunks: 46, Chunks in use: 42. 76.93MiB allocated for chunks. 69.32MiB in use in bin. 62.28MiB client-requested in use in bin.
2021-04-10 17:39:53.878298: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (2097152): 	Total Chunks: 32, Chunks in use: 30. 89.68MiB allocated for chunks. 83.05MiB in use in bin. 72.52MiB client-requested in use in bin.
2021-04-10 17:39:53.878576: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (4194304): 	Total Chunks: 79, Chunks in use: 71. 445.02MiB allocated for chunks. 401.48MiB in use in bin. 369.44MiB client-requested in use in bin.
2021-04-10 17:39:53.878858: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (8388608): 	Total Chunks: 239, Chunks in use: 233. 2.66GiB allocated for chunks. 2.59GiB in use in bin. 2.47GiB client-requested in use in bin.
2021-04-10 17:39:53.879149: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (16777216): 	Total Chunks: 56, Chunks in use: 31. 1.10GiB allocated for chunks. 597.29MiB in use in bin. 394.20MiB client-requested in use in bin.
2021-04-10 17:39:53.891668: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (33554432): 	Total Chunks: 7, Chunks in use: 1. 254.57MiB allocated for chunks. 46.22MiB in use in bin. 46.22MiB client-requested in use in bin.
2021-04-10 17:39:53.892079: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (67108864): 	Total Chunks: 1, Chunks in use: 1. 74.38MiB allocated for chunks. 74.38MiB in use in bin. 46.22MiB client-requested in use in bin.
2021-04-10 17:39:53.892351: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-04-10 17:39:53.892600: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-04-10 17:39:53.892857: I tensorflow/core/common_runtime/bfc_allocator.cc:993] Bin for 46.22MiB was 32.00MiB, Chunk State: 
2021-04-10 17:39:53.893015: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 32.11MiB | Requested Size: 30.41MiB | in_use: 0 | bin_num: 17, prev:   Size: 20.68MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1, next:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.893399: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 33.75MiB | Requested Size: 30.00MiB | in_use: 0 | bin_num: 17, prev:   Size: 5.62MiB | Requested Size: 5.62MiB | in_use: 1 | bin_num: -1, next:   Size: 5.62MiB | Requested Size: 5.62MiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.893755: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 33.75MiB | Requested Size: 30.00MiB | in_use: 0 | bin_num: 17, prev:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1, next:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.894112: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 33.75MiB | Requested Size: 30.41MiB | in_use: 0 | bin_num: 17, prev:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1, next:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.894470: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 33.75MiB | Requested Size: 30.00MiB | in_use: 0 | bin_num: 17, prev:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1, next:   Size: 11.25MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.894830: I tensorflow/core/common_runtime/bfc_allocator.cc:999]   Size: 41.23MiB | Requested Size: 34.81MiB | in_use: 0 | bin_num: 17, prev:   Size: 16.94MiB | Requested Size: 11.25MiB | in_use: 1 | bin_num: -1, next:   Size: 512.0KiB | Requested Size: 512.0KiB | in_use: 1 | bin_num: -1
2021-04-10 17:39:53.895189: I tensorflow/core/common_runtime/bfc_allocator.cc:1006] Next region of size 33554432
2021-04-10 17:39:53.895369: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701400000 of size 147456 next 52
2021-04-10 17:39:53.895559: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701424000 of size 294912 next 59
2021-04-10 17:39:53.895749: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70146c000 of size 589824 next 63
2021-04-10 17:39:53.895941: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7014fc000 of size 32768 next 70
2021-04-10 17:39:53.896130: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701504000 of size 256 next 265
2021-04-10 17:39:53.896318: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701504100 of size 256 next 73
2021-04-10 17:39:53.896504: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701504200 of size 1155584 next 57
2021-04-10 17:39:53.896654: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70161e400 of size 983040 next 217
2021-04-10 17:39:53.896803: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170e400 of size 1024 next 226
2021-04-10 17:39:53.896947: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170e800 of size 1024 next 227
2021-04-10 17:39:53.906290: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170ec00 of size 1024 next 228
2021-04-10 17:39:53.906451: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170f000 of size 1024 next 229
2021-04-10 17:39:53.906601: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170f400 of size 1024 next 231
2021-04-10 17:39:53.906755: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170f800 of size 1024 next 232
2021-04-10 17:39:53.906903: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70170fc00 of size 1024 next 233
2021-04-10 17:39:53.907051: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701710000 of size 1024 next 234
2021-04-10 17:39:53.907210: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701710400 of size 1024 next 236
2021-04-10 17:39:53.907353: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701710800 of size 1024 next 237
2021-04-10 17:39:53.907497: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701710c00 of size 1024 next 238
2021-04-10 17:39:53.907642: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701711000 of size 1024 next 239
2021-04-10 17:39:53.907789: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701711400 of size 368640 next 832
2021-04-10 17:39:53.907937: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70176b400 of size 602112 next 58
2021-04-10 17:39:53.908081: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017fe400 of size 1024 next 83
2021-04-10 17:39:53.908223: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017fe800 of size 1024 next 85
2021-04-10 17:39:53.908366: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017fec00 of size 512 next 350
2021-04-10 17:39:53.908508: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017fee00 of size 256 next 581
2021-04-10 17:39:53.908651: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017fef00 of size 256 next 568
2021-04-10 17:39:53.908794: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017ff000 of size 256 next 1169
2021-04-10 17:39:53.908937: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017ff100 of size 512 next 1370
2021-04-10 17:39:53.909079: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017ff300 of size 256 next 89
2021-04-10 17:39:53.909221: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7017ff400 of size 2359296 next 86
2021-04-10 17:39:53.909369: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a3f400 of size 1024 next 87
2021-04-10 17:39:53.909513: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a3f800 of size 1024 next 90
2021-04-10 17:39:53.909661: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a3fc00 of size 1536 next 728
2021-04-10 17:39:53.909806: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a40200 of size 256 next 1162
2021-04-10 17:39:53.909951: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a40300 of size 256 next 92
2021-04-10 17:39:53.910092: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a40400 of size 131072 next 93
2021-04-10 17:39:53.910238: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a60400 of size 1024 next 94
2021-04-10 17:39:53.910381: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a60800 of size 1024 next 95
2021-04-10 17:39:53.910526: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a60c00 of size 768 next 739
2021-04-10 17:39:53.910682: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a60f00 of size 256 next 989
2021-04-10 17:39:53.910822: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a61000 of size 256 next 325
2021-04-10 17:39:53.910967: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a61100 of size 768 next 97
2021-04-10 17:39:53.911105: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701a61400 of size 2359296 next 98
2021-04-10 17:39:53.922315: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca1400 of size 1024 next 99
2021-04-10 17:39:53.922478: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca1800 of size 1024 next 100
2021-04-10 17:39:53.922623: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca1c00 of size 512 next 1413
2021-04-10 17:39:53.922770: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca1e00 of size 512 next 1234
2021-04-10 17:39:53.922915: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca2000 of size 512 next 1164
2021-04-10 17:39:53.923059: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca2200 of size 256 next 1145
2021-04-10 17:39:53.923202: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca2300 of size 256 next 102
2021-04-10 17:39:53.923345: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ca2400 of size 2359296 next 103
2021-04-10 17:39:53.923492: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee2400 of size 1024 next 104
2021-04-10 17:39:53.923638: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee2800 of size 1024 next 105
2021-04-10 17:39:53.923784: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee2c00 of size 1024 next 132
2021-04-10 17:39:53.923930: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3000 of size 1024 next 107
2021-04-10 17:39:53.924073: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3400 of size 256 next 176
2021-04-10 17:39:53.924217: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3500 of size 256 next 179
2021-04-10 17:39:53.924360: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3600 of size 256 next 180
2021-04-10 17:39:53.924507: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3700 of size 256 next 181
2021-04-10 17:39:53.924651: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701ee3800 of size 147456 next 182
2021-04-10 17:39:53.924798: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f07800 of size 256 next 183
2021-04-10 17:39:53.924942: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f07900 of size 256 next 184
2021-04-10 17:39:53.925085: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f07a00 of size 256 next 185
2021-04-10 17:39:53.925228: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f07b00 of size 256 next 186
2021-04-10 17:39:53.925371: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f07c00 of size 147456 next 187
2021-04-10 17:39:53.925518: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2bc00 of size 256 next 188
2021-04-10 17:39:53.925661: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2bd00 of size 256 next 189
2021-04-10 17:39:53.925805: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2be00 of size 256 next 190
2021-04-10 17:39:53.925951: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2bf00 of size 256 next 191
2021-04-10 17:39:53.926097: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2c000 of size 512 next 195
2021-04-10 17:39:53.926240: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2c200 of size 512 next 197
2021-04-10 17:39:53.926384: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2c400 of size 512 next 198
2021-04-10 17:39:53.926526: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2c600 of size 512 next 196
2021-04-10 17:39:53.926670: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2c800 of size 512 next 199
2021-04-10 17:39:53.926815: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2ca00 of size 512 next 200
2021-04-10 17:39:53.926958: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2cc00 of size 512 next 201
2021-04-10 17:39:53.927103: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2ce00 of size 512 next 202
2021-04-10 17:39:53.937629: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 701f2d000 of size 32768 next 203
...

2021-04-10 17:39:54.017930: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70b7be800 of size 2048 next 242
2021-04-10 17:39:54.018076: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70b7bf000 of size 2048 next 245
2021-04-10 17:39:54.018222: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70b7bf800 of size 2048 next 243
2021-04-10 17:39:54.018368: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70b7c0000 of size 12845056 next 18446744073709551615
2021-04-10 17:39:54.018542: I tensorflow/core/common_runtime/bfc_allocator.cc:1006] Next region of size 2147483648
2021-04-10 17:39:54.018679: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 70c400000 of size 90624 next 1208
2021-04-10 17:39:54.018828: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c416200 of size 256 next 1440
2021-04-10 17:39:54.018974: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c416300 of size 1792 next 1090
2021-04-10 17:39:54.019120: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c416a00 of size 256 next 413
2021-04-10 17:39:54.019263: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c416b00 of size 1024 next 1093
2021-04-10 17:39:54.019424: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c416f00 of size 1280 next 1402
2021-04-10 17:39:54.019570: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c417400 of size 256 next 1353
2021-04-10 17:39:54.019716: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c417500 of size 256 next 658
2021-04-10 17:39:54.019859: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c417600 of size 256 next 850
2021-04-10 17:39:54.020005: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c417700 of size 256 next 1230
2021-04-10 17:39:54.020151: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 70c417800 of size 88832 next 1179
2021-04-10 17:39:54.020298: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c42d300 of size 110592 next 1037
2021-04-10 17:39:54.020447: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c448300 of size 73728 next 1468
2021-04-10 17:39:54.020595: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c45a300 of size 32768 next 982
2021-04-10 17:39:54.020743: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c462300 of size 37632 next 1070
2021-04-10 17:39:54.020889: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c46b600 of size 51712 next 173
2021-04-10 17:39:54.021034: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c478000 of size 2048 next 244
2021-04-10 17:39:54.021178: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c478800 of size 2048 next 246
2021-04-10 17:39:54.021342: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c479000 of size 2048 next 247
2021-04-10 17:39:54.021491: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c479800 of size 2048 next 248
2021-04-10 17:39:54.021636: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c47a000 of size 524288 next 249
2021-04-10 17:39:54.021861: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c4fa000 of size 2048 next 250
2021-04-10 17:39:54.022007: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c4fa800 of size 2048 next 251
2021-04-10 17:39:54.022159: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c4fb000 of size 2048 next 252
2021-04-10 17:39:54.032860: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c4fb800 of size 2048 next 253
2021-04-10 17:39:54.033218: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70c4fc000 of size 9437184 next 254
2021-04-10 17:39:54.033515: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70cdfc000 of size 2048 next 255
2021-04-10 17:39:54.033825: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70cdfc800 of size 2048 next 256
2021-04-10 17:39:54.034058: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70cdfd000 of size 2048 next 257
2021-04-10 17:39:54.034325: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70cdfd800 of size 2048 next 258
2021-04-10 17:39:54.034524: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70cdfe000 of size 9437184 next 259
2021-04-10 17:39:54.034725: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70d6fe000 of size 2048 next 260
2021-04-10 17:39:54.034923: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70d6fe800 of size 2048 next 261
2021-04-10 17:39:54.035156: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70d6ff000 of size 2048 next 262
2021-04-10 17:39:54.035305: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70d6ff800 of size 2048 next 263
2021-04-10 17:39:54.035454: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 70d700000 of size 25952256 next 1332
2021-04-10 17:39:54.035606: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70efc0000 of size 3538944 next 274
2021-04-10 17:39:54.035757: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 70f320000 of size 5898240 next 285
2021-04-10 17:39:54.035904: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 70f8c0000 of size 35389440 next 926
2021-04-10 17:39:54.036054: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 711a80000 of size 5898240 next 958
2021-04-10 17:39:54.036202: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 712020000 of size 11796480 next 813
2021-04-10 17:39:54.036353: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 712b60000 of size 11796480 next 1110
2021-04-10 17:39:54.036506: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7136a0000 of size 13762560 next 267
2021-04-10 17:39:54.036655: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7143c0000 of size 17694720 next 999
2021-04-10 17:39:54.036806: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7154a0000 of size 13238272 next 537
2021-04-10 17:39:54.036958: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 716140000 of size 14379008 next 1214
2021-04-10 17:39:54.037109: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 716ef6800 of size 18394112 next 326
2021-04-10 17:39:54.037259: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 718081400 of size 11799552 next 717
2021-04-10 17:39:54.037410: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 718bc2000 of size 31457280 next 445
2021-04-10 17:39:54.037561: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 71a9c2000 of size 11796480 next 533
2021-04-10 17:39:54.037714: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 71b502000 of size 19660800 next 561
2021-04-10 17:39:54.037902: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 71c7c2000 of size 15728640 next 1348
2021-04-10 17:39:54.038055: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 71d6c2000 of size 5898240 next 516
... <repeated similar messages> ...

2021-04-10 17:39:54.269981: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 825046000 of size 5898240 next 1227
2021-04-10 17:39:54.270126: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 8255e6000 of size 11796480 next 1042
2021-04-10 17:39:54.270273: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 826126000 of size 11796480 next 768
2021-04-10 17:39:54.270418: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 826c66000 of size 48470016 next 784
2021-04-10 17:39:54.270564: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 829a9f800 of size 5898240 next 582
2021-04-10 17:39:54.270743: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 82a03f800 of size 11796480 next 1150
2021-04-10 17:39:54.270889: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 82ab7f800 of size 11796480 next 5
2021-04-10 17:39:54.271031: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 82b6bf800 of size 5898240 next 1473
2021-04-10 17:39:54.271175: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 82bc5f800 of size 11796480 next 1006
2021-04-10 17:39:54.271320: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 82c79f800 of size 11796480 next 695
2021-04-10 17:39:54.271465: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 82d2df800 of size 77993728 next 18446744073709551615
2021-04-10 17:39:54.271628: I tensorflow/core/common_runtime/bfc_allocator.cc:1031]      Summary of in-use Chunks by size: 
2021-04-10 17:39:54.271811: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 206 Chunks of size 256 totalling 51.5KiB
2021-04-10 17:39:54.271954: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 91 Chunks of size 512 totalling 45.5KiB
2021-04-10 17:39:54.272096: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 13 Chunks of size 768 totalling 9.8KiB
2021-04-10 17:39:54.272237: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 85 Chunks of size 1024 totalling 85.0KiB
2021-04-10 17:39:54.272380: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 12 Chunks of size 1280 totalling 15.0KiB
2021-04-10 17:39:54.272522: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 1536 totalling 6.0KiB
2021-04-10 17:39:54.272711: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 7 Chunks of size 1792 totalling 12.2KiB
2021-04-10 17:39:54.272866: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 79 Chunks of size 2048 totalling 158.0KiB
2021-04-10 17:39:54.273008: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 7 Chunks of size 2304 totalling 15.8KiB
2021-04-10 17:39:54.273148: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 2816 totalling 2.8KiB
2021-04-10 17:39:54.273289: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 2 Chunks of size 3072 totalling 6.0KiB
2021-04-10 17:39:54.273445: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 2 Chunks of size 3328 totalling 6.5KiB
2021-04-10 17:39:54.273596: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 2 Chunks of size 3584 totalling 7.0KiB
2021-04-10 17:39:54.273743: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 3840 totalling 11.2KiB
2021-04-10 17:39:54.273940: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 4608 totalling 18.0KiB
2021-04-10 17:39:54.284435: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 6656 totalling 6.5KiB
2021-04-10 17:39:54.284660: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 9216 totalling 27.0KiB
2021-04-10 17:39:54.284855: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 2 Chunks of size 10752 totalling 21.0KiB
2021-04-10 17:39:54.285052: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 12544 totalling 12.2KiB
2021-04-10 17:39:54.285249: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 18432 totalling 18.0KiB
2021-04-10 17:39:54.285449: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 19712 totalling 19.2KiB
2021-04-10 17:39:54.285648: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 21248 totalling 20.8KiB
2021-04-10 17:39:54.285847: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 28672 totalling 28.0KiB
2021-04-10 17:39:54.286046: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 30720 totalling 30.0KiB
2021-04-10 17:39:54.286241: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 5 Chunks of size 32768 totalling 160.0KiB
2021-04-10 17:39:54.286461: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 37632 totalling 110.2KiB
2021-04-10 17:39:54.286680: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 45568 totalling 44.5KiB
2021-04-10 17:39:54.286883: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 51712 totalling 50.5KiB
2021-04-10 17:39:54.287089: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 73728 totalling 288.0KiB
2021-04-10 17:39:54.287294: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 92160 totalling 90.0KiB
2021-04-10 17:39:54.287501: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 110592 totalling 432.0KiB
2021-04-10 17:39:54.287708: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 122880 totalling 120.0KiB
2021-04-10 17:39:54.287918: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 131072 totalling 512.0KiB
2021-04-10 17:39:54.288126: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 132608 totalling 129.5KiB
2021-04-10 17:39:54.288329: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 142336 totalling 139.0KiB
2021-04-10 17:39:54.288534: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 16 Chunks of size 147456 totalling 2.25MiB
2021-04-10 17:39:54.288737: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 162304 totalling 158.5KiB
2021-04-10 17:39:54.288945: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 162560 totalling 158.8KiB
2021-04-10 17:39:54.289151: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 168960 totalling 165.0KiB
2021-04-10 17:39:54.289388: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 175616 totalling 171.5KiB
2021-04-10 17:39:54.289689: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 196608 totalling 192.0KiB
2021-04-10 17:39:54.289999: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 9 Chunks of size 294912 totalling 2.53MiB
2021-04-10 17:39:54.290229: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 338176 totalling 330.2KiB
2021-04-10 17:39:54.290430: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 368640 totalling 1.05MiB
2021-04-10 17:39:54.290626: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 406272 totalling 

... <repeated for many times>...

2021-04-10 17:39:54.320661: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 21073920 totalling 20.10MiB
2021-04-10 17:39:54.320857: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 6 Chunks of size 21626880 totalling 123.75MiB
2021-04-10 17:39:54.321054: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 21680128 totalling 20.68MiB
2021-04-10 17:39:54.321248: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 21683200 totalling 62.04MiB
2021-04-10 17:39:54.321503: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 31457280 totalling 30.00MiB
2021-04-10 17:39:54.321696: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 33423360 totalling 31.88MiB
2021-04-10 17:39:54.321888: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 48470016 totalling 46.22MiB
2021-04-10 17:39:54.322079: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 77993728 totalling 74.38MiB
2021-04-10 17:39:54.322271: I tensorflow/core/common_runtime/bfc_allocator.cc:1038] Sum Total of in-use chunks: 3.87GiB
2021-04-10 17:39:54.322455: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] total_region_allocated_bytes_: 5060693760 memory_limit_: 5060693856 available bytes: 96 curr_region_allocation_bytes_: 4294967296
2021-04-10 17:39:54.322766: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Stats: 
Limit:                      5060693856
InUse:                      4152633344
MaxInUse:                   5019460608
NumAllocs:                       91245
MaxAllocSize:               2325741568
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-04-10 17:39:54.323282: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ****************************************************************************************************
2021-04-10 17:39:54.323508: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_grad_input_ops.cc:1103 : Resource exhausted: OOM when allocating tensor with shape[4,96,98,322] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Epoch1/2:  52%|█████▏    | 27/52 [00:41<00:38,  1.53s/it]
Traceback (most recent call last):
  File ""D:/MA/Recources/monodepth2_tf2/new_trainer.py"", line 549, in <module>
  File ""D:/MA/Recources/monodepth2_tf2/new_trainer.py"", line 481, in train
    
  File ""D:/MA/Recources/monodepth2_tf2/new_trainer.py"", line 438, in run_epoch
    self.optimizer.apply_gradients(zip(grads, trainable_weights_all))
  File ""D:/MA/Recources/monodepth2_tf2/new_trainer.py"", line 458, in grad
    
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\backprop.py"", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\eager\backprop.py"", line 162, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\nn_grad.py"", line 596, in _Conv2DGrad
    data_format=data_format),
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1255, in conv2d_backprop_input
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\Dexxh\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[4,96,98,322] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]

Process finished with exit code 1

```"
48455,Failed to compile on Linux x86_64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.10
- TensorFlow installed from (source or binary):source
- TensorFlow version:master
- Python version:3.8.5
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):10.2.0
- CUDA/cuDNN version:CUDA 11.2.2.1 cuDNN 8.1.1.33
- GPU model and memory:Nvidia RTX2060M 6G



**Describe the problem**
Failed to compile on Linux x86_64 .
Configure :
```
❯ ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]: 


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```
Build command : `bazel build //tensorflow/tools/pip_package:build_pip_package --config=opt --config=cuda --config=mkl -j 6 --config=noaws`
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Error logs :
[error.txt](https://github.com/tensorflow/tensorflow/files/6289961/error.txt)




"
48454,Support for convolutional ops for complex variable inputs,"**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I hope to add support for complex variable optimization to Tensorflow. I have made a custom optimizer that is capable of C->C domain convergence. Currently, when I pass a complex tensor to tf.nn.convolution, I get a NotOkStatusException (complex64 not supported), but it doesn't say where the code is failing. According to the documentation, the Convolution class is simply an implementation of Cross-Correlation rather than a convolution, but even NumPy's correlate (and convolve) supports complex inputs. I do not see where the code is failing exactly and why?

**Will this change the current api? How?**
This will add support for complex64 and complex128 datatypes to some ops involved in convolution operations

**Who will benefit with this feature?**
Any signal processing specialist would be able to use complex inputs directly rather than separating and stacking the real and imaginary components separately. Pytorch supports complex domain optimizations, so doing the same for Tensorflow would be beneficial for the community.
"
48453,Fail to build on Linux aarch64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu devel 21.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):source
- TensorFlow version:master 
- Python version:3.9.4
- Installed using virtualenv? pip? conda?:N/A
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):GCC 10.2.1
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A



**Describe the problem**

I want to build tensorflow on Linux aarch64 platform , and it fails .
Commands : 
`./configure `
`bazel build //tensorflow/tools/pip_package:build_pip_package`
Output : 
[![asciicast](https://asciinema.org/a/KsEIoMqtDsDvInf0fbPTxBCBs.svg)](https://asciinema.org/a/KsEIoMqtDsDvInf0fbPTxBCBs)
Error message : 
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/local_config_cc/BUILD:47:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'aarch64'

Command : `bazel build //tensorflow/tools/pip_package:build_pip_package --toolchain_resolution_debug`
Output : [![asciicast](https://asciinema.org/a/hok5YFKqreDYj7X5pLFDIsbVQ.svg)](https://asciinema.org/a/hok5YFKqreDYj7X5pLFDIsbVQ)

Logs : 
`INFO: ToolchainResolution: Target platform @local_config_platform//:host: Selected execution platform @local_execution_config_platform//:platform,                    INFO: ToolchainResolution: Target platform @local_config_platform//:host: Selected execution platform @local_execution_config_platform//:platform,                    INFO: ToolchainResolution:     Type @bazel_tools//tools/cpp:toolchain_type: target @local_config_platform//:host: Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a; mismatching values: arm, android                                        INFO: ToolchainResolution:   Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_config_platform//:host: execution @local_execution_config_platform//:platform: Selected toolchain @local_config_cc//:cc-compiler-piii                   INFO: ToolchainResolution:   Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_config_platform//:host: execution @local_config_platform//:host: Selected toolchain @local_config_cc//:cc-compiler-piii                                 INFO: ToolchainResolution:     Type @bazel_tools//tools/cpp:toolchain_type: target @local_config_platform//:host: Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a; mismatching values: arm, android                                        INFO: ToolchainResolution: Target platform @local_config_platform//:host: Selected execution platform @local_execution_config_platform//:platform, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-piii          INFO: ToolchainResolution:     Type @bazel_tools//tools/python:toolchain_type: target @local_config_platform//:host: Rejected toolchain @local_execution_config_python//:py_runtime_pair; mismatching values: platform_constraint                        INFO: ToolchainResolution:   Type @bazel_tools//tools/python:toolchain_type: target platform @local_config_platform//:host: execution @local_execution_config_platform//:platform: Selected toolchain @local_config_python//:py_runtime_pair             INFO: ToolchainResolution:   Type @bazel_tools//tools/python:toolchain_type: target platform @local_config_platform//:host: execution @local_config_platform//:host: Selected toolchain @local_config_python//:py_runtime_pair                           INFO: ToolchainResolution:     Type @bazel_tools//tools/cpp:toolchain_type: target @local_config_platform//:host: Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a; mismatching values: arm, android                                        INFO: ToolchainResolution:   Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_config_platform//:host: execution @local_execution_config_platform//:platform: Selected toolchain @local_config_cc//:cc-compiler-piii                   INFO: ToolchainResolution:   Type @bazel_tools//tools/cpp:toolchain_type: target platform @local_config_platform//:host: execution @local_config_platform//:host: Selected toolchain @local_config_cc//:cc-compiler-piii                                 INFO: ToolchainResolution:     Type @bazel_tools//tools/cpp:toolchain_type: target @local_config_platform//:host: Rejected toolchain @local_config_cc//:cc-compiler-armeabi-v7a; mismatching values: arm, android                                        INFO: ToolchainResolution: Target platform @local_config_platform//:host: Selected execution platform @local_execution_config_platform//:platform, type @bazel_tools//tools/cpp:toolchain_type -> toolchain @local_config_cc//:cc-compiler-piii, type @bazel_tools//tools/python:toolchain_type -> toolchain @local_config_python//:py_runtime_pair                                                                            INFO: ToolchainResolution: Removed execution platform @local_config_platform//:host from available execution platforms, it is missing constraint @local_execution_config_platform//:platform_constraint                                                  INFO: ToolchainResolution: Target platform @local_config_platform//:host: Selected execution platform @local_execution_config_platform//:platform,                    INFO: ToolchainResolution: Removed execution platform @local_config_platform//:host from available execution platforms, it is missing constraint @local_execution_config_platform//:platform_constraint`

P.S. Python 3.9.4 is the default and minimal version available in current Ubuntu repo , and I can't add the older repos or the system will be corrupted . "
48452,Manipulate Input/Output of TF model ,"### 1. System information

- Mac OS 11.2.3
- TensorFlow 2.4.1 from COLAB
- Android 10
- MLKit by Google object-detection-custom:16.3.1

### 2. The problem 

I need to manipulate TensorFlow models to make them compatible with MLKit by Google for my Android Application.
A TFlite model is compatible with MLKit if the Tensors are like that:
INPUT
- The model must have only one input tensor with the following constraints:
- The data is in RGB pixel format.
- The data is UINT8 or FLOAT32 type. If the input tensor type is FLOAT32, it must specify the NormalizationOptions by attaching Metadata.
- The tensor has 4 dimensions : BxHxWxC, where: B is the batch size. It must be 1 (inference on larger batches is not supported). W and H are the input width and height. C is the number of expected channels. It must be 3.

OUTPUT
- The model must have at least one output tensor with N classes and either 2 or 4 dimensions: (1xN) , (1x1x1xN)

With this TFlite model [ssd_mobilenet_v1](https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2)
Currently when I use a model with different predicted output (The problem is mainly in the output), I have the this error:
` com.google.android.gms.tasks.RuntimeExecutionException: com.google.mlkit.common.MlKitException: Failed to initialize detector. Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).
` 

I'm new to TensorFlow but I know that TF models can be converted using the TFlite converter but I don't know how to use for manipulating the Input/Ouput. Can someone explain me how to interpret the commands to give to the converter?

#### Other Infos

TF model to convert: [SSD MobileNetV2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2). From all the outputs I need only this: ""detection_scores: a tf.float32 tensor of shape [N] containing detection scores.""
"
48451,Warning when using a TFAutoModelWithLMHead logit outputs in the graph mode,"I've built a translation model following [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention). The overall structure of my code is the same, although I added GPT in the decoder function to get help from this language model and create better translations. Right now this is how I import the model:
```
gpt_model = TFAutoModelWithLMHead.from_pretrained(model_name_or_path,
                                                  output_attentions=False,
                                                  return_dict=False,
                                                  output_hidden_states=False,
                                                  use_cache=False
                                                  )

gpt_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
```

and in the dencoder function, I give one input_id to the GPT model, and output the logits in the graph mode:

`gpt_model(input_id)[0][:, -1, :]`

 Right now my question is why do I get the following warning and how can I silence them properly when training in graph mode?
```
epoch  0
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f90975dfde0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```

and the following error:
```
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
```

The second warning doesn't seem consistent as I have already set all these values to False in the model config. I also tried decorating the model in the following way and then using it, but it didn't work either. (I don't know if it's even correct to do so)

`gpt_model = tf.autograph.experimental.do_not_convert(gpt_model)
`

I'm running all the code in Colab with GPU mode on.

Thanks in advance"
48450,Tensorflow is extremely slow during startup,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, x86_64, 5.10.28-1-lts
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary, from conda
- TensorFlow version (use command below): Tensorflow 2.4.1
- Python version: Python 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cudatoolkit 10.1.243, cudnn 7.6.5 (both from conda); cuda 11.2.2 (from Arch pacman, system-wide)
- GPU model and memory: Nvidia GTX 3080, 10018 MiB

**Describe the current behavior**

I just write some code to train a simple model, and the Tensorflow loading time is too long. Here are some log:

```
>>> python Train.py
2021-04-10 14:16:40.493971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-10 14:16:46.296137: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-10 14:16:46.306974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-10 14:16:46.522294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:73:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-04-10 14:16:46.522374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-10 14:16:46.558851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-04-10 14:16:46.558999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-04-10 14:16:46.578194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-10 14:16:46.582360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-10 14:16:46.612937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-10 14:16:46.618842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-04-10 14:16:46.673830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-04-10 14:16:46.676207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-10 14:16:46.678545: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-10 14:16:46.689923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:73:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-04-10 14:16:46.689992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-10 14:16:46.690048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-04-10 14:16:46.690069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-04-10 14:16:46.690089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-10 14:16:46.690108: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-10 14:16:46.690129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-10 14:16:46.690149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-04-10 14:16:46.690169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-04-10 14:16:46.691482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-04-10 14:16:46.691918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
```
I would like to draw your attention to the fact that there is **a seven-minute gap** between the above and the following logs (they are continuously output, I did not intercept them)
```
2021-04-10 14:23:02.128175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-10 14:23:02.128248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-04-10 14:23:02.128266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-04-10 14:23:02.130070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9037 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:73:00.0, compute capability: 8.6)
2021-04-10 14:23:02.131846: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-10 14:23:03.051975: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-10 14:23:03.052030: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-10 14:23:03.052589: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2021-04-10 14:23:03.058752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.10.1
2021-04-10 14:23:03.159864: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
2021-04-10 14:23:03.159975: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-04-10 14:23:03.345523: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-10 14:23:03.376433: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
Epoch 1/10
2021-04-10 14:23:07.935855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-04-10 14:24:36.693386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
  1/100 [..............................] - ETA: 26:48:40 - loss: 0.0891 - accuracy: 0.2930
```
And here, another 15 minutes log gap
```
2021-04-10 14:39:18.561866: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-04-10 14:39:18.561893: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-04-10 14:39:18.562690: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
  2/100 [..............................] - ETA: 39s - loss: 0.0836 - accuracy: 0.3271     2021-04-10 14:39:18.780340: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-04-10 14:39:18.784724: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-04-10 14:39:18.786618: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-04-10 14:39:18.790988: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18
2021-04-10 14:39:18.792157: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.trace.json.gz
2021-04-10 14:39:18.825724: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18
2021-04-10 14:39:18.829485: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.memory_profile.json.gz
2021-04-10 14:39:18.829700: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18Dumped tool data for xplane.pb to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.xplane.pb
Dumped tool data for overview_page.pb to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.overview_page.pb
Dumped tool data for input_pipeline.pb to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to ./logs/20210410-142303/train/plugins/profile/2021_04_10_14_39_18/GuServer.kernel_stats.pb

100/100 [==============================] - 999s 243ms/step - loss: 0.0518 - accuracy: 0.5508 - val_loss: 0.0583 - val_accuracy: 0.4046
```
and start here, everything goes well. In short, the Tensorflow need about 23 minutes to startup and work, and I'm not sure what went wrong.

**Standalone code to reproduce the issue**
I believe this problem is not code-specific. I try to run the [example code provided on tensorflow official website](https://www.tensorflow.org/tutorials/quickstart/beginner), and the same problem occurs. So I think this problem can be reproduced with any code which using tensorflow on my computer.
"
48449,Can not convert a tf model to tf.lite format,"tf version: 2.4.1

Here is my model constrution:


```
class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()          
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=""relu""), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

# https://tfhub.dev/tensorflow/albert_en_preprocess/3
preprocessor_file = ""./albert_en_preprocess_3""
preprocessor_layer = hub.KerasLayer(preprocessor_file)


def get_model_transormer(num_classes):
    embed_dim = 32  # Embedding size for each token
    num_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer
    
    preprocessor = hub.load(preprocessor_file)
    vocab_size = preprocessor.tokenize.get_special_tokens_dict()['vocab_size'].numpy()

    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) 

    encoder_inputs = preprocessor_layer(text_input)['input_word_ids']

    embedding_layer = TokenAndPositionEmbedding(encoder_inputs.shape[1], vocab_size, embed_dim)
    x = embedding_layer(encoder_inputs)
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(20, activation=""relu"")(x)
    x = layers.Dropout(0.1)(x)
    outputs = layers.Dense(num_classes, activation=""softmax"")(x)

    #outputs = layers.Dense(1, activation=""sigmoid"")(x)
    model = keras.Model(inputs=text_input, outputs=outputs)

    model.compile(""adam"", ""categorical_crossentropy"", metrics=[""acc""])
    #model.compile(""adam"", ""binary_crossentropy"", metrics=[""accuracy""])
    return model

model = get_model_transormer(4)
model.save('model_charl')
```

After saving the model I want to convert it to tf.lite model:

```
converter = tf.lite.TFLiteConverter.from_saved_model('./model_charl')
#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```


This is the error message:


> 2021-04-10 04:20:30.488392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
> 2021-04-10 04:20:30.488415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
> loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): error: requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
> loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
> ---------------------------------------------------------------------------
> Exception                                 Traceback (most recent call last)
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
>     212                                                  debug_info_str,
> --> 213                                                  enable_mlir_converter)
>     214       return model_str
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
>      37       debug_info_str,
> ---> 38       enable_mlir_converter)
>      39
> 
> Exception: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
> <unknown>:0: note: loc(""StatefulPartitionedCall_1""): called from
> <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
> <unknown>:0: note: loc(""StatefulPartitionedCall_1""): called from
> 
> 
> During handling of the above exception, another exception occurred:
> 
> ConverterError                            Traceback (most recent call last)
> <ipython-input-4-def2ce8a009f> in <module>
>       2 #converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
>       3 converter.optimizations = [tf.lite.Optimize.DEFAULT]
> ----> 4 tflite_quant_model = converter.convert()
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
>     737     converter_kwargs.update(quant_mode.converter_flags())
>     738
> --> 739     result = _convert_saved_model(**converter_kwargs)
>     740     calibrate_and_quantize, flags = quant_mode.quantizer_flags()
>     741     if calibrate_and_quantize:
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)
>     635       None,  # input_data, unused
>     636       None,  # debug_info_str, unused
> --> 637       enable_mlir_converter=True)
>     638   return data
>     639
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
>     214       return model_str
>     215     except Exception as e:
> --> 216       raise ConverterError(str(e))
>     217
>     218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:
> 
> ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
> <unknown>:0: note: loc(""StatefulPartitionedCall_1""): called from
> <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_4831"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_4887"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_4897"") at ""StatefulPartitionedCall@__inference_restored_function_body_12076"") at ""model/keras_layer/StatefulPartitionedCall@__inference__wrapped_model_12243"") at ""StatefulPartitionedCall@__inference_signature_wrapper_13370"") at ""StatefulPartitionedCall_1"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
> <unknown>:0: note: loc(""StatefulPartitionedCall_1""): called from"
48448,Is there a way to set training = True after the model is trained?,"There are similar questions that I have it now, such as this: [#36936](https://github.com/tensorflow/tensorflow/issues/36936).

I did not find a solution or answer.

I have a model that have Batch Normalization and Dropouts. As a result, I always have to set training = True to perform image segmentation, etc. For instance;

```
import tensorflow as tf
my_model = tf.keras.models.load_model(""model"")
result = my_model(inputImage, training = True)

```

If I donot provide training = True, the result.numpy() is nan values.

In addition in Python, I want to use this in tensorflow/java. As a result, I donot know how to provide training = True in tensorflow java and I opened a new issue for tensorflow/java [#284](https://github.com/tensorflow/java/issues/284) for this question as well.

I wonder if there is a way to hack or set the trained_model such that, it does not require` training = True `anymore? I thought if I can do this in Python and save the model again, I may not need it in tensorflow/java again.



"
48439,MultiWorkerMirroredStrategy:tensorflow.python.framework.errors_impl.InternalError: x root error(s) found.,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:source
-   **TensorFlow version (use command below)**:2.3.3
-   **Python version**:3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:7.3.1
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
tensorflow.python.framework.errors_impl.InternalError: 5 root error(s) found.
  (0) Internal:  Missing 1-th output from node replica_3/sequential/dropout/dropout/Dropout (defined at /threading.py:916) 
	 [[GroupCrossDeviceControlEdges_1/Identity_7/_203]]
  (1) Internal:  Missing 1-th output from node replica_3/sequential/dropout/dropout/Dropout (defined at /threading.py:916) 
  (2) Internal:  Missing 1-th output from node replica_3/sequential/dropout/dropout/Dropout (defined at /threading.py:916) 
	 [[GroupCrossDeviceControlEdges_0/SGD/SGD/update_1_1/Const/_147]]
  (3) Internal:  Missing 1-th output from node replica_3/sequential/dropout/dropout/Dropout (defined at /threading.py:916) 
	 [[GroupCrossDeviceControlEdges_2/SGD/SGD/update_0/Const/_163]]
  (4) Internal:  Missing 1-th output from node replica_3/sequential/dropout/dropout/Dropout (defined at /threading.py:916) 
	 [[div_no_nan_1/_119]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_7311]

Function call stack:
train_function -> train_function -> train_function -> train_function -> train_function

The above is the error message.I don't know the cause of it.

### Source code / logs
import json
import math
import os
import sys
import numpy as np
import unet

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0, 1, 2, 3""
os.environ.pop('TF_CONFIG', None)
import tensorflow as tf

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

def build_and_compile_cnn_model():
    model = unet.build()
    model.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-9))
    return model

def _is_chief(task_type, task_id):
    # Note: there are two possible TF_CONFIG configuration.
    # 1) In addition to worker tasks, a chief task type is use;
    # in this case, this function should be modified to
    # return task_type == 'chief'.
    # 2) Only worker task type is used; in this case, worker 0 is
    # regarded as the chief. The implementation demonstrated here
    # is for this case.
    # For the purpose of this colab section, we also add task_type is None
    # case because it is effectively run with only single worker.
    return (task_type == 'worker' and task_id == 0) or task_type is None

def get_temp_dir(dirpath, task_id):
    base_dirpath = 'workertemp' + str(task_id)
    temp_dir = os.path.join(dirpath, base_dirpath)
    tf.io.gfile.makedirs(temp_dir)
    return temp_dir

def write_filepath(filepath, task_type, task_id):
    dirpath = os.path.dirname(filepath)
    base = os.path.basename(filepath)
    if not _is_chief(task_type, task_id):
        dirpath = get_temp_dir(dirpath, task_id)
    return os.path.join(dirpath, base)

if __name__ == '__main__':
    # I use default TFConfigClusterResolver,and set the environment variable below
    # In total two processes are opened and 'index' of the other one is '1'
    # like this
    tf_config = {
        'cluster': {'worker': ['10.1.11.60:15536', '10.1.11.59:19203']},
        'task': {'type': 'worker', 'index': 0}
    }
    os.environ['TF_CONFIG'] = json.dumps(tf_config)

    batch_size = 4

    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    with strategy.scope():
        test_model = build_and_compile_cnn_model()
    # I use handwriting dataset
    train_data = np.load('train_data.npy')
    train_label = np.load('train_label.npy')
    # train_data shape is (60000, 28, 28, 1)
    # Only use the first 1000 pictures to train the model
    train_data = train_data[0:1000]
    train_label = train_label[0:1000]
    len_data = len(train_data)
    len_label = len(train_label)
    
    # Abandon partial batch
    steps_per_epoch = math.floor(len_data / batch_size)
    
    # Set save path just like tutorial
    checkpoint_dir = ""./ckpt""
    checkpoint_name = ""./ckpt/chief_ckpt""
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    
    tf_config = json.loads(os.environ['TF_CONFIG'])
    task_type = tf_config['task']['type']
    task_rank = tf_config['task']['index']
    write_model_path = write_filepath(checkpoint_name, task_type, task_rank)
    
    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=write_model_path + '_{epoch:04d}.h5',
                                                    save_best_only=False,
                                                    save_weights_only=True,
                                                    save_freq='epoch')]
    
    # Model loads latest weights
    checkpoints = [checkpoint_dir + '/' + name for name in os.listdir(checkpoint_dir) if name.endswith('.h5')]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=os.path.getctime)
        test_model.load_weights(latest_checkpoint)
    
    test_model.fit(x=train_data,
                   y=train_label,
                   batch_size=batch_size,
                   epochs=3,
                   steps_per_epoch=steps_per_epoch,
                   callbacks=callbacks,
                   verbose=1 if task_rank == 0 else 0)
    
unet.py:
from tensorflow.keras import layers
from tensorflow.keras.layers import Reshape
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras import *
import tensorflow as tf

def build():
    model = Sequential()
    model.add(layers.Conv2D(14, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv1"", input_shape=(28, 28, 1)))
    model.add(layers.MaxPool2D(pool_size=(2, 2), name=""pool1""))
    model.add(layers.Conv2D(28, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv2_1""))
    model.add(layers.Conv2D(28, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv2_2""))
    model.add(layers.MaxPool2D(pool_size=(2, 2), name=""pool2""))
    model.add(layers.Conv2D(56, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv3_1""))
    model.add(layers.Conv2D(56, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv3_2""))
    model.add(layers.MaxPool2D(pool_size=(2, 2), name=""pool3""))
    model.add(layers.Conv2D(112, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv4_1""))
    model.add(layers.Conv2D(112, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv4_2""))
    model.add(layers.Dropout(0.5))
    model.add(layers.MaxPool2D(pool_size=(2, 2), name=""pool4""))
    model.add(layers.Conv2D(224, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv5_1""))
    model.add(layers.Conv2D(224, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv5_2""))
    model.add(layers.Dropout(0.5))
    model.add(layers.Conv2DTranspose(112, 2, padding='same',activation='relu',kernel_initializer='he_normal',name='up6'))
    model.add(layers.Conv2D(112, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv6_1""))
    model.add(layers.Conv2D(112, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv6_2""))
    model.add(layers.Conv2DTranspose(256,2 , padding='same',activation='relu',kernel_initializer='he_normal',name='up7'))
    model.add(layers.Conv2D(56, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv7_1""))
    model.add(layers.Conv2D(56, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv7_2""))
    model.add(layers.Conv2DTranspose(28, 2, padding='same',activation='relu',kernel_initializer='he_normal', name='up8'))
    model.add(layers.Conv2D(28, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv8_1""))
    model.add(layers.Conv2D(28, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv8_2""))
    model.add(layers.Conv2DTranspose(64,2 , padding='same',activation='relu',kernel_initializer='he_normal',name='up9'))
    model.add(layers.Conv2D(14, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv9_1""))
    model.add(layers.Conv2D(14, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv9_2""))
    model.add(layers.Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal',
    name=""conv9_3""))
    model.add(layers.Conv2D(2, 1, activation='sigmoid', padding='same', kernel_initializer='he_normal',
    name=""conv10""))
    model.add(layers.Flatten())
    model.add(layers.Dense(10, activation='softmax'))
    model.add(Reshape((10,)))
    return model"
48438,Cannot build Tensorflow with Cuda,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5
- Python version: 3.9
- Installed using virtualenv? pip? conda?: venv + pip
- Bazel version (if compiling from source):  3.7.2
- GCC/Compiler version (if compiling from source): gcc version 10.2.1 20210110 (Debian 10.2.1-6)
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla T4

**Describe the problem**

```
Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.2


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.1.1


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/include/linux/,/home/admin/cuda/include/


Traceback (most recent call last):
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 653, in <module>
    main()
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 645, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 583, in find_cuda_config
    result.update(_find_cuda_config(cuda_paths, cuda_version))
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 255, in _find_cuda_config
    cuda_header_path, header_version = _find_header(base_paths, ""cuda.h"",
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 243, in _find_header
    return _find_versioned_file(base_paths, _header_paths(), header_name,
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 233, in _find_versioned_file
    actual_version = get_version(file)
  File ""/home/admin/tensorflow/third_party/gpus/find_cuda_config.py"", line 250, in get_header_version
    version = int(_get_header_version(path, ""CUDA_VERSION""))
ValueError: invalid literal for int() with base 10: ''
Asking for detailed CUDA configuration...
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
(venv) admin@ip-172-30-0-223:~/tensorflow$ ./configure
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/admin/venv/bin/python3]:


Found possible Python library paths:
  /home/admin/venv/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/home/admin/venv/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Could not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
        'local/cuda/extras/CUPTI/include'
of:
        '/lib'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda/targets/x86_64-linux/lib'

Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:
```

What is the correct way to specify CUDA + cuDNN versions and paths? Thank you in advance.
"
48436,Converting training loop to compilable Keras model,"I was recently going through this TF tutorial: https://www.tensorflow.org/tutorials/text/nmt_with_attention
 How do I convert the [training loop](https://www.tensorflow.org/tutorials/text/nmt_with_attention#training) in this tutorial to a Keras model which I can then compile using model.compile?

It is especially the variable input lengths and for loop that it is making this challenging "
48435,new soc_id support for hexagon delegate,"We are developing products based on the Qualcomm Snapdragon 865 and have been using the hexagon delegate to use the dsp on the SXR2130 variant with soc_id 356. Recently Qualcomm has migrated us to the QCS8250 variant of the 865, exact same processor, just differentiated by longer term support in the IOT group. This part has soc_id 481 which appears to not be recognized by the hexagon delegate. It looks like there needs to be an entry for 481 in the SocSkelTable that matches 356.

How does that happen? Does Qualcomm notify Google of new soc_id's and how does a new release happen? We are also working this through our Qualcomm contacts. If this is already known and a release is going to happen please provide any timetable information for a new release.

Thank you.

Jay"
48432,can not converting my model .h5 to .tflite and giving me this error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
48431,Didn't find op for builtin opcode 'SUM' version '1'.,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Linux Ubuntu 18.04:
- TensorFlow installed from binary
- Tensorflow version : 2.3
- Target platform: ARM 64

**Describe the problem**
In the following lines I proceed to describe the steps followed:

1. Adapt MobileNetv2 model. Loaded for training with Keras API. Take out the last layer by setting include_top = false, and adding a customized last Conv2D layer with the Functional API. This is done in order to apply the convolutional sliding window approach. (Bigger input image, than the images used for training).
2. Training done with success. Translation of the model from TF to TF Lite and it runs and provides a sensible matrix of results.
3. Translation of TFLite model to TF Tiny model (model.cc) by using the xxd -i ... command provided in the documentation.
4. When allocating the model, we encounter the following failure:

> libraries ready
> STM32 Tensorflow Lite test
> Model working
> Interpreter working
> Didn't find op for builtin opcode 'SUM' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?
> 
> Failed to get registration from op code SUM
>  
> Failed starting model allocation.
> 
> Allocate working
> Type input: 1
> Bytes input: 1310720
> Size input: 4
> Dim input 0: 1
> Dim input 1: 512
> Dim input 2: 640
> Dim input 3: 1

As seen in the message, dims are correct, type input is correct (Float) and everything else seems to be working fine.

Prior to this issue, a similar fault appeared as it requested to have the op code EXP. This one is already available in Tensorflow Git and it has been already implemented. (exp.cc, exp_test.cc, exp.h, AddEpx() included in AllOps file and make file modified).

The same procedure has been handled for the MobileNetV2 with no modifications and it worked fine. 

I have two main questions: 

- Why do we need extra operators, if we only added one extra Conv2D layer?
- Is SUM implemented, and will we have further operators needed?

**Please provide the exact sequence of commands/steps when you ran into the problem**

Model configuration in python (Tensorflow + Keras API):
`            model = MobileNetV2(include_top=False,weights=None,input_tensor=Input(shape = (512,640,channels) ,dtype = 'float32'),pooling = None,classes = len(class_labels))`
`            last = Conv2D(filters = 5,kernel_size = 3, padding= 'valid',strides=(1,1),activation='softmax', input_shape = (model.layers[-1].output.shape))(model.layers[-1].output)`
 `           model = Model(model.input, last)`

Model Conversion from Tensorflow to Tensorflow Lite:
`    converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)`
 `   converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`
`    model_no_quant_tflite = converter.convert()`
Model Conversion from Tensorflow Lite to Tensorflow Tiny:
`xxd -i model_mobilenet_sliding2.tflite > model.cc
`

Model loading in Tensorflow Tiny (C++):

`namespace{
  tflite::ErrorReporter* error_reporter = nullptr;`
  `const tflite::Model* model = nullptr;`
  `// This pulls in all the operation implementations we need`
  `tflite::AllOpsResolver resolver;`
  `constexpr int kTensorArenaSize = 4 * 1024 * 1024;`
  `uint8_t tensor_arena[kTensorArenaSize];` 
   `uint8_t* img = nullptr;`
  `uint64_t timeImage;`
  `uint64_t timeInvoke;`
  `TfLiteTensor* model_input = nullptr;`
  `TfLiteTensor* model_output = nullptr;
}`

`// Set up logging (modify tensorflow/lite/micro/debug_log.cc)
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &micro_error_reporter;`

`  // Say something to test error reporter
  error_reporter->Report(""STM32 Tensorflow Lite test"");`

`  model = ::tflite::GetModel(model_mobilenet_sliding2_tflite);
  cout << ""Model working""<<endl;`

`  tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,
                                      kTensorArenaSize, &micro_error_reporter);`

  `cout << ""Interpreter working"" << endl;`

 ` interpreter.AllocateTensors();`

 ` cout << ""Allocate working"" << endl;`

`model_input = interpreter.input(0);`

`// Get image from provider.`
    `cout << ""Type input: "" << model_input->type << endl;`
   ` cout << ""Bytes input: "" << model_input->bytes << endl;`
    `cout << ""Size input: "" << model_input->dims->size << endl;`
    `cout << ""Dim input 0: "" << model_input->dims->data[0] <<  endl;`
    `cout << ""Dim input 1: "" <<model_input->dims->data[1] <<  endl;`
    `cout << ""Dim input 2: "" <<model_input->dims->data[2] <<  endl;`
    `cout << ""Dim input 3: "" <<model_input->dims->data[3] <<  endl;`
"
48430,tf.nn.embedding_lookup gradient shape is no longer fully defined in 2.5.0rc0,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: `2.5.0rc0`
- Python version: 3.6.9

**Describe the current behavior**

In TensorFlow 2.5.0rc0, the dense shape of the `tf.nn.embedding_lookup` gradient is no longer fully defined. The shape is `(None, None)` instead of the static shape of the embedding variable.

 The same code worked in previous TensorFlow versions.

**Describe the expected behavior**

The `shape` attribute of the sparse gradient should be the fully defined shape of the embedding variable.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

class MyLayer(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.embedding = tf.Variable(tf.random.uniform([50, 16]))

    def call(self, x):
        return tf.nn.embedding_lookup(self.embedding, x)

layer = MyLayer()

@tf.function
def _run(x):
    with tf.GradientTape() as tape:
        y = layer(x)
        loss = tf.math.reduce_sum(y)
    gradients = tape.gradient(loss, layer.weights)
    print(""Gradient shape:"", gradients[0].shape)

_run(tf.random.uniform([4, 16], minval=0, maxval=50, dtype=tf.int64))
```

**Other info / logs**

Output with 2.5.0rc0 (incorrect):

```text
Gradient shape: (None, None)
```

Output with 2.4.1 (correct):

```text
Gradient shape: (50, 16)
```"
48429,tf.data.experimental.enable_debug_mode() doesn't enable breakpoint,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Desktop
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.12.1-53831-ga8b6d5ff93a 2.5.0-rc0
- Python version: Python 3.6.8 
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU

**Describe the current behavior**

```
import tensorflow as tf

tf.data.experimental.enable_debug_mode()
tf.config.run_functions_eagerly(True)


def func(x):
    x = x + 1  # BREAKPOINT HERE
    return x


dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset = dataset.map(func)
# dataset = dataset.map(lambda x: tf.py_function(func, [x], tf.int32))  # doesn't work either
for item in dataset:
    print(item)

```

The breakpoint is not hit. (using pycharm professional 2020.3 )

**Describe the expected behavior**

I expect that the breakpoint is hit 
"
48428,model.fit() raises exception,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: cuda 10
- GPU model and memory: 12GB

**Describe the current behavior**

model.fit() fails with following error
```text
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/backprop.py in _num_elements(grad)
    615   if isinstance(grad, ops.IndexedSlices):
--> 616     return functools.reduce(operator.mul, grad.values._shape_tuple(), 1)  # pylint: disable=protected-access
    617   raise ValueError(""`grad` not a Tensor or IndexedSlices."")

TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'

The above exception was the direct cause of the following exception:

SystemError                               Traceback (most recent call last)
23 frames
<ipython-input-3-3050b60a914a> in <module>()
     87 
     88 siamese_model.summary()
---> 89 siamese_model.fit(data_gen, epochs=1)

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     71     strategy = distribution_strategy_context.get_strategy()
     72     outputs = strategy.experimental_run_v2(
---> 73         per_replica_function, args=(model, x, y, sample_weights))
     74     # Out of PerReplica outputs reduce or pick values to return.
     75     all_outputs = dist_utils.unwrap_output_dict(

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    759                                 convert_by_default=False)
--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    761 
    762   def reduce(self, reduce_op, value, axis):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1785       kwargs = {}
   1786     with self._container_strategy().scope():
-> 1787       return self._call_for_each_replica(fn, args, kwargs)
   1788 
   1789   def _call_for_each_replica(self, fn, args, kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2130         self._container_strategy(),
   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2132       return fn(*args, **kwargs)
   2133 
   2134   def _reduce_to(self, reduce_op, value, destinations):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
    262       y,
    263       sample_weights=sample_weights,
--> 264       output_loss_metrics=model._output_loss_metrics)
    265 
    266   if reset_metrics:

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)
    309           sample_weights=sample_weights,
    310           training=True,
--> 311           output_loss_metrics=output_loss_metrics))
    312   if not isinstance(outs, list):
    313     outs = [outs]

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)
    266           model._backwards(tape, scaled_total_loss)
    267         else:
--> 268           grads = tape.gradient(scaled_total_loss, trainable_weights)
    269           if isinstance(model.optimizer,
    270                         loss_scale_optimizer.LossScaleOptimizer):

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1012         output_gradients=output_gradients,
   1013         sources_raw=flat_sources_raw,
-> 1014         unconnected_gradients=unconnected_gradients)
   1015 
   1016     if not self._persistent:

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     74       output_gradients,
     75       sources_raw,
---> 76       compat.as_str(unconnected_gradients.value))

/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/backprop.py in _aggregate_grads(gradients)
    596   assert gradients, ""No gradients to aggregate""
    597 
--> 598   if len(gradients) == 1:
    599     return gradients[0]
    600   if all(isinstance(g, ops.Tensor) for g in gradients):

SystemError: <built-in function len> returned a result with an error set
```

**Describe the expected behavior**
model.fit() should work and training should proceed

**Standalone code to reproduce the issue**
Code can be found here:
https://colab.research.google.com/drive/1Ab_brNm3JhKmMZhLNyl014mkeZhlatHn?usp=sharing

Reasoning behind code:
+  Samples are document pairs which need to be classified into 4 categories. 
+ We train a Albert finetuned siamese model.
+ Doc 1 is short while doc 2 is long. So we decided to split doc 2 into 10 (roughly equal sized chunks).
+ After getting the hidden representation for each of the 10 parts of doc 2, we combine them using GlobalMaxPool1D over timesteps.
+ Further we combine the representations of doc1 and doc2 and do the prediction using softmax activation.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48427,what's the api for get_or_create_global_step() in tf2.0,
48426,some qunstions of nnapi delegate,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I don't quite understand the relationship between libtensorflowlite.so and libnnapi_delegate.so，if I only nnapi delegate, is it OK to just use libnnapi_delegate.so?

"
48425,A large mount of data to load and fit to train  causing Anaconda kernel died and restart,"I‘m doing a semantic segmentation task，i have thousands of pictures and labels. The kernel will died and restart when i use anaconda to train pictures that exceed 1600, but the kernel will not died when i train 1000 pictures.But the result is so bad and i want to train more than 10000 pictures.It hint the information when i use the Pycharm:

2021-04-09 11:06:08.187912: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found
2021-04-09 11:06:08.190073: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2021-04-09 11:06:08.191626: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-04-09 11:06:08.192304: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-04-09 11:06:08.192482: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.
Process finished with exit code -1073741819 (0xC0000005)


I tried to solve this problem but failed.
My system information are:
Python version: 3.8
CUDA/cuDNN version: 10.1、7.6
TensorFlow version : 2.4.1
Windows10

Fallowing is my code of loading the datasets：

import cv2
from PIL import Image
import numpy as np
import os

path = 'D:\CDS\cropped\label\_label_1_6.png'
def create_one_hot_labels(path):
    NCLASSES = 6
    img = cv2.imread(path)
    labels = np.zeros((img.shape[0], img.shape[1], NCLASSES))
    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            if (img[i, j, 0] == 0.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 0.):  #black 
                labels[i, j, 0] = 1  #to create one-hot-label making the third dimension have shape of six
            elif (img[i, j, 0] == 0.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 255.): #red
                labels[i, j, 1] = 1 
            elif (img[i, j, 0] == 0.) and (img[i, j, 1] == 255.) and (img[i, j, 2] == 0.): #green
                labels[i, j, 2] = 1
            elif (img[i, j, 0] == 255.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 0.): #blue
                labels[i, j, 3] = 1
            elif (img[i, j, 0] == 255.) and (img[i, j, 1] == 255.) and (img[i, j, 2] == 0.): #cyan
                labels[i, j, 4] = 1
            else:
                labels[i, j, 5] = 1
    labels = np.reshape(labels, (-1, NCLASSES))
    return labels

for i in range(1200):
    one_hot_label = create_one_hot_labels(paths_label[i*16])
    one_hot_label = np.array(one_hot_label).astype(np.float32)
    labels.append(one_hot_label)      #train labels

X_train = []
for j in range(1200):
    img = cv2.imread(paths_image[j*16])
    img = (np.array(img) / 255).astype(np.float32)
    X_train.append(img)           #train data

for i in range(700):
    one_hot_label = create_one_hot_labels(paths_label[i*37])
    one_hot_label = np.array(one_hot_label).astype(np.float32)
    val_labels.append(one_hot_label)

X_val = []
for j in range(700):
    img = cv2.imread(paths_image[j*37])
    img = (np.array(img) / 255.).astype(np.float32)
    X_val.append(img)        #validation data

import tensorflow as tf
import numpy as np

AUTOTUNE = tf.data.experimental.AUTOTUNE
BATCH_SIZE = 20
train_ds = tf.data.Dataset.from_tensor_slices((X_train, labels))
# val_ds = tf.data.Dataset.from_tensor_slices((X_val, val_labels))
val_ds = tf.data.Dataset.from_tensor_slices((X_val, val_labels))
train_ds = train_ds.cache()
train_ds = train_ds.apply(
  tf.data.experimental.shuffle_and_repeat(buffer_size=200))
train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.batch(BATCH_SIZE)

miou = tf.keras.metrics.MeanIoU(num_classes=6)
model.compile(optimizer=keras.optimizers.Adam(3e-3),
              loss='categorical_crossentropy',
              metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=6)])
import tensorflow as tf

EPOCHS = 500
VAL_SUBSPLITS = 5
VALIDATION_STEPS = 700//BATCH_SIZE//VAL_SUBSPLITS
STEPS_PER_EPOCH = 1200//BATCH_SIZE

model_history = model.fit(train_ds, epochs=EPOCHS,
                          steps_per_epoch=STEPS_PER_EPOCH,
                          validation_steps=VALIDATION_STEPS,
                          validation_data=val_ds,

                          callbacks=[early_stopping, tensorboard_callback, csv_log])

So my problem is if the way of loading the dataset is wrong and how can i load more than 10000 pictures to train. Or if i have the problem in the version of the configuration.

Many thanks
"
48424,TypeError: apply_gradients() got an unexpected keyword argument 'global_step' in tf_agents.agents.ReinforceAgent,"<em>First of all I tryed everything in my code and it didn't changed the outcome i dont know where is this global_step coming from not from my code then i belive the problem may be with Tensor if its not i am sorry for posting as Bug Issue but before canceling it Could you at least tell me how can i train my model??
edit: other agents work with the same layout ex DqnAgent worked well 
</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock and non stock
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 / ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NoN
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4
- Python version: 3.8
- Bazel version (if compiling from source): NON
- GCC/Compiler version (if compiling from source):  NON
- CUDA/cuDNN version:  NON gpu also doesn't work
- GPU model and memory: GTX1070 6GB

**Describe the current behavior**

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\agents\reinforce\reinforce_agent.py in _train(self, experience, weights)
    286                                           self.train_step_counter)
    287 
--> 288     self._optimizer.apply_gradients(
    289         grads_and_vars, global_step=0)
    290 

TypeError: apply_gradients() got an unexpected keyword argument 'global_step'

**Describe the expected behavior**

train_loss = tf_agent.train(experience,global_step= tf.Variable(1, name=""global_step""))
print(train_loss)

*** train_los***

**Standalone code to reproduce the issue**

```py
import pyxinput
import time
import cv2
from PIL import ImageGrab
import numpy as np
import keyboard
import tensorflow
import tf_agents
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import torch
#from tf_agents.networks import actor_distribution_networ


from tf_agents.policies import random_py_policy

Tensod_spec = tf_agents.specs.BoundedArraySpec(
   (15,), dtype= np.float32 ,  name=""XimputSpecs""   , minimum=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], maximum=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
)



Tensod_spec2 = tf_agents.specs.TensorSpec(
   [ 440 , 600 , 1], dtype= tf.int32 , name=""ScreenSpecs""
)


Tensor_reward_spe = tf_agents.specs.TensorSpec(
   [1,1], dtype= tf.int32 , name=""Reward""
)

FromEnv = tf_agents.specs.BoundedTensorSpec(shape=(440 , 600 , 1), dtype='uint8', name='observation', minimum=0, maximum=255)
FromEnv2 = tf_agents.specs.BoundedTensorSpec(shape=(1 , 440 , 600 , 1), dtype=tf.int32, name='observation', minimum=0, maximum=255)


fullscreen = [110,130,710,570]

screenpil = ImageGrab.grab(bbox=fullscreen)
showprint = np.array(screenpil)
grayscreen = cv2.cvtColor(showprint, cv2.COLOR_BGR2GRAY)
screenrect = cv2.cvtColor(grayscreen, cv2.COLOR_GRAY2BGR)
grayscreen = grayscreen.reshape( 440, 600, 1)

time_step_spec2 = tf_agents.trajectories.time_step.time_step_spec(
   observation_spec= FromEnv ,
   #reward_spec = Tensor_reward_spec
)

time_step_spec = tf_agents.trajectories.time_step.time_step_spec(
    observation_spec= FromEnv ,
    
    #reward_spec = Tensor_reward_spec
)


actor_net = tf_agents.networks.actor_distribution_network.ActorDistributionNetwork(
   input_tensor_spec =FromEnv,
   output_tensor_spec =  tf_agents.specs.tensor_spec.from_spec(Tensod_spec),
   activation_fn = 'relu' ,
   #conv_layer_params = [(25, 40, 2)] ,
   fc_layer_params=(50,25,15) ,
   #dtype =  'int32'
)
print(actor_net)

train_step_counter =tf.dtypes.cast(1, tf.int32)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.003)

tf_agent = tf_agents.agents.ReinforceAgent(
   time_step_spec = time_step_spec,
   action_spec = tf_agents.specs.tensor_spec.from_spec(Tensod_spec),
   actor_network=actor_net,
   optimizer=optimizer,
   normalize_returns=True,
   #train_step_counter= tf.Variable(1, name=""global_step"")
   )
tf_agent.initialize()

grayscreen2 = grayscreen
grayscreen2 = grayscreen2.reshape(1 , 440, 600, 1)
time_step2 = tf_agents.trajectories.time_step.TimeStep(
   step_type = tf_agents.trajectories.time_step.StepType.FIRST,
   reward = tf.dtypes.cast(1, tf.float32) ,
   discount = tf.dtypes.cast(1, tf.float32),
   observation = grayscreen2
)

policy_state = tf_agent.policy.get_initial_state(batch_size=1)

policy_step = tf_agent.policy.action(time_step2, policy_state)
print(policy_step)


observe = time_step2.observation
#print(observe.dtype)
#observe = observe.astype(int)
#print(observe.shape)

experience = tf_agents.trajectories.trajectory.Trajectory(
action= tf.compat.v2.Variable([tf.compat.v2.Variable(policy_step.action),tf.compat.v2.Variable(policy_step.action),tf.compat.v2.Variable(policy_step.action)]),
reward = tf.compat.v2.Variable([[tf.compat.v2.Variable(time_step2.reward),tf.compat.v2.Variable(time_step2.reward),tf.compat.v2.Variable(time_step2.reward)]]),
step_type = tf.compat.v2.Variable([[tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.FIRST),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)]]),
observation = tf.compat.v2.Variable([tf.compat.v2.Variable(observe),tf.compat.v2.Variable(observe),tf.compat.v2.Variable(observe)]),
policy_info = tf_agent.policy.info_spec,
next_step_type = tf.compat.v2.Variable([[tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST),tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)]]),
discount= tf.compat.v2.Variable([[tf.dtypes.cast(1, tf.float32),tf.dtypes.cast(1, tf.float32),tf.dtypes.cast(1, tf.float32)]]), 

)

train_loss = tf_agent.train(experience)
print(train_loss)
```

`
**Other info / logs** Include any logs or source code that would be helpful to

Full erro

``` py
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-16-ed3ca44dc79e> in <module>
      1 #
----> 2 train_loss = tf_agent.train(experience,global_step= tf.Variable(1, name=""global_step""))
      3 print(train_loss)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\agents\tf_agent.py in train(self, experience, weights, **kwargs)
    516 
    517     if self._enable_functions:
--> 518       loss_info = self._train_fn(
    519           experience=experience, weights=weights, **kwargs)
    520     else:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\utils\common.py in with_check_resource_vars(*fn_args, **fn_kwargs)
    183         # We're either in eager mode or in tf.function mode (no in-between); so
    184         # autodep-like behavior is already expected of fn.
--> 185         return fn(*fn_args, **fn_kwargs)
    186       if not resource_variables_enabled():
    187         raise RuntimeError(MISSING_RESOURCE_VARIABLES_ERROR)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\agents\reinforce\reinforce_agent.py in _train(self, experience, weights)
    286                                           self.train_step_counter)
    287 
--> 288     self._optimizer.apply_gradients(
    289         grads_and_vars, global_step=0)
    290 

TypeError: apply_gradients() got an unexpected keyword argument 'global_step'
```
`

Thank you for your time
"
48418,Deadlock when setting interop thread thread to 1 with a tf Dataset and tf function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux / colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 2.4.1
- TensorFlow version (use command below):
- Python version: Colab
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
For test purposes we usually set the nbr of threads to 1 for the interop and intra op threadings. When using the following code (to reduce batch size from an already batched dataset) we get a deadlock.

We only get the deadlock if:
- inter nbr thread set to 1
- and  
-  _slicer is wrapped in tf function.

**Describe the expected behavior**
No deadlocks
**Standalone code to reproduce the issue**
````
import tensorflow as tf
import numpy as np
tf.config.threading.set_inter_op_parallelism_threads(1)
@tf.function
def _slicer(dd, slice_):
    return tf.nest.map_structure(lambda val: val[slice_[0]:slice_[1]], dd)
def get_small_batch_gen(ds: tf.data.Dataset, batch_size):
    def gen_small_batch():
        for batch in ds:
            big_batch_size = tf.nest.flatten(tf.nest.map_structure(lambda x: x.shape[0], batch))
            for i in range(big_batch_size[0] // batch_size):
                yield _slicer(batch, tf.constant([i * batch_size, (i + 1) * batch_size]))
    return tf.data.Dataset.from_generator(gen_small_batch, output_signature=ds.element_spec)
size = 102
ds = tf.data.Dataset.from_tensor_slices(({i: tf.RaggedTensor.from_row_splits(np.random.choice(10, size=size),
                                                                                  tf.range(size + 1))
                                              for i in ['a', 'b', 'c']}, tf.range(size))).batch(10)
next(iter(get_small_batch_gen(ds, 5)))
`````

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48413,License for using the TF logo,"I would like to add the TF Logo to a commercial product. When can I obtain a copy of the licence for the logo?
"
48412,tf.keras.applications.EfficientNetBXs already contain a rescaling layer to preprocess input. It's confusing!,"https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/applications/efficientnet.py#L735-L737
`tf.keras.applications.efficientnet.preprocess_input `does nothing at all! 
`tf.keras.applications.EfficientNetBXs` already contain a rescaling layer to preprocess input. It's confusing!
`tf.keras.applications.EfficientNetBXs` and `tf.keras.applications.efficientnet.preprocess_input`'s design and behavior are different from other tf.keras.applications modules."
48411,Having trouble building tf wheel from r2.5 branch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.5
- Python version: py36
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I'm building  from tf source r2.5 branch, getting the following error while trying to build the wheel:
[91mERROR: Could not find a version that satisfies the requirement tf-estimator<2.6.0,>=2.5.0rc0 (from tf-nightly==2.5.0rc0) (from versions: none)
ERROR: No matching distribution found for tf-estimator<2.6.0,>=2.5.0rc0 (from tf-nightly==2.5.0rc0)

From what I can see in this commit : https://github.com/tensorflow/tensorflow/commit/ecf0809b8f292636b07bcf10db71861709fd2802
has changed the dependency 
from:
 'tf-estimator-nightly == 2.5.0.dev2021032501',
to:
 'tf-estimator >= 2.5.0rc0 , < 2.6.0',

but tf-estimator 2.5.0rc0 doesn't seem to be available.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48409,ImportError: DLL load failed: No se puede encontrar el módulo especificado.,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from :pip install
- TensorFlow version:1.13.1
- Python version:3.5
- Installed using: virtualenv, pip and conda?:
- Bazel version (if compiling from source): none
- CUDA/cuDNN version:10 and 7.4
- GPU model and memory: GF -GTX1070Ti


**PROBLEM**
I am trying to download and use tensorflow-gpu version 1.13.1 and apparently I've done something wrong and I don't really know where. I am using cuda and cudnn versión that are compatible acording to ""https://www.tensorflow.org/install/source_windows"".


**COMMANDS**
conda create -n tensorflow1 pip python=3.5
pip install --upgrade tensorflow-gpu==1.13.1
python
>>>import tensorflow as tf



**LOG**

Traceback (most recent call last):
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Users\germa\anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el módulo especificado.


Failed to load the native TensorFlow runtime."
48408,Accuracies saved in history.history after model.fit() are different than the ones printed on-screen during training,"**System information**
Using Colab with Keras 2.4.0 and TensorFlow 2.4.1

**Describe the current behavior**
After calling ""history = model.fit(...)"", values saved in history.history['loss'] and history.history['accuracy'] are different from the ones printed on-screen at every epoch during training.

**Describe the expected behavior**
Values should be the same, as is with previous tf versions and with other parameters (like val_accuracy)

**Standalone code to reproduce the issue**
Shared link: https://colab.research.google.com/drive/14Uogeq8wRlZlinaKLbkFr_Bl2aLzUJuy?usp=sharing
(slightly modified version of MNIST tutorial, see the block right after calling model.fit())

See also original discussion: https://stackoverflow.com/questions/67001654/accuracy-in-history-dictionary-different-from-what-printed-on-screen"
48407,GPU delegate run tensorflow-lite C++ example label_image more slowly than CPU,"I compile tensorflow-lite and the example label_image in tensorflow-lite source code success,
I did run with delegates of GPU and also CPU with ADB with running comands:
     CPU:  ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -j 1
     GPU:  ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -g 1
Both seems runs success, but average time or GPU : 60.00 ms, average time or CPU : 40.00 ms
Is it right? GPU is more slow than CPU for 50% ?
If Not, what should I do to make GPU runs at good performance?


System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (SOC: Qualcomm Snapdragon 660)
TensorFlow installed from (source or binary): build by myself
TensorFlow version (use command below): tensorflow-2.4.1
Python version: Python 3.8.5
Bazel version (if compiling from source): 3.1.0
GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
CUDA/cuDNN version:
GPU model and memory:
SOC: Qualcomm Snapdragon 660

Could anyone help? Or give a advice?
Thank you so much~

"
48406,Testcode runs on multiple gpus instead of one,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.9
- Python version: 3.6.13
- Installed using virtualenv? pip? conda?: conda 4.6.11
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GeForce GTX TITAN X - 12G



**Describe the problem**
I'm running a simple test script from the tensorflow documentation, to see if my setup is working correctly. I'm using conda to run tensorflow 1.9. When I'm running my test script, even though I'm specifying the device, it is running on all gpus, instead of the specified one. I can't figure out what the issue is. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

import tensorflow as tf

with tf.device('/device:GPU:3'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
  c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess.run(c))

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48405,Old version of CUDA support ?,"Hi, I wonder if possible to release newest/nightly tensorflow-gpu compatible for old version CUDA ( 9.0 10.0). I want to use tf2.4 (or higher) on a CUDA 10.0 machine since the driver version are not compatible with newer version CUDA. Upgrading driver is impossible for some reason and and it's very inconvenient to build from source. 

It's very important to `pip install tensorflow-gpu # maybe with something like +cuda101` to install tensorflow gpu with certain cuda version. Especially when projects are moved to another machine when older version tensorflow can handle newer version tensorflow codes....

Thanks for you guys open source such a great library, I think this feature might be very useful to all users.
"
48403,Is SavedModel in tf 1.15 same as SavedModel in tf 2.x from the API prospective?,"Hi Team,

As it is noted in https://www.tensorflow.org/hub/tf2_saved_model:


**Users of TensorFlow 1 can update to TF 1.15 and then use the same APIs. Older versions of TF1 do not work.**

Does that means there's no difference between SavedModel format of tf 1.15(not tf.1.14 or other versions behind it) and tf 2.x?

Regards,
Zan"
48402,tflite qat model inference data_type,"I am confused with tflite QAT model. Is it running with uint8 data_type or int8 data_type? 
With this link https://www.tensorflow.org/lite/guide/hosted_models , I download the quant models and find it with uint8 weights.
With this link https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications, I found tflite quantization use int8 data_type.

Which is correct?"
48401,How to build libtensorflowlite.so with TensorFlow ops supported for armdf?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: tf-nightly
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 10.2
- GPU model and memory: GTX 1070ti



**Describe the problem**
I need Tensorflow OPS to run tflite model inference in C++.
But how to build libtensorflowlite.so (for armdf) with TensorFlow ops ?
In [document](https://www.tensorflow.org/lite/guide/ops_select), it mentioned:
_Add the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate_
So how to do this? 
Modify BUILD file in directory tensorflow_src/tensorflow/lite/ ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48397,TopK GPU slower than TopK CPU,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: MX150 4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TopK GPU slower than TopK CPU
**Describe the expected behavior**
TopK GPU faster than TopK CPU
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
``` python
import tensorflow as tf

cls_outputs_reshape = tf.random.uniform((1, 4419360), -30 ,30)
def top_k_func(*args, **kwargs):
    return tf.math.top_k(*args, **kwargs)
top_k = tf.function(top_k_func)
with tf.device('/CPU:0'):
    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)
with tf.device('/GPU:0'):
    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)
tf.profiler.experimental.start('./profile_dir')
with tf.device('/CPU:0'):
    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)
with tf.device('/GPU:0'):
    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)
tf.profiler.experimental.stop()
```
``` bash
tensorboard --logdir=./profile_dir
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
![Screenshot from 2021-04-08 16-21-30](https://user-images.githubusercontent.com/17592563/113993013-9226fb80-9886-11eb-912f-c25bc2927295.png)
"
48396,/micro_speech/recognize_commands_test.cc bug,"Hi, I tried to follow the book TinyML chapter 07 and ran `recognize_command_test` and got an error shown in the img below..
I think in `recognize_commands_test.cc` line 158
`const int bad_dims[] = {2, 1, 3};`  
has to be changed to 
`const int bad_dims[] = {2, 1, 4};` 

![Screenshot from 2021-04-08 16-43-54](https://user-images.githubusercontent.com/46836844/113991238-3875ff80-988d-11eb-9e81-c853ec79b31c.png)
"
48395,Allowing strides with dilated convolutions in tf.keras.layers.Conv2D,"**System information**
- TensorFlow version (you are using): v2.4.1
- Are you willing to contribute it (Yes/No): Depends on acceptable solution



**Describe the feature and the current behavior/state.**
At this point in time `tf.keras.layers.Conv2D` doesn't support dilated convolutions with strides > 1, which would be a nice feature to have. This limitation seems to be a limitation of Keras Conv2D, not a Tensorflow. `tf.nn.conv2d` supports dilated convolutions with strides, but all Keras Conv layers use generic `tf.nn.convolution`, which doesn't support dilated convolutions with strides.

The most natural solution would probably be creating something like `_conv_implementation` method in base Conv layer and override it using `tf.nn.conv1d`, `tf.nn.conv2d`, `tf.nn.conv3d` in corresponding Keras layers. 

**Will this change the current api? How?**
It seems that `tf.nn.convolution` and `tf.nn.conv1d`, `tf.nn.conv2d`, `tf.nn.conv3d` behave identically, so it shouldn't change API or break back-compatibility (except now dilated convolutions will be able to use strides).

**Who will benefit with this feature?**
Anybody who wants to use convolutional neural networks with large receptive field but little computational overhead.
"
48394,Add link and more info in tf/examples/README,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
48392,TensorBoard can't support read data from Kerberos cluster hdfs.,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.8
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 
      tensorboard            2.4.1    
      tensorboard-plugin-wit 1.8.0    
      tensorflow             2.3.1    
      tensorflow-estimator   2.3.0  
- Python version: 3.6.8

**Describe the current behavior**
sudo su hdfs
export KERB_TICKET_CACHE_PATH=/tmp/krb5cc_996
echo 'hdfs' | kinit hdfs@TEST-KDC

bash-4.2$ hdfs dfs -ls /tmp/tensorflow/mnist/
Found 4 items
drwxr-xr-x   - hdfs supergroup          0 2021-03-31 18:03 /tmp/tensorflow/mnist/input_data
drwxr-xr-x   - hdfs supergroup          0 2021-04-07 19:49 /tmp/tensorflow/mnist/working_dir
drwxr-xr-x   - hdfs supergroup          0 2021-03-31 19:25 /tmp/tensorflow/mnist/working_dir_1131
drwxr-xr-x   - hdfs supergroup          0 2021-03-31 19:39 /tmp/tensorflow/mnist/working_dir_231

bash-4.2$ hdfs dfs -ls /tmp/tensorflow/mnist/working_dir
Found 17 items
-rw-r--r--   1 hdfs supergroup        222 2021-04-07 19:49 /tmp/tensorflow/mnist/working_dir/checkpoint
-rw-r--r--   1 hdfs supergroup     656044 2021-03-31 20:09 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617217664.cdhhakerberos-cdh-core-kudu-0.novalocal
-rw-r--r--   1 hdfs supergroup     657680 2021-04-01 02:10 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617239301.cdhhakerberos-cdh-core-kudu-0.novalocal
-rw-r--r--   1 hdfs supergroup     657796 2021-04-07 20:49 /tmp/tensorflow/mnist/working_dir/events.out.tfevents.1617824847.cdhhakerberos-cdh-core-kudu-0.novalocal
-rw-r--r--   1 hdfs supergroup     328014 2021-04-07 19:47 /tmp/tensorflow/mnist/working_dir/graph.pbtxt
-rw-r--r--   1 hdfs supergroup   39295624 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.data-00000-of-00001
-rw-r--r--   1 hdfs supergroup        994 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.index
-rw-r--r--   1 hdfs supergroup     138970 2021-03-31 19:07 /tmp/tensorflow/mnist/working_dir/model.ckpt-0.meta


CLASSPATH=`hadoop classpath --glob` tensorboard --logdir hdfs://default/tmp/tensorflow/mnist/ --host `hostname` --verbosity 0 --logger_levels other.logger:DEBUG --stderrthreshold debug

**Describe the expected behavior**
I expect show data in Tensorboard. But TensorBoard show ""No dashboards are active for the current data set.""

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

2021-04-07 22:22:10.690895: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
I0407 22:22:13.750683 139694758954816 plugin_event_multiplexer.py:106] Event Multiplexer initializing.
I0407 22:22:13.750985 139694758954816 plugin_event_multiplexer.py:125] Event Multiplexer done initializing
I0407 22:22:13.751172 139694758954816 data_ingester.py:124] Launching reload in a daemon thread
I0407 22:22:13.752043 139693371959040 data_ingester.py:98] TensorBoard reload process beginning
I0407 22:22:13.752586 139693371959040 plugin_event_multiplexer.py:201] Starting AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/
I0407 22:22:13.756613 139693371959040 plugin_event_multiplexer.py:207] Done with AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/
I0407 22:22:13.756979 139693371959040 data_ingester.py:102] TensorBoard reload process: Reload the whole Multiplexer
I0407 22:22:13.757233 139693371959040 plugin_event_multiplexer.py:212] Beginning EventMultiplexer.Reload()
I0407 22:22:13.757594 139693371959040 plugin_event_multiplexer.py:256] Reloading runs serially (one after another) on the main thread.
I0407 22:22:13.757895 139693371959040 plugin_event_multiplexer.py:265] Finished with EventMultiplexer.Reload()
I0407 22:22:13.758112 139693371959040 data_ingester.py:107] TensorBoard done reloading. Load took 0.006 secs
TensorBoard 2.4.1 at http://cdhhakerberos-cdh-master1-0.novalocal:6006/ (Press CTRL+C to quit)
I0407 22:22:14.644470 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] ""GET / HTTP/1.1"" 200 -
I0407 22:22:14.728922 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] ""GET /index.js HTTP/1.1"" 200 -
I0407 22:22:14.752892 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:14] ""GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1"" 200 -
I0407 22:22:15.515960 139693380351744 application.py:439] Plugin listing: is_active() for scalars took 0.000 seconds
I0407 22:22:15.516464 139693380351744 application.py:439] Plugin listing: is_active() for custom_scalars took 0.000 seconds
I0407 22:22:15.516749 139693380351744 application.py:439] Plugin listing: is_active() for images took 0.000 seconds
I0407 22:22:15.516995 139693380351744 application.py:439] Plugin listing: is_active() for audio took 0.000 seconds
I0407 22:22:15.517701 139693380351744 application.py:439] Plugin listing: is_active() for debugger-v2 took 0.000 seconds
I0407 22:22:15.518152 139693380351744 application.py:439] Plugin listing: is_active() for graphs took 0.000 seconds
I0407 22:22:15.518453 139693380351744 application.py:439] Plugin listing: is_active() for distributions took 0.000 seconds
I0407 22:22:15.518700 139693380351744 application.py:439] Plugin listing: is_active() for histograms took 0.000 seconds
I0407 22:22:15.518924 139693380351744 application.py:439] Plugin listing: is_active() for text took 0.000 seconds
I0407 22:22:15.519140 139693380351744 application.py:439] Plugin listing: is_active() for pr_curves took 0.000 seconds
I0407 22:22:15.519388 139693380351744 application.py:439] Plugin listing: is_active() for profile_redirect took 0.000 seconds
I0407 22:22:15.519608 139693380351744 application.py:439] Plugin listing: is_active() for hparams took 0.000 seconds
I0407 22:22:15.519902 139693380351744 application.py:439] Plugin listing: is_active() for mesh took 0.000 seconds
I0407 22:22:15.520128 139693380351744 application.py:439] Plugin listing: is_active() for timeseries took 0.000 seconds
I0407 22:22:15.525379 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /icon_bundle.svg HTTP/1.1"" 200 -
I0407 22:22:15.526228 139693380351744 application.py:439] Plugin listing: is_active() for projector took 0.006 seconds
I0407 22:22:15.526491 139693380351744 application.py:439] Plugin listing: is_active() for whatif took 0.000 seconds
I0407 22:22:15.529370 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /data/environment HTTP/1.1"" 200 -
I0407 22:22:15.530524 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /data/plugins_listing HTTP/1.1"" 200 -
I0407 22:22:15.541091 139693380351744 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /data/runs HTTP/1.1"" 200 -
I0407 22:22:15.542141 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /data/environment HTTP/1.1"" 200 -
I0407 22:22:15.544588 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /font-roboto/RxZJdnzeo3R5zSexge8UUZBw1xU1rKptJj_0jans920.woff2 HTTP/1.1"" 200 -
I0407 22:22:15.587946 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /data/runs HTTP/1.1"" 200 -
I0407 22:22:15.699429 139693388744448 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /font-roboto/d-6IYplOFocCacKzxwXSOJBw1xU1rKptJj_0jans920.woff2 HTTP/1.1"" 200 -
I0407 22:22:15.701413 139693397137152 _internal.py:113] 192.168.140.253 - - [07/Apr/2021 22:22:15] ""GET /font-roboto/vPcynSL0qHq_6dX7lKVByXYhjbSpvc47ee6xR_80Hnw.woff2 HTTP/1.1"" 200 -
I0407 22:22:18.765110 139693371959040 data_ingester.py:98] TensorBoard reload process beginning
I0407 22:22:18.765607 139693371959040 plugin_event_multiplexer.py:201] Starting AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/
I0407 22:22:18.765993 139693371959040 plugin_event_multiplexer.py:207] Done with AddRunsFromDirectory: hdfs://default/tmp/tensorflow/mnist/
I0407 22:22:18.766208 139693371959040 data_ingester.py:102] TensorBoard reload process: Reload the whole Multiplexer
I0407 22:22:18.766429 139693371959040 plugin_event_multiplexer.py:212] Beginning EventMultiplexer.Reload()
I0407 22:22:18.766694 139693371959040 plugin_event_multiplexer.py:256] Reloading runs serially (one after another) on the main thread.
I0407 22:22:18.766852 139693371959040 plugin_event_multiplexer.py:265] Finished with EventMultiplexer.Reload()
I0407 22:22:18.766979 139693371959040 data_ingester.py:107] TensorBoard done reloading. Load took 0.002 secs

"
48391,"Toy model crashes on multiple GPUs with ""No unary variant device copy function""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0
- Python version: 3
- Bazel version (if compiling from source): 0.29.1-1.8
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: Various, including TITAN Xp 12 GB

**Describe the current behavior**
A toy model consisting of several Conv2D and SyncBatchNorm layers, when training on 4 GPUs, fails with the following error:
```
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2021-04-08 03:24:12.084199: W external/org_tensorflow/tensorflow/core/grappler/optimizers/meta_optimizer.cc:560] dependency_optimizer failed: Deadline exceeded: dependency_optimizer exceeded deadline., time = 11444.1797ms.
2021-04-08 03:24:18.334984: W external/org_tensorflow/tensorflow/core/common_runtime/process_function_library_runtime.cc:733] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.
Traceback (most recent call last):
  File ""/.../train_keras_model.py"", line 109, in <module>
    main()
  File ""/.../train_keras_model.py"", line 103, in main
    verbose=2,
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py"", line 708, in _call
    return function_lib.defun(fn_with_cond)(*canon_args, **canon_kwds)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1665, in _filtered_call
    self.captured_inputs)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 598, in call
    ctx=ctx)
  File ""/tmp/local_changes/train_keras_model.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter
	 [[{{node cond/inner_args_0_7/_2117/_10568}}]]
	 [[cond/else/_1/StatefulPartitionedCall/Adam/Adam/update_69/update_2/ResourceApplyAdam/_51423]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter
	 [[{{node cond/inner_args_0_7/_2117/_10568}}]]
0 successful operations.
3 derived errors ignored. [Op:__inference_fn_with_cond_527347]

Function call stack:
fn_with_cond -> fn_with_cond
```

**Describe the expected behavior**
Model should train without crashing.

**Standalone code to reproduce the issue**
This standalone code reproduces the problem for us.
```
import logging

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.metrics import AUC
from tensorflow.keras.layers import Dense, Conv2D
from tensorflow.keras.layers.experimental import SyncBatchNormalization

def train_dataset_input():
    batch_size = 4
    x_value = tuple(tf.random.uniform((1, batch_size, 10, 10, 3)) for _ in range(5))
    y_value = tf.random.uniform((1, batch_size, 5))
    dataset = tf.data.Dataset.from_tensor_slices((x_value, y_value))
    dataset = dataset.repeat()
    return dataset

class ConvBN(tf.keras.layers.Layer):
    def __init__(self,):
        super(ConvBN, self).__init__()
        self.conv = Conv2D(filters=32, kernel_size=1)
        self.bn1 = SyncBatchNormalization()
        self.bn2 = SyncBatchNormalization()
        self.bn3 = SyncBatchNormalization()

    def call(self, inputs, training=None, **kwargs):
        x = self.conv(inputs)
        x = self.bn1(x)
        x = self.bn2(x)
        x = self.bn3(x)
        return x

class MultiFrameModel(keras.Model):
    def __init__(self, **kwargs):
        super(MultiFrameModel, self).__init__(name=""multi_frame_model"", **kwargs)
        self.backbone = keras.Sequential([ConvBN() for _ in range(17)])
        self.dense = Dense(5)

    def call(self, samples, training=False):
        backbone_outputs = []
        assert len(samples) == 5
        for img in samples:
            backbone_outputs.append(self.backbone(img))

        concat_sample = tf.concat(values=backbone_outputs, axis=3)
        x = tf.reduce_sum(concat_sample, [2, 3])
        x = self.dense(x)
        return x

class AucFromLogits(AUC):
    def update_state(self, y_true, logits, sample_weight=None):
        y_pred = tf.math.sigmoid(logits)
        super(AucFromLogits, self).update_state(y_true, y_pred, sample_weight)

def main():
    strategy = tf.distribute.MirroredStrategy()
    assert strategy.num_replicas_in_sync == 4

    train_dataset = train_dataset_input()
    with strategy.scope():
        metrics = [
            AucFromLogits(
                name=""auc_from_logits"",
                num_thresholds=100,
                curve=""PR"",
                multi_label=True,
                label_weights=[1.0, 0.0, 0.0, 0.0, 0.0],
            )
        ]

        model = MultiFrameModel()
        model.compile(
            optimizer=keras.optimizers.Adam(),
            loss=keras.losses.MeanSquaredError(),
            metrics=metrics,
        )

    model.fit(
        x=train_dataset,
        validation_data=train_dataset,
        steps_per_epoch=100,  # number of training steps between eval epochs
        epochs=120,  # epochs = total number of training steps / steps_per_epoch
        validation_steps=100,
        validation_freq=1,
        verbose=2,
    )

if __name__ == ""__main__"":
    logging.getLogger().setLevel(logging.INFO)
    main()
```

Please note that this code is sufficient to reproduce the crash in our environment. However, because the crash only occurs when training with multiple GPUs, and Colab only allows training on one GPU, this will run but not reproduce the crash on Colab. I tried splitting the GPU into 4 virtual GPUs, but it did not reproduce the problem.

**Other info / logs**
The following features of the model appear to be important; removing any of them ""fixes"" the problem.
- The model consists of SyncBatchNorm and Conv2D layers, and is fairly deep (it consists of a block repeated 17 times; repeating it, say, 10 times does not trigger the issue).
- The input is a tuple of 5 tensors, and the model backbone is applied to each of the tensors.
- There is a slightly modified AUC metric.
- The model runs on 4 GPUs with the Keras MirroredStrategy.

I'm quite confused as to how these four features should interact to cause a crash, especially the metric; it's quite surprising that adding a metric should somehow cause training to fail."
48389,"Tensorflow support for advance models like VAE-GAN, GAN ","Hello,
I was wondering if you guys are planning to make functional APIs for advanced research-based models like VAE, VAEGAN, GANS, etc.
If so then it will be very easier for the TensorFlow community to explore applications of these models in an array of real-world problems. Since currently the generative model uses multiple losses therefore training them is not intuitive and often ends up in unstable training. 
GradientTape alleviates the problem a little but still, it's a major bottleneck in the widespread adoption of advance deep learning models in business problems.
Link: https://arxiv.org/pdf/1512.09300.pdf
Regards,
Mrinal"
48388,tensorRT does not support defining OP.,"1. Use TensorFlow package: TFRA(https://github.com/tensorflow/recommenders-addons)
2. After saving the model, use tensorRT for model conversion:

TFRA code:
```
deep_dynamic_variables = dynamic_embedding.get_variable(
        name=""deep_dynamic_embeddings"",
        initializer=initializer,
        dim=embedding_size,
        devices=ps_list,
        trainable=is_training,
        partitioner=addone_partition_fn,
        checkpoint=True
    )
```

tensorRT conversion code:
```
import tensorflow as tf
import tensorflow_recommenders_addons as tfra
from tensorflow.python.compiler.tensorrt import trt_convert as trt

path = 'models/de'
converter = trt.TrtGraphConverterV2(input_saved_model_dir=path)
converter.convert()

converted_model_path = 'models/c'
converter.save(converted_model_path)
```
3. Error message:
KeyError: ""The name 'deep_dynamic_embeddings' refers to an Operation not in the graph."""
48380,Docker build: no such package '@local_cuda//': The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source, branch r2.5
- TensorFlow version: r2.5
- Python version: 3.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- CUDA/cuDNN version: 10.2/8.1.1
- GPU model and memory: n/a, but I want to use on P100 and V100 nodes

**Describe the problem**
I'm trying to compile tensorflow from source in a docker container, but I get the following error:
```
no such package '@local_cuda//': The repository '@local_cuda' could not be resolved and referenced by '@cub_archive//:cub'
```
See attached dockerfile
[dockerfile.txt](https://github.com/tensorflow/tensorflow/files/6274261/dockerfile.txt)

If line 101 in the above dockerfile is changed from `yes """" | ./configure && \` to ` ./configure && \`, the same issue persists.

**Any other info / logs**
See attached traceback
[traceback.txt](https://github.com/tensorflow/tensorflow/files/6274260/traceback.txt)


Thank you so much for your help with this issue!"
48377,INFO log get printed in tf2.5-rc0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 20H2 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0rc0
- Python version: 3.9.2
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): msvc 19.28.29913
- CUDA/cuDNN version: 11.2/8.1.1
- GPU model and memory: GTX1650 4GB

**Describe the current behavior**

simply doing ``import tensorflow as tf`` will print out all log at INFO level to Jupyter notebook

![image](https://user-images.githubusercontent.com/28623434/113896873-56156d00-9798-11eb-834a-d5a232222cb7.png)

**Describe the expected behavior**

Logs at INFO level will not get print to Jupyter notebook

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1-rOtR-xYJYymNIv4aksle_ZI9W90WLBh?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48376,tensorflow.experimental.numpy operations return tf.Tensor instead of ndarray<tf.Tensor>,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian 10.9
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0rc0
- Python version: Python 3.7.5

**Describe the current behavior**
In tensorflow 2.5.0rc0 these operations
```python
import tensorflow as tf
import tensorflow.experimental.numpy as tnp

print(tnp.real(tf.constant(1)))
print(tnp.real(tf.constant(1+1j)))
print(tnp.imag(tf.constant(1+1j)))
print(tnp.array(tf.constant(1)))
```
return the following results
```python
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(1.0, shape=(), dtype=float64)
tf.Tensor(1.0, shape=(), dtype=float64)
tf.Tensor(1, shape=(), dtype=int32)
```

**Describe the expected behavior**
In tensorflow 2.4.1 they used to return ndarrays
```python
ndarray<tf.Tensor(1, shape=(), dtype=int32)>
ndarray<tf.Tensor(1.0, shape=(), dtype=float64)>
ndarray<tf.Tensor(1.0, shape=(), dtype=float64)>
ndarray<tf.Tensor(1, shape=(), dtype=int32)>
```

**Other info / logs**
I didn't test any other methods.

**Edit**
I don't know if this actually is a bug or if it was one in tensorflow 2.4.1. But I couldn't find out which one is the intended behaviour. I for my part would expect these functions to return an ndarray but the introduction [article](https://www.tensorflow.org/guide/tf_numpy#tftensor_and_nd_array) seems to have a different opinion."
48375,tf.switch_case not working with KerasTensor,"**System information**
- Documentation exemple : https://www.tensorflow.org/api_docs/python/tf/switch_case
- Windows 10
- TensorFlow version 2
- Python version: 3.7

**Current behavior**

```
import tensorflow as tf
from tensorflow.keras.layers import Input
def f1(): return tf.constant(17)
def f2(): return tf.constant(31)
def f3(): return tf.constant(-1)
t_input = Input(shape=(1,), name=""t_input"")
r = tf.switch_case(t_input, branch_fns={0: f1, 1: f2}, default=f3)
```
```
Traceback (most recent call last):
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-9-bd285228541c>"", line 5, in <module>
    r = tf.switch_case(t_input, branch_fns={0: f1, 1: f2}, default=f3)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3616, in switch_case
    return _indexed_case_helper(branch_fns, default, branch_index, name)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3315, in _indexed_case_helper
    branch_fns, default, branch_index)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3249, in _indexed_case_verify_and_canonicalize_args
    type(branch_index)))
TypeError: branch_index must a Tensor, got <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>
```
"
48374,MultiWorkerMirroredStrategy looks a lot slower than non-distributed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I'm using code from here: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras (MNIST example) with small alterations, see below for the code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Amazon linux - 4.14.225-168.357.amzn2.x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Not applicable
- TensorFlow installed from (source or binary):
Version 2.4.1 installed using `pip3 install --user tensorflow==2.4.1`
- TensorFlow version (use command below):
```
$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
2021-04-07 15:52:47.046112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
v2.4.0-49-g85c8b2a817f 2.4.1
```
- Python version:
3.7.9
- Bazel version (if compiling from source):
Not applicable
- GCC/Compiler version (if compiling from source):
Not applicable
- CUDA/cuDNN version:
Looks like CUDA 11
- GPU model and memory:
Not applicable

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
On a single m5.large node in AWS training with the MNIST model takes 1m 48s. With 3 machines of that type and MultiWorkerMirroredStrategy in place, it takes 4 mins 30 seconds.

**Describe the expected behavior**
The expected behavior would be that the multi-worker execution is much faster than the single-node. The point of using a multi-worker distributed capability is to allow for a much faster, scalable processing (model training).

**Standalone code to reproduce the issue**
**Non-distributed**
```
import json
import os
import sys
import time
import numpy as np
import tensorflow as tf

os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
if ""."" not in sys.path:
    sys.path.insert(0, ""."")

def mnist_dataset(batch_size):
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
    # The `x` arrays are in uint8 and have values in the range [0, 255].
    # You need to convert them to float32 with values in the range [0, 1]
    x_train = x_train / np.float32(255)
    y_train = y_train.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
    return train_dataset

def build_and_compile_cnn_model():
    model = tf.keras.Sequential(
        [
            tf.keras.Input(shape=(28, 28)),
            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(32, 3, activation=""relu""),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=[""accuracy""],
    )
    return model

start_time = time.time()
global_batch_size = 64
multi_worker_dataset = mnist_dataset(global_batch_size)
multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(multi_worker_dataset, epochs=50, steps_per_epoch=70)
elapsed_time = time.time() - start_time
str_elapsed_time = time.strftime(""%H : %M : %S"", time.gmtime(elapsed_time))
print("">> Finished. Time elapsed: {}."".format(str_elapsed_time))
```
**Distributed:**
```
import json
import os
import sys
import time
import numpy as np
import tensorflow as tf

os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
if ""."" not in sys.path:
    sys.path.insert(0, ""."")

def mnist_dataset(batch_size):
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
    # The `x` arrays are in uint8 and have values in the range [0, 255].
    # You need to convert them to float32 with values in the range [0, 1]
    x_train = x_train / np.float32(255)
    y_train = y_train.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
    return train_dataset

def build_and_compile_cnn_model():
    model = tf.keras.Sequential(
        [
            tf.keras.Input(shape=(28, 28)),
            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(32, 3, activation=""relu""),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=[""accuracy""],
    )
    return model

start_time = time.time()

per_worker_batch_size = 64
tf_config = json.loads(os.environ[""TF_CONFIG""])
num_workers = len(tf_config[""cluster""][""worker""])

strategy = tf.distribute.MultiWorkerMirroredStrategy()
global_batch_size = 64
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
multi_worker_dataset = mnist_dataset(global_batch_size)
multi_worker_dataset_with_shrd = multi_worker_dataset.with_options(options)

with strategy.scope():
    # Model building/compiling need to be within `strategy.scope()`.
    multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(multi_worker_dataset_with_shrd, epochs=50, steps_per_epoch=70)
elapsed_time = time.time() - start_time
str_elapsed_time = time.strftime(""%H : %M : %S"", time.gmtime(elapsed_time))
print("">> Finished. Time elapsed: {}."".format(str_elapsed_time))
```

**Other info / logs** Include any logs or source code that would be helpful to
**Non-distributed run log:**
```
2021-04-07 15:29:39.627372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-07 15:29:41.291031: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-07 15:29:41.292029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-07 15:29:41.371907: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-04-07 15:29:41.371985: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-249-213.awsinternal.audiomack.com): /proc/driver/nvidia/version does not exist
2021-04-07 15:29:41.372562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-07 15:29:41.372737: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Epoch 1/50
2021-04-07 15:29:42.005401: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-07 15:29:42.006824: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz
70/70 [==============================] - 3s 32ms/step - loss: 2.2895 - accuracy: 0.1377
Epoch 2/50
70/70 [==============================] - 2s 30ms/step - loss: 2.2368 - accuracy: 0.3166
Epoch 3/50
70/70 [==============================] - 2s 29ms/step - loss: 2.1805 - accuracy: 0.4483
Epoch 4/50
70/70 [==============================] - 2s 31ms/step - loss: 2.1005 - accuracy: 0.5943
Epoch 5/50
70/70 [==============================] - 2s 30ms/step - loss: 2.0082 - accuracy: 0.6545
Epoch 6/50
70/70 [==============================] - 2s 31ms/step - loss: 1.8875 - accuracy: 0.6937
Epoch 7/50
70/70 [==============================] - 2s 30ms/step - loss: 1.7216 - accuracy: 0.7262
Epoch 8/50
70/70 [==============================] - 2s 30ms/step - loss: 1.5662 - accuracy: 0.7549
Epoch 9/50
70/70 [==============================] - 2s 31ms/step - loss: 1.3967 - accuracy: 0.7727
Epoch 10/50
70/70 [==============================] - 2s 31ms/step - loss: 1.2423 - accuracy: 0.7823
Epoch 11/50
70/70 [==============================] - 2s 31ms/step - loss: 1.0856 - accuracy: 0.8145
Epoch 12/50
70/70 [==============================] - 2s 31ms/step - loss: 0.9729 - accuracy: 0.8060
Epoch 13/50
70/70 [==============================] - 2s 30ms/step - loss: 0.8768 - accuracy: 0.8278
Epoch 14/50
70/70 [==============================] - 2s 30ms/step - loss: 0.7688 - accuracy: 0.8481
Epoch 15/50
70/70 [==============================] - 2s 31ms/step - loss: 0.7315 - accuracy: 0.8442
Epoch 16/50
70/70 [==============================] - 2s 29ms/step - loss: 0.6648 - accuracy: 0.8584
Epoch 17/50
70/70 [==============================] - 2s 30ms/step - loss: 0.6287 - accuracy: 0.8635
Epoch 18/50
70/70 [==============================] - 2s 30ms/step - loss: 0.6144 - accuracy: 0.8416
Epoch 19/50
70/70 [==============================] - 2s 31ms/step - loss: 0.5623 - accuracy: 0.8624
Epoch 20/50
70/70 [==============================] - 2s 31ms/step - loss: 0.5672 - accuracy: 0.8665
Epoch 21/50
70/70 [==============================] - 2s 31ms/step - loss: 0.5544 - accuracy: 0.8607
Epoch 22/50
70/70 [==============================] - 2s 31ms/step - loss: 0.5045 - accuracy: 0.8795
Epoch 23/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4929 - accuracy: 0.8729
Epoch 24/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4719 - accuracy: 0.8756
Epoch 25/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4753 - accuracy: 0.8746
Epoch 26/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4741 - accuracy: 0.8728
Epoch 27/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4696 - accuracy: 0.8760
Epoch 28/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4531 - accuracy: 0.8835
Epoch 29/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4407 - accuracy: 0.8855
Epoch 30/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4087 - accuracy: 0.8910
Epoch 31/50
70/70 [==============================] - 2s 30ms/step - loss: 0.4413 - accuracy: 0.8808
Epoch 32/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4110 - accuracy: 0.8839
Epoch 33/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4152 - accuracy: 0.8959
Epoch 34/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4159 - accuracy: 0.8870
Epoch 35/50
70/70 [==============================] - 2s 30ms/step - loss: 0.4061 - accuracy: 0.8767
Epoch 36/50
70/70 [==============================] - 2s 31ms/step - loss: 0.4077 - accuracy: 0.8890
Epoch 37/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3841 - accuracy: 0.8951
Epoch 38/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3976 - accuracy: 0.8895
Epoch 39/50
70/70 [==============================] - 2s 30ms/step - loss: 0.3873 - accuracy: 0.8968
Epoch 40/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3711 - accuracy: 0.8943
Epoch 41/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3778 - accuracy: 0.8906
Epoch 42/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3692 - accuracy: 0.9022
Epoch 43/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3809 - accuracy: 0.8925
Epoch 44/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3921 - accuracy: 0.8910
Epoch 45/50
70/70 [==============================] - 2s 30ms/step - loss: 0.3620 - accuracy: 0.9006
Epoch 46/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3519 - accuracy: 0.9026
Epoch 47/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3732 - accuracy: 0.9030
Epoch 48/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3670 - accuracy: 0.8874
Epoch 49/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3756 - accuracy: 0.8920
Epoch 50/50
70/70 [==============================] - 2s 31ms/step - loss: 0.3572 - accuracy: 0.8954
>> Finished. Time elapsed: 00 : 01 : 48.
```
**Logs from the distributed run:**
```
2021-04-07 15:33:57.473623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-07 15:33:58.819768: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-07 15:33:58.820710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-07 15:33:58.893993: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-04-07 15:33:58.894042: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-249-213.awsinternal.audiomack.com): /proc/driver/nvidia/version does not exist
2021-04-07 15:33:58.895043: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-07 15:33:58.895208: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-07 15:33:58.895622: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-07 15:33:58.899579: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.2.249.213:2121, 1 -> 10.2.252.56:2121, 2 -> 10.2.252.97:2121}
2021-04-07 15:33:58.899839: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://10.2.249.213:2121
2021-04-07 15:34:04.181014: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-07 15:34:04.200033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz
Epoch 1/50
70/70 [==============================] - 9s 85ms/step - loss: 2.2927 - accuracy: 0.1324
Epoch 2/50
70/70 [==============================] - 5s 77ms/step - loss: 2.2476 - accuracy: 0.2718
Epoch 3/50
70/70 [==============================] - 5s 74ms/step - loss: 2.1945 - accuracy: 0.4580
Epoch 4/50
70/70 [==============================] - 5s 74ms/step - loss: 2.1318 - accuracy: 0.5667
Epoch 5/50
70/70 [==============================] - 5s 74ms/step - loss: 2.0503 - accuracy: 0.6541
Epoch 6/50
70/70 [==============================] - 5s 75ms/step - loss: 1.9535 - accuracy: 0.6747
Epoch 7/50
70/70 [==============================] - 5s 77ms/step - loss: 1.8103 - accuracy: 0.7182
Epoch 8/50
70/70 [==============================] - 5s 77ms/step - loss: 1.6723 - accuracy: 0.7167
Epoch 9/50
70/70 [==============================] - 6s 79ms/step - loss: 1.5215 - accuracy: 0.7448
Epoch 10/50
70/70 [==============================] - 6s 79ms/step - loss: 1.3525 - accuracy: 0.7807
Epoch 11/50
70/70 [==============================] - 5s 78ms/step - loss: 1.2328 - accuracy: 0.7737
Epoch 12/50
70/70 [==============================] - 5s 76ms/step - loss: 1.1053 - accuracy: 0.7949
Epoch 13/50
70/70 [==============================] - 5s 74ms/step - loss: 0.9806 - accuracy: 0.8146
Epoch 14/50
70/70 [==============================] - 5s 75ms/step - loss: 0.8712 - accuracy: 0.8216
Epoch 15/50
70/70 [==============================] - 5s 75ms/step - loss: 0.8108 - accuracy: 0.8377
Epoch 16/50
70/70 [==============================] - 5s 75ms/step - loss: 0.7336 - accuracy: 0.8502
Epoch 17/50
70/70 [==============================] - 5s 75ms/step - loss: 0.6911 - accuracy: 0.8486
Epoch 18/50
70/70 [==============================] - 5s 74ms/step - loss: 0.6502 - accuracy: 0.8593
Epoch 19/50
70/70 [==============================] - 5s 75ms/step - loss: 0.6725 - accuracy: 0.8402
Epoch 20/50
70/70 [==============================] - 5s 74ms/step - loss: 0.5815 - accuracy: 0.8741
Epoch 21/50
70/70 [==============================] - 5s 75ms/step - loss: 0.5819 - accuracy: 0.8524
Epoch 22/50
70/70 [==============================] - 5s 75ms/step - loss: 0.5522 - accuracy: 0.8588
Epoch 23/50
70/70 [==============================] - 5s 75ms/step - loss: 0.4860 - accuracy: 0.8780
Epoch 24/50
70/70 [==============================] - 5s 75ms/step - loss: 0.5223 - accuracy: 0.8645
Epoch 25/50
70/70 [==============================] - 5s 74ms/step - loss: 0.5226 - accuracy: 0.8651
Epoch 26/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4903 - accuracy: 0.8724
Epoch 27/50
70/70 [==============================] - 5s 75ms/step - loss: 0.5156 - accuracy: 0.8642
Epoch 28/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4501 - accuracy: 0.8855
Epoch 29/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4403 - accuracy: 0.8916
Epoch 30/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4491 - accuracy: 0.8890
Epoch 31/50
70/70 [==============================] - 5s 77ms/step - loss: 0.4170 - accuracy: 0.8920
Epoch 32/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4621 - accuracy: 0.8793
Epoch 33/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4379 - accuracy: 0.8799
Epoch 34/50
70/70 [==============================] - 5s 75ms/step - loss: 0.4217 - accuracy: 0.8864
Epoch 35/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4094 - accuracy: 0.8932
Epoch 36/50
70/70 [==============================] - 5s 75ms/step - loss: 0.4042 - accuracy: 0.8941
Epoch 37/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4123 - accuracy: 0.8895
Epoch 38/50
70/70 [==============================] - 5s 76ms/step - loss: 0.3895 - accuracy: 0.9001
Epoch 39/50
70/70 [==============================] - 5s 77ms/step - loss: 0.4069 - accuracy: 0.8939
Epoch 40/50
70/70 [==============================] - 5s 75ms/step - loss: 0.3772 - accuracy: 0.8923
Epoch 41/50
70/70 [==============================] - 5s 77ms/step - loss: 0.4016 - accuracy: 0.8863
Epoch 42/50
70/70 [==============================] - 5s 76ms/step - loss: 0.3842 - accuracy: 0.9015
Epoch 43/50
70/70 [==============================] - 5s 75ms/step - loss: 0.4055 - accuracy: 0.8925
Epoch 44/50
70/70 [==============================] - 5s 76ms/step - loss: 0.3969 - accuracy: 0.8927
Epoch 45/50
70/70 [==============================] - 5s 76ms/step - loss: 0.3609 - accuracy: 0.8984
Epoch 46/50
70/70 [==============================] - 5s 76ms/step - loss: 0.4033 - accuracy: 0.8918
Epoch 47/50
70/70 [==============================] - 5s 76ms/step - loss: 0.3661 - accuracy: 0.8934
Epoch 48/50
70/70 [==============================] - 5s 75ms/step - loss: 0.3691 - accuracy: 0.8995
Epoch 49/50
70/70 [==============================] - 5s 77ms/step - loss: 0.3702 - accuracy: 0.8982
Epoch 50/50
70/70 [==============================] - 5s 78ms/step - loss: 0.3697 - accuracy: 0.8991
>> Finished. Time elapsed: 00 : 04 : 34.
```
**Other info**
`TF_CONFIG` is set in the environment of all 3 machines, as follows:
```
{""cluster"": {""worker"": [""xxx:2121"", ""yyy:2121"", ""zzz:2121""]}, ""task"": {""type"": ""worker"", ""index"": 0}}
{""cluster"": {""worker"": [""xxx:2121"", ""yyy:2121"", ""zzz:2121""]}, ""task"": {""type"": ""worker"", ""index"": 1}}
{""cluster"": {""worker"": [""xxx:2121"", ""yyy:2121"", ""zzz:2121""]}, ""task"": {""type"": ""worker"", ""index"": 2}}
```
"
48373,tf.switch_case not working,"**System information**
- Documentation exemple : https://www.tensorflow.org/api_docs/python/tf/switch_case
- Windows 10
- TensorFlow version 2
- Python version: 3.7

**Current behavior**

```
def f1(): return tf.constant(17)
def f2(): return tf.constant(31)
def f3(): return tf.constant(-1)
r = tf.switch_case(tf.convert_to_tensor([1, 0]), branch_fns={0: f1, 1: f2}, default=f3)
```
```
Traceback (most recent call last):
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3437, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-04e5a20bbcd0>"", line 4, in <module>
    r = tf.switch_case(tf.convert_to_tensor([1, 0]), branch_fns={0: f1, 1: f2}, default=f3)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3616, in switch_case
    return _indexed_case_helper(branch_fns, default, branch_index, name)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3321, in _indexed_case_helper
    len(branch_fns) - 1, branch_index)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\util\dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 4483, in where
    return gen_math_ops.select(condition=condition, x=x, y=y, name=name)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 8675, in select
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\gen06917\PycharmProjects\BaysianTarnet\.venv\lib\site-packages\tensorflow\python\framework\ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'then' must be at least a vector, but saw shape: [] [Op:Select]
```
"
48372,Using same group key in CollectiveBcastSend/RecvV2 and CollectiveReduceV2 raises an error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-54425-g18a6a9fc87f 2.6.0-dev20210407
- Python version: 3.8.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.0.3 / 7.6.5
- GPU model and memory: TITAN Xp (12GB)

**Describe the current behavior**

If I use 2 consecutive CollectiveCommunication Operations (CollectiveBcastSend / Recv V2 => CollectiveReduceV2) on the same devices, the program is aborted by InternalError

**Describe the expected behavior**

The program should not be aborted and should execute two collective operations

**Standalone code to reproduce the issue**

```
import tensorflow as tf

from tensorflow.core.protobuf import config_pb2
from tensorflow.python.ops import collective_ops

def main():
  g = tf.Graph()
  with g.as_default():
    c = tf.random.normal(shape=(10, 20))
    with tf.device(""/GPU:0""):
      b0 = collective_ops.broadcast_send_v2(c, group_size=2, group_key=1, instance_key=1)
      r0 = collective_ops.all_reduce_v2(b0, group_size=2, group_key=1, instance_key=2)
    with tf.device(""/GPU:1""):
      b1 = collective_ops.broadcast_recv_v2(c.shape, c.dtype, group_size=2, group_key=1, instance_key=1)
      r1 = collective_ops.all_reduce_v2(b1, group_size=2, group_key=1, instance_key=2)

    with tf.compat.v1.Session(graph=g) as sess:
        options = config_pb2.RunOptions()
        options.experimental.collective_graph_key = 1
        sess.run([r0, r1], options=options)

if __name__ == ""__main__"":
  main()
```

**Other info / logs** Include any logs or source code that would be helpful to

The error logs are
```
Traceback (most recent call last):
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InternalError: [_Derived_]Collective ops is aborted by: Collective Op CollectiveReduceV2: ReduceV2(Add,Id) is assigned to device /job:localhost/replica:0/task:0/device:GPU:0 with type GPU and group_key 1 but that group has type DEFAULT
The error could be from a previous operation. Restart your program to reset.
         [[{{node CollectiveReduceV2_1}}]]
         [[CollectiveReduceV2/_1]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""collective.py"", line 31, in <module>
    main()
  File ""collective.py"", line 28, in main
    print(sess.run([r0, r1], options=options))
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: [_Derived_]Collective ops is aborted by: Collective Op CollectiveReduceV2: ReduceV2(Add,Id) is assigned to device /job:localhost/replica:0/task:0/device:GPU:0 with type GPU and group_key 1 but that group has type DEFAULT
The error could be from a previous operation. Restart your program to reset.
         [[node CollectiveReduceV2_1 (defined at collective.py:21) ]]
         [[CollectiveReduceV2/_1]]

Errors may have originated from an input operation.
Input Source operations connected to node CollectiveReduceV2_1:
 CollectiveBcastRecvV2 (defined at collective.py:20)

Original stack trace for 'CollectiveReduceV2_1':
  File ""collective.py"", line 31, in <module>
    main()
  File ""collective.py"", line 21, in main
    r1 = collective_ops.all_reduce_v2(b1, 2, 1, 2)
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/ops/collective_ops.py"", line 111, in all_reduce_v2
    return gen_collective_ops.collective_reduce_v2(
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/ops/gen_collective_ops.py"", line 779, in collective_reduce_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 3561, in _create_op_internal
    ret = Operation(
  File ""/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 2049, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)
```

Interestingly, if I give a different `group_key` to collective reduce as
```
import tensorflow as tf

from tensorflow.core.protobuf import config_pb2
from tensorflow.python.ops import collective_ops

def main():
  g = tf.Graph()
  with g.as_default():
    c = tf.random.normal(shape=(10, 20))
    with tf.device(""/GPU:0""):
      b0 = collective_ops.broadcast_send_v2(c, group_size=2, group_key=1, instance_key=1)
      r0 = collective_ops.all_reduce_v2(b0, group_size=2, group_key=2, instance_key=2)
    with tf.device(""/GPU:1""):
      b1 = collective_ops.broadcast_recv_v2(c.shape, c.dtype, group_size=2, group_key=1, instance_key=1)
      r1 = collective_ops.all_reduce_v2(b1, group_size=2, group_key=2, instance_key=2)

    with tf.compat.v1.Session(graph=g) as sess:
        options = config_pb2.RunOptions()
        options.experimental.collective_graph_key = 1
        sess.run([r0, r1], options=options)

if __name__ == ""__main__"":
  main()
```
, the program runs without an error.
However, from my understanding, I can use the same `group_key` for different collective operations if the participant devices are same (only `instance_key` should be different) because CollectiveReduce => CollectiveGather works well if I run
```
import tensorflow as tf

from tensorflow.core.protobuf import config_pb2
from tensorflow.python.ops import collective_ops

def main():
  g = tf.Graph()
  with g.as_default():
    c = tf.random.normal(shape=(10, 20))
    with tf.device(""/GPU:0""):
      r0 = collective_ops.all_reduce_v2(c, group_size=2, group_key=2, instance_key=2)
      r0 = collective_ops.all_gather_v2(r0, group_size=2, group_key=2, instance_key=3)
    with tf.device(""/GPU:1""):
      r1 = collective_ops.all_reduce_v2(c, group_size=2, group_key=2, instance_key=2)
      r1 = collective_ops.all_gather_v2(r1, group_size=2, group_key=2, instance_key=3)

    with tf.compat.v1.Session(graph=g) as sess:
        options = config_pb2.RunOptions()
        options.experimental.collective_graph_key = 1
        sess.run([r0, r1], options=options)

if __name__ == ""__main__"":
  main()
```"
48370,Broken/outdated Links in Docs ,"While going through the documentation I encountered a few broken/outdated links, and I've found the correct link for the same 
Kindly assign this issue to me so that I can make the required changes.
"
48369,Gradients are all None in GradientTape,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.15.4

- TensorFlow installed from (source or binary): 2.4.1 from binary
- TensorFlow version (use command below):pip install tf 
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 1.6ghz i5, 8gb ddr3


**Describe the current behavior**
We get the error that no gradients have been found for the variable in our model. When printing the intermittent results, we see that all the gradients are none.
**Describe the expected behavior**
We expected gradients to be not none

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
`import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time
from Retrieve_data import retrieve_data
from sklearn.model_selection import train_test_split
from Model import model_P
import keras as K
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Embedding
from keras.layers import Bidirectional
from keras.layers import Attention
from keras.layers import Concatenate
from keras.layers import TimeDistributed
from keras.layers import Reshape
from keras_self_attention import SeqSelfAttention
from tensorflow import keras


# Prepare the training dataset.
base_directory = '/Users/Desktop/Companies/'
X_pricing, Y_pricing, X_reports = retrieve_data(base_directory, 'ADP')
X_reports_train, X_reports_test, X_pricing_train, X_pricing_test, Y_train, Y_test = train_test_split(X_reports, X_pricing, Y_pricing, test_size=0.50, shuffle=False)

param_grid = {
    'batch_size': 64,
    'epochs': 50,
    'embedding_dimensions': 64,
    'units_LSTM_1': 128,
    'units_LSTM_2': 128,
    'units_BiLSTM_1': 128,
    'units_BiLSTM_2': 128,
    'units_Dense': 128,
    'optimizer': ['ADAM'],
    'loss': 'MeanSquaredError',
    'activation_LSTM_1': 'softsign',
    'activation_LSTM_2': 'softsign',
    'activation_BiLSTM_1': 'softsign',
    'activation_BiLSTM_2': 'softsign',
    'dropout_LSTM_1': 0.5,
    'dropout_LSTM_2': 0.5,
    'dropout_BiLSTM_1': 0.5,
    'dropout_BiLSTM_2': 0.5,
    'metrics': 'mse',
    'learning_rate': 0.000001
}


input_layer = K.Input(shape=(X_pricing.shape[1], X_pricing.shape[2]), batch_size=param_grid['batch_size'])

# Defining the Bidirectional LSTM Layer for Pricing
BiLSTM_pricing = Bidirectional(LSTM(units=param_grid['units_LSTM_1'], return_sequences=True, activation='softsign', dropout=param_grid['dropout_BiLSTM_1']))(input_layer)

    # Defining the Attention Layer for the Pricing
attention_pricing = SeqSelfAttention(attention_activation='softsign')(BiLSTM_pricing)

    # Adding Output Layer
output_layer = Dense(units=param_grid['units_Dense'], activation='relu')(attention_pricing)

    # Building Model

PricingModel = K.Model(inputs=input_layer, outputs=output_layer, name=""PricingModel"")

# Instantiate an optimizer to train the model.
optimizer = keras.optimizers.SGD(learning_rate=1e-3)
# Instantiate a loss function.
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Prepare the metrics.
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

x_train = X_pricing_train
x_test = X_pricing_test
y_train = Y_train
y_test = Y_test


print(x_train.shape)
print(y_train.shape)

print(len(x_train))
# Prepare the training dataset.
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(param_grid['batch_size'])


epochs = 1
for epoch in range(epochs):
    print(""\nStart of epoch %d"" % (epoch,))
    start_time = time.time()

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:

            logits = PricingModel(x_batch_train, training=True)
            logits = np.reshape(logits, (-1, (logits.shape[2]*logits.shape[1])))
            print(logits)
            print(logits.shape)
            loss_value = loss_fn(y_batch_train, logits)
            print(loss_value)
        tape.watch(loss_value)
        grads = tape.gradient(loss_value, PricingModel.trainable_weights)
        #print(PricingModel.trainable_weights)
        #print(len(PricingModel.trainable_weights))
        print(grads)
        optimizer.apply_gradients(zip(grads, PricingModel.trainable_weights))

        # Update training metric.
        train_acc_metric.update_state(y_batch_train, logits)

        # Log every 200 batches.
        if step % 200 == 0:
            print(
                ""Training loss (for one batch) at step %d: %.4f""
                % (step, float(loss_value))
            )
            print(""Seen so far: %d samples"" % ((step + 1) * param_grid['batch_size']))

    # Display metrics at the end of each epoch.
    train_acc = train_acc_metric.result()
    print(""Training acc over epoch: %.4f"" % (float(train_acc),))

    # Reset training metrics at the end of each epoch
    train_acc_metric.reset_states()

`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The code produces the following error:
ValueError: No gradients provided for any variable: ['bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/bias:0', 'seq_self_attention/seq_self_attention_Add_Wt:0', 'seq_self_attention/seq_self_attention_Add_Wx:0', 'seq_self_attention/seq_self_attention_Add_bh:0', 'seq_self_attention/seq_self_attention_Add_Wa:0', 'seq_self_attention/seq_self_attention_Add_ba:0', 'dense/kernel:0', 'dense/bias:0'].


"
48368,AttributeError: 'NoneType' object has no attribute 'shape',"Hello,

I'm trying to train a model using the `model.fit()` function, but it keeps throwing me an error which I cannot resolve. 
The error is:
```
Traceback (most recent call last):
  File ""train.py"", line 355, in <module>
    main()
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 620, in wrapper
    return func(*args, **kwargs)
  File ""train.py"", line 338, in main
    history = model.fit(
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 725, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3196, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AttributeError: in user code:

    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:758 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:387 update_state
        self.build(y_pred, y_true)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:317 build
        self._metrics = nest.map_structure_up_to(y_pred, self._get_metric_objects,
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1159 map_structure_up_to
        return map_structure_with_tuple_paths_up_to(
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1257 map_structure_with_tuple_paths_up_to
        results = [
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1258 <listcomp>
        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1161 <lambda>
        lambda _, *values: func(*values),  # Discards the path arg.
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:418 _get_metric_objects
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:418 <listcomp>
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    /Users/pietmuller/Dokumente/code/AI/E2EASRS/tensor_venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:439 _get_metric_object
        y_t_rank = len(y_t.shape.as_list())

    AttributeError: 'NoneType' object has no attribute 'shape'
```

This is my `.fit()` function:

```
history = model.fit(
        train_ds, 
        validation_data=val_ds, 
        epochs=EPOCHS, 
        batch_size=BATCH_SIZE,
        callbacks=[earlyStopping, modelCheckpoint]
    )
```

My train dataset is a `tf.data.Dataset` and is of type: `<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>` 

The model is building and compiling fine, but training...

When I try to look at the dataset using:

```
for batch in train_ds.take(1):
    input_shape = batch[""spectrogram""].shape
    label_shape = batch[""label""].shape
    print('Input shape:', input_shape)
```

It outputs the shape just fine.

I'm looking forward for help:)"
48367,Tensorflow 1.13.1 GPU xla training hang at cuDevicePrimaryCtxRetain (NVIDIA MPS),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binay
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4.1.5-1+cuda10.0
- GPU model and memory: Tesla V100-SXM2, 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

python process hang

```
ma-user     273     11  0 Apr03 ?        00:06:34 /home/ma-user/anaconda/bin/python thumt/bin/trainer_beta.py --input=en.train__ur.train --batch_size=2048 --spm_model=spm.model --references=ur.dev --vocab=en.vocab.txt__ur.vocab.txt --type=deep_25_3 --validation=en.dev --data_url=/home/ma-user/ma/inputs/data_url_0/ --train_url=/home/ma-user/ma/outputs/train_url_0/
```

py-spy dump info

![py-spy](https://user-images.githubusercontent.com/8072378/113856754-3dc34380-97d4-11eb-98d9-32481aa51db3.png)


final logs

```
INFO:tensorflow:Total trainable variables size: 159322624
INFO:tensorflow:Total trainable variables size: 159322624
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Create EvaluationHook.
INFO:tensorflow:Create EvaluationHook.
INFO:tensorflow:Making dir: /home/ma-user/ma/inputs/train_url_0/eval
INFO:tensorflow:Making dir: /home/ma-user/ma/inputs/train_url_0/eval
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:set by user per_process_gpu_memory_fraction = 0.450000
INFO:tensorflow:set by user per_process_gpu_memory_fraction = 0.450000
2021-04-03 04:15:11.694498: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
```

gdb bt (partial)

```
Thread 91 (Thread 0x7f01057fe700 (LWP 511)):
#0  0x00007f043b8d726d in __lll_lock_wait ()
   from /lib/x86_64-linux-gnu/libpthread.so.0
#1  0x00007f043b8d3288 in pthread_rwlock_rdlock ()
   from /lib/x86_64-linux-gnu/libpthread.so.0
#2  0x00007f035129d5a1 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#3  0x00007f035129eb1e in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#4  0x00007f035129f2c9 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#5  0x00007f0351348fbe in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#6  0x00007f035134b0d7 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#7  0x00007f0351275719 in ?? () from /usr/local/nvidia/lib64/libcuda.so.1
#8  0x00007f03513e715e in cuDevicePrimaryCtxRetain ()
   from /usr/local/nvidia/lib64/libcuda.so.1
#9  0x00007f03605b942d in stream_executor::cuda::CUDADriver::CreateContext(int, stream_executor::DeviceOptions const&, stream_executor::cuda::CudaContext**) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#10 0x00007f03605c15a4 in stream_executor::cuda::CUDAExecutor::Init(int, stream_executor::DeviceOptions) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#11 0x00007f03604fa7b7 in stream_executor::StreamExecutor::Init(int, stream_executor::DeviceOptions) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#12 0x00007f03605c839d in stream_executor::cuda::CudaPlatform::GetUncachedExecutor(stream_executor::StreamExecutorConfig const&) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#13 0x00007f03605c78dc in std::_Function_handler<stream_executor::port::StatusOr<std::unique_ptr<stream_executor::StreamExecutor, std::default_delete<stream_executor::StreamExecutor> > > (), stream_executor::cuda::CudaPlatform::GetExecutor(stream_executor::StreamExecutorConfig const&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#14 0x00007f0360432343 in stream_executor::ExecutorCache::GetOrCreate(stream_executor::StreamExecutorConfig const&, std::function<stream_executor::port::StatusOr<std::unique_ptr<stream_executor::StreamExecutor, std::default_delete<stream_executor::StreamExecutor> > > ()> const&) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#15 0x00007f03605c7960 in stream_executor::cuda::CudaPlatform::GetExecutor(stream_executor::StreamExecutorConfig const&) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#16 0x00007f03683499a6 in xla::PlatformUtil::GetStreamExecutors(stream_executor::Platform*)::{lambda()#2}::operator()() const ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007f036015fdc6 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#18 0x00007f036015ec84 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/ma-user/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#19 0x00007f0352626c5c in std::execute_native_thread_routine_compat (
    __p=<optimized out>)
    at /home/msarahan/miniconda2/conda-bld/compilers_linux-64_1507259624353/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110
#20 0x00007f043b8ce6ba in start_thread ()
   from /lib/x86_64-linux-gnu/libpthread.so.0
#21 0x00007f043b60441d in clone () from /lib/x86_64-linux-gnu/libc.so.6
```

specially, the training job use the NVIDIA GPU by NVIDIA MPS

mps control log (i can't see any 04-03 log)

```
[2021-04-02 20:15:11.789 Control    10] Accepting connection...
[2021-04-02 20:15:11.789 Control    10] NEW CLIENT 0 from user 1000: Server already exists
[2021-04-06 01:26:33.903 Control    10] Accepting connection...
[2021-04-06 01:26:33.903 Control    10] User did not send valid credentials
[2021-04-06 01:26:33.903 Control    10] Accepting connection...
[2021-04-06 01:26:33.903 Control    10] NEW CLIENT 0 from user 1000: Server already exists
```

mps server log (i can't see any 04-03 log too)

```
[2021-04-02 20:15:11.789 Other   331] Volta MPS: Creating worker thread
[2021-04-02 20:15:11.789 Other   331] Volta MPS: Device Tesla V100-SXM2-32GB (uuid 0x9be86048-0x5b1bd801-0x1850e545-0xac577163) is associated
[2021-04-06 00:59:57.679 Other   331] Receive command failed, assuming client exit
[2021-04-06 00:59:57.679 Other   331] Volta MPS: Client process disconnected
[2021-04-06 00:59:57.681 Other   331] Receive command failed, assuming client exit
[2021-04-06 00:59:57.681 Other   331] Receive command failed, assuming client exit
[2021-04-06 00:59:57.681 Other   331] Volta MPS: Client disconnected. Number of active client contexts is 9
[2021-04-06 00:59:57.681 Other   331] Volta MPS: Client disconnected. Number of active client contexts is 8
```

it's very odd, how can this thing happen, cuda can't connect to the MPS ? thanks for any help for this problem
"
48366,tf.image.resize_with_pad slows the computation enormously,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): tf-nightly==2.6.0.dev20210331
- TensorFlow version (use command below): 2.6.0-dev20210331
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2.2 / 8.1.1
- GPU model and memory: RTX 3090 24267MiB

**Describe the current behavior**
When using `tf.image.resize_with_pad` with `tf.data.Dataset`, the training of the model becomes so slow. Surprisingly, commenting `tf.image.resize_with_pad` out and using `tf.image.resize` instead, speeds up the computation at least > 5x times.   


**Describe the expected behavior**

Shouldn't both of them give more or less the same performance? Or is this a bug?

**Standalone code to reproduce the issue**
Sample used function:

```
    @tf.function
    def resize(self, image):
        # for portrait images, the aspect ratio would stay the same
        # no distortion of images, would be padded with zeros
        image = tf.image.resize_with_pad(
            image,
            self.target_height,
            self.target_width,
            method=tf.image.ResizeMethod.BILINEAR,
            antialias=False
        )

        # it is much much faster with normal resize
        # image = tf.image.resize(image, [self.target_height, self.target_width])

        return image
```

"
48365,Segmentation fault in tf-opt while running a tf dialect mlir file ," tf-opt segfaults on running the  tf.mlir file attached below 
tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --tf-shape-inference  -xla-legalize-tf=allow-partial-conversion --hlo-legalize-to-linalg -linalg-fusion-for-tensor-ops --linalg-bufferize --print-ir-before-all --print-ir-after-all resnet50_v1_tf_trimmed.mlir 
GDB log is attached 
The error seems to come from /home/ubuntu/.cache/bazel/_bazel_ubuntu/f3ca1101791d1383bd78d7eef31c6279/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/llvm-project/mlirnclude/mlir/Dialect/Linalg/IR/LinalgOps.cpp.inc 

printOperandsOrIntegersSizesList

void InitTensorOp::print(::mlir::OpAsmPrinter &p) {
  p << ""linalg.init_tensor"";
  p << ' ';
  printOperandsOrIntegersSizesList(p, *this, sizes(), static_sizesAttr());
  p.printOptionalAttrDict((*this)->getAttrs(), /*elidedAttrs=*/{""static_sizes""});
  p << ' ' << "":"";
  p << ' ';
  p << ::llvm::ArrayRef<::mlir::Type>(result().getType());
}
On changing the batchsize to a known value like from ? to  the segfault goes away. 

 %2 = ""tf.Placeholder""() {device = """", shape = #tf.shape<?x224x224x3>} : () -> tensor<?x224x224x3xf32>
 
 %2 = ""tf.Placeholder""() {device = """", shape = #tf.shape<4x224x224x3>} : () -> tensor<4x224x224x3xf32>

Looks like it's due to an unknown dim of the input tensor .

  %cst_9 =  constant dense<0xFF800000> : tensor<f32>
  %454 =  linalg.init_tensor [3, 3] : tensor<3x3xf32>
  %455 =  linalg.init_tensor [TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump:
 #0 0x000055e21d04e433 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2af433)
 #1 0x000055e21d04bcc2 llvm::sys::RunSignalHandlers() (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2accc2)
 #2 0x000055e21d04c9d8 SignalHandler(int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2ad9d8)
 #3 0x00007fad38f618a0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x128a0)
 #4 0x000055e21cfd8620 (anonymous namespace)::SSANameState::printValueID(mlir::Value, bool, llvm::raw_ostream&) const (.constprop.646) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb239620)
 #5 0x000055e21cfd9b4d (anonymous namespace)::OperationPrinter::printOperand(mlir::Value) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb23ab4d)
 #6 0x000055e21cf52ea7 mlir::printOperandsOrIntegersSizesList(mlir::OpAsmPrinter&, mlir::Operation*, mlir::OperandRange, mlir::ArrayAttr) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1b3ea7)
 #7 0x000055e21ccf4ad8 mlir::linalg::InitTensorOp::print(mlir::OpAsmPrinter&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf55ad8)
 #8 0x000055e21cd3e6c5 _ZN4mlir2OpINS_6linalg12InitTensorOpEINS_7OpTrait10ZeroRegionENS3_9OneResultENS3_14OneTypedResultINS_10TensorTypeEE4ImplENS3_13ZeroSuccessorENS3_16VariadicOperandsENS_23MemoryEffectOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf9f6c5)
 #9 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)
#10 0x000055e21cfe4c43 (anonymous namespace)::OperationPrinter::print(mlir::Block*, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245c43)
#11 0x000055e21cfe512f (anonymous namespace)::OperationPrinter::printRegion(mlir::Region&, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24612f)
#12 0x000055e21cf957b6 mlir::impl::printFunctionLikeOp(mlir::OpAsmPrinter&, mlir::Operation*, llvm::ArrayRef<mlir::Type>, bool, llvm::ArrayRef<mlir::Type>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1f67b6)
#13 0x000055e21cfbc8c2 print(mlir::FuncOp, mlir::OpAsmPrinter&) (.constprop.300) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8c2)
#14 0x000055e21cfbc8e9 _ZN4mlir2OpINS_6FuncOpEINS_7OpTrait9OneRegionENS2_10ZeroResultENS2_13ZeroSuccessorENS2_12ZeroOperandsENS2_11AffineScopeENS2_24AutomaticAllocationScopeENS_19CallableOpInterface5TraitENS2_12FunctionLikeENS2_19IsIsolatedFromAboveENS_17SymbolOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8e9)
#15 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)
#16 0x000055e21cfe541c mlir::Operation::print(llvm::raw_ostream&, mlir::AsmState&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24641c)
#17 0x000055e21cfe54d4 mlir::Operation::print(llvm::raw_ostream&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2464d4)
#18 0x000055e21cedfec6 printIR(mlir::Operation*, bool, llvm::raw_ostream&, mlir::OpPrintingFlags) (.constprop.117) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140ec6)
#19 0x000055e21cee02b3 void llvm::function_ref<void (llvm::raw_ostream&)>::callback_fn<(anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*)::'lambda'(llvm::raw_ostream&)>(long, llvm::raw_ostream&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1412b3)
#20 0x000055e21cedfa12 (anonymous namespace)::BasicIRPrinterConfig::printAfterIfEnabled(mlir::Pass*, mlir::Operation*, llvm::function_ref<void (llvm::raw_ostream&)>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140a12)
#21 0x000055e21cee075f (anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb14175f)
#22 0x000055e21ceffd71 mlir::PassInstrumentor::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb160d71)
#23 0x000055e21cf062ed mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672ed)
#24 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)
#25 0x000055e21cf07791 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>) std::for_each<llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)>(llvm::SmallVector<mlir::OpPassManager, 1u>*, llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168791)
#26 0x000055e21cf058c9 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1668c9)
#27 0x000055e21cf062df mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672df)
#28 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)
#29 0x000055e21cf07d24 mlir::PassManager::run(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168d24)
#30 0x000055e218f746a7 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, mlir::PassPipelineCLParser const&) (.constprop.155) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d56a7)
#31 0x000055e218f74a4d processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, bool, bool, bool, bool, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&) (.constprop.154) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5a4d)
#32 0x000055e218f74d51 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5d51)
#33 0x000055e218f7563f mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d663f)
#34 0x000055e2128ce58c main (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2f58c)
#35 0x00007fad38967b97 __libc_start_main /build/glibc-2ORdQG/glibc-2.27/csu/../csu/libc-start.c:344:0
#36 0x000055e2129bf4da _start (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xc204da)
 #0 0x000055e21d04e433 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2af433)
 #1 0x000055e21d04bcc2 llvm::sys::RunSignalHandlers() (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2accc2)
 #2 0x000055e21d04c9d8 SignalHandler(int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2ad9d8)
 #3 0x00007fad38f618a0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x128a0)
 #4 0x000055e21cfd8620 (anonymous namespace)::SSANameState::printValueID(mlir::Value, bool, llvm::raw_ostream&) const (.constprop.646) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb239620)
 #5 0x000055e21cfd9b4d (anonymous namespace)::OperationPrinter::printOperand(mlir::Value) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb23ab4d)
 #6 0x000055e21cf52ea7 mlir::printOperandsOrIntegersSizesList(mlir::OpAsmPrinter&, mlir::Operation*, mlir::OperandRange, mlir::ArrayAttr) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1b3ea7)
 #7 0x000055e21ccf4ad8 mlir::linalg::InitTensorOp::print(mlir::OpAsmPrinter&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf55ad8)
 #8 0x000055e21cd3e6c5 _ZN4mlir2OpINS_6linalg12InitTensorOpEINS_7OpTrait10ZeroRegionENS3_9OneResultENS3_14OneTypedResultINS_10TensorTypeEE4ImplENS3_13ZeroSuccessorENS3_16VariadicOperandsENS_23MemoryEffectOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf9f6c5)
 #9 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)
#10 0x000055e21cfe4c43 (anonymous namespace)::OperationPrinter::print(mlir::Block*, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245c43)
#11 0x000055e21cfe512f (anonymous namespace)::OperationPrinter::printRegion(mlir::Region&, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24612f)
#12 0x000055e21cf957b6 mlir::impl::printFunctionLikeOp(mlir::OpAsmPrinter&, mlir::Operation*, llvm::ArrayRef<mlir::Type>, bool, llvm::ArrayRef<mlir::Type>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1f67b6)
#13 0x000055e21cfbc8c2 print(mlir::FuncOp, mlir::OpAsmPrinter&) (.constprop.300) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8c2)
#14 0x000055e21cfbc8e9 _ZN4mlir2OpINS_6FuncOpEINS_7OpTrait9OneRegionENS2_10ZeroResultENS2_13ZeroSuccessorENS2_12ZeroOperandsENS2_11AffineScopeENS2_24AutomaticAllocationScopeENS_19CallableOpInterface5TraitENS2_12FunctionLikeENS2_19IsIsolatedFromAboveENS_17SymbolOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8e9)
#15 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)
#16 0x000055e21cfe541c mlir::Operation::print(llvm::raw_ostream&, mlir::AsmState&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24641c)
#17 0x000055e21cfe54d4 mlir::Operation::print(llvm::raw_ostream&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2464d4)
#18 0x000055e21cedfec6 printIR(mlir::Operation*, bool, llvm::raw_ostream&, mlir::OpPrintingFlags) (.constprop.117) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140ec6)
#19 0x000055e21cee02b3 void llvm::function_ref<void (llvm::raw_ostream&)>::callback_fn<(anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*)::'lambda'(llvm::raw_ostream&)>(long, llvm::raw_ostream&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1412b3)
#20 0x000055e21cedfa12 (anonymous namespace)::BasicIRPrinterConfig::printAfterIfEnabled(mlir::Pass*, mlir::Operation*, llvm::function_ref<void (llvm::raw_ostream&)>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140a12)
#21 0x000055e21cee075f (anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb14175f)
#22 0x000055e21ceffd71 mlir::PassInstrumentor::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb160d71)
#23 0x000055e21cf062ed mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672ed)
#24 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)
#25 0x000055e21cf07791 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>) std::for_each<llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)>(llvm::SmallVector<mlir::OpPassManager, 1u>*, llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168791)
#26 0x000055e21cf058c9 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1668c9)
#27 0x000055e21cf062df mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672df)
#28 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)
#29 0x000055e21cf07d24 mlir::PassManager::run(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168d24)
#30 0x000055e218f746a7 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, mlir::PassPipelineCLParser const&) (.constprop.155) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d56a7)
#31 0x000055e218f74a4d processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, bool, bool, bool, bool, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&) (.constprop.154) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5a4d)
#32 0x000055e218f74d51 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5d51)
#33 0x000055e218f7563f mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d663f)
#34 0x000055e2128ce58c main (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2f58c)
#35 0x00007fad38967b97 __libc_start_main /build/glibc-2ORdQG/glibc-2.27/csu/../csu/libc-start.c:344:0
#36 0x000055e2129bf4da _start (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xc204da)

-------------------- GDB log ------
%454 =  linalg.init_tensor [3, 3] : tensor<3x3xf32>
  %455 =  linalg.init_tensor [tf-opt: external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1118: ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value*, mlir::OpOperand*, mlir::detail::OpResultImpl*>; T = mlir::Value; PointerT = mlir::Value; ReferenceT = mlir::Value; size_t = long unsigned int]: Assertion `Index < size() && ""invalid index for value range""' failed.
Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
51	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff0d1b8b1 in __GI_abort () at abort.c:79
#2  0x00007ffff0d0b42a in __assert_fail_base (fmt=0x7ffff0e92a38 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x555572d2ca38 ""Index < size() && \""invalid index for value range\"""",
    file=file@entry=0x555572d2c8a0 ""external/llvm-project/llvm/include/llvm/ADT/STLExtras.h"", line=line@entry=1118,
    function=function@entry=0x555572d3d6a0 <llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[](unsigned long) const::__PRETTY_FUNCTION__> ""ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value""...) at assert.c:92
#3  0x00007ffff0d0b4a2 in __GI___assert_fail (assertion=0x555572d2ca38 ""Index < size() && \""invalid index for value range\"""", file=0x555572d2c8a0 ""external/llvm-project/llvm/include/llvm/ADT/STLExtras.h"",
    line=1118,
    function=0x555572d3d6a0 <llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[](unsigned long) const::__PRETTY_FUNCTION__> ""ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value""...) at assert.c:101
#4  0x00005555628b36c1 in llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[] (this=0x7fffffffc450, Index=0) at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1118
#5  0x0000555568efb182 in <lambda(mlir::Attribute)>::operator()(mlir::Attribute) const (__closure=0x7fffffffc370, a=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:84
#6  0x0000555568efbf3a in llvm::interleave<const mlir::Attribute*, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, llvm::interleave(const Container&, StreamT&, UnaryFunctor, const llvm::StringRef&) [with Container = mlir::ArrayAttr; UnaryFunctor = printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>; StreamT = mlir::OpAsmPrinter; T = const mlir::Attribute]::<lambda()>, void>(const mlir::Attribute *, const mlir::Attribute *, <lambda(mlir::Attribute)>, llvm::<lambda()>) (begin=0x55557c087c08, end=0x55557c087c28, each_fn=..., between_fn=...) at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1740
#7  0x0000555568efbe91 in llvm::interleave<mlir::ArrayAttr, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, mlir::OpAsmPrinter, const mlir::Attribute>(const mlir::ArrayAttr &, mlir::OpAsmPrinter &, <lambda(mlir::Attribute)>, const llvm::StringRef &) (c=..., os=..., each_fn=..., separator=...)
    at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1762
#8  0x0000555568efbd33 in llvm::interleaveComma<mlir::ArrayAttr, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, mlir::OpAsmPrinter, const mlir::Attribute>(const mlir::ArrayAttr &, mlir::OpAsmPrinter &, <lambda(mlir::Attribute)>) (c=..., os=..., each_fn=...)
    at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1776
#9  0x0000555568efb24e in printOperandsOrIntegersListImpl<-1l> (p=..., values=..., arrayAttr=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:81
#10 0x0000555568efaf40 in mlir::printOperandsOrIntegersSizesList (p=..., op=0x55557b4738b0, values=..., integers=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:103
#11 0x0000555568952ad0 in mlir::linalg::InitTensorOp::print (this=0x7fffffffc588, p=...)
    at bazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen/mlir/Dialect/Linalg/IR/LinalgOps.cpp.inc:313
#12 0x0000555568a60d2b in mlir::Op<mlir::linalg::InitTensorOp, mlir::OpTrait::ZeroRegion, mlir::OpTrait::OneResult, mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl, mlir::OpTrait::ZeroSuccessor, mlir::OpTrait::VariadicOperands, mlir::MemoryEffectOpInterface::Trait>::printAssembly (op=0x55557b4738b0, p=...) at external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:1709
#13 0x0000555569018294 in mlir::AbstractOperation::printAssembly (this=0x55557bf662f8, op=0x55557b4738b0, p=...) at external/llvm-project/mlir/include/mlir/IR/OperationSupport.h:93
#14 0x000055556900f789 in (anonymous namespace)::OperationPrinter::printOperation (this=0x7fffffffca60, op=0x55557b4738b0) at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2434
#15 0x000055556900f4c0 in (anonymous namespace)::OperationPrinter::print (this=0x7fffffffca60, op=0x55557b4738b0) at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2397
#16 0x0000555569010060 in (anonymous namespace)::OperationPrinter::print (this=0x7fffffffca60, block=0x55557c03d9b0, printBlockArgs=false, printBlockTerminator=true)
    at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2534
 
[resnet50_v1_tf_trimmed.tgz.txt](https://github.com/tensorflow/tensorflow/files/6270791/resnet50_v1_tf_trimmed.tgz.txt)

"
48364,LSTM layer not possible with channels_first,"The following:
```
tf.keras.backend.set_image_data_format(""channels_first"")
tf.keras.layers.LSTM(units=256)(x)
```
Returns:
```ValueError: Shape must be at least rank 3 but is rank 2 for '{{node lstm/lstm_cell_1/BiasAdd}} = BiasAdd[T=DT_HALF, data_format=""NCHW""](lstm/lstm_cell_1/MatMul, lstm/lstm_cell_1/split_1)' with input shapes: [?,256], [256].```

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- CUDA/cuDNN version: 11.1
- GPU model and memory: NVIDIA V100"
48363,"In the ""LinalgFusionOfTensorOps"" node fusion pass linalg.conv Operator does not seems to be inside the Generic Operator Region unlike other pointwise operators add/mul .. "," Looks like the linalg.conv operator  are not placed inside the Generic Operator Region
 For our custom requirement we would like to use and leverage on the ""Node fusion capability of Regions"" offered by the ""LinalgFusionOfTensorOps""  , The pass  seems to work point wise operators but not for the  operators like linalg.conv/MaxPool/BatchNorm/AVvgPool etc  
 
We have our own customized HW accelerated kernels for the above operator sets , We can do away with the affine_map<(...)>  information also.

What does it need to be done in any reference sample or pointer would be appreciated?
 
   %2 =  linalg.init_tensor [2, 112, 112, 1, 64] : tensor<2x112x112x1x64xf32>
  %cst =  constant 0.000000e+00 : f32
  %3 =  linalg.fill(%2, %cst) : tensor<2x112x112x1x64xf32>, f32 -> tensor<2x112x112x1x64xf32>
  **%4 =  linalg.depthwise_conv_2d_input_nhwc_filter_hwcf {strides =  dense<2> : tensor<2xi64>} ins(%arg0, %arg1 : tensor<2x224x224x3xf32>, tensor<7x7x3x64xf32>) outs(%3 : tensor<2x112x112x1x64xf32>) -> tensor<2x112x112x1x64xf32>**
  %5 =  linalg.tensor_reshape %4 [affine_map<(d0, d1, d2, d3, d4) -> (d0)>, affine_map<(d0, d1, d2, d3, d4) -> (d1)>, affine_map<(d0, d1, d2, d3, d4) -> (d2)>, affine_map<(d0, d1, d2, d3, d4) -> (d3, d4)>] : tensor<2x112x112x1x64xf32> into tensor<2x112x112x64xf32>
  %6 =  shape.shape_of %5 : tensor<2x112x112x64xf32> -> tensor<?xindex>
  %7 =  shape.to_extent_tensor %6 : tensor<?xindex> -> tensor<4xindex>
  %8 =  linalg.init_tensor [2, 112, 112, 64] : tensor<2x112x112x64xf32>
  %9 =  linalg.generic {indexing_maps =  [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types =  [""parallel"", ""parallel"", ""parallel"", ""parallel""]} ins(%arg2 : tensor<64xf32>) outs(%8 : tensor<2x112x112x64xf32>) {
  ^bb0(%arg321: f32, %arg322: f32):  // no predecessors
    linalg.yield %arg321 : f32
  } -> tensor<2x112x112x64xf32>
  %10 =  linalg.init_tensor [2, 112, 112, 64] : tensor<2x112x112x64xf32>
  %11 =  linalg.generic {indexing_maps =  [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types =  [""parallel"", ""parallel"", ""parallel"", ""parallel""]} ins(%5, %9 : tensor<2x112x112x64xf32>, tensor<2x112x112x64xf32>) outs(%10 : tensor<2x112x112x64xf32>) {
  ^bb0(%arg321: f32, %arg322: f32, %arg323: f32):  // no predecessors
    **%58 =  addf %arg321, %arg322 : f32**
    linalg.yield %58 : f32
  } -> tensor<2x112x112x64xf32>
"
48362,"While converting from HLO to linalg the conversion of mhlo.convolution(contains padding ) -> linalg.conv fails  with error ""non-zero padding unsupported yet"" ","While converting from HLO to linalg the conversion of mhlo.convolution(contains padding ) -> linalg.conv fails 
with error ""non-zero padding unsupported yet""  
 
 ""mhlo.convolution""(%510, %532) {batch_group_count =  1 : i64, dimension_numbers =  {input_batch_dimension =  0 : i64, input_feature_dimension =  3 : i64, input_spatial_dimensions =  dense<[1, 2]> : tensor<2xi64>, kernel_input_feature_dimension =  2 : i64, kernel_output_feature_dimension =  3 : i64, kernel_spatial_dimensions =  dense<[0, 1]> : tensor<2xi64>, output_batch_dimension =  0 : i64, output_feature_dimension =  3 : i64, output_spatial_dimensions =  dense<[1, 2]> : tensor<2xi64>}, feature_group_count =  1 : i64, padding =  dense<1> : tensor<2x2xi64>, rhs_dilation =  dense<1> : tensor<2xi64>, window_strides =  dense<1> : tensor<2xi64>} : (tensor<4x56x56x64xf32>, tensor<3x3x64x64xf32>) -> tensor<4x56x56x64xf32>

The documentation @ https://mlir.llvm.org/docs/Dialects/Linalg/#linalgpad_tensor-mlirlinalgpadtensorop
for linalg.conv (::mlir::linalg::ConvOp) seems to contain attribute 
padding | ::mlir::DenseIntElementsAttr | 64-bit signless integer elements attribute
 
Is the padding support missing in linalg.conv as of now and is due for implementation if so when could it be added."
48361,Nested model not training for distributed gradient tapes,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu18.04
- TensorFlow installed from: pip
- TensorFlow version: 2.4.1
- Python version: 3.7.1
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla V100 each with 16 GB


**Describe the current behavior**
Nested model (Encoder) not training/ model parameters not changing

**Describe the expected behavior**
Nested (Encoder) model should be trained



I had created a model for an encoder that is used in both the generator as well as the discriminator. While the same encoder model shows in total params for both the generator and discriminator, it doesn't train at all. When the same encoder model is later called on random inputs it will always give the same result of 0.

    
    tf.keras.backend.clear_session()

    gamma_init = tf.keras.initializers.RandomNormal(mean=0.02, stddev=0.02) 
    def ENCODER(shape=(int(456/2), int(456/2), 3)):
        y0 = tf.keras.Input(shape=shape,name = 'encoder')
        y1 = tf.keras.applications.MobileNetV3Small(include_top=False,weights=None,input_shape=shape)(y0)
        y1 = tf.keras.layers.GlobalAveragePooling2D()(y1)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(y1)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        encoding = tf.keras.layers.Dense(32)(x)
        encoding = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1),name=""L2_normalized_encodings"")(encoding)
        model = tf.keras.Model(y0, encoding)
        return model
    
    def EMBEDDER(shape = (1,),classes = NUM_CLASSES):
        input_ = tf.keras.Input(shape=shape,name = 'embedder')
        embedings_raw = tf.keras.layers.Embedding(NUM_CLASSES,32)(input_)
        embedings = tf.keras.layers.Lambda(lambda x:x[:,0])(embedings_raw)
        embedings = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1),name=""L2_normalized_embeddings"")(embedings)
        return tf.keras.Model(input_,embedings)
    
    def DISCRIMANTOR(encoder):
        generation = tf.keras.Input(shape=(int(456/2), int(456/2), 3), name='input_1_disc')
        embedings = tf.keras.Input(shape=(32,), name='input_2_disc')
        delta_layer = encoder(generation,training=True)
        ##SIGMOID OUTPUT MODULE
        modified_delta = tf.keras.layers.Add()([delta_layer,embedings])
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(modified_delta)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        x = tf.keras.layers.Dense(16)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        x = tf.keras.layers.Dense(8)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        output_layer = tf.keras.layers.Dense(1,activation = 'sigmoid')(x)
        return tf.keras.Model(inputs =[generation,embedings],outputs = [output_layer])

    def GENERATOR():
        encodings = tf.keras.Input(shape=(32,), name='input_1_gen')
        embedings = tf.keras.Input(shape=(32,), name='input_2_gen')
        composed_layer = tf.keras.layers.Concatenate(axis=-1)([encodings,embedings])
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(composed_layer)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        x = tf.keras.layers.Dense(256)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        encodings_for_label = tf.keras.layers.Dense(128*2*2)(x)
        image_format = tf.keras.layers.Reshape((2,2, 128), name='de_reshape')(encodings_for_label)
        first_image = tf.keras.layers.Conv2DTranspose(filters = 256,kernel_size=(2, 2) ,strides = 2)(image_format)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(first_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        second_image = tf.keras.layers.Conv2DTranspose(filters = 256,kernel_size=(3, 3),strides = 2)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        third_image = tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(3, 3) ,strides = 2)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        fourth_image = tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(1, 1))(tf.keras.layers.Add()([x,tf.keras.layers.Conv2DTranspose(filters = 128,kernel_size=(7, 7),strides = 4)(first_image)]))
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(fourth_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        second_image = tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(2, 2),strides = 3)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        third_image = tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(2, 2) ,strides = 2)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        
        fourth_image =  tf.keras.layers.Conv2DTranspose(filters = 32,kernel_size=(1, 1))(tf.keras.layers.Add()([x,tf.keras.layers.Conv2DTranspose(filters = 64,kernel_size=(6, 6),strides = 6)(fourth_image)]))
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(fourth_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        second_image = tf.keras.layers.Conv2DTranspose(filters = 16,kernel_size=(1, 1),strides = 2)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(second_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        third_image = tf.keras.layers.Conv2DTranspose(filters = 8,kernel_size=(1, 1))(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(third_image)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        fourth_image = tf.keras.layers.Conv2DTranspose(filters = 3,kernel_size = 1,name=""fourth_output"")(x)
        return tf.keras.Model(inputs=[encodings,embedings],outputs = [fourth_image]) def DISCRIMANTOR(encoder):
        generation = tf.keras.Input(shape=(int(456/2), int(456/2), 3), name='input_1_disc')
        embedings = tf.keras.Input(shape=(32,), name='input_2_disc')
        delta_layer = encoder(generation,training=True)
        ##SIGMOID OUTPUT MODULE
        modified_delta = tf.keras.layers.Add()([delta_layer,embedings])
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(modified_delta)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        x = tf.keras.layers.Dense(16)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        x = tf.keras.layers.Dense(8)(x)
        x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(x)
        x = tf.keras.layers.Dropout(0.4)(x)
        x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)
        output_layer = tf.keras.layers.Dense(1,activation = 'sigmoid')(x)
        return tf.keras.Model(inputs =[generation,embedings],outputs = [output_layer])
```


for the training, I use the following loop:- 

with stratergy.scope()
    input_image =tf.keras.Input(shape=(228,228, 3))
    input_label =tf.keras.Input(shape=(1,))
    ENC = ENCODER()
    encoding = ENC(input_image)
    embeding = EMBEDDER()(input_label)
    image_new = GENERATOR()([encoding,embeding])
    reality_check = DISCRIMANTOR(ENC)([input_image,embeding])
    discriminator = tf.keras.Model(inputs = [input_image,input_label],outputs = [reality_check])   
    gen_trainer = tf.keras.Model(inputs = [input_image,input_label],outputs = [encoding,image_new])

def train_complete():
    def train_step(inputs):
        image, labels = inputs
        #Precitions for generator:- [image,encoding_original,encoding_new]
        #Precitions for discriminator:- [softmax output,embedding+centre]
        #DISCRIMINATOR TRAINING
        discrim_labels = tf.concat([tf.ones_like(labels),tf.zeros_like(labels)],axis = 0)
        gen_discim = tf.concat([tf.zeros_like(labels),tf.ones_like(labels)],axis = 0)
        with tf.GradientTape(persistent=True) as tape:
            image_,encoding_original,encoding_new = gen_trainer(inputs,training=True)
            image_loss = GEN_IMAGE_LOSS(image,image_)
            encoding_loss = ENCODER_LOSS(encoding_original,encoding_new)
            
            binary_preds = discriminator([tf.concat([image/1.,image_],axis = 0),tf.concat([labels,labels],axis = 0)],training=True)
            binary_loss = DISC_SOFTMAX_LOSS(discrim_labels,binary_preds)
            loss_final_gen = encoding_loss + 10*image_loss + DISC_SOFTMAX_LOSS(gen_discim,binary_preds)
            loss_final_disc = binary_loss + centre_trip_loss
            
        gradients = tape.gradient(loss_final_disc, discriminator.trainable_variables)
        
        var_list_disc = discriminator.trainable_variables
        
        #CLIP BY VALUE
        sam_gradients = [tf.clip_by_value(grad, -1., 1.) for grad in gradients]
        #APPLY TO GRADIENTS
        discriminator.optimizer.apply_gradients(zip(sam_gradients, discriminator.trainable_variables),experimental_aggregate_gradients=False)
        
        
        ##GENERATOR TRAINING
          
        gradients = tape.gradient(loss_final_gen, gen_trainer.trainable_variables)
        
        var_list_gen = gen_trainer.trainable_variables
        
        #CLIP BY VALUE
        sam_gradients = [tf.clip_by_value(grad, -1., 1.) for grad in gradients]
        #APPLY TO GRADIENTS
        gen_trainer.optimizer.apply_gradients(zip(sam_gradients, gen_trainer.trainable_variables),experimental_aggregate_gradients=False)
        del tape
        return binary_loss,image_loss,encoding_loss
    @tf.function
    def distributed_train_step(inputs):
        loss1,loss2,loss3= strategy.run(train_step, args=(inputs,))
        #train_metric.update_state(strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None))
        loss1 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss1,axis=None)
        loss2 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss2,axis=None)
        loss3 = strategy.reduce(tf.distribute.ReduceOp.SUM, loss3,axis=None)
        return loss1,loss2,loss3
    return distributed_train_step
train_function = train_complete()
for _ in range(80):
    total_loss = np.array([0.0,0.0,0.0])
    num_batches = 0
    for x in train_dataset:
          total_loss += train_function(x)
          num_batches += 1
          train_loss = total_loss / num_batches
```

The loss functions here are as follows:-

```
    @tf.function
    def DISC_EMBED_LOSS(labels, predictions):
        per_example_loss = TripletSemiHardLoss(distance_metric = pairwise_distance,reduction=tf.keras.losses.Reduction.NONE)(labels, predictions)
        return per_example_loss/ strategy.num_replicas_in_sync
    @tf.function
    def ENCODER_LOSS(labels,predictions):
        per_example_loss = tf.nn.compute_average_loss(1.+tf.keras.losses.CosineSimilarity(axis=-1,reduction=tf.keras.losses.Reduction.NONE)(labels,predictions), global_batch_size=GLOBAL_BATCH_SIZE)
        return per_example_loss
    @tf.function
    def DISC_SOFTMAX_LOSS(labels, predictions):
        per_example_loss =  tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE,from_logits=True)(labels, predictions)
        return per_example_loss/ strategy.num_replicas_in_sync
    @tf.function
    def GEN_IMAGE_LOSS(labels, predictions):
        per_example_loss = tf.math.reduce_mean(tf.math.abs(labels/255. - predictions/255.))
        return per_example_loss/ strategy.num_replicas_in_sync
```

With all this being as it is when I call the encoder on any other value the prediction is always 0 irrespective of input or number of epochs. Which I believe is due to it not getting trained. What could be the issue? Does the error lie with gradient tapes, because when the encoder was trained by itself with train_on_batch, the encoder's weights were updated. Also, note there isn't any graph disconnection error in the generator and discriminator. 

"
48359,3D NMS,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):tensorflow2.3.0
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.**
Does tensorflow currently provide NMS for 3D boxes?
**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
Can you add an algorithm to non_max_suppression of the 3D frame"
48358,predict_on_batch score from tensorflow 1.15 to tensorflow 2.3,"Dear all,

I'm facing a strange behavior in keras-retinanet 1.0. I have recently updated retinanet from version 0.5 to version 1.0. I have two classes, one with a validation mAP of 0.87 and the other with a validation mAP of 0.3 both in version 0.5 and 1.0. I have trained retinanet 0.5 on the training set, and, after doing that, I have converted the trained model to an inference model. I have used predict_on_batch (as in examples) to get the predictions on the test set and I have saved them in a csv file. The predictions obtained with version 0.5 contain both classes with a score that goes from 0.2 to 1, as expected. But the predictions obtained using version 1.0 provide the predictions of one class (the one with 0.87 mAP) with a score between 0 and 1, while for the prediction of the other class the score is always below 0.5. I set up the score threshold to 0.5 during training in both cases (version 0.5 and 1.0). I don't understand why in version 1.0 the model seems not to find any object of the second class with a score greater than 0.5. The first version is in Tensorflow 1.15 with Keras 2.2.4 while the second is in Tensorflow 2.3 with Keras 2.4. Can someone help me? Does he function predict_on_batch change from tensorflow 1.15 to Tensorflow 2.3?

Best regards,
mdatres"
48357,tensorflow-lite C++ example label_image failed for oemconfig.so and libhexagon_nn_skel.so,"I compile tensorflow-lite and the example label_image in tensorflow-lite source code success,
I could run success with delegates of NNAPI, XNNPACK, GPU and also CPU with ADB, but always fail with Hexagon.
The running comand for Hexagon is: ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -j 1
The CMD window only show following message and paused for long time:
INFO: Loaded model tflite_model_int8.tflite
INFO: resolved reporter
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so_


The log of ""adb logcat"" shows:
04-07 14:52:43.607  3114  3114 I tflite  : Initialized TensorFlow Lite runtime.
04-07 14:52:43.688  3114  3114 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1732: Successfully opened fastrpc_shell_3
04-07 14:52:43.723  3114  3114 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1878: Successfully created user PD on domain 3 (attrs 0x0)
04-07 14:52:43.724  3114  3117 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:277: FastRPC latency thread started for QoS
04-07 14:52:43.728  3114  3115 E ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:745:Error 0x2: fopen failed for oemconfig.so. (No such file or directory)
04-07 14:52:43.729  3114  3115 E ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:745:Error 0x2: fopen failed for libhexagon_nn_skel.so. (No such file or directory)
04-07 14:52:44.363   887  1096 E storaged: getDiskStats failed with result NOT_SUPPORTED and size 0
04-07 14:52:46.371  1431  1431 E         : [eebbk_backup_restore]  get host by name error retry times: 245s!
04-07 14:52:47.503  1433  1635 D BES     : AEEIOCTL_RT_MON_Kick IOCTL,cmd= 2147774474, lParam=300. 


I did copy an empty file ""oemconfig.so"" to the path of my executeable file label_image.
And also I did download 3 different shared libraries “libhexagon_nn_skel.so”, “libhexagon_nn_skel_v65.so”, “libhexagon_nn_skel_v66.so” by hexagon_nn_skel.run and copy to the path of my executeable file label_image.
Should I copy ""oemconfig.so"" and “libhexagon_nn_skel.so” to other path?

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (Chinese made pad for kid learning)
- TensorFlow installed from (source or binary): build by myself
- TensorFlow version (use command below): tensorflow-2.4.1
- Python version: Python 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version:
- GPU model and memory:


Could anyone help? Or give a advice?
Thank you so much~
"
48356,WGAN-GP Keras tutorial is slow to start the first epoch,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab and Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary in both cases
- TensorFlow version (use command below): 2.4.1 in both cases
- Python version: 3.9 on my computer, whatever google colab uses
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2.1/8.1.0.77 locally, whatever google colab uses
- GPU model and memory: GeForce 1080, whatever google colab uses


**Describe the current behavior**

When I run the  [WGAN-GP tutorial from the Keras website](https://keras.io/examples/generative/wgan_gp/), I notice that the first epoch takes a really long time to start in both CPU and GPU, up to a minute.

**Describe the expected behavior**

The first epoch should start promptly, as it happens with the other tutorials. For example the [VAE](https://keras.io/examples/generative/vae/) and traditional [DCGAN](https://keras.io/examples/generative/dcgan_overriding_train_step/)

**Standalone code to reproduce the issue**

The [WGAN-GP colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/wgan_gp.ipynb). Choose a runtime accelerator and click on run all. The first epoch will always have a delay when starting training.

The [VAE colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb) and [DCGAN colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb) do not exhibit this regression.

**Other info**

I can reproduce the same performance issue on my local computer, so this is not a colab issue."
48355,"Convert a model trained with TensorFlow1, and convert the model to TFLite with TensorFlow 2 ","Hi,

If the model is trained in TensorFlow1, is it possible to convert it to TensorFlow Lite model with the tool version 2.3.1 and be parsed with TFLite delegate parser?"
48354,Which cuda versions does tf-nightly 2.6 require?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly-2.6.0.dev20210402
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip
- Nvidia Driver: 460.39
- CUDA version: cuda 11.0.2
- cudNN version: libcudnn8=8.0.5.39-1+cuda11.0
- GPU model and memory: GeForce RTX 3090 24GB

**Describe the problem**

When tensorflow attemps to load cuda libraries, it wants to load libcusolver.so.11. However cuda11.0 installs libcusolver.so.10.
The webpage of tf-nightly claims that cuda11.0 is the way to go. What are the correct dependencies? 

When using tensorflow 2.4.1 all libraries are loaded correctly. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(venv) ...$ python3
Python 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
\>>> import tensorflow as tf
2021-04-07 09:26:18.837456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
INFO:tensorflow:Enabling eager execution
INFO:tensorflow:Enabling v2 tensorshape
INFO:tensorflow:Enabling resource variables
INFO:tensorflow:Enabling tensor equality
INFO:tensorflow:Enabling control flow v2
\>>> tf.\_\_version\_\_
'2.6.0-dev20210402'
\>>> a = tf.constant(shape=(100), value=1)
2021-04-07 09:26:41.014017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-07 09:26:41.062091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:21:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
2021-04-07 09:26:41.062145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-04-07 09:26:41.064840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-04-07 09:26:41.064923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-04-07 09:26:41.066001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-07 09:26:41.066298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-07 09:26:41.066485: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64
2021-04-07 09:26:41.067051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-04-07 09:26:41.067192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-07 09:26:41.067209: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-07 09:26:41.067605: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-07 09:26:41.068940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-07 09:26:41.068989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      
"
48352,Conv2d gives an error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):  9.3.0
- CUDA/cuDNN version: 11.0 / 11.0.207
- GPU model and memory: GeForce RTX 2060

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I try to run any model with Conv it raises an unexpected error. When I run the same code within an environment without gpu (cpu only) it works properly. 
tf.test.is_gpu_avialable returns True and it seems the gpu is installed properly. Other applications (ray rllib) can run on the gpu with no problem. 

**Describe the expected behavior**
NotFoundError                             Traceback (most recent call last)
<ipython-input-13-4c2161bc3738> in <module>
----> 1 model(pp)

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    422         a list of tensors if there are more than one outputs.
    423     """"""
--> 424     return self._run_internal_graph(
    425         inputs, training=training, mask=mask)
    426 

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    558 
    559         args, kwargs = node.map_arguments(tensor_dict)
--> 560         outputs = node.layer(*args, **kwargs)
    561 
    562         # Update tensor_dict.

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
    246       inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs))
    247 
--> 248     outputs = self._convolution_op(inputs, self.kernel)
    249 
    250     if self.use_bias:

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in convolution_v2(input, filters, strides, padding, data_format, dilations, name)
   1011     dilations=None,
   1012     name=None):
-> 1013   return convolution_internal(
   1014       input,  # pylint: disable=redefined-builtin
   1015       filters,

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in convolution_internal(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)
   1141         op = conv1d
   1142 
-> 1143       return op(
   1144           input,
   1145           filters,

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in _conv2d_expanded_batch(input, filters, strides, padding, data_format, dilations, name)
   2595     # We avoid calling squeeze_batch_dims to reduce extra python function
   2596     # call slowdown in eager mode.  This branch doesn't require reshapes.
-> 2597     return gen_nn_ops.conv2d(
   2598         input,
   2599         filter=filters,

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)
    930       return _result
    931     except _core._NotOkStatusException as e:
--> 932       _ops.raise_from_not_ok_status(e, name)
    933     except _core._FallbackException:
    934       pass

~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6860   message = e.message + ("" name: "" + name if name is not None else """")
   6861   # pylint: disable=protected-access
-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)
   6863   # pylint: enable=protected-access
   6864 

~/.virtualenvs/trader/lib/python3.8/site-packages/six.py in raise_from(value, from_value)



**Standalone code to reproduce the issue**
# Create the base model from the pre-trained model ResNet50 V2
IMG_SHAPE = (32, 32, 3)
base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model(sample)


Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48350,divide_no_nan gives nans for small complex values in non-singleton tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
`divide_no_nan` (and `divide`) give `nan` results when working with very small complex values in tensors with more than one element.

**Describe the expected behavior**
Neither `divide` nor `divide_no_nan` (but especially the latter) should give a `nan` when dividing a value by itself.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.constant([1e-160, 1], tf.complex128)
tf.math.divide_no_nan(x, x)
```
https://colab.research.google.com/drive/1I1vDbMhDYYncWMxX6jg4_nSyNmeb90z0?usp=sharing"
48349,"What is the diference between NNAPI, GPU and Hexagon delegates","Could anyone describe the diference between NNAPI, NPU, GPU and Hexagon delegates?
Does it means NNAPI will use one delegate in NPU, GPU or Hexagon(DSP) according to device? If device only embedded GPU, NNAPI will use GPU; if device only embedded DSP, NNAPI will use DSP; and so on?
Or does NNAPI only use NPU delegate?

Thank you so much~"
48343,TFLite Object Detection Inference Limit 10,"@tensorflow/micro

**System information**

- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.4.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Jupyter Notebook

**Describe the problem**
Trying to make detections over an image with a tflite model. It executes correctly, but only detects 10 elements per image, no matter the image provided. 


**Reference** [**tutorial** ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) - Export & run with TensorFlow Lite step- <br>
**1.** Export model to tflite graph: 

```
python models/research/object_detection/export_tflite_graph_tf2.py \
  --pipeline_config_path output/pipeline.config \
  --trained_checkpoint_dir output/checkpoint \
  --output_directory tflite
```
 <br>

`pipeline.config`: 

```
# SSD with Mobilenet v2 FPN-lite (go/fpn-lite) feature extractor, shared box
# predictor and focal loss (a mobile version of Retinanet).
# Retinanet: see Lin et al, https://arxiv.org/abs/1708.02002
# Trained on COCO, initialized from Imagenet classification checkpoint
# Train on TPU-8
#
# Achieves 28.2 mAP on COCO17 Val

model {
  ssd {
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    num_classes: 2
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.0
        unmatched_threshold: 0.0
        ignore_thresholds: true
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    encode_background_as_zeros: true
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: [1.0, 2.0, 0.5]
        scales_per_octave: 2
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 640
        width: 640
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        depth: 128
        class_prediction_bias_init: -4.6
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            random_normal_initializer {
              stddev: 0.01
              mean: 0.0
            }
          }
          batch_norm {
            scale: true,
            decay: 0.997,
            epsilon: 0.001,
          }
        }
        num_layers_before_predictor: 4
        share_prediction_tower: true
        use_depthwise: true
        kernel_size: 3
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v2_fpn_keras'
      use_depthwise: true
      fpn {
        min_level: 3
        max_level: 7
        additional_layer_depth: 128
      }
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          random_normal_initializer {
            stddev: 0.01
            mean: 0.0
          }
        }
        batch_norm {
          scale: true,
          decay: 0.997,
          epsilon: 0.001,
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    loss {
      classification_loss {
        weighted_sigmoid_focal {
          alpha: 0.25
          gamma: 2.0
        }
      }
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    normalize_loc_loss_by_codesize: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.0
        max_detections_per_class: 200
        max_total_detections: 200
      }
      score_converter: SIGMOID
    }
  }
}

train_config: {
  fine_tune_checkpoint_version: V2
  fine_tune_checkpoint: ""checkpoint/ckpt-0""
  fine_tune_checkpoint_type: ""detection""
  batch_size: 8
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  num_steps: 50000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: .08
          total_steps: 50000
          warmup_learning_rate: .026666
          warmup_steps: 1000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 200
  unpad_groundtruth_tensors: false
}

train_input_reader: {
  label_map_path: ""/opt/ml/input/data/train/label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/opt/ml/input/data/train/train.records""
  }
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  batch_size: 1;
}

eval_input_reader: {
  label_map_path: ""/opt/ml/input/data/train/label_map.pbtxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""/opt/ml/input/data/train/validation.records""
  }
}
```

**2.** Convert to model.tflite: 
<br>

```
tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite
```

**3.** Use .tflite model 

```python 
import tensorflow as tf
import os
import numpy as np
import boto3
import cv2 
from PIL import Image
from six import BytesIO
from object_detection.utils import config_util

from object_detection.utils import config_util
from object_detection.builders import model_builder

def reshape_img(image): 

    """"""
    image: numpy.ndarray
    """"""

    width = 640
    height = 640 
    img = cv2.resize(image, (width,height))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = np.expand_dims(img, axis=0)
    return img

def images_reshaped(all_images):

   """"""
   all_images: list
   """"""

    i=0
    im_reshaped = []
    for im in all_images:
            im_reshaped.append(reshape_img(im))
        i += 1
    return im_reshaped

def detect(interpreter, input_tensor, pipeline_config_path):

    """"""
    interpreter: interpreter class
    input_tensor: tensor class
    pipeline_config_path: str
    """"""

    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()


    # We use the original model for pre-processing, since the TFLite model doesn't
    # include pre-processing.
    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
    model_config = configs['model']
    detection_model = model_builder.build(model_config=model_config, is_training=True)
    preprocessed_image, shapes = detection_model.preprocess(input_tensor)

    interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
    
    interpreter.invoke()
    boxes = interpreter.get_tensor(output_details[0]['index'])
    classes = interpreter.get_tensor(output_details[1]['index'])
    scores = interpreter.get_tensor(output_details[2]['index'])
    print(f""LEN {len(boxes.tolist()[0])}"")

    return boxes, classes, scores


def inference_img(test_images_np, interpreter, pipeline_config_path):

    """"""
    test_images_np: list 
    interpreter: interpreter class 
    pipeline_config_path: str
    """"""

    label_id_offset = 1
    for i in range(len(test_images_np)):
        input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)
        print(f""TENSOR SHAPE: {input_tensor.shape}"")
        boxes, classes, scores = detect(interpreter, input_tensor, pipeline_config_path)

if __name__ == ""__main__"":
   interpreter = tf.lite.Interpreter(model_path=""tflite/model.tflite"")
   test_images_np = images_reshaped(all_images)
   inference_img(test_images_np, interpreter, 'pipeline.config')
```

**OUTPUT**
```
LEN 10
LEN 10
LEN 10
LEN 10
LEN 10
...

```

"
48338,Unused variable body_arg_types in  class PointwiseToLinalgConverter   hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc,"File  hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc
Function : template <typename OpTy, bool isLHLO = true>
class PointwiseToLinalgConverter : public OpConversionPattern<OpTy> { }
----
 SmallVector<Type, 4> body_arg_types, body_result_types, op_result_types;
 ValueRange inputs(args.take_front(num_inputs));
 for (Value in : inputs)
      body_arg_types.emplace_back(getElementTypeOrSelf(in.getType()));
 
seems body_arg_types is populated but not used elsewhere in the function. 
 
commit ac5f2a1f0db1ba1e585559ad4e593fdb3e73713a (HEAD -> master, origin/nightly, origin/master, origin/HEAD
"
48334,some confuse about tf.shape(x) and x.shape,"When I learning the [document](https://tensorflow.google.cn/tutorials/text/transformer) about transformer, I meet some different about tf.shape(x) and x.shape
`tf.version == 2.3.0`
code:

```
class Encoder(tf.keras.layers.Layer): 
    def call(self, x, training, mask):
    ##########  it will raise a error when using x.shape[1]  ##########
        seq_len = tf.shape(x)[1]         
        x = self.embedding(x)
        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]
        
        x = self.dropout(x)
        
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)

        return x
```
the error when using `x.shape[1]` to replace `tf.shape(x)[1]`:
```
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Incompatible shapes: [64,38,128] vs. [1,8216,128]
	 [[node transformer_14/encoder_19/add (defined at <ipython-input-104-62f8963562be>:19) ]]
  (1) Invalid argument:  Incompatible shapes: [64,38,128] vs. [1,8216,128]
	 [[node transformer_14/encoder_19/add (defined at <ipython-input-104-62f8963562be>:19) ]]
	 [[gradient_tape/transformer_14/decoder_14/embedding_34/embedding_lookup/Reshape/_466]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_step_1459223]

Errors may have originated from an input operation.
Input Source operations connected to node transformer_14/encoder_19/add:
 transformer_14/encoder_19/mul (defined at <ipython-input-104-62f8963562be>:18)

Input Source operations connected to node transformer_14/encoder_19/add:
 transformer_14/encoder_19/mul (defined at <ipython-input-104-62f8963562be>:18)
```
**it waste a lot of time to find it, who can explain the different betweent them, thanks**"
48333,TFLite object detection model inference returns 0.0,"@tensorflow/micro

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Testing on computer with python 

**Describe the problem**
After exporting a functional `saved_model.pb` to  a `model.tflite`  and try to make inferences with the `Interpreter` library, the results are 0.0 when tested with images that work over the `saved_model.pb`. 


**Reference** [**tutorial** ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) - Export & run with TensorFlow Lite step- <br>
**1.** Export model to tflite graph: 

```
python models/research/object_detection/export_tflite_graph_tf2.py \
  --pipeline_config_path output/pipeline.config \
  --trained_checkpoint_dir output/checkpoint \
  --output_directory tflite
```
 <br>

**3.** Convert to model.tflite: 
<br>
```
tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite
```
**3.** Test .tflite model 

```python 
import tensorflow as tf
import os
import numpy as np
import boto3
import cv2 
from PIL import Image
from six import BytesIO
from object_detection.utils import config_util

from object_detection.utils import config_util
from object_detection.builders import model_builder

def reshape_img(image): 

    """"""
    image: numpy.ndarray
    """"""

    width = 640
    height = 640 
    img = cv2.resize(image, (width,height))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = np.expand_dims(img, axis=0)
    return img

def images_reshaped(all_images):

   """"""
   all_images: list
   """"""

    i=0
    im_reshaped = []
    for im in all_images:
            im_reshaped.append(reshape_img(im))
        i += 1
    return im_reshaped

def detect(interpreter, input_tensor, pipeline_config_path):

    """"""
    interpreter: interpreter class
    input_tensor: tensor class
    pipeline_config_path: str
    """"""

    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(f""INPUT DETAILS:\n{input_details}"")
    print(f""OUTPUT DETAILS:\n{output_details}"")

    # We use the original model for pre-processing, since the TFLite model doesn't
    # include pre-processing.
    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
    model_config = configs['model']
    detection_model = model_builder.build(model_config=model_config, is_training=True)
    preprocessed_image, shapes = detection_model.preprocess(input_tensor)

    interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
    
    interpreter.invoke()
    boxes = interpreter.get_tensor(output_details[0]['index'])
    classes = interpreter.get_tensor(output_details[1]['index'])
    scores = interpreter.get_tensor(output_details[2]['index'])
  
    print(f""BOXES {boxes}"")
    print(f""CLASSES {classes}"")
    print(f""SCORES {scores}"")

    return boxes, classes, scores


def inference_img(test_images_np, interpreter, pipeline_config_path):

    """"""
    test_images_np: list 
    interpreter: interpreter class 
    pipeline_config_path: str
    """"""

    label_id_offset = 1
    for i in range(len(test_images_np)):
        input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)
        print(f""TENSOR SHAPE: {input_tensor.shape}"")
        boxes, classes, scores = detect(interpreter, input_tensor, pipeline_config_path)

if __name__ == ""__main__"":
   interpreter = tf.lite.Interpreter(model_path=""tflite/model.tflite"")
   test_images_np = images_reshaped(all_images)
   inference_img(test_images_np, interpreter, 'pipeline.config')
```

**OUTPUT:**
```
TENSOR SHAPE: (1, 640, 640, 3)
INPUT DETAILS:
[{'name': 'input', 'index': 12, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
OUTPUT DETAILS:
[{'name': 'Identity', 'index': 0, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 1, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 4, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_3', 'index': 7, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
BOXES 0.0
CLASSES 0.0
SCORES 0.0
```"
48332,"Output shapes of then and else branches do not match  (s64[1,64]) vs. (s64[64])","<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):2.4
- Python version:3.7.9
- accelerator : TPU
**Describe the current behavior**
Model is not training on TPU but it is on GPU with same code 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[](https://www.kaggle.com/prudhvi9999/oversampled?scriptVersionId=58909509)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Code that throwing error when running on TPU
`InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-19-c05fea4c2943> in <module>
      1 model.fit(train_ds,epochs=EPOCHS,validation_data=val_ds,
----> 2           steps_per_epoch=TS,validation_steps=VS,callbacks=[lr_scheduler])

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1103               logs = tmp_logs  # No error, now safe to assign to logs.
   1104               end_step = step + data_handler.step_increment
-> 1105               callbacks.on_train_batch_end(end_step, logs)
   1106               if self.stop_training:
   1107                 break

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    452     """"""
    453     if self._should_call_train_batch_hooks:
--> 454       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
    455 
    456   def on_test_batch_begin(self, batch, logs=None):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    294       self._call_batch_begin_hook(mode, batch, logs)
    295     elif hook == 'end':
--> 296       self._call_batch_end_hook(mode, batch, logs)
    297     else:
    298       raise ValueError('Unrecognized hook: {}'.format(hook))

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)
    314       self._batch_times.append(batch_time)
    315 
--> 316     self._call_batch_hook_helper(hook_name, batch, logs)
    317 
    318     if len(self._batch_times) >= self._num_batches_for_timing_check:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)
    354       hook = getattr(callback, hook_name)
    355       if getattr(callback, '_supports_tf_logs', False):
--> 356         hook(batch, logs)
    357       else:
    358         if numpy_logs is None:  # Only convert once.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
   1018 
   1019   def on_train_batch_end(self, batch, logs=None):
-> 1020     self._batch_update_progbar(batch, logs)
   1021 
   1022   def on_test_batch_end(self, batch, logs=None):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)
   1082     if self.verbose == 1:
   1083       # Only block async when verbose = 1.
-> 1084       logs = tf_utils.to_numpy_or_python_type(logs)
   1085       self.progbar.update(self.seen, list(logs.items()), finalize=False)
   1086 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)
    512     return t  # Don't turn ragged or sparse tensors to NumPy.
    513 
--> 514   return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    515 
    516 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    657 
    658   return pack_sequence_as(
--> 659       structure[0], [func(*x) for x in entries],
    660       expand_composites=expand_composites)
    661 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    657 
    658   return pack_sequence_as(
--> 659       structure[0], [func(*x) for x in entries],
    660       expand_composites=expand_composites)
    661 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)
    508   def _to_single_numpy_or_python_type(t):
    509     if isinstance(t, ops.Tensor):
--> 510       x = t.numpy()
    511       return x.item() if np.ndim(x) == 0 else x
    512     return t  # Don't turn ragged or sparse tensors to NumPy.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)
   1069     """"""
   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1073 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1037       return self._numpy_internal()
   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1040 
   1041   @property

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 9 root error(s) found.
  (0) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
  (1) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_35]]
  (2) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_11]]
  (3) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_5]]
  (4) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_29]]
  (5) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_47]]
  (6) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_41]]
  (7) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_17]]
  (8) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])
	 [[{{node cond}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]
	 [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_23]]
0 successful operations.
0 ... [truncated]`"
48331,Support for VersionId in s3 file system,"- TensorFlow version 2.4.1

Support **VersionId** argument in `s3_file_system` operations like reading (currently this is not supported by tensorflow even though it is supported by the underlying aws sdk).

I want to be able to load a model with a specific **VersionId** from a versioned AWS S3-bucket in **tf-serving**. "
48329,model_main_tf2.py -> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 142: invalid start byte,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): example script
- OS Platform and Distribution : win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): bin
- TensorFlow version (use command below): tensorflow-cpu 2.4.1
- Python version: 3.8.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: intel integrated 


**Describe the current behavior**

> (tensorflow) C:\Users\kacpe\Desktop\tensorflow\models\research\object_detection>python model_main_tf2.py --pipeline_config_path=training/pipeline.config  --model_dir=training  --alsologtostderr
> 2021-04-06 14:21:49.605417: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
> 2021-04-06 14:21:49.605814: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
> 2021-04-06 14:22:08.223915: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2021-04-06 14:22:08.229736: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
> 2021-04-06 14:22:08.230068: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
> 2021-04-06 14:22:08.242115: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-HO16S6U
> 2021-04-06 14:22:08.242701: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-HO16S6U
> 2021-04-06 14:22:08.244472: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-04-06 14:22:08.245897: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
> WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
> W0406 14:22:08.247780  9044 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
> INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
> I0406 14:22:08.247780  9044 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
> INFO:tensorflow:Maybe overwriting train_steps: None
> I0406 14:22:08.263412  9044 config_util.py:552] Maybe overwriting train_steps: None
> INFO:tensorflow:Maybe overwriting use_bfloat16: False
> I0406 14:22:08.263412  9044 config_util.py:552] Maybe overwriting use_bfloat16: False
> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b1
> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88
> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 4
> I0406 14:22:08.357155  9044 efficientnet_model.py:147] round_filter input=32 output=32
> I0406 14:22:08.510153  9044 efficientnet_model.py:147] round_filter input=32 output=32
> I0406 14:22:08.510153  9044 efficientnet_model.py:147] round_filter input=16 output=16
> I0406 14:22:09.333078  9044 efficientnet_model.py:147] round_filter input=16 output=16
> I0406 14:22:09.333078  9044 efficientnet_model.py:147] round_filter input=24 output=24
> I0406 14:22:10.936553  9044 efficientnet_model.py:147] round_filter input=24 output=24
> I0406 14:22:10.936553  9044 efficientnet_model.py:147] round_filter input=40 output=40
> I0406 14:22:12.487046  9044 efficientnet_model.py:147] round_filter input=40 output=40
> I0406 14:22:12.487046  9044 efficientnet_model.py:147] round_filter input=80 output=80
> I0406 14:22:14.495594  9044 efficientnet_model.py:147] round_filter input=80 output=80
> I0406 14:22:14.495594  9044 efficientnet_model.py:147] round_filter input=112 output=112
> I0406 14:22:16.817428  9044 efficientnet_model.py:147] round_filter input=112 output=112
> I0406 14:22:16.817428  9044 efficientnet_model.py:147] round_filter input=192 output=192
> I0406 14:22:19.744267  9044 efficientnet_model.py:147] round_filter input=192 output=192
> I0406 14:22:19.744267  9044 efficientnet_model.py:147] round_filter input=320 output=320
> I0406 14:22:20.961132  9044 efficientnet_model.py:147] round_filter input=1280 output=1280
> I0406 14:22:21.257415  9044 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')
> WARNING:tensorflow:From C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:545: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
> Instructions for updating:
> rename to distribute_datasets_from_function
> W0406 14:22:21.567980  9044 deprecation.py:333] From C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:545: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
> Instructions for updating:
> rename to distribute_datasets_from_function
> Traceback (most recent call last):
>   File ""model_main_tf2.py"", line 113, in <module>
>     tf.compat.v1.app.run()
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 303, in run
>     _run_main(main, args)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 251, in _run_main
>     sys.exit(main(argv))
>   File ""model_main_tf2.py"", line 104, in main
>     model_lib_v2.train_loop(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 545, in train_loop
>     train_input = strategy.experimental_distribute_datasets_from_function(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 340, in new_func
>     return func(*args, **kwargs)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1143, in experimental_distribute_datasets_from_function
>     return self.distribute_datasets_from_function(dataset_fn, options)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1134, in distribute_datasets_from_function
>     return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\mirrored_strategy.py"", line 545, in _distribute_datasets_from_function
>     return input_lib.get_distributed_datasets_from_function(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 161, in get_distributed_datasets_from_function
>     return DistributedDatasetsFromFunction(dataset_fn, input_workers,
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1272, in __init__
>     _create_datasets_from_function_with_input_context(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1936, in _create_datasets_from_function_with_input_context
>     dataset = dataset_fn(ctx)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 536, in train_dataset_fn
>     train_input = inputs.train_input(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\inputs.py"", line 893, in train_input
>     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py"", line 210, in build
>     decoder = decoder_builder.build(input_reader_config)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\decoder_builder.py"", line 52, in build
>     decoder = tf_example_decoder.TfExampleDecoder(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\data_decoders\tf_example_decoder.py"", line 414, in __init__
>     _ClassTensorHandler(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\data_decoders\tf_example_decoder.py"", line 88, in __init__
>     name_to_id = label_map_util.get_label_map_dict(
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\utils\label_map_util.py"", line 201, in get_label_map_dict
>     label_map = load_labelmap(label_map_path_or_proto)
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\object_detection\utils\label_map_util.py"", line 168, in load_labelmap
>     label_map_string = fid.read()
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 117, in read
>     self._preread_check()
>   File ""C:\Users\kacpe\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 79, in _preread_check
>     self._read_buf = _pywrap_file_io.BufferedInputStream(
> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 142: invalid start byte
> 

**Describe the expected behavior**

train the model

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**pipeline.config :**

`model {
  ssd {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 640
        max_dimension: 640
        pad_to_max_dimension: true
      }
    }
    feature_extractor {
      type: ""ssd_efficientnet-b1_bifpn_keras""
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.9999998989515007e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.029999999329447746
          }
        }
        activation: SWISH
        batch_norm {
          decay: 0.9900000095367432
          scale: true
          epsilon: 0.0010000000474974513
        }
        force_use_bias: true
      }
      bifpn {
        min_level: 3
        max_level: 7
        num_iterations: 4
        num_filters: 88
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 1.0
        x_scale: 1.0
        height_scale: 1.0
        width_scale: 1.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.9999998989515007e-05
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.009999999776482582
            }
          }
          activation: SWISH
          batch_norm {
            decay: 0.9900000095367432
            scale: true
            epsilon: 0.0010000000474974513
          }
          force_use_bias: true
        }
        depth: 88
        num_layers_before_predictor: 3
        kernel_size: 3
        class_prediction_bias_init: -4.599999904632568
        use_depthwise: true
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 3
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.5
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 1.5
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
    add_background_class: false
  }
}
train_config {
  batch_size: 8
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_scale_crop_and_pad_to_square {
      output_size: 640
      scale_min: 0.10000000149011612
      scale_max: 2.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.07999999821186066
          total_steps: 300000
          warmup_learning_rate: 0.0010000000474974513
          warmup_steps: 2500
        }
      }
      momentum_optimizer_value: 0.8999999761581421
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/efficientdet_d1_coco17_tpu-32/checkpoint/ckpt-0""
  num_steps: 300000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  fine_tune_checkpoint_type: ""detection""
  use_bfloat16: true
  fine_tune_checkpoint_version: V2
}
train_input_reader: {
  label_map_path: ""C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/training/labelmap.bptxt""
  tf_record_input_reader {
    input_path: ""C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/images/train.record""
  }
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  batch_size: 1;
}

eval_input_reader: {
  label_map_path: ""C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/training/labelmap.bptxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/images/test.record""
  }
}
`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


"
48328,saved model error,"Following is my step:
1. turn bert-base-chinese to saved model
`
input_ids = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=""input_ids"")
input_mask = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=""input_mask"")
segment_ids = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=""segment_ids"")

bert_config = modeling.BertConfig.from_json_file(""chinese_L-12_H-768_A-12/bert_config.json"")
model = modeling.BertModel(
    config=bert_config,
    is_training=False,
    input_ids=input_ids,
    input_mask=input_mask,
    token_type_ids=segment_ids,
    use_one_hot_embeddings=False
)

tvars = tf.trainable_variables()
init_checkpoint = ""chinese_L-12_H-768_A-12/bert_model.ckpt""
(assignment_map, initialized_variable_names) = \
    modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    output = tf.placeholder(shape=[None, 768], dtype=tf.float32, name=""bert/pooler/dense/Tanh"")
    tf.saved_model.simple_save(
        session=sess,
        export_dir=""./saved"",
        inputs={""inpu_ids"": input_ids,
                ""inpu_mask"": input_mask,
                ""segmen_ids"": segment_ids},
        outputs={""outpu"": output}
    )
`

2. do inference on saved model using c++

then i got this error:

E0406 11:25:56.438611   107 savedmodel_backend.cc:410] 2 root error(s) found.
  (0) Invalid argument: You must feed a value for placeholder tensor 'bert/pooler/dense/Tanh_1' with dtype float and shape [?,768]
         [[{{node bert/pooler/dense/Tanh_1}}]]
         [[bert/pooler/dense/Tanh_1/_399]]
  (1) Invalid argument: You must feed a value for placeholder tensor 'bert/pooler/dense/Tanh_1' with dtype float and shape [?,768]
         [[{{node bert/pooler/dense/Tanh_1}}]]
0 successful operations.
0 derived errors ignored. 

Is there anything wrong about the way I turn ckpt to saved model?"
48327,"I am unable to install Tensorflow on my mac without Virtual Environment, how can I fix this","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48326,How can I train tensorflow model to take specific rows of historical training data in each training iteration that is related to current row?,"I have a buses dataset between stations with the entering time for each stop. Each row has information about the bus current station including (time entering this station, long/lat, date, and also the target station with its arrival time and distance(I add a sample of data below). What I want to do is to estimate the bus arrival time to any of the following station in the route. Since the data is not recorded with time-stamp I can't use LSTM to train the model using the previous window time(for example two hours before). How can I train the model for each link between two stops with only historical records that are two hours before. Any idea to build a training function that takes the previous links with two hours before from the data as the batch size.
![Screenshot from 2021-04-06 16-19-14](https://user-images.githubusercontent.com/60028253/113696247-53693800-9704-11eb-9925-2b4d347cf453.png)
"
48325,tensorflow-2.4.1 crash by pthread_mutex_lock while enable Hexagon ,"I enable Hexagon to run tensorflow-lite by ADB, it crash everytime. In ADB shell window, the error log is: 
FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x70d105cf70)
Aborted


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Android 9.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (Chinese made pad for kid learning)
- TensorFlow installed from (source or binary): build by myself
- TensorFlow version (use command below): tensorflow-2.4.1
- Python version: Python 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version:
- GPU model and memory:


The code I use to enable Hexagon is following:
const char library_directory_path[] = ""/data/local/tmp/limd"";
TfLiteHexagonInitWithPath(library_directory_path);  // Needed once at startup.
TfLiteHexagonDelegateOptions params = { 2, 0, true, false};
auto* delegate_ptr = TfLiteHexagonDelegateCreate(&params);
if(delegate_ptr == nullptr)
{
    std::cout << ""error! delegate_ptr is NULL !!"" << std::endl;
}
tflite::Interpreter::TfLiteDelegatePtr delegate(delegate_ptr,
  [](TfLiteDelegate* delegate) {
    TfLiteHexagonDelegateDelete(delegate);
    TfLiteHexagonTearDown();
  });
interpreter->ModifyGraphWithDelegate(delegate.get());


adb logcat shows:
04-06 16:15:48.900  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1732: Successfully opened fastrpc_shell_3
04-06 16:15:48.929  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1878: Successfully created user PD on domain 3 (attrs 0x0)
04-06 16:15:48.930  3777  3781 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:277: FastRPC latency thread started for QoS
04-06 16:15:48.992  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1034: remote_handle64_open: Successfully opened handle 0x2e628a80 for file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3
04-06 16:15:48.995  3777  3777 I tflite  : TfLiteHexagonDelegate delegate: 69 nodes delegated out of 71 nodes with 1 partitions.
04-06 16:15:49.282  3777  3777 F libc    : FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x7c2da59f70)
04-06 16:15:49.282  3777  3777 F libc    : Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 3777 (main), pid 3777 (main)
04-06 16:15:49.310  3784  3784 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone
04-06 16:15:49.312  1023  1023 I /system/bin/tombstoned: received crash request for pid 3777
04-06 16:15:49.313  3784  3784 I crash_dump64: performing dump of process 3777 (target tid = 3777)
04-06 16:15:49.314  3784  3784 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
04-06 16:15:49.314  3784  3784 F DEBUG   : Build fingerprint: 'EEBBK/H7000/H7000:9.0/PKQ1.190319.001/cp08240552:userdebug/release-keys'
04-06 16:15:49.314  3784  3784 F DEBUG   : Revision: '0'
04-06 16:15:49.314  3784  3784 F DEBUG   : ABI: 'arm64'
04-06 16:15:49.314  3784  3784 F DEBUG   : pid: 3777, tid: 3777, name: main  >>> ./main <<<
04-06 16:15:49.315  3784  3784 F DEBUG   : signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
04-06 16:15:49.315  3784  3784 F DEBUG   : Abort message: 'FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x7c2da59f70)'
04-06 16:15:49.315  3784  3784 F DEBUG   :     x0  0000000000000000  x1  0000000000000ec1  x2  0000000000000006  x3  0000000000000008
04-06 16:15:49.315  3784  3784 F DEBUG   :     x4  0000000000008080  x5  0000000000008080  x6  0000000000008080  x7  0000000000000038
04-06 16:15:49.315  3784  3784 F DEBUG   :     x8  0000000000000083  x9  a0ddb7fa2b51b658  x10 0000000000000000  x11 fffffffc7fffffdf
04-06 16:15:49.315  3784  3784 F DEBUG   :     x12 0000000000000001  x13 00000000606c18b5  x14 0010363e80ab4a00  x15 00009dbf4cfefd8c
04-06 16:15:49.315  3784  3784 F DEBUG   :     x16 0000007c2f3ed2b8  x17 0000007c2f30ea50  x18 0000000000000010  x19 0000000000000ec1
04-06 16:15:49.315  3784  3784 F DEBUG   :     x20 0000000000000ec1  x21 0000007c2e6ccc00  x22 0000007c2da59f58  x23 00000057483c5078
04-06 16:15:49.315  3784  3784 F DEBUG   :     x24 0000007c2f3f6000  x25 0000005747be4b04  x26 0000007c2f3f6000  x27 0000000000000018
04-06 16:15:49.315  3784  3784 F DEBUG   :     x28 0000007c2fd983d0  x29 0000007fc348a9b0
04-06 16:15:49.315  3784  3784 F DEBUG   :     sp  0000007fc348a970  lr  0000007c2f300084  pc  0000007c2f3000ac
04-06 16:15:49.316  3784  3784 I unwind  : Malformed section header found, ignoring...
04-06 16:15:49.317  3784  3784 F DEBUG   : 
04-06 16:15:49.317  3784  3784 F DEBUG   : backtrace:
04-06 16:15:49.317  3784  3784 F DEBUG   :     #00 pc 00000000000220ac  /system/lib64/libc.so (abort+116)
04-06 16:15:49.317  3784  3784 F DEBUG   :     #01 pc 000000000009377c  /system/lib64/libc.so (__fortify_fatal(char const*, ...)+120)
04-06 16:15:49.317  3784  3784 F DEBUG   :     #02 pc 0000000000092de4  /system/lib64/libc.so (HandleUsingDestroyedMutex(pthread_mutex_t*, char const*)+52)
04-06 16:15:49.317  3784  3784 F DEBUG   :     #03 pc 0000000000092c60  /system/lib64/libc.so (pthread_mutex_lock+228)
04-06 16:15:49.317  3784  3784 F DEBUG   :     #04 pc 000000000000ca00  /data/local/tmp/limd/libhexagon_interface.so
04-06 16:15:49.317  3784  3784 F DEBUG   :     #05 pc 000000000000b528  /data/local/tmp/limd/libhexagon_interface.so
04-06 16:15:49.317  3784  3784 F DEBUG   :     #06 pc 000000000001181c  /data/local/tmp/limd/libtensorflowlite_hexagon_jni.so
04-06 16:15:49.317  3784  3784 F DEBUG   :     #07 pc 000000000001197c  /data/local/tmp/limd/libtensorflowlite_hexagon_jni.so
04-06 16:15:49.317  3784  3784 F DEBUG   :     #08 pc 000000000010cf68  /data/local/tmp/limd/main
04-06 16:15:49.317  3784  3784 F DEBUG   :     #09 pc 000000000010cc10  /data/local/tmp/limd/main
04-06 16:15:49.354  1436  1584 W NativeCrashListener: Couldn't find ProcessRecord for pid 3777
04-06 16:15:49.316  3784  3784 I chatty  : uid=2000(shell) crash_dump64 identical 1 line
04-06 16:15:49.317  3784  3784 I unwind  : Malformed section header found, ignoring...
04-06 16:15:49.355  1023  1023 E /system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_49
04-06 16:15:49.351  3777  3777 W main    : type=1400 audit(0.0:92): avc: denied { search } for name=""core"" dev=""mmcblk0p59"" ino=917506 scontext=u:r:shell:s0 tcontext=u:object_r:bee_core_data_file:s0 tclass=dir permissive=0


Could anyone help? Or give a advice?
Thank you so much~
"
48324,MultiWorkerMirroredStrategy:Training is slow with 50 workers,"I am trying to use 50 workers to train my model and made it.However,training is very slow.I'd like to know if there are some tips to 
improve the performance in terms of reducing training time.I would appreciate it if you could give me some advice."
48322,fake_quant_with_min_max_vars can only get five significant figures instead of six,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): NO
- TensorFlow version (use command below):  2.4.1
- Python version: 3.7.3
- Bazel version (if compiling from source): NO
- GCC/Compiler version (if compiling from source): NO 
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100-SXM2-32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.4.0-49-22-g85c8b2a817f 2.4.1

**Describe the current behavior**
""tf.quantization.fake_quabt_with_min_max_vars"" can only get five significant figures.

**Describe the expected behavior**
""tf.quantization.fake_quabt_with_min_max_vars"" should get six significant figures when float32 has six significant figures.
With regard to pytorch ""torch.fake_quantize_per_tensor_affine"" always has six significant figures.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

import tensorflow as tf

min_var = -10.5268
max_var = 15.1868

output_1 = tf.quantization.fake_quant_with_min_max_vars(0.27, min_var, max_var, num_bits=8, narrow_range=True)
print(output_1 ) #output_1 is 0.3037033

output_2 = tf.quantization.fake_quant_with_min_max_vars(-0.27, min_var, max_var, num_bits=8, narrow_range=True)
print(output_2 ) #output_2 is -0.30370426

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48321,Outdates or Broken Link in  tensorflow/examples documentation ,"While going through  TensorFlow/examples/CONTRIBUTING.md i found out some broken or outdated links in the docs I would like to fix it with the correct once, *please assign the issue to me*"
48320,Track SignatureDef in TFlite models built using lite.TFLiteConverter.from_concrete_functions(),"
**System information**
- TensorFlow version (you are using): 2.5.0-rc0
- Are you willing to contribute it (Yes/No): Yes


**Description**
- TF2.5 introduces `get_signature_runner()` API which allows `tensorflow.lite.interpreter` to operate on the model using user provided input an output names.
- SignatureDef is captured in the tflite models in the conversion process using `tensorflow.lite.TFLiteConverter.from_saved_model()` 
- However, `tensorflow.lite.TFLiteConverter.from_concrete_functions()` doesn't yet seem to capture the SignatureDef 

Reproduced here: https://gist.github.com/avroshk/87f4ad6232118f52500238e75caedba2

In reference to the conversation here: https://github.com/tensorflow/tensorflow/issues/32180#issuecomment-813737802

**Will this change the current api? How?**
- This feature should not need to change anything in the current api. It will help bring TFLite conversion methods at par in terms of signature_def availability 


"
48319,tensorflow illegal instruction error,"hello,
when importing tensorflow i am getting an illegal instruction error.
mine is new model CPU ,but AVX instruction are missing?What should i do?plz help"
48317,MultiWorkerMirroredStrategy how to use GPU and something about datashard,"1.When I used MultiWorkerMirroredStrategy to train my model,it works normally but didn't use GPUs,so I supposed if I omit something about the config of GPUs of the Strategy.
2.I wonder if the Strategy would do datashard automatically for me if I do nothing but just send dataset to 'model.fit' without 
using tf.dataset?


"
48316,Error validating data cardinality when fitting the model,"
**System information**
- I am writing code to create models using tensorflow
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version 2.4.1
- Python version: 3.7.4
- CUDA/cuDNN version: not installed
- GPU model and memory: GeForce 940MX (256MB)

System information extracted from tf_env_collect.sh
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6260387/tf_env.txt)

**Current behavior**
When fitting the model, inputs and outputs are joined in single tuple before calling the function _check_data_cardinality in tensorflow/python/keras/engine/data_adapter.py. Then, inside the _check_data_cardinality function, a flatten method is called for joined x and y. Since input and output are of different dimension, this results in error still inside _check_data_cardinality function. I have tried input and output manipulation to prevent the error, since this could be caused by invalid input and/or output format, but had no success.

**Expected behavior**
Do not throw an error, do not mix dimension validation of input and output together.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1qZG82zYoir92D0cIVZYwihvQJtC_L66O?usp=sharing

**Other info / logs**
```
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

<ipython-input-1-1810d9a9a642> in <module>()
     31 
     32 
---> 33 test_concatenated_neural_network()

4 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _check_data_cardinality(data)
   1527           label, "", "".join(str(i.shape[0]) for i in nest.flatten(single_data)))
   1528     msg += ""Make sure all arrays contain the same number of samples.""
-> 1529     raise ValueError(msg)
   1530 
   1531 

ValueError: Data cardinality is ambiguous:
  x sizes: 3, 3, 3, 3, 3, 3, 3, 3
  y sizes: 4
Make sure all arrays contain the same number of samples.
```
"
48314,substantial overhead in tf.GradientTape.jacobian?,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.9 (Nitrogen)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): conda defaults channel (using ""tensorflow-gpu"", even though only CPUs are being used at the moment)
- TensorFlow version (use command below): unknown 2.4.1
- Python version: 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

There seems to be a substantial overhead in using the tape.jacobian(), compared to running a for-loop with tape.gradient().

** When ""n_div = 3"" in the code below:

Forward pass took 0.015 [s]
jacobian took 3.562 [s]
3 gradient calls took 0.055 [s]

** When ""n_div = 1001"" in the code below:

Forward pass took 0.014 [s]
jacobian took 3.549 [s]
1001 gradient calls took 18.359 [s]

**Describe the expected behavior**

The direct use of a for-loop with tape.gradient() implies that each backpropagation takes almost the same time as the forward pass, and the total running time scales roughly linearly with ""n_div"". This agrees with my expectation.

I was expecting tape.jacobian() to take, at most, the running time of the tape.gradient() with a for-loop in the worst case scenario of not utilizing any parallelism, and at best, almost the same time as one backpropagation. However, the jacobian's running time appears dominated by some overhead actions.

The server I tested the minimal code below has no GPU installed. Eventually I want to run a more complicated version of this on a cluster node with a GPU with the tape.jacobian() taking only one backpropagation time for ""n_div"" ~ 100.

**Standalone code to reproduce the issue**

```
import time

import numpy as np
import tensorflow as tf

def simple_func(x0, dp0):

    x1 = x0 / (1 + dp0) * 1.0001

    return x1

def scalar_example(n_repeat, dp0=0.0, show_print=True):

    x0 = tf.constant(0.0, name='x0')
    dp0 = tf.constant(dp0, name='dp0')

    t0 = time.time()
    x1, dp1 = x0, dp0
    with tf.GradientTape(persistent=False) as tape:
        tape.watch(x0)
        tape.watch(dp0)

        for _ in range(n_repeat):
            x1 = simple_func(x1, dp1)
    if show_print:
        print(f'Forward pass took {time.time()-t0:.3f} [s]')

    t0 = time.time()
    m11 = tape.gradient(x1, x0)
    if show_print:
        print(f'Gradient calc for m11 took {time.time()-t0:.3f} [s]')

        print('Finished')

    return m11

def vector_example(n_repeat, n_div):

    x0 = tf.constant(0.0, name='x0')
    dp0 = tf.constant(0.0, name='dp0')

    dp_offsets = np.linspace(-4e-3, +4e-3, n_div)
    dp0vec = dp0 + dp_offsets

    t0 = time.time()
    x1, dp1 = x0, dp0vec
    with tf.GradientTape(persistent=False) as tape:
        tape.watch(x0)
        tape.watch(dp0)

        for _ in range(n_repeat):
            x1 = simple_func(x1, dp1)
    print(f'Forward pass took {time.time()-t0:.3f} [s]')

    t0 = time.time()
    m11 = tape.jacobian(x1, x0)
    print(f'Gradient calc for m11 took {time.time()-t0:.3f} [s]')

    print('Finished')

    return m11

if __name__ == '__main__':

    n_repeat = 100
    n_div = 1001

    print(f'n_repeat = {n_repeat:d}, n_div = {n_div:d}')
    t0 = time.time()
    m11_list_vec = vector_example(n_repeat, n_div)
    tElapsed_vec = time.time()-t0

    t0 = time.time()
    dp_offsets = np.linspace(-4e-3, +4e-3, n_div, dtype=np.float32)
    m11_list_sca = np.array([scalar_example(n_repeat, dp0=dp0, show_print=False).numpy()
                             for dp0 in dp_offsets])
    tElapsed_sca = time.time()-t0

    print([n_repeat, n_div, m11_list_vec.numpy(),
           m11_list_sca,
           tElapsed_vec, tElapsed_sca])
```

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48313,Training Operations Dependencies ,"**System information**
- TensorFlow version: 2.4.1
- Are you willing to contribute it (Yes/No): Yes when published 

**Description**
I am trying to add a feature that enhances the optimization performance and it can be done in two ways:
1. Creating custom optimizers: by inheriting  [optimizer_v2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L290-L302) 
2. Extending existing optimizers

I just implemented the first option and it works just find, now I need to add the feature to existing optimizers to measure the performance enhancement, e.g. SGD and Adam. I started with ([tensorflow/tensorflow/core/kernels/training_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc) &  [training_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.h)) but then I realized that there are many files that need to be changed. 
In particular, I need to add functions like [ApplyGradientDescent](https://github.com/tensorflow/tensorflow/blob/68e6c8da5a56b4895114ec47fc6aeccb8027de88/tensorflow/core/kernels/training_ops.h#L32).
Can I get a list of files (including build files and wrappers) that need to be changed to get such function working?


Thanks,
"
48312,Custom layer failed import message is not clear,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Linux etc...
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.7
- CUDA/cuDNN version: 11
- GPU model and memory:  RTX 3090

**Describe the current behavior**

Currently when using custom layers, when the import is missing during the reloading the raised `ValueError` says:
```
ValueError: Input tensors to a Similarity>SimilarityModel must come from `tf.keras.Input`. Received: None (missing previous layer metadata).
``

Trace:
```
~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\keras\engine\functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)
    118     generic_utils.validate_kwargs(kwargs, {})
    119     super(Functional, self).__init__(name=name, trainable=trainable)
--> 120     self._init_graph_network(inputs, outputs)
    121 
    122   @trackable.no_automatic_dependency_tracking

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\keras\engine\functional.py in _init_graph_network(self, inputs, outputs)
    155         base_layer_utils.create_keras_history(self._nested_outputs)
    156 
--> 157     self._validate_graph_inputs_and_outputs()
    158 
    159     # A Network does not create weights of its own, thus it is already

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\keras\engine\functional.py in _validate_graph_inputs_and_outputs(self)
    689                          'must come from `tf.keras.Input`. '
    690                          'Received: ' + str(x) +
--> 691                          ' (missing previous layer metadata).')
    692       # Check that x is an input tensor.
```

**Describe the expected behavior**

The RaiseError should be more explicit about which layer was not properly imported so it's easier to debug. "
48311,"""A simple graph image"" in ""Introduction to graphs and tf.function"" at TF guides not loading in Safari Browser.","## URL(s) with the issue:
https://www.tensorflow.org/guide/intro_to_graphs

Link to the documentation entry:
https://github.com/tensorflow/docs/blob/master/site/en/guide/intro_to_graphs.ipynb

## Description of issue (what needs changing):

""A simple graph image"" in [""Introduction to graphs and tf.function""](https://www.tensorflow.org/guide/intro_to_graphs) not loading in Safari Browser.

![screenshot](https://github.com/Suraj1199/test/blob/master/File_000.jpeg)

Are you planning to also submit a pull request to fix the issue? 
Yes"
48309,Documentation for loading `Estimator` and using it to `predict`,"Documentation and examples describing how to use a trained estimator to make predictions exist ([ref:boosted_trees_model_understanding](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)). 

Similarly, there are examples on how to save a trained estimator ([ref](https://www.tensorflow.org/guide/estimator))

However, there is a dearth of examples/documentation on how to load a saved estimator, and then use it for predictions. Currently, I am using this [stack-overflow issue](https://stackoverflow.com/questions/58959582/saving-loading-and-predicting-from-a-tensorflow-estimator-model-2-0/60230173#comment118347400_60230173) to help figure out how to do this.
"
48308,Keras loss is stuck at the very same value during training after the first epoch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): Binary via Python pip
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: Python 3.8.5
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla P100-PCIE with 16 GB memory

**Describe the current behavior**
I am writing a Keras-based autoencoder, using my own data. That dataset includes about 20k training and about 4k validation images. All of them are very similar, all [show the very same object](https://imgur.com/a/ClbMJ0H). My model looks like this:

```
Model: ""autoencoder""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 300, 300, 1)]     0
_________________________________________________________________
encoder (Functional)         (None, 16)                5779216
_________________________________________________________________
decoder (Functional)         (None, 300, 300, 1)       6176065
=================================================================
Total params: 11,955,281
Trainable params: 11,954,897
Non-trainable params: 384
_________________________________________________________________
Model: ""encoder""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 300, 300, 1)]     0
_________________________________________________________________
conv2d (Conv2D)              (None, 150, 150, 32)      320
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 150, 150, 32)      0
_________________________________________________________________
batch_normalization (BatchNo (None, 150, 150, 32)      128
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 75, 75, 64)        18496
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 75, 75, 64)        0
_________________________________________________________________
batch_normalization_1 (Batch (None, 75, 75, 64)        256
_________________________________________________________________
flatten (Flatten)            (None, 360000)            0
_________________________________________________________________
dense (Dense)                (None, 16)                5760016
=================================================================
Total params: 5,779,216
Trainable params: 5,779,024
Non-trainable params: 192
_________________________________________________________________
Model: ""decoder""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_2 (InputLayer)         [(None, 16)]              0
_________________________________________________________________
dense_1 (Dense)              (None, 360000)            6120000
_________________________________________________________________
reshape (Reshape)            (None, 75, 75, 64)        0
_________________________________________________________________
conv2d_transpose (Conv2DTran (None, 150, 150, 64)      36928
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 150, 150, 64)      0
_________________________________________________________________
batch_normalization_2 (Batch (None, 150, 150, 64)      256
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 300, 300, 32)      18464
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 300, 300, 32)      0
_________________________________________________________________
batch_normalization_3 (Batch (None, 300, 300, 32)      128
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 300, 300, 1)       289
_________________________________________________________________
activation (Activation)      (None, 300, 300, 1)       0
=================================================================
Total params: 6,176,065
Trainable params: 6,175,873
Non-trainable params: 192
```

Then I initialize my model like this:

    IMGSIZE = 300
    EPOCHS = 20
    LR = 0.0001

    (encoder, decoder, autoencoder) = ConvAutoencoder.build(IMGSIZE, IMGSIZE, 1)
    sched = ExponentialDecay(initial_learning_rate=LR, decay_steps=EPOCHS, decay_rate=LR / EPOCHS)
    autoencoder.compile(loss=""mean_squared_error"", optimizer=Adam(learning_rate=sched))

Then I train my model like this:

    image_generator = ImageDataGenerator(rescale=1.0 / 255)
    train_gen = image_generator.flow_from_directory(
        os.path.join(args.images, ""training""),
        class_mode=""input"",
        color_mode=""grayscale"",
        target_size=(IMGSIZE, IMGSIZE),
        batch_size=BS,
    )
    val_gen = image_generator.flow_from_directory(
        os.path.join(args.images, ""validation""),
        class_mode=""input"",
        color_mode=""grayscale"",
        target_size=(IMGSIZE, IMGSIZE),
        batch_size=BS,
    )
    hist = autoencoder.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, batch_size=BS)

My batch size `BS` is 32 and I start with an initial Adam learning rate of 0.001 (but I also tried values like 0.1 down to 0.0001). I also tried to increase the latent dimensionality to something like 1024, but that doesn't solve my issue either.

**Describe the expected behavior**
During training the loss goes down in the first epoch from about 0.5 to about 0.2 - and then beginning from the second epoch that loss sticks at the very same value, e.g. 0.1989, and then it stays there ""forever"", regardless of how many epochs I train and/or the initial learning rate I use.

I would expect that the loss goes down a bit further, or would change with different learning rates. But as said that didn't help. Even another model layout did not solve my issue - the training loss is stuck to a value and it stays at this value forever.
"
48307,"TfLite Interpreter fails to execute quantized model, succeeds on non-quantized","
**System information**

- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Nightly: 2.6.0-dev20210402. installed using pip install tf-nightly
- Python version: 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Given a TensorFlow model, I convert it to TfLite in three ways:
1. Plain conversion without post training quantization 
2. Post-integer quantization when only the weights are quantized
3. Full integer quantization. 

When running inference using those three models:
1. the non-quantized version works as expected in 21 seconds
2. the version where just the weights are quantized does not terminate even after 10 minutes
3. when using the model with full integer quantization, the interpreter crashes with the following error message:
""external/ruy/ruy/apply_multiplier.cc:52: RUY_CHECK_LE condition not satisfied:   [ shift <= 7 ]   with values   [ 108 <= 7 ].""

**Describe the expected behavior**

the interpreter running the model with full integer quantization should terminate with a result similar to the non-quantized version

**Standalone code to reproduce the issue**
see linked [zip file](https://drive.google.com/file/d/1c_sKgIbDzROM9DV99lN__GHYuJOlAZBS/view?usp=sharing).

- quantize.py is the code used to convert and quantize TensorFlow to tflite
- tfliteinf.py is the code used to run inference
- poolnet_small1_tf is the original TensorFlow SavedModel
- trevi_small1.bmp is the input image
- expected.png is the expected output image
- norm_images is a directory containing the representative dataset

"
48306,TFLite: Manipulating the hardware accelerators is not allowed in the Task library currently. Only CPU is allowed,"Hello, 

I am trying to run the image classification code [here](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android). It works fine until I switch to the NNAPI option in the device. When I do that it throws the following error:
```
2021-04-05 15:26:22.086 23313-23349/org.tensorflow.lite.examples.classification E/tensorflow: ClassifierActivity: Failed to create classifier.
    java.lang.IllegalArgumentException: Manipulating the hardware accelerators is not allowed in the Task library currently. Only CPU is allowed.
        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:169)
        at org.tensorflow.lite.examples.classification.tflite.ClassifierQuantizedEfficientNet.<init>(ClassifierQuantizedEfficientNet.java:33)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:90)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:147)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.lambda$onInferenceConfigurationChanged$0$ClassifierActivity(ClassifierActivity.java:126)
        at org.tensorflow.lite.examples.classification.-$$Lambda$ClassifierActivity$83lGy2TUjuj0M5n4BhMB9qlLgSY.run(Unknown Source:8)
        at android.os.Handler.handleCallback(Handler.java:938)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:233)
        at android.os.HandlerThread.run(HandlerThread.java:67)
```

I am using Xiaomi Mi 11 as the test device (Snapdragon 888, Adreno 660) with the latest updates. Could you please help me with this issue?

Thanks!"
48305,OperatorNotAllowedInGraphError,"- TensorFlow version :2.4.0(gpu)

I am a beginner to tensorflow, when I run the following code,it report error:     OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.

```
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

def create_placeholders(n_x):
    x = tf.placeholder(tf.float32,  [None, n_x], name = ""x"")    
    return x

def get(x):
    [r, c] = tf.shape(x)
    mat = tf.zeros([r, c])
    return np.array(mat, dtype='float32')

@tf.function
def model(h):
    (m, n_x) = h.shape
    x = create_placeholders(n_x) 
    with tf.name_scope(""x_layer""):
        z = get(x)

    with tf.Session() as sess:
        v = tf.Variable(tf.ones([2,3]), name=""v"") 
        z = sess.run([z], feed_dict={x: v})           

h = tf.Variable(tf.ones([3,3]), name=""h"")
model(h)
```

I don't know what's wrong in code， how can i solve it?"
48302,MHLO to LMHLO function signature conversion: missing output argument and stale comments,"With the tensorflow git HEAD at the current master tip a83625aff5add82a150b41922ff99ad0f27fbb35 (Apr 5, 2021), I see the following issues: most of them appear are be due to stale code and an incomplete update post migration to new utilities.

INPUT
```
func @main(%arg0: tensor<1024x1024xf64>, %arg1: tensor<1024x1024xf64>) -> tensor<1024x1024xf64> {
    %0 = ""mhlo.dot""(%arg0, %arg1) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<1024x1024xf64>
    ""mhlo.return""(%0) : (tensor<1024x1024xf64>) -> ()
  }
```

OUTPUT
```
$ tf-opt -hlo-legalize-to-lhlo  dot.mlir 
module  {
  func @main(%arg0: memref<1024x1024xf64>, %arg1: memref<1024x1024xf64>) -> memref<1024x1024xf64> {
    %0 = memref.alloc() : memref<1024x1024xf64>
    ""lmhlo.dot""(%arg0, %arg1, %0) {dot_dimension_numbers = {lhs_batching_dimensions = dense<> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}} : (memref<1024x1024xf64>, memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()
    ""lmhlo.copy""(%0, %arg1) : (memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()
    ""lmhlo.terminator""() : () -> ()
  }
}
```
The tensor being returned has been copied into the wrong memref, and the signature hasn't been converted. Looks like there are assumptions on what it would convert correctly. If the `mhlo.return` is replaced with a standard return, the output is semantically correct:
```
func @main(%arg0: memref<1024x1024xf64>, %arg1: memref<1024x1024xf64>) -> memref<1024x1024xf64> {
    %0 = memref.alloc() : memref<1024x1024xf64>
    ""lmhlo.dot""(%arg0, %arg1, %0) {dot_dimension_numbers = {lhs_batching_dimensions = dense<> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}} : (memref<1024x1024xf64>, memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()
    return %0 : memref<1024x1024xf64>
  }
```
But this is inconsistent with the large doct comment at `hlo_legalize_to_lhlo.cc` which says the return value would be converted to an output argument. The comment is probably stale and makes no mention of `-buffer-results-to-out-params` which I think is necessary to get such a conversion. 

```
  // FuncOp signature conversion example:
  //
  // func @func_op(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {
  //   %0 = ""mhlo.maximum""(%arg0, %arg1) : (tensor<4xf32>, tensor<4xf32>) ->
  //   tensor<4xf32> %1 = ""mhlo.add""(%arg0, %0)  : (tensor<4xf32>,
  //   tensor<4xf32>) -> tensor<4xf32> return %1 : tensor<4xf32>
  // }
  //
  // Transformed function with an extra argument for the result. The types have
  // been converted from tensor to memref.
  //
  // func @func_op(%arg0: memref<4xf32>,
  //               %arg1: memref<4xf32>,
  //               %arg2: memref<4xf32>) {
  //   %0 = alloc() : memref<4xf32>
  
  //   ""lmhlo.maximum""(%arg0, %arg1, %0) :
  //         (memref<4xf32>, memref<4xf32>, memref<4xf32>) -> ()
  //   %1 = alloc() : memref<4xf32>
  //   ""lmhlo.add""(%arg0, %0, %1) :
  //         (memref<4xf32>, memref<4xf32>, memref<4xf32>) -> ()
  //   ""lmhlo.copy""(%1, %arg2) : (memref<4xf32>, memref<4xf32>) -> ()
  //   ""lmhlo.terminator""() : () -> ()
  // }
```
Even with a `-buffer-results-to-out-params`, a `linalg.copy` is generated instead of an `lmhlo.copy` and so this comment should be updated to reflect changes in MLIR transform utilities.

CC: @silvasean @stellaraccident "
48301,tensorflow MLIR binaries build fails due to missing dependency declarations,"With the TF git version `a83625aff5add82a150b41922ff99ad0f27fbb35` (as of Apr 5), MLIR binaries fail to build:

$ bazel build  --config=monolithic --config=noaws  --copt=-UNDEBUG --linkopt='-fuse-ld=lld'  tensorflow/compiler/mlir:all

ERROR: /home/uday/tensorflow-upstream/tensorflow/compiler/mlir/hlo/BUILD:496:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:hlo_dialect_registration':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/init.cc':
  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'
INFO: Elapsed time: 2.001s, Critical Path: 1.51s
INFO: 395 processes: 159 remote cache hit, 70 internal, 166 local.
FAILED: Build did NOT complete successfully

gcc (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)
bazel 3.7.2
CentOS 8 Linux

CC: @jurahul @joker-eph "
48300, TypeError: '>' not supported between instances of 'NoneType' and 'float',"System Information

- Running in Google Collab
- Python 3.7
- TensorFlow Version: 2.1.0
- Keras Version: 2.3.0
- Notebook is set to run on GPU
![P1](https://user-images.githubusercontent.com/71000110/113538851-85aa5680-9599-11eb-8dfd-e8075f1d0612.PNG)

When I attempted to run the code above I got this error message:

![P2](https://user-images.githubusercontent.com/71000110/113538936-bee2c680-9599-11eb-8012-a6b2a647d471.PNG)

Any thoughts on how to solve this?


"
48299,boringssl error : invalid use of incomplete type 'BIO',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: raspberrypi 3
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.3, 2.4.1, 2.0.0 (these versions are tried)
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: by bazel
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): local : 7.5.0
- CUDA/cuDNN version: 10
- GPU model and memory:



**Describe the problem**

using openssl version 1.0.2g, 1.1.1h, 1.1.1k ..., all trial has been failed.
I couldn't complete tensorflow lite lib building using below command, any suggestion would be really helpful regard to this issue.
(I tried boringssl version downgrade / upgrade, but it didn't worked)
![tried_version_boringssl](https://user-images.githubusercontent.com/50652715/113537568-06a63400-9614-11eb-8a31-fcaf4467a3aa.png)



**Provide the exact sequence of commands / steps that you executed before running into the problem**

sudo bazel build \
--config=elinux_armhf \
--config=monolithic \
--config=noaws \
--config=nohdfs \
--config=nonccl \
--config=nogcp \
--config=v1 \
--copt=-O \
-c opt \
--verbose_failures \
--define=tensorflow_mkldnn_contraction_kernel=0 \
--define=with_xla_support=false \
--define=with_gcp_support=false \
--define=tflite_convert_with_select_tf_ops=true \
--define=with_select_tf_ops=true \
//tensorflow/lite:libtensorflowlite.so

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

/home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/boringssl/BUILD:137:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF2_BEHAVIOR=0 \
    TF_CONFIGURE_IOS=0 \
  /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/include/c++/8.3.0/ -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.o' -fPIC -iquote external/boringssl -iquote bazel-out/armhf-opt/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/armhf-opt/bin/external/boringssl/src/include -w -DAUTOLOAD_DYNAMIC_KERNELS -O '-std=c++14' -DOPENSSL_NO_ASM -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/boringssl/src/ssl/bio_ssl.cc -o bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.o)
Execution platform: @local_execution_config_platform//:platform
external/boringssl/src/ssl/bio_ssl.cc: In function 'SSL* get_ssl(BIO*)':
external/boringssl/src/ssl/bio_ssl.cc:16:37: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
   return reinterpret_cast<SSL *>(bio->ptr);
                                     ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_read(BIO*, char*, int)':
external/boringssl/src/ssl/bio_ssl.cc:40:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_ACCEPT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:45:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_CONNECT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_write(BIO*, const char*, int)':
external/boringssl/src/ssl/bio_ssl.cc:80:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->retry_reason = BIO_RR_CONNECT;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'long int ssl_ctrl(BIO*, int, long int, void*)':
external/boringssl/src/ssl/bio_ssl.cc:101:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->shutdown = num;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:102:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->ptr = ptr;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:103:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->init = 1;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:107:17: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       return bio->shutdown;
                 ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc:110:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
       bio->shutdown = num;
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_free(BIO*)':
external/boringssl/src/ssl/bio_ssl.cc:148:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}
   if (bio->shutdown) {
          ^~
In file included from /usr/include/openssl/crypto.h:25,
                 from /usr/include/openssl/comp.h:16,
                 from /usr/include/openssl/ssl.h:17,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}
 typedef struct bio_st BIO;
                ^~~~~~
external/boringssl/src/ssl/bio_ssl.cc: At global scope:
external/boringssl/src/ssl/bio_ssl.cc:170:25: error: variable 'const BIO_METHOD ssl_method' has initializer but incomplete type
 static const BIO_METHOD ssl_method = {
                         ^~~~~~~~~~
In file included from /usr/include/openssl/ssl.h:18,
                 from external/boringssl/src/ssl/bio_ssl.cc:10:
external/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected identifier before numeric constant
 long BIO_set_ssl(BIO *bio, SSL *ssl, int take_owership) {
      ^~~~~~~~~~~
external/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected ',' or '...' before numeric constant
external/boringssl/src/ssl/bio_ssl.cc: In function 'long int BIO_ctrl(BIO*, int)':
external/boringssl/src/ssl/bio_ssl.cc:178:39: error: 'take_owership' was not declared in this scope
   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);
                                       ^~~~~~~~~~~~~
external/boringssl/src/ssl/bio_ssl.cc:178:54: error: 'ssl' was not declared in this scope
   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);
                                                      ^~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 8014.552s, Critical Path: 118.45s
INFO: 6038 processes: 6038 local.
FAILED: Build did NOT complete successfully

"
48298,Sample or example?,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization

## Description of issue (what needs changing):

### Clear description

Lots of the description use the word _sample_. However, [Google Machine Learning Glossary](https://developers.google.com/machine-learning/glossary) does not have an entry of _sample_. It does have _example_:

> example
> One row of a dataset. An example contains one or more features and possibly a label. See also labeled example and unlabeled example.

Since this class works on dataset, I think the description is talking about examples.

Before we have samples, there must be a process of sampling. I don't think TextVectorization is doing sampling. Therefore it's unproper to call examples samples.

Should ""sample"" be replaced by ""example""? 
"
48297,Add option to reset a tf.data.Iterator / tf.distribute.DistributedIterator,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
We can iterate over a `tf.data.dataset` using `for element in datasetA:`. This automatically ""restarts"" if we run this for multiple epochs, as seen in this code:
```
datasetA = tf.data.Dataset.from_tensor_slices([1,  2,  3,  4,  5, 6])

for epoch in range(10):
  print(f""Epoch {epoch}"")
  for element in datasetA:
    print(element)
``` 
When using an iterator `iterB = iter(datasetB)` we can exhaust the iteration, as seen in this code:

```
datasetA = tf.data.Dataset.from_tensor_slices([1,  2,  3,  4,  5, 6])
datasetB = tf.data.Dataset.from_tensor_slices([11, 22, 33, 44])

iterB = iter(datasetB)
epochs = 5

for epoch in range(epochs):
  print(f""Epoch {epoch}"")
  for element in datasetA:
    print(element)
    elementB = iterB.get_next()
    print(elementB)
```

We can handle this by catching the error or using `get_next_as_optional()`, and **manually** restart the iteration:
```
for epoch in range(epochs):
  print(f""Epoch {epoch}"")
  for element in datasetA:
    print(element)
    elementB = iterB.get_next_as_optional()
    if not elementB.has_value():
      iterB = iter(datasetB) #<--- restarts the iterator manually
      elementB = iterB.get_next_as_optional()

    print(elementB.get_value())
```
I propose a method like `.reset_iterator()` or `restart_iterator()` that handles this case conveniently. The same holds for distributed iterators.

**Will this change the current api? How?**
This won't have any effect

**Who will benefit with this feature?**
- Users that iterate over a dataset element by element, without using `for elem in dataset:` loops
- Users iterating multiple datasets **pairwise** simultaneously (see this [question](https://stackoverflow.com/questions/66929798/how-to-do-a-pairwise-iteration-over-two-unequal-length-tf-datasets/66931999#66931999))

**Any Other info.**
As an alternative, make the `.take(count)` operation advance the internal state, so that repeatedly taking `count` elements yields the next element, rather than the same.
"
48296,Core ML delegate for TensorFlow Lite does not run examples/lite/examples/image_classification/ios on Neural Engine.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, according to https://www.tensorflow.org/lite/performance/coreml_delegate.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPad Air(Gen.4)
- TensorFlow installed from (source or binary): I have no idea since I used pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
- TensorFlow version (use command below): I have no idea since I used pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Using Core ML delegate does not have any performance boost compared with the original code on A14 processor.

**Describe the expected behavior**
Using Core ML delegate should have performance boost compared with the original code by A14 Neural Engine.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

examples/lite/examples/image_classification/ios/ in https://github.com/y-ich/examples/tree/coreml_delegate

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48295,Do TensorFlow probability (TFP) layers have,"**System information**
- TensorFlow version (you are using): 2.4 (binary)

**Describe the feature and the current behavior/state.**
`tf.keras.layers.Layer` -> class from which all layers inherit.



**Who will benefit from this feature?**
Users of TFP who would like to inherit the base class and add their own layers. 

"
48293,Build from source on OSX 10.13.6 wrongly produces .whl package for 10.14,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: v2.4.1, v2.5.0rc0
- Python version: 3.9
- Installed using virtualenv? pip? conda?: Build from source
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**

Running `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` on OSX 10.13.6 as per instruction produces a .whl package for OSX 10.14 and not 10.13.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
pip install -U --user pip numpy wheel
pip install -U --user keras_preprocessing --no-deps
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
bazel build //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```

Package in /tmp/tensorflow_pkg is named `tensorflow-2.4.1-cp39-cp39-macosx_10_14_x86_64.whl`

"
48292,Custom Layer Using Packages Other Than Tensorflow,"**System information**
- TensorFlow version (2.4.1):
- Are you willing to contribute it (If you pay yes):



**Describe the feature and the current behavior/state.**
Currently you can't use packages other than tensorflow to build a layer 

**Will this change the current api? How?**
I don't know

**Who will benefit with this feature?**
Everyone who wants to use packages like graph analysis or topological data analysis to build a custom layer for specific works ."
48290,Problems with tf.function,"@tensorflow/micro

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.10
- TensorFlow version: 2.4.1
- Python version : 3.8

**Describe the problem**

I would like to run inference on a microcontroller above by using a model characterized by Conv1D layers.  I thought I would use something like this:

`// Pull in only needed operations (should match NN layers).`
`// Template parameter <n> is number of ops to be added. Available ops:`
`// tensorflow/lite/micro/kernels/micro_ops.h`

`static tflite::MicroMutableOpResolver <1> micro_op_resolver;`
`tflite_status = micro_op_resolver.Conv1D();`
 
 `if (tflite_status != kTfLiteOk) {`
   `error_reporter->Report(""Could not add Conv1D op"");`
`while(1);`
`}`

However, the operation above shouldn't be supported by TensorFlow Lite.
Then if I use this simple code:

`import tensorflow as tf`
`tf.config.run_functions_eagerly(True)`

`input_shape = (1, 7, 1)`
`x = tf.random.normal(input_shape)`

`@tf.function`
`def convol1d():`
   `y=tf.keras.layers.Conv1D(1, 3, input_shape=input_shape[1:], name=""Conv1D"")(x)`
`return y`

`data = convol1d()`
`print(""\n\n data is:"", data)`

`tflite_model_name = 'convol1d' `
`converter = tf.lite.TFLiteConverter.from_concrete_functions([convol1d.get_concrete_function()])`
`converter.allow_custom_ops = True`
`tflite_model = converter.convert()`
`open(tflite_model_name + '.tflite', 'wb').write(tflite_model)`

If I run it, it appears the following error:

`ValueError: tf.function-decorated function tried to create variables on non-first call.`

Instead, I would expect it:

`Error:
Didn't find custom operator for name 'Conv1D'`
`Registration failed.`


Thanks in advance.





"
48289,Autograph produces code with invalid syntax when transforming expressions containing tensor slicing with a colon in tf 2.6.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 11
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-54030-g23c3e3c4ad1 2.6.0-dev20210331
- Python version: 3.9.2
- CUDA/cuDNN version: using a CPU
- GPU model and memory: using a CPU

**Describe the current behavior**
When transforming expressions containing tensor slicing, autograph puts the contents of the brackets in parentheses (wraps them in a tuple). This produces invalid syntax if one of the characters in the brackets is a colon.

**Describe the expected behavior**
Autograph should transform expressions involving tensor slicing with colons without producing code that contains invalid syntax. 

**Standalone code to reproduce the issue**
```
import tensorflow as tf

@tf.function
def test_function(x):
    return x[:,0]

test_function(tf.ones((1, 1)))
```


**Other info / logs**

The above code produces the following warning:
```
WARNING:tensorflow:AutoGraph could not transform <function test_function at 0x7fd9511f7040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: invalid syntax (tmpna4wrl96.py, line 12)
```
Full output is attached as [output.log](https://github.com/tensorflow/tensorflow/files/6252266/output.log).

The contents of the temporary file produced by autograph are:
```
# coding=utf-8
def outer_factory():

    def inner_factory(ag__):

        def tf__test_function(x):
            with ag__.FunctionScope('test_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
                do_return = False
                retval_ = ag__.UndefinedReturnValue()
                try:
                    do_return = True
                    retval_ = ag__.ld(x)[(:, 0)]
                except:
                    do_return = False
                    raise
                return fscope.ret(retval_, do_return)
        return tf__test_function
    return inner_factory
```

The syntax error is in the expression ```retval_ = ag__.ld(x)[(:, 0)]```. Removing the parentheses around ```:, 0``` fixes the error.

This problem does not appear when using tf 2.2.0. I haven't tested with other versions."
48288,UnboundLocalError 'strategy' referenced before assignment during SSD mobilenet FPN evaulation ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no custom code. Using stock example in tensorflow
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1 
- GPU model and memory: RTX 2080, 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I used this `python model_main_tf2.py --model_dir=models\my_mobilenet640v2 --pipeline_config_path=models\my_mobilenet640v2\pipeline.config --checkpoint_dir=models\my_mobilenet640v2 --wait_interval=60 --eval_timeout=60` in stock model_main_tf2.py to evaluate a new trained object detection model using my own custom set of image data while training on top of a pre-trained model.

It throw a error 'UnboundLocalerror: local variable 'strategy' referenced before assignment.
pastebin of error: https://pastebin.com/rjfY6dhp
**Describe the expected behavior**
strategy scope should not have have error
Will ignoring this error cause any potential problems when exporting to tflite using export_tflite_ssd_graph.py ?

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48287,Embedding Projector lacks scrolling on Nearest points in the original space.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Chrome Browser
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: N/A
-   **TensorFlow version (use command below)**: N/A
-   **Python version**: N/A
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: Go to projector.tensorflow.org and select a word. Check the Nearest points in the original space.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
48285,Update/Refactor Community Page,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/community/contribute/community

## Description of issue (what needs changing):
There some outdated and missing info in this page. See the thread at
https://github.com/tensorflow/docs/pull/1868#issuecomment-812568979

/cc @theadactyl "
48282,[RNN] input->type != output->type (UINT8 != FLOAT32)Node number 18 (TANH) failed to prepare,"### 1. System information

- Windows 10
- pip install tf-nightly
- 2.6.0.dev20210330

### 2. Code
The following code converts and quantizes a stateless lstm layer with additional inputs and outputs to handle the state by the user program. This was suggested at https://www.tensorflow.org/lite/convert/rnn
> It is still possible to model a stateful Keras LSTM layer using the underlying stateless Keras LSTM layer and managing the state explicitly in the user program. Such a TensorFlow program can still be converted to TensorFlow Lite using the feature being described here.

```
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf
import numpy as np

rng = np.random.default_rng()


def representative_dataset():
    yield [rng.random((1, 1024), dtype=np.float32),
           rng.random((1, 1, 1024), dtype=np.float32),
           rng.random((1, 1024), dtype=np.float32)]


def create_keras_model(keras_file_name, stateful=True):
    input = keras.Input(shape=(1, 1024,), name=""input"")
    hidden_state = keras.Input(shape=(1024,), name=""lstm_hidden_state"")
    cell_state = keras.Input(shape=(1024,), name=""lstm_cell_state"")
    output, new_hidden_state, new_cell_state = layers.LSTM(units=1024, return_sequences=True, unroll=True,
                                                           return_state=True,
                                                           name=""lstm"")(input, initial_state=[hidden_state, cell_state])
    model = keras.Model([input, hidden_state, cell_state],
                        [output, new_hidden_state, new_cell_state])
    model.summary()
    model.compile()
    model.save(keras_file_name)


def convert_to_tflite(keras_file_name, tflite_filename):
    converter = tf.lite.TFLiteConverter.from_saved_model(keras_file_name)
    #converter.experimental_new_converter = True
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.target_spec.supported_types = [tf.int8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    tflite_model = converter.convert()
    with open(tflite_filename, 'wb') as f:
        f.write(tflite_model)


def main():
    keras_file_name = ""example_model""
    tflite_filename = ""example_model.tflite""
    create_keras_model(keras_file_name)
    convert_to_tflite(keras_file_name, tflite_filename)

    # run model with random input
    interpreter = tf.lite.Interpreter(tflite_filename)
    interpreter.allocate_tensors()
    for input_detail in interpreter.get_input_details():
        scale, zero_point = input_detail['quantization']
        input_tensor = rng.random(input_detail[""shape""], dtype=np.float32)
        input_tensor = input_detail[""dtype""](input_tensor / scale + zero_point)
        interpreter.set_tensor(input_detail[""index""], input_tensor)

    interpreter.invoke()


if __name__ == '__main__':
    main()
```

![grafik](https://user-images.githubusercontent.com/16699443/113432571-e7be5c80-93dd-11eb-8bff-629fcaf1b275.png)


### 3. Failure after conversion

Running the model after conversion fails with:
```
Traceback (most recent call last):
  File ""C:/Users/user/PycharmProjects/Project/LSTM_tflite_playground.py"", line 62, in <module>
    main()
  File ""C:/Users/user/PycharmProjects/Project/LSTM_tflite_playground.py"", line 51, in main
    interpreter.allocate_tensors()
  File ""C:\Users\user\PycharmProjects\Project\venv\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 408, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: tensorflow/lite/kernels/activations.cc:393 input->type != output->type (UINT8 != FLOAT32)Node number 18 (TANH) failed to prepare.
```
A normal stateless model without the additional in and outputs runs fine. Is there any workaround? 
"
48281,TensorflowLite (API C++)  No NNAPI acceleration,"**System information**
- OS Platform and Distribution : Linux Ubuntu 20.04
- Mobile device : HUAWEI Honor 8X, Samsung S20, ...
- Android ABI : ARM64
- TensorFlow installed : from source
- TensorFlow version : v2.4.1 and master branch
- Python version: 3.8.8 (conda forge)
- Bazel version : 4.0.0 (conda forge)
- NDK : 21.4.7075529
- NDK API : 29

Hello, I try to perform some pubic and private models with TensorflowLite-API-C++ on several phone under Android 10 at least.

To do that, I try to use GPU delegate, NNAPI, XNNPACK methods based on tensorflow/lite/examples/label_image code. 

But unfortunately, I don't have any acceleration difference between NNAPI and pure CPU. Sometime with NNAPI is even worst.

To be sure that the bug not come from my code I use label_image (from tensorflow/lite/examples/label_image)  to test difference between nnapi and cpu acceralation and with several models like mobilenet, inceptionV4, .. (from https://www.tensorflow.org/lite/guide/hosted_models)

So, I follow the documentation of the label_image sample (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) : 

> git clone --depth 1 https://github.com/tensorflow/tensorflow.git tf_arm64
> cd tf_arm64
> ./configure
> bazel build -c opt --config=android_arm64   //tensorflow/lite/examples/label_image:label_image
> adb push bazel-bin/tensorflow/lite/examples/label_image/label_image /data/local/tmp
> adb shell
> ... 
> ./label_image -c 10 -a 1 ...
> ...
> ./label_image -c 10 ...

But with all the models I don't have any NNAPI acceleration like it's not used. I have the same behavior with Samsung S20 and others mobile phone, It's normal ?
 
I want try NNAPI because with GPU delegate my model have unsupported graph node like  :

>   E tflite  : Following operations are not supported by GPU delegate:
>   E tflite  : GATHER: Operation is not supported.
>   E tflite  : 143 operations will run on the GPU, and the remaining 9 operations will run on the CPU.
>   E tflite  : TfLiteGpuDelegate Init: SPLIT: ReadNonConstantTensor: value is a constant tensor: 224
>   I tflite  : Created 0 GPU delegate kernels.
>   E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized
>   E tflite  : Node number 152 (TfLiteGpuDelegateV2) failed to prepare.
>   E tflite  : Restored original execution plan after delegate application failure.
>   I libTFLAppAndroidConsole_arm64-v8a.so: Failed to apply  GPU  delegate.

If you have any advice. 
"
48280,"The new Apple M1 MLcompute Tensorflow2.4 not compatible with Numpy 1.20.1, after attempting installing ~>1.19.2, got error","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Air M1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Github Apple MLcompute repo
- TensorFlow version: 2.4
- Python version: 3.9
- Installed using virtualenv? pip? conda?: conda, miniforge3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
After #48237, I uninstalled 1.20.1 and 1.18.5 and tried to install 1.19.2 or ~>1.19.2. error 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I uninstalled numpy 1.20.1 and 1.18.5 and tried to install numpy~=1.19.2 and got this error:
```
ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48279,tf.while loop in MultiWorkerMirroredStrategy leads to training error: Complete shape not known for Adam/allreduce/CollectiveReduce,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuda 11.0/cudnn 8
- GPU model and memory: NVIDIA GTX 1080 8GB

**Describe the current behavior:**

In a cutom tf.keras.Model using a tf.while loop within train_step works fine when using non ditributed training or tf.distribute.MirroredStrategy.
When using tf.distribute.MultiWorkerMirroredStrategy following error is shown upon training startup which then leads to NaN loss when training a complex model:

> 2021-04-02 12:56:07.219137: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:455] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219172: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219180: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219187: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:928] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce

I have prepared an example to reproduce the problem in the code below. The issue is present for GPU and CPU. Due to the simplified example, it does not lead to immediate NaN loss as it does in my more complex model. However, I hope it is sufficient to reproduce and identify the issue. When switching to SGD the error message does not appear. Nevertheless, when using tf.distribute.MultiWorkerMirroredStrategy it goes to immediate NaN loss in my more complex model as well.

**Describe the expected behavior:**

Training should work with tf.distribute.MultiWorkerMirroredStrategy as it does for standalone training or training when using tf.distribute.MirroredStrategy.

**Standalone code to reproduce the issue**
Needed files are provided as well in this zipfile:
[example.zip](https://github.com/tensorflow/tensorflow/files/6249694/example.zip)
Use code below or unzip the files in a directory and run python test_worker_0.py on one instance and python test_worker_1.py on another instance of the same machine.

Code for TestModel.py:
```
import tensorflow as tf
import numpy as np

loss_tracker = tf.keras.metrics.Mean(name=""loss"")


class TestModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super(TestModel, self).__init__(*args, **kwargs)
        self.Dense = tf.keras.layers.Dense(80)
        self.MSE = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)

    def train_step(self, data):
        batch_size = tf.squeeze(tf.slice(tf.shape(data), [0], [1]), -1)
        #tried hardcoding batch_size to e.g. 16 with two workers -> does not resolve the issue
        max_length = 10
        i_start = tf.constant(0, dtype=tf.int32)
        dummy_inputs = tf.zeros([batch_size, 1, 200])
        gen_outputs = tf.zeros([batch_size, 0, 80], tf.float32)

        def body(i, input, output_full):
            output_single = self.Dense(input)
            output_full = tf.concat([output_full, output_single], 1)
            i_next = i + 1
            return i_next, input, output_full

        with tf.GradientTape() as tape:
            _, _, gen_data = tf.while_loop(cond=lambda i,
                                                       input,
                                                       output_full: tf.less(i, max_length),
                                           body=body,
                                           loop_vars=(i_start,
                                                      dummy_inputs,
                                                      gen_outputs,),
                                           shape_invariants=(i_start.get_shape(),
                                                             tf.TensorShape([None, None, 200]),
                                                             tf.TensorShape([None, None, 80])))
            loss = self.MSE(data, gen_data)
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        loss_tracker.update_state(loss)

        return {""loss"": loss_tracker.result()}


def get_dummy_data():
    x = np.random.random((128, 10, 80))
    dataset = tf.data.Dataset.from_tensor_slices(x)
    dataset.options().experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    dataset = dataset.batch(32)
    return dataset


def get_model():
    model = TestModel()
    optimizer = tf.keras.optimizers.Adam()
    model.compile(optimizer=optimizer)
    return model

```

Code for test_worker_0.py:
```
import os
import json
import tensorflow as tf
os.environ.pop('TF_CONFIG', None)
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf_config = {
    'cluster': {
        'worker': ['localhost:12345', 'localhost:23456']
    },
    'task': {'type': 'worker', 'index': 0}
}
os.environ['TF_CONFIG'] = json.dumps(tf_config)

communication_options = tf.distribute.experimental.CommunicationOptions(
    implementation=tf.distribute.experimental.CommunicationImplementation.RING)
strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)

import TestModel

with strategy.scope():
    model = TestModel.get_model()

data = TestModel.get_dummy_data()
model.fit(data, epochs=4)
```

Code for test_worker_1.py:
```
import os
import json
import tensorflow as tf
os.environ.pop('TF_CONFIG', None)
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf_config = {
    'cluster': {
        'worker': ['localhost:12345', 'localhost:23456']
    },
    'task': {'type': 'worker', 'index': 1}
}
os.environ['TF_CONFIG'] = json.dumps(tf_config)

communication_options = tf.distribute.experimental.CommunicationOptions(
    implementation=tf.distribute.experimental.CommunicationImplementation.RING)
strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)

import TestModel

with strategy.scope():
    model = TestModel.get_model()

data = TestModel.get_dummy_data()
model.fit(data, epochs=4)
```

**Other info / logs**
Full logs from a run of the example:

> 2021-04-02 12:56:04.115797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
> 2021-04-02 12:56:04.797812: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2021-04-02 12:56:04.798297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
> 2021-04-02 12:56:04.819525: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
> 2021-04-02 12:56:04.819543: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: test
> 2021-04-02 12:56:04.819548: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: test
> 2021-04-02 12:56:04.819598: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.39.0
> 2021-04-02 12:56:04.819613: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.39.0
> 2021-04-02 12:56:04.819618: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.39.0
> 2021-04-02 12:56:04.819910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-04-02 12:56:04.821691: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2021-04-02 12:56:04.823450: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2021-04-02 12:56:04.827242: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}
> 2021-04-02 12:56:04.828921: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:23456
> 2021-04-02 12:56:04.885062: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
> 2021-04-02 12:56:04.885517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3700075000 Hz
> Epoch 1/4
> 2021-04-02 12:56:07.219137: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:455] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219172: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219180: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 2021-04-02 12:56:07.219187: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:928] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce
> 4/4 [==============================] - 2s 3ms/step - loss: 0.3312
> Epoch 2/4
> 4/4 [==============================] - 0s 3ms/step - loss: 0.3290
> Epoch 3/4
> 4/4 [==============================] - 0s 3ms/step - loss: 0.3270
> Epoch 4/4
> 4/4 [==============================] - 0s 3ms/step - loss: 0.3250

"
48278,Issue: tf.function not working when dealing with tf.stack,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version 2.3.0


**Describe the current behavior**
tf.function is not working in converting functions containing tf.stack.

**Describe the expected behavior**
I would expect tf.function to actually manage to convert the function

**Standalone code to reproduce the issue**
```
import tensorflow as tf

c = tf.Variable([[1., 5.], [2., 4.]])

@tf.function
def toy_fct(x):
    y = tf.stack([x[0,:], x[1,:]], axis=0)
    return y

toy_fct(c)
```

**Other info / logs**
The warning given is:

> WARNING:tensorflow:AutoGraph could not transform <function toy_fct at 0x000001DC9A13A670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function toy_fct at 0x000001DC9A13A670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 5.],
       [2., 4.]], dtype=float32)>
"
48277,TFLite Android 'Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei Nova 3i
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to convert this YoloV3 model, written for tensorflow v2.0 to a tflite model. https://github.com/YunYang1994/tensorflow-yolov3
The model is first compiled and saved. Then, it is converted.
Here is my code for converting the model:
```
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(""model"")
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_model = converter.convert()
open(""converted_model/model.tflite"", ""wb"").write(tflite_model)
```

The tflite model is imported into Android Studio using New > Other > Tflite Model
However, when running the model an error occurred:
```
Registration failed.
    
        at android.app.ActivityThread.deliverResults(ActivityThread.java:5078)
        at android.app.ActivityThread.handleSendResult(ActivityThread.java:5120)
        at android.app.servertransaction.ActivityResultItem.execute(ActivityResultItem.java:49)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2199)
        at android.os.Handler.dispatchMessage(Handler.java:112)
        at android.os.Looper.loop(Looper.java:216)
        at android.app.ActivityThread.main(ActivityThread.java:7625)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:524)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:987)
     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'
```
I am using
```
    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'
    implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.4.0'
```

Thanks.
"
48275,tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux bullseye/sid
- TensorFlow installed from (source or binary): Not sure (I did not install it myself)
- TensorFlow version (use command below): 2.5.0-dev20210323
- Python version: Python 3.9.2
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla V100

You can also obtain the TensorFlow version with:
v1.12.1-53554-gb725e835c68 2.5.0-dev20210323


**Describe the current behavior**
There was an update on multiple aspects of the machine I train my models and after that I get the following error.  **Not sure what version exactly is not matching or not working, although due to the connected_components in the logs Im guessing it might have something to do with TF-Addons ('0.13.0-dev').**

When trying to train the model I get the following error:
```
  File ""/home/pduque/.local/lib/python3.9/site-packages/comet_ml/monkey_patching.py"", line 317, in wrapper
    return_value = original(*args, **kwargs)
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/training.py"", line 1154, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py"", line 872, in __call__
    result = self._call(*args, **kwds) 
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py"", line 3023, in __call__
    return graph_function._call_flat(  
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py"", line 1960, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py"", line 591, in call          
    outputs = execute.execute(                                                                                          
  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute               tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, 

tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.                                             (0) Internal:  invalid resource handle                                                                                
         [[{{node PartitionedCall/connected_components/Unique}}]]
         [[ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_bool_Squeeze_1/_72]]
  (1) Internal:  invalid resource handle
         [[{{node PartitionedCall/connected_components/Unique}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_10900]

Function call stack:
train_function -> train_function

```

**Describe the expected behavior**

Model should train normally without this error.

**Standalone code to reproduce the issue**
I use connected_components in the loss function, other than that is a regular model.
```
def lession_recall(y_true, y_pred):
    conn_comp_true = tensorflow_addons.image.connected_components(tf.cast(tf.squeeze(y_true, axis=[-1]), tf.bool))
    conn_comp_pred = conn_comp_true * tf.cast(tf.squeeze(y_pred, axis=[-1]), tf.int32)

    n_conn_comp_true, _ = tf.unique(backend.flatten(conn_comp_true))
    n_conn_comp_pred, _ = tf.unique(backend.flatten(conn_comp_pred))
    n_conn_comp_true = tf.size(input=n_conn_comp_true) - 1
    n_conn_comp_pred = tf.size(input=n_conn_comp_pred) - 1

    recall = tf.cond(pred=tf.equal(n_conn_comp_true, 0),
                     true_fn=lambda: tf.cast(1.0, dtype=tf.float64), false_fn=lambda: n_conn_comp_pred / n_conn_comp_true)
    return recall
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48271,Compatibility with numpy 1.20,"Hi
May I know if there is any note about the numpy version and tensorflow? It seems that the master branch doesn't work with numpy==1.20.0.

Is that right? If yes, is there any workaround for that?"
48269,tf.signal.rFFT in TFLITE is not working in JAVA,"**System information**
- OS Platform and Distribution (Android 10,11):
- TensorFlow installed from : source
- TensorFlow version: 2.4.1

I am simply trying to make a model which takes time series data as input and compute rFFT/irFFT and return time series data.
I used tf.signal.rfft and tf.signal.irfft for computations. I have converted it to TFLITE and its working perfectly fine in python. But when I load it into android studio it gave me Null in interpreter. Following is code for python:

```
from tensorflow.keras.layers import  Lambda, Input
import tensorflow as tf

inp = keras.Input(shape=((197429)))
O = Lambda(stftLayer)(inp)
Z = Lambda(istftLayer)(O)
model = keras.Model(inputs=inp, outputs=Z, name=""fft_model"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS ]
tflite_model = converter.convert()
with open('FFT.tflite', 'wb') as f:
    f.write(tflite_model)
```

When I give voice signal to this TFLITE model on python it returns same signal perfectly. But When I Load it into Android studio and run following code, it do not load TFLITE model properly and show NULL in Interpreter :

    private MappedByteBuffer loadModelFile() throws IOException {
    AssetFileDescriptor fileDescriptor = this.getAssets().openFd(""FFT.tflite"");
    FileInputStream fileInputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = fileInputStream.getChannel();
    long startOffSets = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffSets, declaredLength);
    
    }
    import org.tensorflow.lite.Interpreter;
    Interpreter tflite;
    
    try {
    tflite = new Interpreter(loadModelFile());
    
        } catch (Exception e) {
            e.printStackTrace();
        }

When I run model it gives 
**Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.run(java.lang.Object, java.lang.Object)' on a null object reference**

Following is gradle setting:

    android {
        compileSdkVersion 30
        buildToolsVersion ""30.0.3""
    
        defaultConfig {
            applicationId ""com.example.dtln_test""
            minSdkVersion 27
            targetSdkVersion 30
            versionCode 1
            versionName ""1.0""
    
            testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""
        }
    
        buildTypes {
            release {
                minifyEnabled false
                proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
            }
        }
        compileOptions {
            sourceCompatibility JavaVersion.VERSION_1_8
            targetCompatibility JavaVersion.VERSION_1_8
        }
        aaptOptions{
            noCompress = ""tflite""
    
        }
        buildFeatures {
            mlModelBinding true
        }
    }
    
    dependencies {
        implementation fileTree(dir: 'libs', include: ['*.jar'])
    
        //implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
        implementation 'androidx.appcompat:appcompat:1.1.0'
        implementation 'androidx.constraintlayout:constraintlayout:1.1.3'
        implementation 'org.tensorflow:tensorflow-lite:+'
        implementation 'org.tensorflow:tensorflow-lite-support:+'
        ///////////////////////////////////////////////////////////
       // implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'
      //  implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0-rc1'
    //    implementation 'org.tensorflow:tensorflow-lite-gpu:2.2.0'
        testImplementation 'junit:junit:4.12'
        implementation 'com.google.android.material:material:1.2.0-alpha03'
        androidTestImplementation 'androidx.test.ext:junit:1.1.1'
        androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'
    }

Any help will be greatly appreciated !!
Thanks
"
48268,XLA compilation bug in TPU involving indirection through index tensors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48267,ImportError: No module named '_pywrap_tensorflow_internal',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64amd
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.5.0
- Python version:3.5.2 , 3.6,3.7,3.8 & 3.9.2 64amd 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\sign-language-gesture-recognition-master>python retrain.py --bottleneck_dir=bottlenecks --summaries_dir=training_summaries/long --output_graph=retrained_graph.pb --output_labels=retrained_labels.txt --image_dir=train_frames
Traceback (most recent call last):
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\sign-language-gesture-recognition-master\retrain.py"", line 132, in <module>
    import tensorflow as tf
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Vanshika Gupta\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48266,Failed to get convolution algorithm,"**System information**
- OS Platform and Distribution Linux Ubuntu 18.04 aws dlami
- TensorFlow version : tensorflow==2.5.0rc0
- Python version:  3.6
- CUDA/cuDNN version: cuda 11.0
- GPU model and memory:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   31C    P0    32W /  70W |   6220MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2685      C   python                           6217MiB |
+-----------------------------------------------------------------------------+

** current behaviour**
```2021-04-02 05:38:06.872091: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-04-02 05:38:06.888553: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2499995000 Hz
2021-04-02 05:38:07.028741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-04-02 05:38:07.528481: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-04-02 05:38:07.531403: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-04-02 05:38:07.532382: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops_fused_impl.h:698 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1727, in predict
    tmp_batch_outputs = self.predict_function(iterator)
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 872, in __call__
    result = self._call(*args, **kwds)
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 940, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 596, in call
    ctx=ctx)
  File ""/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node model/block1_conv1/Relu (defined at <stdin>:1) ]] [Op:__inference_predict_function_618]

Function call stack:
predict_function
```



I am getting error with latest tensorflow 2.5.0 its working fine with tensorflow 2.4.0. Also it's work with tensorflow 2.5.0 in colab but in my server is giving me error."
48265,What does it mean Trackable objects? ,"According to the read me, it [says](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/README.md#:~:text=Trackable%20objects%20and%20TensorFlow%20functions,wrapped%20as%20trackable%20objects%2Ftf.)

> Trackable objects and TensorFlow functions are represented as nodes in the trackable object graph, and each node in the graph stores information about their python properties. Since many attributes in Keras Layers/Models are not Trackable objects or tf. functions, these attributes are wrapped as trackable objects/tf.

By this, I understand one thing that, it represents a node in the `tf` graph. But this information is not enough. What does it mean exactly? "
48264,ERROR: An error occurred during the fetch of repository 'local_config_python':,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 2.7.2
- GCC/Compiler version (if compiling from source): not sure how to check this
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
Building TF with bazel is not working. The cause seems to be:
**ERROR: An error occurred during the fetch of repository 'local_config_python'**:
I am not sure what exactly the error means or how to fix it. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(build_tf_apr21) C:\git_repositories\BlinkGuard\code\tensorflow>python configure.py
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is C:\Users\gerri\anaconda3\envs\build_tf_apr21\python.exe]:

Found possible Python library paths:
  C:\Users\gerri\anaconda3\envs\build_tf_apr21\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\gerri\anaconda3\envs\build_tf_apr21\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n
Not overriding eigen strong inline, some compilations could take more than 20 mins.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(build_tf_apr21) C:\git_repositories\BlinkGuard\code\tensorflow>bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-
msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=152
INFO: Reading rc options for 'build' from c:\git_repositories\blinkguard\code\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe
INFO: Reading rc options for 'build' from c:\git_repositories\blinkguard\code\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_to
olchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --a
nnounce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --d
efine=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\git_repositories\blinkguard\code\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --action_env PYTHON_LIB_PATH=C:/Users/gerri/anac
onda3/envs/build_tf_apr21/lib/site-packages --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --copt=/d2ReducedOptimizeHugeFunctions
 --host_copt=/d2ReducedOptimizeHugeFunctions
INFO: Found applicable config definition build:short_logs in file c:\git_repositories\blinkguard\code\tensorflow\.bazelrc: --output_filter=DONT_MATCH_AN
YTHING
INFO: Found applicable config definition build:v2 in file c:\git_repositories\blinkguard\code\tensorflow\.bazelrc: --define=tf_api_version=2 --action_en
v=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\git_repositories\blinkguard\code\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DE
FINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_
LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkop
t=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\git_repositories\blinkguard\code\tensorflow\.bazelrc: --define framework_shared_obj
ect=false
DEBUG: C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA prop
rietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree t
o the terms of the license agreement, do not use the software.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz failed: class c
om.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Build options --copt, --define, and --host_copt have changed, discarding analysis cache.
INFO: Repository local_execution_config_python instantiated at:
  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace
  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains
  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
Repository rule local_python_configure defined at:
  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>
INFO: Repository local_config_python instantiated at:
  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace
  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains
Repository rule python_configure defined at:
  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl"", line 209, column 22, in _create_local_python_reposito
ry
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl"", line 143, column 52, in _check_python_bin
                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), ""-c"", cmd])
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 77, column 26, in get_bash_bin
                bash_bin_path = which(repository_ctx, ""bash"")
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 27, column 23, in which
                return execute(
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 219, column 13, in execute
                fail(
Error in fail: Repository command failed
INFO: Could not find files for the given pattern(s).
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl"", line 209, column 22, in _create_local_python_reposito
ry
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl"", line 143, column 52, in _check_python_bin
                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), ""-c"", cmd])
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 77, column 26, in get_bash_bin
                bash_bin_path = which(repository_ctx, ""bash"")
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 27, column 23, in which
                return execute(
        File ""C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl"", line 219, column 13, in execute
                fail(
Error in fail: Repository command failed
INFO: Could not find files for the given pattern(s).
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -040
0""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:23:14: in <toplevel>
  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 4017 targets configured).
INFO: Found 0 targets...
ERROR: command succeeded, but not all targets were analyzed
INFO: Elapsed time: 4.195s, Critical Path: 0.00s
INFO: 1 process: 1 internal.
FAILED: Build did NOT complete successfully
"
48257,The mbed project generation bits might be a bit out of date.,"The mbed project generation bits might be a bit out of date.

Tagging @MatthiasHertel80 to check what the current recommended approach is for using TFLM with mbed.

From my perspective (as a TFLM maintainer, but not the one maintaining the mbed integration), you should be able to rename [this folder](https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1) to mbed,

and modify this line:
https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc#L2

to check for the target (instead of tage) similar to arduino:
https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1


Then `TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn` should do the trick.

_Originally posted by @advaitjain in https://github.com/tensorflow/tensorflow/issues/46721#issuecomment-772749626_"
48256,Data cardinality,"Hi, I am training a neural network use two inputs with different numbers of images, like this:

input1 = Input((500,500,3), name='input1')
input2 = Input((700, 500, 3,), name='input2')

l1 = Dense(64, activation='relu')(input1)
l1 = Dense(1, activation='sigmoid', name='output1')(l1)

l2 = Dense(64, activation='relu')(input2)
l2 = Dense(1, activation='sigmoid', name='output2')(l2)

model = keras.Model(inputs=[input1, input2], outputs=[l1, l2])
keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)
model.compile(optimizer='adam', loss={'output1':keras.losses.MeanSquaredError(),'output2':keras.losses.MeanSquaredError()})
model.summary()

and you can check it by som dummy x1 and x2 but with different first dimension, for example:
x1.shape = (50, 500, 500, 3)
y1.shape = (50,)
x2.shape = (100,700,500,3)
y2.shape = (100,)
model.fit({'input1':x1, 'input2':x2}, {'output1':y1, 'output2':y2}, epochs=10)

I got the ValueError: Data cardinality is ambiguous: x sizes: 50, 100 y sizes: 50, 100 Please provide data which shares the same first dimension.

Can anyone help me to solve the problem?
Any help will be appreciated!
"
48255,Trying To Resolve Eigen Library For TFLM Visual Studio Build,"**System information**
- Host OS Platform and Distribution: Windows 10
- TensorFlow installed from Source
- Tensorflow version 2.4.1
- Target platform Visual Studio



**Describe the problem**
I am attempting to build TFLM in Visual Studio, thus am unable to rely on the Bazel, CMake, or Make build tools (which seemed to have issue for TFLM example projects). I have a build that works without some kernels, like LSTM.

I am needing to resolve my Eigen library dependency to be able to load my models properly in C++. This dependency first presents when I add the tensorflow/lite/kernels/add.cc file. This C++ includes the header file tensorflow/lite/kernels/internal/optimized/optimized_ops.h, which in turn includes two header file folders from the Eigen library:


#include ""third_party/eigen3/Eigen/Core""
#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""

I manually attempted to download the Eigen library from here and add it into my project without any guidance from a build process, which seems to be the only way to troubleshoot with TFLM:

https://gitlab.com/libeigen/eigen

There is an eigen/src/Core folder here (doesn't exactly match ""Eigen/Core""):

https://gitlab.com/libeigen/eigen/-/tree/master/Eigen/src/Core

However, there is nothing matching the unsupported/Eigen/CXX11/Tensor pattern at all.

Can someone please help me understand where/how to pull the Eigen library to satisfy the tensorflow/lite/kernels/add.cc dependency as a next step to resolving the rest of my dependency issues? 

I am fairly new to Bazel, CMake, and Make, but have been attempting to use them to build TFLM example projects from the tensorflow/lite/micro/examples folder, for example magic_wand, but there always seems to be issues with the build (for example, problems with checksums like this: https://github.com/tensorflow/tensorflow/issues/47467)

Thanks!"
48254,rsync error with TFLM github CI,"With https://github.com/tensorflow/tensorflow/pull/48241, we started getting an rsync error (after the TFLM CI build passed all the checks):
```
Finished all micro tests at Fri Apr  2 01:09:41 UTC 2021


[ID: 3784634] Build finished after 1341 secs, exit value: 0


Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
rsync: send_files failed to open ""/tmpfs/src/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flexbuffers.h"": Permission denied (13)
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1677) [generator=3.1.3]
```

See this build log for one example:
https://source.cloud.google.com/results/invocations/44cbfc69-3cc3-4fbb-90a5-fa500a7c79bb/log

The exact reason for this is unclear and it is not reproducible with the internal kokoro builds (which is why #48241 got merged.

There doesn't seem to be a good reason to roll-back #48241 so we will attempt a quick fix-forward. If that does not work, #48241 will be rolled back."
48251,Corstone download appears occasionally fail,"
Here is one CI run where the corstone download failed unexpectedly:
https://source.cloud.google.com/results/invocations/d8f3e531-377b-4e7a-b7e6-18a039a7eedb/log

```
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
--2021-04-01 21:36:06--  https://developer.arm.com/-/media/Arm%20Developer%20Community/Downloads/OSS/FVP/Corstone-300/FVP_Corstone_SSE-300_Ethos-U55_11.12_57.tgz
Resolving developer.arm.com (developer.arm.com)... 23.6.139.135
Connecting to developer.arm.com (developer.arm.com)|23.6.139.135|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 419 [text/html]
Saving to: ‘/tmp/tmp.Q0ggPBjdiR/temp_file’

     0K                                                       100% 8.59M=0s

2021-04-01 21:36:12 (8.59 MB/s) - ‘/tmp/tmp.Q0ggPBjdiR/temp_file’ saved [419/419]

tensorflow/lite/micro/tools/make/targets/cortex_m_corstone_300_makefile.inc:7: *** Something went wrong with the Arm Corstone-300 software download: Bad checksum. Expected: 08cc89b02a41917c2224f390f3ac0b47, Got: 97e661159b989fa04a4c122a981758ef.  Stop.
```
"
48242,Ubuntu Debbian to MacOS Terminal Prompt and TF,"Hi,

I have traditionally used TF 2.3.1 on an linux ubuntu debbian where I ran this project through a jupyter notebook.  No GPU was needed to run anything.  

I am now working locally on my Mac OS Terminal where I believe I can only conda install TF 2.0.  When I try pip installing an upgrade, I can no longer even import tensorflow.  I get this error importing tensorflow:

File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

""Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/blah/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib""


Anyways, when I run the identical code with just tensorflow 2.0 in jupyter I am getting an error in my pipeline: 

26     return (0.5 / float(tf.shape(x_true)[0])) * tf.square(tf.linalg.norm(x_true - x_prime))    #original
     27 
     28 

TypeError: float() argument must be a string or a number, not 'Tensor'



Unfortunately, I am not as well versed in the differences between TFs and installing various versions.  I would prefer not to go through the entire pipeline to change TF syntax but am willing to do so if that is the only work around.  Would anybody have an idea what may be wrong?  Thanks.



 




"
48239,Autograph transformation failure with tensorflow 2.4.1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX1080Ti, 11 GB VRAM

**Describe the current behavior**

tensorflow asked me to report a bug

```
 $ export AUTOGRAPH_VERBOSITY=10
 $ python repro.py
2021-04-01 20:28:34.334930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-01 20:28:35.704317: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-01 20:28:35.705182: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-01 20:28:35.871925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:35.873276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:35.874585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:07:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:35.875977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:08:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:35.876804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties: 
pciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:35.878258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties: 
pciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:35.879686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties: 
pciBusID: 0000:87:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:35.881117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties: 
pciBusID: 0000:88:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:35.881154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-01 20:28:35.882875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-04-01 20:28:35.882923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-04-01 20:28:35.884498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-01 20:28:35.884771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-01 20:28:35.886423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-01 20:28:35.887438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-04-01 20:28:35.891003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-04-01 20:28:35.910781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-04-01 20:28:35.911152: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-01 20:28:37.137066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:37.138107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:37.139125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:07:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:37.140194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:08:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s
2021-04-01 20:28:37.140952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties: 
pciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:37.142042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties: 
pciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:37.143120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties: 
pciBusID: 0000:87:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:37.144269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties: 
pciBusID: 0000:88:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-04-01 20:28:37.144311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-01 20:28:37.144341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-04-01 20:28:37.144354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-04-01 20:28:37.144366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-04-01 20:28:37.144377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-04-01 20:28:37.144389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-04-01 20:28:37.144401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-04-01 20:28:37.144414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-04-01 20:28:37.161814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-04-01 20:28:37.161872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-04-01 20:28:40.704083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-01 20:28:40.704138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 4 5 6 7 
2021-04-01 20:28:40.704147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y N N N N 
2021-04-01 20:28:40.704152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y N N N N 
2021-04-01 20:28:40.704159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y N N N N 
2021-04-01 20:28:40.704164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N N N N N 
2021-04-01 20:28:40.704186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 4:   N N N N N Y Y Y 
2021-04-01 20:28:40.704211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 5:   N N N N Y N Y Y 
2021-04-01 20:28:40.704217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 6:   N N N N Y Y N Y 
2021-04-01 20:28:40.704223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 7:   N N N N Y Y Y N 
2021-04-01 20:28:40.715354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7424 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2021-04-01 20:28:40.718506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7424 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
2021-04-01 20:28:40.726289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 7424 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080, pci bus id: 0000:07:00.0, compute capability: 6.1)
2021-04-01 20:28:40.732324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 7424 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080, pci bus id: 0000:08:00.0, compute capability: 6.1)
2021-04-01 20:28:40.737468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 130 MB memory) -> physical GPU (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
2021-04-01 20:28:40.741858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10269 MB memory) -> physical GPU (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)
2021-04-01 20:28:40.748570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10269 MB memory) -> physical GPU (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:87:00.0, compute capability: 6.1)
2021-04-01 20:28:40.753390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10269 MB memory) -> physical GPU (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:88:00.0, compute capability: 6.1)
2021-04-01 20:28:40.754020: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
WARNING:tensorflow:AutoGraph could not transform <bound method LIFCell.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7ff1b3f268d0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function pre_train_step at 0x7ff1500db3b0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function voltage_reg_loss at 0x7ff1b3f66c20> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-04-01 20:28:41.569861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-01 20:28:41.595639: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2394455000 Hz
2021-04-01 20:28:41.782810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10

```

**Describe the expected behavior**

Run code without being told to report an error. Also, the full code seems to be running a lot slower than expected, but I can't tell if this is because of this warning or some other aspect of the full project.

By the way, based on previous github issues, I insured that `gast` version `==0.2.2` is installed.

**Standalone code to reproduce the issue**

```py
import tensorflow as tf
import numpy as np

from os.path import join as opj

from collections import namedtuple


def pseudo_derivative(v_scaled, dampening_factor):
    '''
    Define the pseudo derivative used to derive through spikes.
    :param v_scaled: scaled version of the voltage being 0 at threshold and -1 at rest
    :param dampening_factor: parameter that stabilizes learning
    :return:
    '''
    return dampening_factor*tf.maximum(1 - tf.abs(v_scaled), 0)
    #return dampening_factor*tf.exp(-2.0*tf.abs(v_scaled))


@tf.custom_gradient
def spike_function(v_scaled, dampening_factor):
    '''
    The tensorflow function which is defined as a Heaviside function (to compute the spikes),
    but with a gradient defined with the pseudo derivative.
    :param v_scaled: scaled version of the voltage being -1 at rest and 0 at the threshold
    :param dampening_factor: parameter to stabilize learning
    :param derivative_width: parameter which will rescale the width of the pseudo-derivative
    :return: the spike tensor
    '''
    z_ = tf.greater(v_scaled, 0.)
    z_ = tf.cast(z_, dtype=tf.float32)

    grad = lambda dy: [dy*pseudo_derivative(v_scaled, dampening_factor), tf.zeros_like(dampening_factor)]

    return tf.identity(z_, name=""SpikeFunction""), grad

@tf.function
def voltage_reg_loss(v_scaled):

    reg_thresh_pos = 0.4
    reg_thresh_neg = -2.0

    per_neuron_time_step = tf.nn.relu(v_scaled - 0.4)**2 + tf.nn.relu(-2.0 - v_scaled)**2

    loss_volt = tf.math.reduce_mean(per_neuron_time_step)**2

    return loss_volt


class LIFCell(tf.keras.layers.Layer):

    def __init__(self,
                 n_rec,
                 tau=20.,
                 thr=1.,
                 dt=1,
                 n_refractory=5,
                 dampening_factor=.3):

        super().__init__(self)

        self.n_rec = n_rec

        self._dt = float(dt)
        self._decay = tf.exp(-dt / tau)
        self._n_refractory = n_refractory


        self.input_weights = None
        self.bias_currents = None
        self.recurrent_weights = None
        self.disconnect_mask = None

        self.threshold = thr
        self._dampening_factor = dampening_factor

    def build(self, input_shape):

        initializer = tf.keras.initializers.GlorotNormal()

        self.input_weights = self.add_weight(shape=(input_shape[-1], self.n_rec),
                                             initializer=initializer,
                                             name='input_weights')

        self.disconnect_mask = tf.cast(np.diag(np.ones(self.n_rec, dtype=np.bool)), tf.bool)

        self.recurrent_weights = self.add_weight(shape=(self.n_rec, self.n_rec),
                                                 initializer=initializer,
                                                 name='recurrent_weights')

    @property
    def state_size(self):
        return self.n_rec, self.n_rec, self.n_rec

    @property
    def output_size(self):
        return self.n_rec, self.n_rec, self.n_rec

    def zero_state(self, batch_size, dtype=tf.float32):
        v0 = tf.zeros((batch_size, self.n_rec), dtype)
        r0 = tf.zeros((batch_size, self.n_rec), tf.int32)
        z_buf0 = tf.zeros((batch_size, self.n_rec), tf.float32)
        return v0, r0, z_buf0

    def get_initial_state(self, batch_size, inputs, dtype=tf.float32):
        v0 = tf.zeros((batch_size, self.n_rec), dtype)
        r0 = tf.zeros((batch_size, self.n_rec), tf.int32)
        z_buf0 = tf.zeros((batch_size, self.n_rec), tf.float32)
        return v0, r0, z_buf0

    @tf.function
    def call(self, inputs, initial_state):
        old_v = initial_state[0]
        old_r = initial_state[1]
        old_z = initial_state[2]

        no_autapse_w_rec = tf.where(self.disconnect_mask, tf.zeros_like(self.recurrent_weights), self.recurrent_weights)

        i_in = tf.matmul(inputs, self.input_weights)
        i_rec = tf.matmul(old_z, no_autapse_w_rec)

        i_reset = -self.threshold * old_z
        input_current = (1.0 - self._decay)*(i_in + i_rec) + i_reset

        new_v = self._decay * old_v + input_current

        is_refractory = tf.greater(old_r, 0)
        v_scaled = (new_v - self.threshold) / self.threshold
        new_z = spike_function(v_scaled, self._dampening_factor)
        new_z = tf.where(is_refractory, tf.zeros_like(new_z), new_z)
        new_r = tf.clip_by_value(old_r - 1 + tf.cast(new_z * self._n_refractory, tf.int32), 0, self._n_refractory)

        new_state = (new_v, new_r, new_z)
        output = (new_v, new_z)

        return output, new_state


def create_pretrain_model(cell, seq_len=1000, n_input=40):

    inputs = tf.keras.layers.Input(shape=(seq_len, n_input))
    batch_size = tf.shape(inputs)[0]

    rnn = tf.keras.layers.RNN(cell, return_sequences=True)

    initial_state = cell.zero_state(batch_size)
    v, z = rnn(inputs, initial_state=initial_state)

    return tf.keras.Model(inputs=inputs, outputs=[v, z])


if __name__ == '__main__':

    with tf.device('/GPU:0'):

        # initialize the cell
        cell = LIFCell(512)

        # create the model with some inputs
        model = create_pretrain_model(cell, seq_len=1000, n_input=40)

        # save initial weights
        model.save_weights(opj('ckpt', 'init_weights'))

        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

        # optimize the network simply so that ""voltages"" tend to lie within a certain range
        @tf.function
        def pre_train_step(samples):

            with tf.GradientTape() as tape:

                v, z = model(samples)

                scaled_voltage = (v - 1.0)

                loss = voltage_reg_loss(scaled_voltage)

            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))

            return v, z

        # only 10 iterations
        for i in range(10):

            # input is irrelevant for reproducing the bug
            samples = tf.random.uniform((100, 1000, 40))

            # training step
            v, z = pre_train_step(samples)

        # save pre-trained weights
        model.save_weights(opj('ckpt', 'pretrain_weights'))
```
"
48238,tf.meshgrid throws exception when input tensors have different dtypes,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Reproducible on several OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: See above
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Reproducible with or without GPU


**Describe the current behavior**

Currently, the way TensorFlow tests its own `meshgrid` implementation is by comparing the output to the result of NumPy's implementation ([code](https://github.com/tensorflow/tensorflow/blob/32f8688c1c108ab2df794fe4ceb9e058641359b0/tensorflow/python/kernel_tests/array_ops_test.py#L552)).

In NumPy's implementation, when you call the function with arrays of different types, the types of the input arrays are mapped to the output arrays:

```python
np.meshgrid([1, 2, 3], [4.0, 5.0, 6.0])
[array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]]), array([[4., 4., 4.],
       [5., 5., 5.],
       [6., 6., 6.]])]
```

NumPy's code even works with tensors:

```python
a = tf.constant([1,2,3])
b = tf.constant([4,5,6], dtype='float32')
np.meshgrid(a, b)
[array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]], dtype=int32), array([[4., 4., 4.],
       [5., 5., 5.],
       [6., 6., 6.]], dtype=float32)]
```

However, the TensorFlow Python implementation throws an exception:

```python
tf.meshgrid(tf.constant([0, 1]), tf.constant([2., 3.]))
InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:Mul]
```

It seems like this is being thrown by an internal matrix multiplication operation.

**Describe the expected behavior**

TensorFlow's Python function to match how NumPy handles inputs with different data types.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
tf.meshgrid(tf.constant([0, 1]), tf.constant([2., 3.]))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This was discovered during [code review](https://github.com/tensorflow/tfjs/pull/4855#pullrequestreview-624437214) for implementing `tf.meshgrid` for TensorFlow.js

I'd also be willing to submit a PR with a patch and an additional unit test to cover this case.
"
48237,The new Apple M1 MLcompute Tensorflow2.4 not compatible with Numpy1.20.1 or 1.18.5,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Apple Macbook M1 air):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Apple M1 MLcompute optimized build
- TensorFlow version:2.4
- Python version:3.9
- Installed using virtualenv? pip? conda?: conda through conda-miniforge
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am running the new apple native tensorflow package 2.4, and ran into a problem I did not have before. This jupyter notebook code works in old intel based environment where an older tensorflow version was used. But with M1 apple MLcompute TensorFlow2.4 it is not compatible with Numpy 1.20 or 1.18(I downgraded numpy to try). The error log:**Provide the exact sequence of commands / steps that you executed before running into the 

problem**
Screen catch error log:
```
NotImplementedError: Cannot convert a symbolic Tensor (lstm_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
    ---------------------------------------------------------------------------
    NotImplementedError                       Traceback (most recent call last)
    <ipython-input-20-73358e637fe3> in <module>
          4 model = Sequential()
          5 model.add(Embedding(vocab_size+1, W2V_SIZE, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))
    ----> 6 model.add(LSTM(500, dropout=0.2, recurrent_dropout=0.2))
          7 model.add(Dense(units = 10000, kernel_initializer = 'glorot_uniform', activation = 'relu'))
          8 model.add(Dropout(0.35))
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
        515     self._self_setattr_tracking = False  # pylint: disable=protected-access
        516     try:
    --> 517       result = method(self, *args, **kwargs)
        518     finally:
        519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
        221       # If the model is being built continuously on top of an input layer:
        222       # refresh its output.
    --> 223       output_tensor = layer(self.outputs[0])
        224       if len(nest.flatten(output_tensor)) != 1:
        225         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
        658 
        659     if initial_state is None and constants is None:
    --> 660       return super(RNN, self).__call__(inputs, **kwargs)
        661 
        662     # If any of `initial_state` or `constants` are specified and are Keras
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
        944     # >> model = tf.keras.Model(inputs, outputs)
        945     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    --> 946       return self._functional_construction_call(inputs, args, kwargs,
        947                                                 input_list)
        948 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
       1083           layer=self, inputs=inputs, build_graph=True, training=training_value):
       1084         # Check input assumptions set after layer building, e.g. input shape.
    -> 1085         outputs = self._keras_tensor_symbolic_call(
       1086             inputs, input_masks, args, kwargs)
       1087 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
        815       return nest.map_structure(keras_tensor.KerasTensor, output_signature)
        816     else:
    --> 817       return self._infer_output_signature(inputs, args, kwargs, input_masks)
        818 
        819   def _infer_output_signature(self, inputs, args, kwargs, input_masks):
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
        856           # TODO(kaftan): do we maybe_build here, or have we already done it?
        857           self._maybe_build(inputs)
    --> 858           outputs = call_fn(inputs, *args, **kwargs)
        859 
        860         self._handle_activity_regularization(inputs, outputs)
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
       1161     # LSTM does not support constants. Ignore it during process.
       1162     orig_initial_state = initial_state
    -> 1163     inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)
       1164 
       1165     if isinstance(mask, list):
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in _process_inputs(self, inputs, initial_state, constants)
        857         initial_state = self.states
        858     elif initial_state is None:
    --> 859       initial_state = self.get_initial_state(inputs)
        860 
        861     if len(initial_state) != len(self.states):
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in get_initial_state(self, inputs)
        640     dtype = inputs.dtype
        641     if get_initial_state_fn:
    --> 642       init_state = get_initial_state_fn(
        643           inputs=None, batch_size=batch_size, dtype=dtype)
        644     else:
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in get_initial_state(self, inputs, batch_size, dtype)
       2504 
       2505   def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
    -> 2506     return list(_generate_zero_filled_state_for_cell(
       2507         self, inputs, batch_size, dtype))
       2508 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype)
       2985     batch_size = array_ops.shape(inputs)[0]
       2986     dtype = inputs.dtype
    -> 2987   return _generate_zero_filled_state(batch_size, cell.state_size, dtype)
       2988 
       2989 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in _generate_zero_filled_state(batch_size_tensor, state_size, dtype)
       3001 
       3002   if nest.is_nested(state_size):
    -> 3003     return nest.map_structure(create_zeros, state_size)
       3004   else:
       3005     return create_zeros(state_size)
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
        657 
        658   return pack_sequence_as(
    --> 659       structure[0], [func(*x) for x in entries],
        660       expand_composites=expand_composites)
        661 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
        657 
        658   return pack_sequence_as(
    --> 659       structure[0], [func(*x) for x in entries],
        660       expand_composites=expand_composites)
        661 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py in create_zeros(unnested_state_size)
       2998     flat_dims = tensor_shape.TensorShape(unnested_state_size).as_list()
       2999     init_state_size = [batch_size_tensor] + flat_dims
    -> 3000     return array_ops.zeros(init_state_size, dtype=dtype)
       3001 
       3002   if nest.is_nested(state_size):
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
        199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
        200     try:
    --> 201       return target(*args, **kwargs)
        202     except (TypeError, ValueError):
        203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in wrapped(*args, **kwargs)
       2817 
       2818   def wrapped(*args, **kwargs):
    -> 2819     tensor = fun(*args, **kwargs)
       2820     tensor._is_zeros_tensor = True
       2821     return tensor
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)
       2866           # Create a constant if it won't be very big. Otherwise create a fill
       2867           # op to prevent serialized GraphDefs from becoming too large.
    -> 2868           output = _constant_if_small(zero, shape, dtype, name)
       2869           if output is not None:
       2870             return output
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _constant_if_small(value, shape, dtype, name)
       2802 def _constant_if_small(value, shape, dtype, name):
       2803   try:
    -> 2804     if np.prod(shape) < 1000:
       2805       return constant(value, shape=shape, dtype=dtype, name=name)
       2806   except TypeError:
    <__array_function__ internals> in prod(*args, **kwargs)
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial, where)
       3028     10
       3029     """"""
    -> 3030     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
       3031                           keepdims=keepdims, initial=initial, where=where)
       3032 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
         85                 return reduction(axis=axis, out=out, **passkwargs)
         86 
    ---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
         88 
         89 
    ~/miniforge3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __array__(self)
        850 
        851   def __array__(self):
    --> 852     raise NotImplementedError(
        853         ""Cannot convert a symbolic Tensor ({}) to a numpy array.""
        854         "" This error may indicate that you're trying to pass a Tensor to""
    NotImplementedError: Cannot convert a symbolic Tensor (lstm_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48236,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  Specified a list with shape [32,3] from a tensor with shape [2,3] 	 [[node model/lstm/TensorArrayUnstack/TensorListFromTensor (defined at /PycharmProjects/Latestcodeemma/venv/lib/python3.8/site-packages/kerashypetune/kerashypetune.py:206) ]] [Op:__inference_train_function_10478]","**We are writing a neural network for text analysis. To do this we want to perform a gridsearch for the hyperparameters. However, in our code we get the following error:**

tensorflow.python.framework.errors_impl.InvalidArgumentError:  Specified a list with shape [32,3] from a tensor with shape [2,3]
	 [[node model/lstm/TensorArrayUnstack/TensorListFromTensor (defined at PycharmProjects/venv/lib/python3.8/site-packages/kerashypetune/kerashypetune.py:206) ]] [Op:__inference_train_function_10478]

Function call stack:
train_function

**This would indicate that some dimensions do not line up, however, all our dimensions of the data set are correct as we can fit the model  correctly, but the grid search does not work. 
The full error is:**

Traceback (most recent call last):
  File ""/Users/Desktop/grid_search.py"", line 141, in <module>
    kgs.search([X_pricing_train, X_reports_train], Y_train, validation_data=([X_pricing_test, X_reports_test], Y_test), callbacks=callback)
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/kerashypetune/kerashypetune.py"", line 206, in search
    model.fit(x = x, 
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/Users/PycharmProjects//venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 555, in call
    outputs = execute.execute(
  File ""/Users/PycharmProjects/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Specified a list with shape [32,3] from a tensor with shape [2,3]
	 [[node model/lstm/TensorArrayUnstack/TensorListFromTensor (defined at /PycharmProjects/venv/lib/python3.8/site-packages/kerashypetune/kerashypetune.py:206) ]] [Op:__inference_train_function_10478]

Function call stack:
train_function


**Does someone know how to solve this error or where it could potentially go wrong? In the form of what kind of dimensions might not line up?**

**The way we defined our grid search is the following:**

kerasgridsearch = KerasGridSearch(hypermodel, param_grid, monitor = 'val_loss', greater_is_better=False, tuner_verbose=1)
callback = K.callbacks.EarlyStopping(monitor=monitor)
kerasgridsearcg.search([X_1_train, X_2_train], Y_train, validation_data=([X_1_test, X_2_test], Y_test), callbacks=callback)
"
48235,Error when loading trained model with tf.keras.models.load_model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.0
- CUDA/cuDNN version: 11.0.221 / 8.0.4 
- GPU model and memory: RTX 3090 24GB


**Describe the current behavior**

When loading a model with `tf.keras.models.load_model` that has been saved with `model.save` the following error occurs:

```
Traceback (most recent call last):
  File ""loading_minimal_example.py"", line 29, in <module>
    model = tf.keras.models.load_model(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 212, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 138, in load
    keras_loader.load_layers(compile=compile)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 380, in load_layers
    self.loaded_nodes[node_metadata.node_id] = self._load_layer(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 417, in _load_layer
    obj, setter = self._revive_from_config(identifier, metadata, node_id)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 431, in _revive_from_config
    obj = self._revive_metric_from_config(metadata)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 540, in _revive_metric_from_config
    obj = metrics.deserialize(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py"", line 3446, in deserialize
    return deserialize_keras_object(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 346, in deserialize_keras_object
    (cls, cls_config) = class_and_config_for_serialized_keras_object(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 311, in class_and_config_for_serialized_keras_object
    deserialized_objects[key] = deserialize_keras_object(
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 360, in deserialize_keras_object
    return cls.from_config(cls_config)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py"", line 642, in from_config
    return super(MeanMetricWrapper, cls).from_config(config)
  File ""/home/bjkomer/anaconda3/envs/myenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 720, in from_config
    return cls(**config)
TypeError: __init__() got an unexpected keyword argument 'reduction'
```

**Describe the expected behavior**

The model will be loaded without errors.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

inp = Input(shape=(None, 10))

out = tf.keras.layers.LSTM(50)(inp)

model = Model(inputs=inp, outputs=out)
  
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
loss = tf.keras.losses.BinaryCrossentropy()

model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=loss,
)


history = model.fit(
    np.zeros((32, 16, 10)),
    np.zeros((32, 50)),
    steps_per_epoch=1,
    epochs=2,
) 

model.save(""my_model"")

del model
  
model = tf.keras.models.load_model(
    ""my_model"", compile=False
)
```

Note that if `model.fit` is not called, the model will be loaded fine. Also, if metrics are not given to `model.compile` it will also be loaded fine. This error only occurs if the model has been trained while reporting metrics.
Edit: the model doesn't need to be recurrent, the issue also happens with a single dense layer
"
48234,Broken syntax highlighting on docs that render python notebooks.,"tensorflow/docs related.

## URL(s) with the issue:

Code example for `MultiHeadAttention` at:

https://www.tensorflow.org/tutorials/text/transformer?hl=en#multi-head_attention

and code example at:

https://www.tensorflow.org/tutorials/text/image_captioning?hl=en#create_a_tfdata_dataset_for_training

## Description of issue (what needs changing):

### Clear description

I actually wasted some amount of time for my own stupidity, but there seems to be a glitch in the syntax highlighting system for notebooks rendered on the docs.

It seems that the syntax highlighter parses `//` as comments in python code blocks, not [`floordiv`](https://docs.python.org/3/library/operator.html#operator.floordiv), which it actually means.

This actually may cause some amount of confusion, especially for people coming from c++ world like myself 
who tend to eye-parse away the `//` as comments and the broken syntax highlighter is encouraging this (+ it looks so natural!).

### Request visuals, if applicable

![image](https://user-images.githubusercontent.com/14329563/113301809-5f3ab000-933a-11eb-82e9-251e0f514af1.png)

![image](https://user-images.githubusercontent.com/14329563/113301852-6a8ddb80-933a-11eb-80a7-1edf624f5047.png)

Also in dark mode:

![image](https://user-images.githubusercontent.com/14329563/113302147-bc366600-933a-11eb-8569-f61efa84de73.png)
"
48232,Loading multiple Models over different Processes,"I am creating a GUI for handling neural nets using Keras.

I run, save and load models in different processes (multiprocessing.Process). I assumed the global state of tensorflow is being cleaned up after the process is being terminated. This works so far well when running and saving the models in different processes.

Somehow when I load a net and the process terminates, somehow the global variables start blocking tensorflow again.

What is a good approach working with multiple models with tensorflow 2. I have found approaches for Tensorflow 1, but a good approach for tensorflow 2 I have not found yet.

It is quite hard to provide a code snippet, since I only need a good solution working with different models and my project is quite complex."
48230,Reset/Reinitialize model weights/parameters,"Motivated by [this](https://github.com/keras-team/keras/issues/341) post, I would like to ask for a method like `model.reset_weights()` to get the reset and init the weights again."
48229,Multiple retracing when shapes changes ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na
- TensorFlow installed from (source or binary): binay pypy
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

```
import tensorflow as tf
@tf.function()  # Setting experimental relax shapes does not change the result
def unique_numpy(arr):
    unique_val, unique_inverse = tf.unique(arr)
    return unique_val
# We trace two functions.
unique_numpy.get_concrete_function(tf.TensorSpec(shape=(None,), dtype=tf.int32))
unique_numpy.get_concrete_function(tf.TensorSpec(shape=(None,), dtype=tf.int64))
print(unique_numpy.pretty_printed_concrete_signatures()) # 2 signatures available 
unique_numpy(tf.constant([0, 2], dtype=tf.int32)) 
print(' New')
print(unique_numpy.pretty_printed_concrete_signatures()) # 3 signatures while it should have re used the tf.TensorSpec(shape=(None,), dtype=tf.int32) trace
```

**Describe the expected behavior**
Since `tf.constant([0, 2], dtype=tf.int32)`respects the signature `tf.TensorSpec(shape=(None,), dtype=tf.int32)`I would have thought that TF would reuse that trace and not trace again the function.

I would like to do that to create a function with the same name that could support the same shape but multiple dtypes.
 
**Standalone code to reproduce the issue**
Done above

**Other info / logs** Include any logs or source code that would be helpful to

"
48228,Centernet Mobilenet Keypoint Model Training on custom datasets issue in Keypoints Scores,"Hi,

TensorFlow : 2.2.0
Python : 3.6
Model: centernet_mobilenet_kpt_fpn

I am trying to train centernet keypoint model on custom datasets. But its keypoints score is very less, if box score, i am getting as 70% then keypoints score i am getting 3%. 
Can anyone tell me why it is happening?"
48227,Tensorflow-lite: unresolved dependencies during linking,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): arm 64 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 85c8b2a817f95a3e979ecd1ed95bff1dc1335cff
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Unresolved dependencies during linking.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I'm creating a very easy model via following python code:

```
import tensorflow as tf
import numpy as np
from tensorflow import keras

model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss='mean_squared_error')

xs = np.array([ -1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([ -3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)

model.fit(xs, ys, epochs=500)

print(model.predict([10.0]))

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open('linear.tflite', 'wb').write(tflite_model)
```


Afterwards I'm trying to create a simple C++ program to load and evaluate the model. But with following code which just creates the model. I'm already getting linker errors:

```
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/tools/gen_op_registration.h>

std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""linear.tflite"");

if(!model){
    printf(""Failed to mmap model\n"");
    exit(0);
}
```

I observe following linker problems:

libtensorflow-lite.so: undefined reference to 'TfLiteXNNPackDelegateOptionsDefault'
libtensorflow-lite.so: undefined reference to 'TfLiteXNNPackDelegateDelete'
libtensorflow-lite.so: undefined reference to 'tflite::tools::ToolParams::Merge(tflite::tools::ToolParams const&, bool)'
libtensorflow-lite.so: undefined reference to 'TfLiteXNNPackDelegateCreate'
libtensorflow-lite.so: undefined reference to 'tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)'

Note that I'm building with TFLITE_ENABLE_XNNPACK=OFF. I'm using CMake for the compilation.

Could anybody suggest on what's going wrong?

Kind regards,

Wannes"
48226,Mobilenet v3 keras model lack a 7x7 avgpool and cannot get reported performance,"Mobilenetv3 keras model is reported as converted from its ckpt model. As I observed its structure, they are inconsistent, as keras model **lacks a 'Avgpool' layer before conv_2 layer**. As a consequence, the released mobilenetv3_large_1.0 model can only achieve top1: 72.7%, 3% lower than ckpt model.
"
48225,how to obtain the location of input/output when create a new Op?,"Hi, I am following the [doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pad_op.cc#L372) to implement a new tensorflow Op. For REGISTER_KERNEL_BUILDER, 
`REGISTER_KERNEL_BUILDER(Name(""PadV2"")
                            .Device(DEVICE_GPU)
                            .TypeConstraint<int32>(""T"")
                            .TypeConstraint<int32>(""Tpaddings"")
                            .HostMemory(""input"")
                            .HostMemory(""paddings"")
                            .HostMemory(""constant_values"")
                            .HostMemory(""output""),
                        PadOp<CPUDevice, int32, int32>);`

For my case, I need to know where the tensor is allocated. For example, for `PadV1`, the `input` might be on the gpu while for `PadV2` the `input` might be on the host memory. How can I obtain those info?
"
48224,How can I find the type of quantization?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

How can I find out the quantization type (e.g. float16 or int8) of the tflite file after loading the .tflite file in a Python environment and creating an Interpreter through tf.lite.Interpreter?
"
48223,fake_quant_with_min_max_vars  can only get five significant figures instead of six,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.3
- Bazel version (if compiling from source): NO
- GCC/Compiler version (if compiling from source): NO
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100-SXM2-32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.4.0-49-22-g85c8b2a817f 2.4.1

**Describe the current behavior**
""tf.quantization.fake_quabt_with_min_max_vars"" can only get five significant figures.

**Describe the expected behavior**
""tf.quantization.fake_quabt_with_min_max_vars"" should get six significant figures when float32 has six significant figures.
With regard to pytorch ""torch.fake_quantize_per_tensor_affine"" always has six significant figures.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

import tensorflow as tf

min_var = -10.5268
max_var = 15.1868

output_1 = tf.quantization.fake_quant_with_min_max_vars(0.27, min_var, max_var, num_bits=8, narrow_range=True)
print(output_1 ) #output_1 is 0.3037033

output_2 = tf.quantization.fake_quant_with_min_max_vars(-0.27, min_var, max_var, num_bits=8, narrow_range=True)
print(output_2 ) #output_2 is -0.30370426

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48221,tf.linalg.eigh and tf.linalg.eigvalsh errors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  Source
- TensorFlow version (use command below):  2.0.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I apologize in advance, I'm working on a machine that is isolated from the internet so have to type everything in by hand.  My issue is that I'm attempting to use tf.linalg.eigh within a loss function and get errors on a fraction of the test data I'm using.  For most data it works, but about 1% fails.  The matrices it fails on work fine with numpy.linalg.eig and they are symmetric matrices.

The error looks like this:

```
InvalidArgumentError: Got info = 5251 for batch index 49, expected info = 0.  Debug_info = heevd
  [[node SelfAdjointEigV2 (defined at .../lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_dirtributed_function_1774378]

Function call stack:
distributed_function
```"
48220,Model to estimator docs use deprecated code,"The example code for converting a keras model to a tf estimator cause a user warning that set_learning_phase is deprecated and will be removed after 2020-10-11, which has since past. 

## URL(s) with the issue:
https://www.tensorflow.org/guide/migrate#using_a_keras_model_definition

## Description of issue (what needs changing):
User warning in the console log:

```
/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
```
"
48219,"Add ""input"" label mode to tf.keras.preprocessing.image_dataset_from_directory","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently, there is no way to get `tf.keras.preprocessing.image_dataset_from_directory` to create a dataset the labels of which are the images themselves, for use in autoencoders. The behavior would be the same as that of `tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory` with the argument `class_mode=""input""`. Perhaps the preexisting argument `label_mode` could be modified to include an `""input""` option.

**Will this change the current api? How?**
Minimal change to `tf.keras.preprocessing.image_dataset_from_directory` is required. Because of the nature of what is essentially a very small addition, it would be extremely unlikely to pose a compatibility issue.

**Who will benefit with this feature?**
Users wishing to benefit from the advantages of the `tf.data` API to train their image data autoencoders.

**Any Other info.**
"
48216,slim,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
48213,Exception when saving custom RNN model with a constant in the call function when using SavedModel format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Redhat 7.8.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.1 and 2.3.1 (tested)
- Python version: 3.6.8
- Bazel version (if compiling from source):  No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Current Behavior**
When saving (SavedModel format) an RNN model with a constant in the call function (like shown below), we get an exception.

**Desired behaviour** 
We should be able to save `model` defined below, in a SavedModel format, just like we can save it in an h5 format.

**Code to reproduce the issue:**
```
import tensorflow as tf
import tensorflow.keras as tfk
import tensorflow.keras.backend as K
import numpy as np

class MinimalRNNCell(tfk.layers.Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states=None, constants=None, training=False, *args, **kwargs):
        prev_output = states[0]
        print(""constants: "", constants)
        h = K.dot(inputs, self.kernel) + constants[0]
        output = h + K.dot(prev_output, self.recurrent_kernel)
        return output, [output]

    def get_config(self):
        return dict(super().get_config(), **{'units': self.units})

cell = MinimalRNNCell(32)
x = tfk.Input((None, 5), name='x')
z = tfk.Input((1), name='z')
layer = tfk.layers.RNN(cell, name='rnn')
y = layer(x, constants=[z])

model = tfk.Model(inputs=[x, z], outputs=[y])
model.compile(optimizer='adam', loss='mse')
model.save('tmp.h5') # This works ok
model_loaded = tfk.models.load_model('tmp.h5', custom_objects={'MinimalRNNCell': MinimalRNNCell})
print(model_loaded.predict([np.array([[[0,0,0,0,0]]]), np.array([[0]])])) # This works ok
model.save('tmp2') # This throws an exception
```

**Other info**
The stdout from the above is
```
constants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'model_1/Cast_1:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'model_1/Cast_1:0' shape=(None, 1) dtype=float32>,)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]
constants:  (<tf.Tensor 'z:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'z:0' shape=(None, 1) dtype=float32>,)
constants:  (<tf.Tensor 'constants:0' shape=(None, None, 5) dtype=float32>,)
```

I'll attach the full exception [here](https://github.com/tensorflow/tensorflow/files/6235635/tensorflow_rnn_exception.txt). Anyone should be able to reproduce. However, the main error is
```
ValueError: Dimensions must be equal, but are 32 and 5 for '{{node add}} = AddV2[T=DT_FLOAT](MatMul, constants)' with input shapes: [?,32], [?,?,5].
```
For some reason, the name of `z` changes to `constants` and the shape changes from the expected `(None, 1)` to `(None, None, 5)`. 

Any ideas appreciated. 

Thanks in advance.

"
48212,Quantization Aware Training model has weird inference behavior,"Hi,
I have a pretrained detection model I trained in Tensorflow 2.3 with fp32 precision. I used this model's weights as initial pretrained weights for Quantization Aware Training (QAT). During training I could see that training converged and gave logical predictions (I visualized results during training).
When trying to load with Tensorflow the Quantization Aware Training weights for inference the model behaves differently: When applying in inference the same preprocessing + normalization of mean/std, the predictions' probabilities are always around 0.25. Also, all the predictions' bounding boxes in inference are really small: around 15 pixels height/width, while during training I could see predictions having logical bounding boxes that are with varied dimensions and with much higher probabilities. This behavior appears even on images that were used for training so it's not related to overfitting issues.

Technical constraints for my implementation:
1) I used Tensorflow's `tf.quantization.quantize_and_dequantize` in specific places in my model: After the activation function which appears in the following pattern: `Conv2D` --> BatchNormalization --> `Activation`. I attach here an example for such pattern: 

```
from tensorflow.keras.layers import Conv2D, ZeroPadding2D, LeakyReLU, ReLU, BatchNormalization

def MyConv(x, filters, kernel_size, strides=1, batch_norm=True, is_quantized=False):
    if strides == 1:
        padding = ""same""
    else:
        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding
        padding = ""valid""
    x = Conv2D(filters=filters, kernel_size=kernel_size,
               strides=strides, padding=padding,
               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)

    if batch_norm:
        x = BatchNormalization()(x)
        if is_quantized:
            # We'll use The leaky version of Relu6 for stability of low-precision computations
            # We define the Leaky Relu using Relu for conversion constrains to TensorRT later
            x = ReLU(negative_slope=0.1, max_value=6)(x)
        else:
            x = LeakyReLU(alpha=0.1)(x)
    if is_quantized:
        # Using tf syntax of quantization - applying quantization only on the conv op, after the activation
        x = tf.quantization.quantize_and_dequantize(x, input_min=-64, input_max=64, range_given=False)
        # The input_min / input_max should be ignored due to range_given=False
    return x
```
This function is called when building the model both for training and inference. 
The reason I place the `quantize_and_dequantize` op specifically after the activation function is because this model should be converter to TensorRT later- their [QAT guidelines](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#quantization-training) note that this is the place to apply quantization op for QAT, due to the layer fusion TensorRT applies in the conversion and optimization process.
I tried also applying QAT using the [Tensorflow Model Optimization Toolkit as describes here](https://www.tensorflow.org/model_optimization/guide/quantization/training_example), but this resulted with a model that includes unsupported nodes for TensorRT such as `ConvInteger`. A TF model that uses quantize_and_dequantize produces model that is possible to convert to TRT, but has the issues I described above due to the irregular behavior in TF during inference. Here is an example to the quant ops in Netron for the inference model:

![qat structure example](https://user-images.githubusercontent.com/23454156/113120276-17bd0280-921a-11eb-9d79-9b0a80e8e7b9.PNG)

This is a Tensorflow-related issue since the issue appears during inference in Tensorflow with QAT weights.
2) I create the model for inference using the following command:
```
tf.keras.backend.set_learning_phase(1)
self.model = self._build_model()
```
where `build_model()` calls internally also the function I attached here which contains the quantization ops after every Activation function.

Is there any additional command that needs to be applied in inference when using `tf.quantize.quantize_and_dequantize` ? I remember in TF 1.x there was `tf.contrib.create_eval_graph`, but I didn't see anything similar to it in TF 2.3.
I prefer a solution that lets me still use `tf.quantize.quantize_and_dequantize` due to the TensorRT constraints I described (since I can't use Model Optimization Toolkit's `quantize_model`).

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow version: 2.3.0
- Python version: 3.6
"
48211,kindly help,"Epoch 1/20
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-107-c74f48714daf> in <module>()
      2 # history = model.fit_generator(datagen.flow(train_x,train_y, batch_size=batch_size),
      3 #                               epochs = epochs, validation_data = (val_x,val_y), steps_per_epoch=train_x.shape[0] // batch_size)
----> 4 history = model.fit(train_x, train_y, epochs=20, batch_size=128, validation_split=0.1)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step
        y_pred = self(x, training=True)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility
        ' but received input with shape ' + display_shape(x.shape))


    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (None, 28, 28, 3)"
48210,[XLA PjRT] Ongoing progress and future plan of PjRT distributed runtime,"Hi TF developers, 

This issue intends to get more knowledge about the ongoing and future plan of XLA PjRT, especially on the distributed runtime of GPU.

Although there are some related commits (e.g., 44e771aa50ad8be454b7f4469ad04a815d03cca1 and 8a72c4466a0f44de9bd1cfbf47a701727731036c), existing PjRT code seems quite preliminary in supporting distributed training on multiple hosts -- As far as I could see, it now only provides basic control operations such as setting up the connections and sharing GPU topology, and it is not trivial to run a real distributed demo with communication.  Another issue https://github.com/google/jax/issues/2731 seems also aligns with this point. 

So I would like to confirm if TF team is also working on this and may probably release some new features soon. For now, our   interest is on using PjRT to run HLO with collective or P2P communication, possibly including all-reduce, all-gather, reduce-scatter, send/recv, etc.

Any response will be appreciated. Thanks. "
48209,Documentation for Time Distributed Layer,"Documentation for ```Time Distributed layer``` in Keras mentions any ```Layer``` applied with it will be ```applied to every temporal slice``` 

But in my assumption - for example consider ```mant-to-many``` sequence model trained for ```NER``` or ```Language Model``` with the following code, ```(if return_sequences==True)``` in the previous ```BiLSTM layer``` then ```Dense(n)``` and ```TimeDistributed(Dense(n))```  are exactly the same and either of them can be used. Is my assumption correct? 
   
```
model = Sequential()
model.add(Embedding(input_dim=voc_size, output_dim=embed_dim, input_length=50))
model.add(Bidirectional(LSTM(units=lstm_units,return_sequences=True),
                                         merge_mode=""ave""))
model.add(TimeDistributed(Dense(n)))
```
    
 "
48206,transpose op is deleted when i use tf.lite.TFLiteConverter.from_keras_model to convert tflite model.,"### 1. System information

- OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installation (pip package):
- TensorFlow library (2.0.0-rc0):

### 2. Code
...
    output = tf.transpose(output, [0, 3, 1, 2])
    output_shape = output.shape
    output = Reshape([output_shape[1] * output_shape[3], output_shape[2]])(tf.transpose(output, [0, 1, 3, 2]))
    output = tf.transpose(output, [0, 2, 1]) **# if output shape is [1, 64, 1], the transpose op is deleted, when i convert keras to tflite. If the output shape is [1, 64, 2], the transpose op is reserved.**
    bn_w = state_dict[bn_name+'.weight'].numpy()
    bn_b = state_dict[bn_name+'.bias'].numpy()
    bn_mean = state_dict[bn_name+'.running_mean'].numpy()
    bn_var = state_dict[bn_name+'.running_var'].numpy()
    output = tf.add(tf.multiply(
        tf.divide(tf.subtract(output, bn_mean), tf.sqrt(bn_var+1e-05)), bn_w), bn_b)
    output = tf.transpose(output, [0, 2, 1])
    output = tf.transpose(Reshape([output_shape[1], output_shape[3],  output_shape[2]])(output), [0, 1, 3, 2])

...
converter = tf.lite.TFLiteConverter.from_keras_model(model)

### 3. Failure after conversion
The conversion is successful, but the generated model is wrong:
- transpose op is deleted in tflite model, which leads to the dimension mismatch after the transpose.  

"
48205,TF2 model output NaN when inferenced is performed using CPU,"I have a TensorFlow model architecture and pre-trained weights. When I use it for prediction on GPU it works well, but on CPU it starts giving NaNs. The model weights are in hdf5 format. There is no batch normalization layer in the model. I am confused why it's not working on the CPU.  
Tensorflow version:2.30.  
Tested on Windows and Linux (same issue on both). A similar error is reported in #35307 . But in TensorFlow version 2, i am not sure how to use clear_device as mentioned in [the comment of this issue](https://github.com/tensorflow/tensorflow/issues/35307#issuecomment-568136811). The model I am using is available [here  ](https://github.com/xliucs/MTTS-CAN/blob/5db0928c7474bc2206d6132cd79b09aadc619897/code/model.py#L203)
"
48204,TF2 equivalent of tf.contrib.graph_editor ,"Hi Team,

In TF1 we have tf.contrib.graph_editor to conveniently edit a trained model. I couldn't find the equivalent in TF2. Editing a trained model is obviously a better design since in many cases separating models into different components (e.g., embedding generation) is a post-training request. It does not make sense to use tf.function to generate different signatures and train the model again.

For TF model devs that provide training libraries for users, this is especially painful since different users have different request for model separation. Separating models on users' custom needs in post training stage is a more scalable solution than the current tf.function solution.

Can you add an equivalent graph editor in TF2?

**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The feature should allow users to separate the model graph and change the input and output of each subgraphs.

**Will this change the current api? How?**
Add a new api support for graph editting

**Who will benefit with this feature?**
All TF users bothered with changing model signature with a trained model

**Any Other info.**
"
48200,There is no Digital Object Identifiers (DOI) to make the code citable for academic references,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/master/README.md

## Description of issue (what needs changing):

The source code should be made citable for academic references: https://guides.github.com/activities/citable-code/

### Clear description

_""A DOI, or Digital Object Identifier, is a string of numbers, letters and symbols used to permanently identify an article or document and link to it on the web. A DOI will help your reader easily locate a document from your citation.""_  - [source](https://library.uic.edu/help/article/1966/what-is-a-doi-and-how-do-i-use-them-in-citations).

### Usage example

See GitHub instructions on ""making your code citable"":  https://guides.github.com/activities/citable-code/
"
48199,TFLite ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi Redmi Note 7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0-dev20210322
- Python version: 3.8

**Describe the current behavior**
When I am running the TFLite benchmark on my phone with my model I get the following error `ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.` when I try to run on GPU although every tensor are in static size.

**Describe the expected behavior**
I took care of having static-sized tensors everywhere so I expect to be able to run the model fully on GPU.

**Standalone code to reproduce the issue**
The network can be downloaded [removed].

I run the benchmark which can be downloaded from Tensorflow [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_arm_benchmark_model).
I run it with the following commands
```
adb push android_arm_benchmark_model /data/local/tmp/benchmark
adb shell chmod +x /data/local/tmp/benchmark
adb push model.tflite /data/local/tmp/model.tflite
adb shell ""/data/local/tmp/benchmark""  --graph=""/data/local/tmp/model.tflite"" --input_layer=input --input_layer_shape=1,360,640,3 --use_gpu=true
```

Regarding conversion, I am converting my model using this script:
```
model = load_model() # I would rather not share this

# Create dataset
def get_func():
    return lambda obj: func(obj)

def func(obj, y=640, x=360):
    image = obj[""image""]
    shape = tf.shape(image)
    height, width = shape[0], shape[1]
    ratio_y = y / height
    ratio_x = x / width
    image = tf.image.resize(image, (y, x))
    scale = tf.convert_to_tensor([ratio_y, ratio_x, ratio_y, ratio_x], dtype=tf.float32)
    return {""Input"": image, ""Scale_Input"": scale}

dataset = tfds.load(
    name=""coco/2017"",
    split=""train"",
)
dataset = dataset.map(get_func())

# Transform the dataset into a representative dataset as in the TF guide
def representative_dataset_generator():
    for input_value in dataset.batch(1).take(10):
        yield [input_value[""Input""], input_value[""Scale_Input""]]


# Converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Set the representative dataset in order to quantize the activations
converter.representative_dataset = representative_data_gen

# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]

# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

# Additional tricks
converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
]

tf_lite_quant_model = converter.convert()

# saving converted model in TFLite file
with open(""model.tflite"", ""wb"") as tf_file:
    tf_file.write(tf_lite_quant_model)
```
"
48197,@tf.function in TF2.x,"Where is tf.function defined? 
Where can I see its implementation?
How can it be invoked (as a decorator) when we compile the model using TF.Keras?"
48196,Getting sub-layer output of a custom layer breaks in a Tensorflow 2.4.1,"**System information**
- Custom code:

```
class MyCustomLayer(keras.layers.Layer):
    def __init__(self, num_filters=64, kernel_size=3):
        keras.layers.Layer.__init__(self)
        self.conv_1 = keras.layers.Conv2D(filters=num_filters, kernel_size=kernel_size)
        
    def call(self, inputs):
        return self.conv_1(inputs)

x = keras.Input(shape=(None, None, 3))
my_custom_layer = MyCustomLayer()
y = my_custom_layer(x)
```

- OS Platform and Distribution: Mac OS Big Sur v 11.2.2, Linux Ubuntu 20.04
- TensorFlow installed from binary
- TensorFlow version: code fails using TF 2.4.1, but it works using TF  2.3.1
- Python version: 3.8

**Describe the current behavior**

The following line works:

```
my_custom_layer.output
# Out: <KerasTensor: shape=(None, None, None, 64) dtype=float32 (created by layer 'my_custom_layer')>
```

The following line breaks in Tensorflow 2.4.1:
```
my_custom_layer.conv_1.output
```

However, it works in Tensorflow 2.3.1 without any problem.

**Describe the expected behavior**

`my_custom_layer.conv_1.output` should return the output tensor of the convolutional layer.

**Standalone code to reproduce the issue**

Link to Colab: https://colab.research.google.com/drive/16a2IZfzrv4V0oOdxWp3ctt0VOj3wkyaI?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-74d9fc3b4fbb> in <module>
----> 1 my_custom_layer.conv_1.output

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in output(self)
   2152     """"""
   2153     if not self._inbound_nodes:
-> 2154       raise AttributeError('Layer ' + self.name + ' has no inbound nodes.')
   2155     return self._get_node_attribute_at_index(0, 'output_tensors', 'output')
   2156 

AttributeError: Layer conv2d has no inbound nodes.
```

Here there is the question that I initially raised on stackoverflow: https://stackoverflow.com/questions/66872434
"
48195,"Recurrent layers fail on (at least some) Linuxes where use_bias=True, succeeds on Macos","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (AWS Deep Learning AMI)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3, 2.4.1, several 2.5 and 2.6-nightly samples
- Python version: 3.6
- CUDA/cuDNN version: 11
- GPU model and memory: NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB]; BUT also occurs on CPU wheels

Creating a subclass model with a~~n LSTM~~ recurrent layer when `use_bias=True` (i.e. the default) fails. The code can be found in the following gist:

https://gist.github.com/zoxoi/9c331f9df1fa22112e061638ea200ec8

Note that the model

```python
class FailModel(Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.lstm = layers.LSTM(64, use_bias=True)

    def call(self, input, training=False):
        return self.lstm(input)
```

is the only important bit, as far as I know. The rest is a minor bit of supporting code just to trigger the issue such as compiling the model and creating a dummy dataset generator. The full traceback is in the included gist, to save space, but the ultimate error given is:

```    
ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=""NCHW""](add, bias)' with input shapes: [?,256], [256].
```

A few facts I've noticed:
- It doesn't seem to fail when *calling* the model, this behavior is limited to things such as `model.fit()`. Explicitly calling `model(tf.zeros(1, 96, 100))` works fine from my testing.
- The value given in the error `ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=""NCHW""](add, bias)' with input shapes: [?,256], [256].` seems to always be 4x the number of cells (e.g. for 64 it's 250, 100 it's 400 etc). I don't know if this is surprising or not.
- Since it may be related to NCHW format, I tried changing the `time_major` argument to true, and both leaving the data the same and (more correctly) manually permuting/transposing the input into time major order, no success.
- This bug happens on multiple TF versions, 2.3 was the earliest I can test but persists up to the most recent 2.6 nightly.
- This happens even if you force TF to not discover a GPU, or install tf-cpu instead.
- However, this does NOT happen on my Macos Catalina 10.15.7 laptop, with an integrated GPU (i.e. tf-cpu is automatically installed), the layer works as intended with no errors.

For anybody else dealing with this: a workaround is to set **use_bias=False** when creating the LSTM. This has tradeoffs however, (beyond just the potential mathematical issues with using a bias allowing some correction), since it won't use the cuDNN kernel if use_bias is off which from my testing seems to have a fairly radical speed penalty (it's training more slowly than it is on my laptop CPU by about 40ms/step). 

If you want to test on the **exact** same setup I'm using, I'm experiencing the issue on the AWS Deep Learning AMI Version 42.1.

Further important things I haven't tested:
- ~~If this is limited to LSTM or affects other recurrent layers.~~ [Update: It affects GRU and SimpleRNN as well, updating title]
- If this affects Windows in either configuration
- ~~If, even though it fails on CPU, it's perhaps related to CUDA 11 specifically somehow~~ [Also failed on a GPU-less Linux AMI]
- If this affects the functional API as well, I've only used subclass models"
48191,Regenerate Keras Python code based on model.summary() ,"Is it possible to generate python code based on model.summary() information, the generated code (maybe write it to `filename.py` file) is the regular Keras model Sequential like the following:

```
model = Sequential()
model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu',
                 input_shape=(time_window_size, 1)))
model.add(MaxPooling1D(pool_size=4))
model.add(LSTM(64))
model.add(Dense(units=time_window_size, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metric])
model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metric])
model.compile(optimizer=""sgd"", loss=""mse"", metrics=[metric])
```

I am sure someone somewhere did it but I searched GitHub and Google and could not find it, you may have already came across such function. Please share your thoughts."
48190,High Inference time on warmup state in android  ,"**System information**
- OS Platform and Distribution (Windows 10 Pro/Andriod 10,11):
- Mobile device (Redmi note 8)
- TensorFlow installed from (source or binary): tflite .so built from source
- TensorFlow version: 2.4
- Python version: 3.8

**Problem:** I am working on Real time Audio processing application in android studio. I made a model using LSTMs and converted the model to the TFLITE format. After that when I tested model by giving some data it gives inconsistent inference time every time. Model inference time is 3-4ms. If I run inference in a loop, 3-4ms is the average inference time. If I shift towards the real-time scenario, I have to perform inference every 10ms. My inference time increases in this scenario. This is probably because the inference is not running back to back but actually waits till we get audio data again (10ms callback -3ms inference time = 7ms wait in thread for more audio data). I read on tensorflow site (https://www.tensorflow.org/lite/performance/measurement) that this is due to the warmup state and steady state. For warmup state it gives 9ms inference time and on steady state it gives almost 3ms inference time. But in my case, since audio is coming continuously and I want the model to run in steady state. Is there any way to run the TFLITE model in steady state once its loaded in the application ? 
   

**Describe the current behavior**
Android tflite model warmup everytime when inference function is called
**Describe the expected behavior**
Android tflite model should warmup once at start and stay in steady state after that.

I run the model on tensorflow benchmark tool with and without delay. When I run model without any delay it gives avg 2ms time for inference. When I put 5ms delay between each inference then model inference time inreased to 6ms. Please refer to screenshots below:

**Without any delay**
![without delay](https://user-images.githubusercontent.com/63999516/113251302-80b87f00-92db-11eb-9a7f-1f160c506476.png)

**With 5ms delay between each inference**
![with 5m delay](https://user-images.githubusercontent.com/63999516/113251321-87df8d00-92db-11eb-9a71-56d535b7558a.png)

Any help is greatly appreciated, thank you!"
48189,Broken Link in MLIR docs,"## URL(s) with the issue:

https://www.tensorflow.org/mlir

<img width=""1406"" alt=""Screen Shot 2021-03-30 at 10 25 10 PM"" src=""https://user-images.githubusercontent.com/8815362/112996050-cc1d4100-91a6-11eb-8b25-54934afe1d09.png"">

`MLIR on GitHub` box link is broken. That link is pointing `https://github.com/llvm/llvm-project/tree/master/mlir` that returns 404. I think `https://github.com/llvm/llvm-project/tree/main/mlir` is valid link.
"
48188,RaggedTensor with irregular shape breaks when yielded from `tf.keras.utils.Sequence`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: -


**Describe the current behavior**

I am trying to pass tensors with irregular shapes to a custom keras model. I am using a custom data generator which inherits from `tf.keras.utils.Sequence`. This custom data generator is important because we want to control how data is batched after each epoch. I've reproduced my issue in this sample code:

```
import numpy as np
import tensorflow as tf
import random

class BatchGen(tf.keras.utils.Sequence):
    def __len__(self):
        return 100
    
    def __getitem__(self, idx):
        for i in range(100):
            return self.gen_ragged_tensors()
    
    @staticmethod
    def gen_ragged_tensors():
        seq_len = np.random.randint(1,3)
        a = np.zeros((seq_len,2), dtype=np.float32)

        seq_len = np.random.randint(1,3)
        b = np.ones((seq_len,2), dtype=np.float32)
        c = tf.ragged.constant([a,b], dtype=tf.float32)
        return c

class Model(tf.keras.models.Model):
    
    def calc_loss(self, batch_in):
        # dummy operation
        return tf.reduce_mean(batch_in - tf.constant(0, dtype=tf.float32))
    
    @tf.function
    def train_step(self, batch_in):
        
        with tf.GradientTape(persistent=True) as tape:
            prediction_loss = self.calc_loss(batch_in)
        prediction_gradients = tape.gradient(prediction_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(prediction_gradients, self.trainable_variables))
        self.add_loss(lambda: prediction_loss)
        return {""loss"": prediction_loss}
        
    def call(self, inputs, training):
        return self.train_step(inputs)

model = Model()
model.compile(optimizer=tf.keras.optimizers.Adam(0.001))


data_gen = BatchGen()
model.fit(data_gen, epochs=10)
```

Running the above code gives the following error - 
```
Epoch 1/10
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-71ab935a83be> in <module>
----> 1 model.fit(data_gen, epochs=10)

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    844               *args, **kwds)
    845       # If we did not create any variables the trace we have is good enough.
--> 846       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    847 
    848     def fn_with_cond(*inner_args, **inner_kwds):

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1846                            resource_variable_ops.BaseResourceVariable))],
   1847         captured_inputs=self.captured_inputs,
-> 1848         cancellation_manager=cancellation_manager)
   1849 
   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1922       # No tape is watching; skip to running the function.
   1923       return self._build_call_outputs(self._inference_function.call(
-> 1924           ctx, args, cancellation_manager=cancellation_manager))
   1925     forward_backward = self._select_forward_and_backward_functions(
   1926         args,

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    548               inputs=args,
    549               attrs=attrs,
--> 550               ctx=ctx)
    551         else:
    552           outputs = execute.execute_with_cancellation(

~/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float32, but the yielded element was <tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0]], [[1.0, 1.0], [1.0, 1.0]]]>.
Traceback (most recent call last):

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 843, in generator_py_func
    ret, dtype=dtype.as_numpy_dtype))

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 204, in _convert
    result = np.asarray(value, dtype=dtype, order=""C"")

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/numpy/core/numeric.py"", line 538, in asarray
    return array(a, dtype, copy=False, order=order)

ValueError: setting an array element with a sequence.


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 244, in __call__
    ret = func(*args)

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper
    return func(*args, **kwargs)

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 848, in generator_py_func
    ""element was %s."" % (dtype.name, ret)), sys.exc_info()[2])

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/six.py"", line 702, in reraise
    raise value.with_traceback(tb)

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 843, in generator_py_func
    ret, dtype=dtype.as_numpy_dtype))

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 204, in _convert
    result = np.asarray(value, dtype=dtype, order=""C"")

  File ""/Users/daksh/miniconda3/envs/rasa_master/lib/python3.7/site-packages/numpy/core/numeric.py"", line 538, in asarray
    return array(a, dtype, copy=False, order=order)

TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float32, but the yielded element was <tf.RaggedTensor [[[0.0, 0.0], [0.0, 0.0]], [[1.0, 1.0], [1.0, 1.0]]]>.


	 [[{{node PyFunc}}]]
	 [[IteratorGetNext]] [Op:__inference_train_function_170]

Function call stack:
train_function
```

The error seems to be coming because the inner rows of the ragged tensor have different size of the first dimension.

Note, that there is no error when I replace `gen_ragged_tensors` with:

```
def gen_ragged_tensors():
        seq_len = np.random.randint(1,3)
        a = np.zeros((seq_len,2), dtype=np.float32)
        b = np.ones((seq_len,2), dtype=np.float32)
        c = tf.ragged.constant([a,b], dtype=tf.float32, inner_shape=(2, seq_len, 2))
        return c
```

This is not useful in reality because this needs me to firstly use the same size of first dimension for all rows of the ragged tensor and secondly requires me to specify the `inner_shape` which is not guaranteed to be known because we want to pass irregular shaped tensors to our model.

Any suggestions on what might be going wrong are very helpful.


**Describe the expected behavior**

Successfully being able to pass irregular shaped ragged tensors into a custom keras model. These ragged tensors should be yielded by a data generator of type `tf.keras.utils.Sequence`.

**Standalone code to reproduce the issue**
Provided above
"
48185,#Question on efficient data input pipeline.,"Hello,
I trained a small ML model, where I created and extracted training data in a Pythonic way. According to TF docs, it seems there is an efficient alternative using the module tf.data. I'm learning about tf.data from a Stanford Gitbub code written in TF Version 1. Thus, it is necessary o use tf.compat.v1 to use these commands on TF version 2. Since my goal is to build efficient solutions, is it still a good idea to learn these tf.compat.v1 commands or their purpose is solely to migrate codes written in TF v1 to TFv2?
"
48183,Minor updates to documentation ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): N/A
- Tensorflow version (commit SHA if source): 899fdb415dc970d5bca7d98a7dddf95968ef07c2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): N/A

**Describe the problem**
Some documentation is outdated and needs an update.
The memory mgmt markdown needs an update regarding offline planned tensor allocations.

**Please provide the exact sequence of commands/steps when you ran into the problem**
N/A
"
48180,Unable to install tensorflow 2.4.1 with python3.8.2 pip,"

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
48179,Converting saved_model to .tflite with empty Signatures Key,"### 1. System information

- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installation: pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow == 2.4.1

### 2. Code

```
import tensorflow as tf

model_dir = 'model'

converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS
]

tflite_model = converter.convert()
fo = open(""model.tflite"", ""wb"")
fo.write(tflite_model)
fo.close
```
Link to model: https://drive.google.com/file/d/1hU2zOpMPqH0gaASiDLmTZ6MFTotMEfw9/view?usp=sharing


### 3. Failure after conversion
Conversion failed.

```
ValueError: Only support a single signature key.
```

### 5. (optional) Any other info / logs
```
model_dir = 'model'
saved_model = tf.saved_model.load(export_dir=model_dir)
Signature =  saved_model.signatures
print(Signature, ""Len: "", len(Signature))
```
Output:
```
_SignatureMap({}) Len:  0
```"
48178,Dataset random state not saved in checkpoint when reshuffling each iteration,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: NVIDIA Titan V

**Describe the current behavior**
I am creating a dataset with
``` python
dataset = tf.data.Dataset.from_tensor_slices((np.arange(5)))
dataset.shuffle(5, reshuffle_each_iteration=True)
dataset = dataset.batch(5)
...
for epoch in range(1000):
  iterator = iter(dataset)
```
I am saving this dataset as part of a checkpoint after every epoch. When I stop training, load the checkpoint, and create an iterator for a new epoch from the dataset, the batch is not the same as if I had kept training. Instead, it uses the global random seed, so the iterator is identical to the iterator from the very first epoch.

**Describe the expected behavior**
The dataset random state should be held over. That is, when an iterator is created from this dataset, it should be identical to if I had never stopped training.

**Standalone code to reproduce the issue**
https://gist.github.com/keyonvafa/2120c836a71dd783f7da0e73116681b6

At the first iteration, this prints: tf.Tensor([1 3 2 4 0], shape=(5,), dtype=int64). When we stop training and restore a checkpoint, it will go back and print the same thing. Removing the global random seed would fix this but the code would no longer be reproducible between runs.

Thank you!
"
48177,Time Series Prediction Bug,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Anaconda Distribution
- TensorFlow version (use command below): unknown 2.3.0
- Python version: 2.8.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` 


**Describe the current behavior**
****
WARNING:tensorflow:AutoGraph could not transform <bound method split_window of Total window size: 22
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Label indices: [21]
Label column name(s): ['Temperature_mean']> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <bound method split_window of Total window size: 22
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Label indices: [21]
Label column name(s): ['Temperature_mean']> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
(TensorSpec(shape=(None, 21, 6), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))

**Describe the expected behavior**
****
According to the tutorial this should be the output:
(TensorSpec(shape=(None, 6, 19), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))


**Standalone code to reproduce the issue**
****
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Following this TF tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series#2_split
The error happens in ""w1.train.element_spec"" part


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48176,FP16 support for grouped convolutions,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently, grouped convolutions (at least for Conv2D) doesn't see any speedup from FP16 training, although memory usage is still lowered training in FP16. When doing a mixed precision test, running tf.nn.conv2d and K.conv2d with grouped convolutions in FP16 has equal performance to FP32 and running the same operations without groups (same number of input/output channels) has about an 80% speedup in FP16.

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
48175,tensorflow: AutoGraph could not transform data,"Error reported in Python 3.9.2 tf-nightly:
""WARNING:tensorflow:AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x0000019783941EE0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: invalid syntax (tmp0qln7o16.py, line 48)
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert""

This feature works in Tensorflow for Python 3.8 but not in tf-nightly for Python 3.9"
48174,https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard contains broken links,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard

## Description of issue (what needs changing):

The doc for the `embeddings_metadata` argument contains a broken link. It says ""See the details about metadata files format."", and the link redirects to https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional, which is broken.
"
48171,tf.cast zeroes out input tensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but a stock example would fail too, e.g. ""style_transfer.ipynb"" fails on my setup when trying to convert image into a floating point tensor.
- OS Platform and Distribution: Windows 7, 64 bit
- TensorFlow installed from: binary, installed using command ""pip install tensorflow==2.3.2""
- TensorFlow version: v2.3.1-38-g9edbe5075f7 2.3.2
- Python version: WinPython64 ver.3.8.7.0cod (Python version 3.8.7)
- CUDA/cuDNN version: CUDA Version 10.1.243, i.e. 10.1 update 2 / cuDNN ver.7.6.5.32
- GPU model and memory:  Nvidia Geforce GTX 750 TI, memory: 2GB

**Describe the current behavior**
CUDA is successfully recognized by the TensorFlow - I get the following message when the TensorFlow is started:
`(2021-03-29 22:59:14.061267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll)`
CUDA also runs its own basic self-tests without apparent issues.

Unfortunately, when I am trying to run more complex code, error messages are generated (e.g. when trying to run TensorFlow's style transfer tutorial ""style_transfer.ipynb"". I traced the issue to the casting of integer numbers to floating point numbers, which generates the floating point zeros. There was an earlier bug described here: https://github.com/tensorflow/tensorflow/issues/14147 which sounds similar.

However, I do not seem to need to use a lot of GPU memory to trigger the issue. Even the following very basic test:
```
import tensorflow as tf
ivalue = tf.constant(10)
print(ivalue)
fvalue = tf.cast(ivalue, tf.float32)
print(fvalue)
```
produces output

```
tf.Tensor(10, shape=(), dtype=int32)
tf.Tensor(0.0, shape=(), dtype=float32)
```
However, if I disable the CUDA acceleration by issuing the following commands at the start of my script:
```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
```
the script works correctly, as expected.

**Describe the expected behavior**

I expect the script above to output
```
tf.Tensor(10, shape=(), dtype=int32)
tf.Tensor(10.0, shape=(), dtype=float32)
```
without having to disable CUDA.

**Standalone code to reproduce the issue**
Please see above. Please let me know if I need to run a more complex code to help identify what is wrong more precisely.

**Other info / logs**
I actually attempted to install several combinations of versions of TensorFlow and CUDA, trying to see if it will make any difference. Specifically, I tried installing TensorFlow 2.2.0, 2.3.0 and 2.3.2 with various minor versions of CUDA 10.1 and cuDNN 7.6. It made no difference at all."
48170,Difference between android and iOS inference result,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, no example code found
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 11.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 6 & iPhone8
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): TensorflowLite Objective C

**Context**

I'm currently working on an object detection on mobile project.
I have good result with tensorflowlite on android but we are facing a problem on ios.

I am using the objective-c version of tensorflow lite because I get the images from[ react-natice-camera](https://github.com/react-native-camera/react-native-camera) which is also coded in objective-c.

**Expected behavior**

I expect to have same inference result on iOS and android detection (classname, confidence and bbox location) but the results are very different on iOS. 
In fact 50% of the detections are the same but the other 50% are different. The locations of the bbox reaches very high values (> 1.56^31 e.g.) while the value must be between 0 and 1.

Some example of result:

| Android bbox (x1,y1,x2,y2) |  iOS bbox (x1,y1,x2,y2)| Notes|  
|---|---|---|
| 0.291 - 0.328 - 0.325 - 0.445  |  0.29 - 0.331 - 0.3257- 0.445 | same |  
| 0.295 - 0.509 - 0.327 - 0.600  |  0.292 - 0.511 - 0.327- 0.600 | same |  
| 0.295 - 0.493 - 0.550 - 0.582   |  0.0- 0.0 - 0.0- 0.0 |  different, but almost same classe & confidence | 
| 0.311 - 0.423 - 0.511 - 0.542   |  0.0- 5.56e^10 - 0.0- 0.0 |  overflow ?, but almost same classe & confidence | 

## Some part of the objective C code

Same images are used in android and objective c
Images are in rgb converted in grayscaled

Images are ing RGBA format

I don't use quantisized model so pixel are stored in float32 array


**Pre processing the image**

```
    float* array = malloc((320*320*3) * sizeof(float));
    int count = 0;
    for (int i = 0 ; i < height*width ; ++i)
    {
       
        int r   = ((int) rawData[byteIndex] );
        int g = ((int) rawData[byteIndex + 1] );
        int b  = ((int) rawData[byteIndex + 2] );

        byteIndex += 4; //passing to next rgba tuple
        
        float gray = r * 0.3f + g *0.59f + b*0.11f;
        gray = (gray - 127.5)/127.5;

        array[count++]=gray;
        array[count++]=gray;
        array[count++]=gray;
    }
```

This code is ok, I've done a lot of test with custom image.

I've already check this [issue](https://github.com/tensorflow/tensorflow/issues/40442) but nothing change


**Get inference result**

I use a SSDMobileNetV2 with [mnasFPN ](https://arxiv.org/pdf/1912.01106.pdf)network so I have 4 tensors output (classe, confidence, bbox locations, nb detections)

I retrieve the inference data with this code:

```
    //Getting input tensor
    TFLTensor *inputTensor = [interpreter inputTensorAtIndex:0 error:&error];

    //Copying input data into input tensor
    [inputTensor copyData:dataIm error:&error];

    //Executing model
    [interpreter invokeWithError:&error];
    
    //Getting the 4 output tensors
    TFLTensor *outputLocations = [interpreter outputTensorAtIndex:0 error:&error];
    TFLTensor *outputClasses = [interpreter outputTensorAtIndex:1 error:&error];
    TFLTensor *outputScore = [interpreter outputTensorAtIndex:2 error:&error];
    TFLTensor *outputNumDetections = [interpreter outputTensorAtIndex:3 error:&error];
    
    //Converting to NSData
    NSData* dataOutputLocations = [outputLocations dataWithError:&error];
    NSData* dataOutputClasses = [outputClasses dataWithError:&error];
    NSData* dataOutputScore = [outputScore dataWithError:&error];
    NSData* dataOutputNumDetections = [outputNumDetections dataWithError:&error];

    //Converting NSData to float array

    float numDetections;
    [dataOutputNumDetections getBytes:&numDetections length:(sizeof(float))];
    
    float outputLocArray[(int)numDetections * 4];
    float outputClassArray[(int)numDetections];
    float outputScoreArray[(int)numDetections];
    
    [dataOutputLocations getBytes:&outputLocArray length:(sizeof(float))*numDetections];
    [dataOutputClasses getBytes:&outputClassArray length:(sizeof(float))*numDetections];
    [dataOutputScore getBytes:&outputScoreArray length:(sizeof(float))*numDetections];

    //store final result into Dictionnary
    NSMutableArray* results = [NSMutableArray array];
    for(int i = 0; i < (int) numDetections; i++)
    {
        
        int detected_class = (int) outputClassArray[i];
        float score = outputScoreArray[i];
        float ymin = fmax(0,outputLocArray[i*4]);
        float xmin = fmax(0,outputLocArray[i*4+1]);
        float ymax = outputLocArray[i*4+2];
        float xmax = outputLocArray[i*4+3];
        
        if(score > 0.0f)
        {
            NSMutableDictionary* res = [NSMutableDictionary dictionary];
            NSString* classname = [@(detected_class) stringValue];
            
            [res setObject:classname forKey:@""classname""];
            [res setObject:@(score) forKey:@""confidence""];
            [res setObject:@(xmin) forKey:@""xmin""];
            [res setObject:@(xmax) forKey:@""xmax""];
            [res setObject:@(ymin) forKey:@""ymin""];
            [res setObject:@(ymax) forKey:@""ymax""];
  
            

            [results addObject:res];
        }
    }
```

By doing this I have the strange result above, with float value which seems to overflow.

I couldn't find an example code in objective-c so I'm not sure what is wrong.
Maybe a conversion error?"
48167,Running L-BFGS-B optimizer in TF2,"
### System information

-   This concerns a customized script applying PINN
-   Runs both (quite well) on Jupyter Notebooks, and Colab
-   TF2 (and T1 in other environment) installed using Anaconda, and Colab
-   TF 2.4.1
-   Python 3.8.2 (3.7 in Colab)
-   
-   No CUDA used on local host (yet), automatically assigned on Colab
-   NVIDIA 1070 (local host, not yet used), automatically assigned on Colab

Issue at hand:
_______________________________________________________________________________________________

Originally the optimizer based on L-BFGS-B only runs on TF1 via

self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, 
                                                                        method = 'L-BFGS-B', 
                                                                        options = {'maxiter': 50000,
                                                                                   'maxfun': 50000,
                                                                                   'maxcor': 50,
                                                                                   'maxls': 50,
                                                                                   'ftol' : 1.0 * np.finfo(float).eps})


The '.contrib' module has been left out of TF2, and so far no straightforward solution found anywhere that works well.

Reason for request:
PINN is a significant and growing development for science / engineering applications. Hence not having 
this functionality implemented in a usable and accessible way in TF2 is an issue.

Hence in short, this is a feature request:

Please enable straightforward to use implementation of the Broyden - Fletcher - Goldfarb - Shanno optimization into TF2.
Ideally, accessible through the Keras framework (be it functional API or not).


Thanks and best regards,

Jan van de Mortel


"
48166,Prediction from saved Estimator is Tensor with no numpy method,"**System information**
- OS Platform and Distribution: MacOS 11.2.3
- TensorFlow installed from: source
- TensorFlow version (use command below): GIT_VERSION: v1.12.1-48291-g8867b44e4cd  VERSION: 2.5.0
- Python version: 3.7.10
- Bazel version (if compiling from source):  4.0.0-homebrew
- GCC/Compiler version (if compiling from source): clang

I have an Estimator model that I have fit:

```python
    estimator = tf.estimator.DNNClassifier(
        feature_columns=get_feature_columns(),
        hidden_units=[128, 128],
        n_classes=n_classes,
        activation_fn=tf.nn.swish,
        dropout=DROPOUT,
        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
    )

    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(batch_size=128), max_steps=10_000)
    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(training=False))

    eval_result, _ = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

and have saved it to disk using `export_saved_model`. When I load the model into a new Python session, it generates predictions without error:

```python
imported = tf.compat.v2.saved_model.load(estimator_path)

def dfrow_to_example(input_df):
    example = tf.train.Example()
    for feature_name, value in input_df.iteritems():
        if value.dtype == np.int64:
            example.features.feature[feature_name].int64_list.value.extend(
            list(value.values))
        elif value.dtype == np.float64:
            example.features.feature[feature_name].float_list.value.extend(
            list(value.values))
    return example


def predict_input_fn(input_df):
    example = dfrow_to_example(input_df)
    return tf.constant([example.SerializeToString()])

predict_fn = imported.signatures['predict']

prediction = predict_fn(examples=predict_input_fn(input_data))
```

However, I am returned a dict of Tensors, but they do not have `numpy()` methods, so I cannot access the values:

```python
{'classes': <tf.Tensor 'StatefulPartitionedCall_4:3' shape=(1, 1) dtype=string>,
 'probabilities': <tf.Tensor 'StatefulPartitionedCall_4:5' shape=(1, 18) dtype=float32>,
 'all_class_ids': <tf.Tensor 'StatefulPartitionedCall_4:0' shape=(1, 18) dtype=int32>,
 'all_classes': <tf.Tensor 'StatefulPartitionedCall_4:1' shape=(1, 18) dtype=string>,
 'logits': <tf.Tensor 'StatefulPartitionedCall_4:4' shape=(1, 18) dtype=float32>,
 'class_ids': <tf.Tensor 'StatefulPartitionedCall_4:2' shape=(1, 1) dtype=int64>}
```

```

prediction['probabilities'].numpy()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/Yankees/nn_matchup/models/matchup_model_test_saved.py in <module>
----> 1 prediction['probabilities'].numpy()

AttributeError: 'Tensor' object has no attribute 'numpy'
```

Have I done something wrong with my input function? 
"
48165,Python tensorflow and tensorflow-cpu dependencies,"Although undocumented, it seems that the [tensorflow-cpu](https://pypi.org/project/tensorflow-cpu/) package is meant to be a CPU-only version of tensorflow to enable smaller package sizes.

Given that there is no dependency relationship between `tensorflow-cpu` and `tensorflow`, ecosystem libraries have no way of specifying that they need either package (see for example [this issue](https://discuss.python.org/t/conditional-package-install-depending-on-other-packages-in-environment/4140)).

In hindsight, the most obvious solution would have been to make a `tensorflow-core` package or something, but idk if that's possible without a lot of work or breaking things.

I'm not sure what the solution is here, but I just wanted to raise the issue."
48162,LoadDataset op uses a bytes string as its name,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux NixOS 20.09
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When trying to use an `op_regex` in `tf.debugging.experimental.enable_dump_debug_info` while using `tf.data.experimental.load` you get an error that you `cannot use a string pattern on a bytes-like object`. Looking at the stack trace (below) it appears the error comes from the fact that the `LoadDataset` op is using a bytes string for the op name while other ops in `load` use regular strings. 

**Describe the expected behavior**
You should be able to use a regex to filter ops as described on the api reference page for `tf.data.experimental.load`.

**Other info / logs**
With a string regex
```
Traceback (most recent call last):
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/mnt/NVME/CubeCobraRecommender/src/ml/train_draftbots.py"", line 114, in <module>
    train_dataset = load_picks(pick_cache_dir / 'train', batch_size)
  File ""/mnt/NVME/CubeCobraRecommender/src/non_ml/parse_picks.py"", line 395, in load_picks
    return tf.data.experimental.load(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py"", line 201, in load
    return _LoadDataset(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py"", line 129, in __init__
    variant_tensor = gen_experimental_dataset_ops.load_dataset(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py"", line 5636, in load_dataset
    return load_dataset_eager_fallback(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py"", line 5698, in load_dataset_eager_fallback
    _result = _execute.execute(b""LoadDataset"", 1, inputs=_inputs_flat,
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 140, in execute_with_callbacks
    callback(op_name, tuple(inputs), attrs, tensors, name)
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 595, in callback
    writer.WriteExecution(self._dump_eager_tensors(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 505, in _dump_eager_tensors
    if (self._should_dump_tensor(op_type, tensor.dtype) and
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 675, in _should_dump_tensor
    re.match(self._op_regex, op_type))
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/re.py"", line 191, in match
    return _compile(pattern, flags).match(string)
TypeError: cannot use a string pattern on a bytes-like object
```
and with a bytes regex
```
Traceback (most recent call last):
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/mnt/NVME/CubeCobraRecommender/src/ml/train_draftbots.py"", line 114, in <module>
    train_dataset = load_picks(pick_cache_dir / 'train', batch_size)
  File ""/mnt/NVME/CubeCobraRecommender/src/non_ml/parse_picks.py"", line 395, in load_picks
    return tf.data.experimental.load(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py"", line 201, in load
    return _LoadDataset(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py"", line 122, in __init__
    self._reader_func = dataset_ops.StructuredFunctionWrapper(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3525, in __init__
    self._function = wrapper_fn.get_concrete_function()
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3051, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3019, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3196, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 906, in func_graph_from_py_func
    func_args = _get_defun_inputs_from_args(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1142, in _get_defun_inputs_from_args
    return _get_defun_inputs(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1215, in _get_defun_inputs
    placeholder = graph_placeholder(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/graph_only_ops.py"", line 45, in graph_placeholder
    callback_outputs = op_callbacks.invoke_op_callbacks(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/op_callbacks.py"", line 202, in invoke_op_callbacks
    new_outputs = callback(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 581, in callback
    return self._instrument_symbolic_tensors(
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 415, in _instrument_symbolic_tensors
    if (not self._should_dump_tensor(op_type, tensor.dtype) or
  File ""/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 675, in _should_dump_tensor
    re.match(self._op_regex, op_type))
  File ""/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/re.py"", line 191, in match
    return _compile(pattern, flags).match(string)
TypeError: cannot use a bytes pattern on a string-like object
```"
48159,ImportError on Raspberry Pi 4 Model B: cannot import name 'symbol_database' from 'google.protobuf' ,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution: Linux raspberrypi 5.10.17-v7l+ #1403 SMP Mon Feb 22 11:33:35 GMT 2021 armv7l GNU/Linux
- Raspberry Pi 4 Model B
- TensorFlow installed from binary
- TensorFlow version: 2.4.0
- Python version: 3.7
- Google Protobuf version: 3.15.6

"
48157,`SignatureDef` of saved Keras models should match the Keras input and output `dict`,"**System information**
- TensorFlow version (you are using): 2.6.0-dev20210329
- Are you willing to contribute it (Yes/No): Yes (although I'd need some pointers to where to look for this particular issue)

**Describe the feature and the current behavior/state.**

Currently when saving Keras models using the saved model format, the input and output names in the `SignatureDef` refer to the names of the tensors of the model. Unfortunately these names are not fixes since they depend on implementation details like the number of times the model has been built (this might be problematic during fine tuning). This makes it hard to construct multi input and output models that are intended to be used with saved models during serving or with TFLite.
In contrast to Keras the saved model format and therefore also the TFLite converter do not preserve the output ordering, which makes using a list as output of the Keras model not a valid use. For more information why this leads to problems please see #47927.

It would be great if the `SignatureDef` of the saved model would match the dictionary keys of the input and output of the Keras model or if saved models would preserve the order of inputs and outputs matching Python and Keras:

```python
import tensorflow as tf

def build_model():
    input_tensor = tf.keras.layers.Input((32, 32, 128))
    x = input_tensor

    boxes = tf.keras.layers.Conv2D(4, (3, 3))(x)
    boxes = tf.reshape(boxes, (-1, 4))

    scores = tf.keras.layers.Conv2D(1, (3, 3))(x)
    scores = tf.reshape(scores, (-1,))

    return  tf.keras.Model({""inputs"": input_tensor}, {""boxes"": boxes, ""scores"": scores})

model = build_model()
model = build_model()

model.save(""/tmp/model"")

restored_fn = tf.saved_model.load(""/tmp/model"")
concrete_fn = restored_fn.signatures[""serving_default""]

# Would be great if these would match the Keras model:
# signature_wrapper(*, input_6)
# Args:
#   input: float32 Tensor, shape=(None, 32, 32, 128)
# Returns:
#   {'boxes': <1>, 'scores': <2>}
#     <1>: float32 Tensor, shape=(None, 4)
#     <2>: float32 Tensor, shape=(None,)

# Instead the keys are layer names which depends on implementation details and on how often the model
# was instantiated: (e.g input_6, tf.reshape_8, tf.reshape_9)
print(concrete_fn.pretty_printed_signature())
```

To make this work currently one needs to use the following workaround which requires users to learn about the low-level `tf.function` API despite that all the information was already present in the Keras model:
```python
@tf.function(input_signature=[tf.TensorSpec([None, 32, 32, 128], dtype=tf.float32, name=""input"")])
def override_output_signatures(input_tensor):
   outputs = model(input_tensor)
   return {""boxes"": outputs[""boxes""], ""scores"": outputs[""scores""]}

signatures = override_output_signatures.get_concrete_function()
tf.saved_model.save(model, export_dir=""/tmp/saved_model"", signatures=signatures)
```

For a complete reproducible example please see [this notebook](https://colab.research.google.com/drive/1CtZ0O7vNN6ODCQfSlon92JDNzfN9FQPe?usp=sharing).

**Will this change the current api? How?**
This proposal wouldn't require any the API changes on the Python side since this usage is already supported in Keras, however it might change the names of serialized multi input/output models. However, I don't think this is a problem in general since currently Keras saved models do not have deterministic output naming either.

**Who will benefit with this feature?**

People working with multi input and output models that require serialization to saved models would greatly benefit from a way to easily set function signatures when saving Keras models. For more motivation for TFLite users, please see #47927.
"
48156,Huge code size after compile the TFLM hello world example for ARM,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): SHA
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM Cortex M33 bare mental (SSE 200)

**Describe the problem**
##########
I want to implement the TFLM into my own ARM board, which has a flash not more than 512KB. And my compiler is arm compiler.  The first step I did is to try the hello world example. I expected a binary file less than 20KB as it says in the website, but my compilation result is 348KB. I don't know how to handle that.

**Please provide the exact sequence of commands/steps when you ran into the problem**
#############
First, I generated a static library by using `make -f tensorflow/lite/micro/tools/make/Makefile TOOLCHAIN=armclang TARGET_TOOLCHAIN_ROOT=/usr/pack/armmisc-201912-tonl/ARMCompiler6.13/bin/ TARGET=cortex_m_generic TARGET_ARCH=cortex-m33 microlite ARMLMD_LICENSE_FILE:=xxx`. Even the generated static library `libtensorflow-microlite.a` is 1.1MB, which is really large compared other issues posted here.
Then, I modified my makefile, adding the lib directory into the dependency,
```
$(OUTPUT_DIR)/$(BUILD_S_NS)/$(TEST_NAME).elf:	$(CPU_MCU_OBJ) $(MICROLITE_LIB_PATH)  
	@set -euo pipefail;\  
	echo ""Making $@ ..."";\  
	mkdir -p $(@D);\  
	export ARMLMD_LICENSE_FILE=$(ARMLMD_LICENSE_FILE);\
	$(LD) $(LDFLAGS) -o $@ $^ > $@.log`
```
I also changed the ops from `all_ops_resolver` to `micro_mutable_op_resolver.h`, and modified the code in `main_function.cc hello_world_test.cc` to 
```
// static tflite::AllOpsResolver resolver;
  
  static tflite::MicroMutableOpResolver<1> micro_op_resolver(error_reporter);
  if (micro_op_resolver.AddFullyConnected() != kTfLiteOk) {
    return;
  }

```
The `CPU_MCU_OBJ` includes the five source code which should be compiled, `main.cc main_functions.cc models.cc constants.cc output_handler.cc`.  It will generate `.elf .hex .disass .map`, I really care about the size of `.hex` file, which is 348KB, far more larger than it indicated less than 20KB in the website. 
"
48155,Filename clashes in /tmp during flatbuffer download,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 1e8f4666f2fbc1bdd4ce2797b218de0453cffc63
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): all

**Describe the problem**

The script `tensorflow/lite/micro/tools/make/flatbuffers_download.sh` creates a temporary files/dirs in `/tmp`
whose names are not uniqified and not all of which are subsequently removed.  This means builds by different users on a shared server host fail and junk is left lying around in `/tmp`.

A small patch correcting these issues by using `mktemp`  is attached.  This approach is the same
as that used in the  `tensorflow/lite/micro/tools/make/download_and_extract.sh` script.

[tmp_filename_clash_fix.patch.txt](https://github.com/tensorflow/tensorflow/files/6221742/tmp_filename_clash_fix.patch.txt)


**Please provide the exact sequence of commands/steps when you ran into the problem**

make -f tensorflow/lite/micro/tools/make/Makefile

"
48154,Compact Multi-Class Boosted Trees configuration ,"I am trying implement the TFBT algorithm from the [Compact multi-class boosted trees](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA) paper. As I understand this [paper](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA) used the [BoostedTreeClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933) with [this](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403) config.

- Would you please indicate the same **setting** and **hyperparameter** which you used in your paper?
- Also, would you verify that the mentioned paper used the above code?
- How can I have Fit method in [BoostedTreeClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933)?

"
48153,Using GridSearchCV in BoostedTreesClassifier,"I'm trying to Implement [GridsearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on [BoostedTreesClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933). Since [BoostedTreesClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933) has not the fit method, is there any alternative to implement the GridsearchCV?"
48152,TF2 getting tf.keras.metrics.AUC().result() greater than 1 on mirroredStrategy,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:  yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:source
-   **TensorFlow version (use command below)**:2.3.0
-   **Python version**:3.7
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**: 2 Tesla V100 GPU
-   **Exact command to reproduce**:


### Describe the problem

Hi. I find the tf.keras.metrics.AUC() is returning wrong values when using MirroredStrategy on 2 GPUs, while it works correctly on non-distributed settings. For the sake of data privacy I cannot provide my data or code to reproduce, but I believe it is the same problem described as in 
https://stackoverflow.com/questions/62405592/tensorflow-2-metrics-produce-wrong-results-with-2-gpus. 
which uses mnist data to reproduce. We both get an AUC value greater than 3, while other metrics look normal. 

Thanks."
48151,"tf. feature_column.input_layer(dataVale, feature_embedding) sometimes gets out of order when processing feature_embedding lists","When bucketized_column is used to divide buckets and generate one-hot code, the running result is inconsistent with the actual input feature list.

windows10,tensorflow==1.13.1,python==3.7.7

-----------------------------------------python-----------------------------------------------------------
import tensorflow as tf
import json
import pandas as pd


def get_feature_columns(tain_file):
    data = pd.read_csv(tain_file, nrows=5).round(5)
    feature_size = data.columns.shape[0] - 1
    feature_columns = data.columns[:-1].tolist()
    return feature_columns, feature_size


def read_row(csv_row):
    record_defaults = [[0.]] * FEATURE_SIZE + [[0]]
    row = tf.decode_csv(csv_row, record_defaults=record_defaults)
    return dict(zip(FEATURE_COLUMNS, row[:-1])), row[-1]


def _feature_column(COLUMNS):
    with open('./feature_fm_bucket.json', 'r') as f:
        boundary_dict = json.load(f)
    boundary_dict = {k: sorted(x for x in v if abs(x) != float(""inf"")) for k, v in boundary_dict.items()}
    length = 0
    for kk in boundary_dict:
        length += len(kk)

    feature_embedding = []
    for col in COLUMNS:
        buket_col = tf.feature_column.bucketized_column(
            tf.feature_column.numeric_column(col), boundary_dict[col])
        feature_embedding.append(buket_col)

    return feature_embedding, boundary_dict


feature_name = 'user_click_num_7d'
# FEATURE_COLUMNS = [feature_name]

tain_file = './demo.csv'

FEATURE_COLUMNS, FEATURE_SIZE = get_feature_columns(tain_file)

feature_embedding, boundary_dict = _feature_column(FEATURE_COLUMNS)

TRAIN_FILENAMES = [tain_file]

batch_size = 1
filenames = TRAIN_FILENAMES
dataset = tf.data.Dataset.from_tensor_slices(filenames)
dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(TRAIN_FILENAMES).skip(1))
dataset = dataset.map(lambda line: read_row(line), num_parallel_calls=15,
                      ).batch(batch_size).repeat(1)

iterator = dataset.make_one_shot_iterator()
ds = iterator.get_next()
dataVale = ds[0]

order = tf.feature_column.input_layer(dataVale, feature_embedding)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())

value = sess.run(order)
first_line_value = value[0]

count = 0
for i, v1 in enumerate(FEATURE_COLUMNS):
    print(""line="" + str(i + 1) + "", "", v1, end=', one_hot=[')
    jj = 0
    for j in range(len(boundary_dict[v1]) + 1):
        vv = first_line_value[count]
        count += 1
        print(vv, end=',')
        if vv == 1:
            jj = j

    print(end='], index=')
    print(jj, end='')
    print()
-----------------------------------------------------python-------------------------------------------

-----------------------------------------feature_fm_bucket.json----------------------------------------------------

{""mt_ses_pv_maxmin_30d"":[0.01,0.03,0.06,0.1,0.16,0.24,0.37,0.58],""it_uv_15d"":[2,5,6,7,8,9,10,11,12],""it_ck_pv_ws_3_30d"":[0.04,0.07,0.09,0.11,0.13,0.15,0.18,0.22,0.26,0.31,0.38,0.49,0.64,0.8,0.92,0.96,0.98],""it_ck_pv_ws_3_15d"":[0.06,0.1,0.13,0.16,0.19,0.22,0.25,0.28,0.32,0.37,0.43,0.53,0.65,0.8,0.92,0.96,0.98],""rt_it_uv_3h"":[1,2,3,4,5,6,7],""it_lk_uv_7"":[0.001,0.002,0.003,0.005,0.008,0.01],""it_lk_uv_30d"":[0.0005,0.001,0.003],""it_collect_uv_15d"":[0.001,0.002,0.003,0.006],""rt_it_fp_ws_st_ro_1_3h"":[0.01,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.17,0.19],""it_st_pv_ws_1_3d"":[0.27,0.34,0.38,0.41,0.44,0.46,0.48,0.5,0.52,0.54,0.55,0.58,0.6,0.64,0.68,0.75,0.83,0.93],""mt_ses_dur_weight_30d"":[0.01,0.02,0.04,0.06,0.1,0.2],""rt_ur_tr"":[0.01,0.02,0.03,0.04,0.05,0.07],""it_ck_pv_ws_1_15d"":[0.02,0.04,0.06,0.07,0.09,0.1,0.11,0.13,0.15,0.17,0.2,0.24,0.3,0.37,0.47,0.6,0.75,0.88,0.96],""mt_nlp_tag_max"":[0.02,0.04,0.05,0.07,0.08,0.1,0.12,0.15,0.17,0.19,0.21,0.24,0.28,0.33,0.41],""it_fp_ck_pv_15d"":[5,6,7,8,9,10,11,12],""ur_ck_num_7"":[0.7,1,2,2.5,3,3.5,4,4.7],""it_prop_ar_favorites_num"":[2,3,4],""rt_it_fp_ws_st_ro_3h"":[0.06,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.19,0.2,0.23,0.25,0.29,0.32,0.38],""it_prop_ar_articlenum"":[2,3,4],""it_lk_uv_15d"":[0.001,0.003,0.005,0.007,0.1,0.2],""rt_mt_it_dgc"":[0.57,0.7,0.78,0.83,0.87,0.9,0.92,0.96,0.98],""it_fp_st_pv_7"":[7,8,9,10,11,12,13],""it_fp_ck_pv_3d"":[4,5,6,7,8,9,10,11],""it_lk_uv_1"":[0.0001,0.0002,0.0003,0.0005,0.001,0.003,0.01],""rt_ur_st_show_num"":[1,2,3,4,5],""rt_it_pv"":[3,4,5,6,7,8],""it_ck_pv_ws_1_30d"":[0.01,0.03,0.04,0.05,0.06,0.07,0.08,0.1,0.12,0.14,0.17,0.21,0.27,0.35,0.46,0.6,0.75,0.88,0.96],""mt_ses_pv_weight_7"":[0.01,0.02,0.04,0.07,0.13,0.26],""it_fp_st_pv_30d"":[7,8,9,10,11,12,13,14],""it_prop_ar_articlenum_4"":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],""it_prop_ar_articlenum_5"":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],""it_prop_ar_articlenum_2"":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1],""it_prop_ar_articlenum_3"":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1],""it_fp_ck_pv_30d"":[5,6,7,8,9,10,11,12],""rt_it_fp_ck_pv"":[3,4,5,6,7,8],""it_rp_uv_3d"":[0.001,0.002,0.003,0.004,0.01],""rt_it_fp_ws_ck_ro_3h"":[0.01,0.03,0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.15,0.16,0.18,0.21,0.25,0.3],""rt_it_uv"":[3,4,5,6,7,8],""rt_mt_ses_wsim_top5"":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],""it_quality_effect_score"":[0.8,0.88,0.93,0.96,0.97,0.98,0.99],""rt_mt_ses_wsim_top1"":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],""rt_mt_ses_wsim_top2"":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],""ur_dr_30d"":[5,6,7,8],""it_fp_ag_pv_tr_3d"":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],""it_ck_pv_ws_1_3d"":[0.19,0.26,0.3,0.34,0.37,0.39,0.42,0.44,0.46,0.48,0.51,0.53,0.56,0.59,0.64,0.7,0.79,0.89,0.97],""mt_ses_dur_weight_15d"":[0.01,0.03,0.04,0.07,0.11,0.22],""mt_ses_lst"":[1,2],""it_fp_ws_pv_tr_30d"":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],""it_uv_30d"":[1,5,6,7,8,9,10,11,12],""it_fp_ag_pv_tr_15d"":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.2],""mt_ses_ss_7"":[1,2,3,5,8],""it_cb_ro_7"":[1.12,1.2,1.23,1.27,1.3,1.32,1.34,1.36,1.39,1.41,1.44,1.48,1.51,1.57,1.65,1.82],""rt_it_fp_ws_tr"":[0.02,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15],""rt_it_fp_ws_st_ro_1h"":[0.01,0.02,0.03,0.04,0.06],""it_quality_rare_score"":[0.24,0.38,0.48,0.57,0.63,0.73,0.76,0.79,0.81,0.83,0.86,0.88,0.89,0.91,0.95,0.98,0.99],""it_collect_uv_7"":[0.001,0.002,0.004],""mt_ses_dur_15d"":[2,3,4,5,6,7],""it_st_pv_ws_3_7"":[0.25,0.33,0.37,0.41,0.44,0.46,0.49,0.52,0.55,0.59,0.63,0.7,0.79,0.89,0.97],""it_rp_uv_30d"":[0.001,0.002,0.003,0.005,0.011],""mt_ses_dur_30d"":[1,2,3,4,5,6,7],""rt_it_fp_ws_ck_ro_1_3h"":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.14],""it_fp_ck_pv_1"":[3,4,5,6,7,8,9,10],""it_fp_ck_pv_7"":[4,5,6,7,8,9,10,11],""rt_it_fp_ck_pv_1h"":[1,2,3,4,5],""rt_it_fp_ws_tr_1h"":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.11],""rt_it_pv_1h"":[1,2,3,4,5],""ur_ck_num_30d"":[1,2,3,4,5,6],""mt_ses_ss_90d"":[1,2,3,4,5,9,12],""it_quality_cover_score"":[0.42,0.52,0.53,0.54,0.55,0.56,0.6,0.65],""it_fp_ag_pv_tr_30d"":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.19],""context_event_hour"":[8,12,14,18,21],""it_uv_1"":[3,4,5,6,7,8,9,10],""rt_mt_ses_maxmin"":[0],""it_uv_7"":[1,5,6,7,8,9,10,11,12],""it_rp_uv_15d"":[0.001,0.002,0.003,0.005,0.009],""it_rp_uv_1"":[0.0005,0.001,0.003],""mt_it_lst_type"":[1,2,4,6],""it_st_pv_ws_1_7"":[0.09,0.14,0.16,0.19,0.21,0.22,0.24,0.26,0.28,0.3,0.33,0.37,0.41,0.46,0.56,0.67,0.8,0.91],""it_fp_ws_pv_tr_1"":[0.03,0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17],""it_fp_ag_pv_tr_1"":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],""rt_mt_ses_sim_lst4"":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],""rt_mt_ses_sim_lst5"":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],""it_collect_uv_1"":[0.001,0.002],""rt_it_pv_3h"":[1,2,3,4,5,6,7],""rt_mt_ses_sim_lst1"":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],""rt_mt_ses_sim_lst2"":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],""rt_mt_ses_sim_lst3"":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],""it_prop_ar_comment_num"":[2,3,4],""it_lk_uv_3d"":[0.0002,0.0003,0.0006,0.002],""ur_dr_7"":[5,6,7,8],""rt_mt_ses_wsim_top4"":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],""it_st_pv_ws_1_15d"":[0.04,0.06,0.08,0.09,0.11,0.12,0.14,0.15,0.17,0.2,0.23,0.27,0.33,0.41,0.52,0.66,0.79,0.91],""it_cb_ro_1"":[1.11,1.19,1.23,1.26,1.29,1.31,1.34,1.36,1.39,1.41,1.45,1.48,1.52,1.58,1.67,1.86],""it_ck_pv_ws_3_7"":[0.19,0.26,0.31,0.35,0.39,0.42,0.44,0.47,0.5,0.54,0.58,0.65,0.73,0.83,0.92,0.96,0.98],""mt_ses_ss_60d"":[1,2,3,4,5,9,17],""mt_nlp_sec_tag_avg"":[0.03,0.05,0.07,0.09,0.11,0.12,0.14,0.18,0.23,0.33],""rt_ur_ck_num"":[1,1.4,1.9,2,2.4,2.7,3],""ur_dr_15d"":[5,6,7,8],""rt_it_uv_1h"":[1,2,3,4,5],""it_st_pv_ws_3_30d"":[0.05,0.08,0.11,0.13,0.15,0.18,0.21,0.25,0.29,0.35,0.43,0.54,0.69,0.88,0.97],""mt_ses_dur_maxmin_15d"":[0.02,0.04,0.08,0.13,0.21,0.34,0.55],""it_fp_ws_pv_tr_15d"":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],""it_fp_ag_pv_tr_7"":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],""rt_it_fp_ws_tr_3h"":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.14],""it_fp_st_pv_3d"":[6,7,8,9,10,11,12,13],""rt_it_fp_ck_pv_3h"":[1,2,3,4,5,6,7],""rt_mt_ses_wsim_top3"":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],""ur_ck_num_15d"":[1,2,3,4,5],""it_prop_time_decay"":[0.12,0.15,0.2,0.24,0.27,0.31,0.36,0.41,0.46,0.5,0.57,0.61,0.65,0.69,0.74,0.77,0.81,0.84],""it_prop_ar_fellowsnum"":[2,3,4,5],""it_prop_ar_lk_num"":[2,3,4],""mt_ses_pref"":[0.01,0.03,0.05,0.07,0.11,0.17,0.29],""it_uv_3d"":[1,4,5,6,7,8,9,10,11],""rt_mt_it_weights"":[0.06,0.16,0.25,0.33,0.37,0.47,0.5,0.6,0.7],""it_fp_ws_pv_tr_7"":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],""mt_ses_pv_maxmin_7"":[0.02,0.05,0.09,0.18,0.32,0.56],""it_ck_pv_ws_1_7"":[0.06,0.1,0.12,0.15,0.17,0.19,0.2,0.22,0.24,0.27,0.3,0.33,0.37,0.42,0.51,0.62,0.76,0.88,0.96],""it_rp_uv_7"":[0.001,0.002,0.003,0.007],""it_cb_ro_15d"":[1.13,1.2,1.23,1.27,1.3,1.32,1.34,1.36,1.39,1.41,1.44,1.47,1.5,1.55,1.62,1.81],""it_fp_ws_pv_tr_3d"":[0.03,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],""mt_ses_pv_15d"":[1,2,3,4,5],""it_cb_ro_30d"":[1.13,1.2,1.24,1.27,1.3,1.32,1.33,1.35,1.38,1.4,1.43,1.46,1.5,1.55,1.61,1.78],""mt_ses_pv_weight_30d"":[0.01,0.02,0.04,0.06,0.09,0.18],""mt_nlp_tag_num"":[1,2,3,6],""mt_ses_ss_15d"":[1],""rt_it_fp_ws_ck_ro_1h"":[0.0003,0.0008,0.01,0.02,0.03,0.04],""mt_ses_dur_7"":[1,3,4,5,6],""mt_ses_pv_maxmin_15d"":[0.02,0.04,0.08,0.13,0.22,0.35,0.57],""it_cb_ro_3d"":[1.12,1.2,1.23,1.27,1.3,1.32,1.35,1.37,1.4,1.42,1.45,1.49,1.53,1.58,1.65,1.87],""it_quality_time_score"":[0.12,0.27,0.39,0.45,0.49,0.54,0.58,0.61,0.65,0.68,0.71,0.74,0.76,0.77,0.78,0.9,0.96],""it_collect_uv_3d"":[0.001,0.002,0.003],""it_st_pv_ws_1_30d"":[0.02,0.04,0.05,0.06,0.08,0.09,0.1,0.12,0.14,0.17,0.2,0.24,0.31,0.4,0.51,0.66,0.79,0.91],""mt_nlp_tag_avg"":[0.02,0.04,0.05,0.06,0.07,0.09,0.1,0.12,0.14,0.16,0.19,0.22,0.26,0.31,0.39],""it_collect_uv_30d"":[0.001,0.002,0.003,0.004,0.007],""it_prop_publishtime"":[347,422,504,604,722,844,976,1104,1376,1563,1769,2035,2340,2606,2808,3158,3751,4188,16536],""mt_ses_dur_maxmin_30d"":[0.01,0.03,0.05,0.09,0.14,0.22,0.34,0.54],""mt_nlp_tag_sum"":[0.03,0.05,0.06,0.08,0.11,0.13,0.16,0.18,0.21,0.24,0.28,0.33,0.39,0.49,0.72],""mt_ses_pv_30d"":[1,2,3,4,5],""it_prop_ar_share_cnt"":[2,3,4],""it_fp_st_pv_1"":[6,7,8,9,10,11,12],""mt_ses_ss_30d"":[1,2,3,4,5,7,14],""mt_nlp_sec_tag_sum"":[0.03,0.06,0.09,0.11,0.14,0.18,0.22,0.28,0.36,0.49],""rt_mt_ses_sim_top4"":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],""rt_mt_ses_sim_top5"":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],""mt_ses_pv_7"":[1,2,3,4],""mt_ses_pv_weight_15d"":[0.01,0.02,0.04,0.06,0.11,0.21],""rt_mt_ses_sim_top1"":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],""rt_mt_ses_sim_top2"":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],""rt_mt_ses_sim_top3"":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],""mt_nlp_sec_tag_max"":[0.03,0.06,0.08,0.1,0.12,0.14,0.17,0.21,0.25,0.34],""it_st_pv_ws_3_15d"":[0.09,0.13,0.16,0.19,0.22,0.25,0.28,0.31,0.36,0.41,0.47,0.58,0.71,0.88,0.97],""it_fp_st_pv_15d"":[7,8,9,10,11,12,13,14],""rt_mt_it_rank"":[0.5,0.72,0.85],""mt_ses_dur_weight_7"":[0.01,0.02,0.04,0.08,0.13,0.28],""mt_ses_dur_maxmin_7"":[0.02,0.05,0.1,0.18,0.32,0.55]}

-----------------------------------------feature_fm_bucket.json----------------------------------------------------


-----------------------------------------demo.csv----------------------------------------------------

context_event_hour,it_cb_ro_15d,it_cb_ro_1,it_cb_ro_30d,it_cb_ro_3d,it_cb_ro_7,it_ck_pv_ws_1_15d,it_ck_pv_ws_1_30d,it_ck_pv_ws_1_3d,it_ck_pv_ws_1_7,it_ck_pv_ws_3_15d,it_ck_pv_ws_3_30d,it_ck_pv_ws_3_7,it_collect_uv_15d,it_collect_uv_1,it_collect_uv_30d,it_collect_uv_3d,it_collect_uv_7,it_fp_ag_pv_tr_15d,it_fp_ag_pv_tr_1,it_fp_ag_pv_tr_30d,it_fp_ag_pv_tr_3d,it_fp_ag_pv_tr_7,it_fp_ck_pv_15d,it_fp_ck_pv_1,it_fp_ck_pv_30d,it_fp_ck_pv_3d,it_fp_ck_pv_7,it_fp_st_pv_15d,it_fp_st_pv_1,it_fp_st_pv_30d,it_fp_st_pv_3d,it_fp_st_pv_7,it_fp_ws_pv_tr_15d,it_fp_ws_pv_tr_1,it_fp_ws_pv_tr_30d,it_fp_ws_pv_tr_3d,it_fp_ws_pv_tr_7,it_lk_uv_15d,it_lk_uv_1,it_lk_uv_30d,it_lk_uv_3d,it_lk_uv_7,it_prop_ar_articlenum,it_prop_ar_comment_num,it_prop_ar_favorites_num,it_prop_ar_fellowsnum,it_prop_ar_lk_num,it_prop_ar_share_cnt,it_prop_publishtime,it_prop_time_decay,it_quality_cover_score,it_quality_effect_score,it_quality_rare_score,it_quality_time_score,it_rp_uv_15d,it_rp_uv_1,it_rp_uv_30d,it_rp_uv_3d,it_rp_uv_7,it_st_pv_ws_1_15d,it_st_pv_ws_1_30d,it_st_pv_ws_1_3d,it_st_pv_ws_1_7,it_st_pv_ws_3_15d,it_st_pv_ws_3_30d,it_st_pv_ws_3_7,it_uv_15d,it_uv_1,it_uv_30d,it_uv_3d,it_uv_7,mt_nlp_sec_tag_avg,mt_nlp_sec_tag_max,mt_nlp_sec_tag_sum,mt_nlp_tag_avg,mt_nlp_tag_max,mt_nlp_tag_num,mt_nlp_tag_sum,mt_ses_ss_15d,mt_ses_ss_30d,mt_ses_ss_60d,mt_ses_ss_7,mt_ses_ss_90d,mt_it_lst_type,mt_ses_dur_15d,mt_ses_dur_30d,mt_ses_dur_7,mt_ses_dur_maxmin_15d,mt_ses_dur_maxmin_30d,mt_ses_dur_maxmin_7,mt_ses_dur_weight_15d,mt_ses_dur_weight_30d,mt_ses_dur_weight_7,mt_ses_lst,mt_ses_pref,mt_ses_pv_15d,mt_ses_pv_30d,mt_ses_pv_7,mt_ses_pv_maxmin_15d,mt_ses_pv_maxmin_30d,mt_ses_pv_maxmin_7,mt_ses_pv_weight_15d,mt_ses_pv_weight_30d,mt_ses_pv_weight_7,ur_ck_num_15d,ur_ck_num_30d,ur_ck_num_7,ur_dr_15d,ur_dr_30d,ur_dr_7,rt_it_fp_ck_pv,rt_it_fp_ck_pv_1h,rt_it_fp_ck_pv_3h,rt_it_fp_ws_ck_ro_1_3h,rt_it_fp_ws_ck_ro_1h,rt_it_fp_ws_ck_ro_3h,rt_it_fp_ws_tr,rt_it_fp_ws_tr_1h,rt_it_fp_ws_tr_3h,rt_it_fp_ws_st_ro_1_3h,rt_it_fp_ws_st_ro_1h,rt_it_fp_ws_st_ro_3h,rt_it_pv,rt_it_pv_1h,rt_it_pv_3h,rt_it_uv,rt_it_uv_1h,rt_it_uv_3h,rt_mt_it_dgc,rt_mt_it_rank,rt_mt_it_weights,rt_mt_ses_maxmin,rt_mt_ses_sim_lst1,rt_mt_ses_sim_lst2,rt_mt_ses_sim_lst3,rt_mt_ses_sim_lst4,rt_mt_ses_sim_lst5,rt_mt_ses_sim_top1,rt_mt_ses_sim_top2,rt_mt_ses_sim_top3,rt_mt_ses_sim_top4,rt_mt_ses_sim_top5,rt_mt_ses_wsim_top1,rt_mt_ses_wsim_top2,rt_mt_ses_wsim_top3,rt_mt_ses_wsim_top4,rt_mt_ses_wsim_top5,rt_ur_ck_num,rt_ur_tr,rt_ur_st_show_num,label
11,1.398,1.374,1.398,1.406,1.404,0.08342,0.08342,0.45096,0.17688,0.17996,0.17996,0.38242,0.0038,0.00282,0.0038,0.00282,0.00322,0.16433,0.15416,0.16416,0.15696,0.16032,11.03429,8.61505,11.03429,9.36134,10.28606,12.83994,10.48414,12.83994,11.21267,12.11637,0.16137,0.14493,0.16137,0.15045,0.15611,0.00209,0.00019,0.00209,0.00015,0.00036,2.0,4.0,4.0,3.0,3.0,4.0,1666,0.435,0.564,0.99997,0.917776,0.63613,0.00308,0.00172,0.00308,0.00168,0.00241,0.09246,0.09246,0.47343,0.19089,0.19325,0.19325,0.39934,11.189,8.722,11.189,9.429,10.335,0.16,0.16,0.16,0.186,0.186,1,0.186,0.0,0.0,0.0,0.0,0.0,1.0,5.784,5.784,3.401,0.313,0.239,0.049,0.031,0.02,0.007,0.0,0.047,4.159,4.159,0.693,0.13,0.1,0.004,0.014,0.011,0.001,4.92,5.308,4.234,7.257,7.105,6.953,6.863,4.174,5.943,0.09343,0.03659,0.32193,0.11717,0.06461,0.10553,0.15844,0.06329,0.37468,6.828,4.111,5.916,6.783,4.007,5.858,0.889,1.0,0.667,0.0,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.0,0.0,0.0,0.0,0.0,1.946,0.02483,3.714,1

-----------------------------------------demo.csv----------------------------------------------------

The running results are as follows:
In column 108, ur_ck_num_7sub bucket value:
[0.7,1,2,2.5,3,3.5,4,4.7]

The ur_ck_num_7value value in column 108 is initial 4.4.
line=108,  ur_ck_num_7, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=5;
line=149,  rt_ur_ck_num, one_hot=[0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=4

The ur_ck_num_7 value in column 108 is changed to 114.234.
line=108,  ur_ck_num_7, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=5：The data has not changed.;
line=149,  rt_ur_ck_num, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,], index=5：The data changes.

We expected line 108 to change, but it was line 149 where the data changed."
48149,Keras `predict_step` is not preserved across save and restore,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 and macOS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0-dev20210329
- Python version: 3.7 and 3.8

**Describe the current behavior**

When implementing custom prediction logic for Keras models using [`predict_step`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_step) as explained [here](https://keras.io/guides/customizing_what_happens_in_fit/), saving and restoring the Keras model with the saved model format ignores the custom prediction logic. Unfortunately the code silently fails and doesn't inform the user that this is not supported, which could lead to detrimental bugs.

The issue is explained in detail with a minimal example in [this colab notebook](https://colab.research.google.com/drive/1q2f1tXS3fUaO4WH1TJYmEYFO7QQoh8Z9?usp=sharing).

I know I can save a custom serving function using
```python
class MyModel(tf.keras.Model):
    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
    def serve(self, data):
        ...
```
as described [here](https://www.tensorflow.org/api_docs/python/tf/saved_model/save).
But I feel the current behaviour breaks with user expectations since the saved model format is now the default saving format but doesn't support all of the features and might silently fail resulting in unexpected behaviour.
This makes it necessary for users to break the abstraction and start using low level TF APIs instead, which I think doesn't fit well with the progressive disclosure of complexity that Keras tends to strive for.

**Describe the expected behavior**

Keras models should preserve custom `predict_step` logic when saving and restoring models.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

class FullyConnectedModel(tf.keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(10)

    def predict_step(self, data):
        logits = self(data, training=False)
        return tf.argmax(logits, axis=-1)

    def call(self, inputs):
        return self.dense(inputs)

x, y = np.random.uniform(size=(128, 20)).astype(np.float32), np.random.randint(0, 10, size=(128))

model = FullyConnectedModel()
model.compile(optimizer=""sgd"", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.fit(x, y, epochs=2, batch_size=32)

model.save(""/tmp/model"", save_traces=True)
reloaded_model = tf.keras.models.load_model(""/tmp/model"")

y_pred = model.predict(x)
reloaded_y_pred = reloaded_model.predict(x)

np.testing.assert_allclose(reloaded_y_pred, y_pred)
```
See [this notebook](https://colab.research.google.com/drive/1q2f1tXS3fUaO4WH1TJYmEYFO7QQoh8Z9?usp=sharing) for more information."
48148,`TFLiteConverter` breaks `SignatureDefs` when used with full integer quantization,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installation (pip package or built from source): binary 
- TensorFlow library (version, if pip package or github SHA, if built from source): `2.5.0-dev20210319` and `2.6.0-dev20210329`

**Describe the current behavior**

Converting `SignatureDefs` together with full int8 quantization by setting
```python
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
```
breaks the signature runner since the signature still refers to tensors that [have been removed when converting to full int8 quantization](https://github.com/tensorflow/tensorflow/blob/305926798c36e2babfd31395317db33c55490d5b/tensorflow/lite/python/lite.py#L745).

Please see [this notebook](https://colab.research.google.com/drive/1stwy8M-tLDBCPW8_BRXpBs5ZOzlm94Jz?usp=sharing) for the full code to reproduce the problem.

For more background on why this is problematic for multi-output models please checkout #47927
"
48147,"""CancelledError:  [_Derived_]RecvAsync is cancelled.  Function call stack: train_function"" persists for some code","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.0
- CUDA/cuDNN version: 11.0 (TF was installed by following the procedure in https://www.tensorflow.org/install/gpu)
- GPU model and memory:  3080 RTX 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

After setting up a new rig, I ran two different LSTM models. Initially both models failed to initiate training by giving the error below:
     

     CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]
	 [[gradient_tape/sequential_1/embedding/embedding_lookup/Reshape/_38]] [Op:__inference_train_function_3172]
    Function call stack:
    train_function

I was able to fix one model by adding `import os ; os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""]=""true""`, but another model still fails to run. Here is the whole error message from the model:

    Epoch 1/15
    ---------------------------------------------------------------------------
    CancelledError                            Traceback (most recent call last)
    <ipython-input-96-9b266da78330> in <module>
          1 es = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 4, verbose =1)
          2 mc = ModelCheckpoint('best_model_naver.h5', monitor = 'val_acc', mode = 'max', save_best_only = True)
    ----> 3 history = model.fit(X_train, y_train, batch_size = 128, callbacks = [es, mc], epochs = 15, validation_split = .2)
    
    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
       1098                 _r=1):
       1099               callbacks.on_train_batch_begin(step)
    -> 1100               tmp_logs = self.train_function(iterator)
       1101               if data_handler.should_sync:
       1102                 context.async_wait()

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
        826     tracing_count = self.experimental_get_tracing_count()
        827     with trace.Trace(self._name) as tm:
    --> 828       result = self._call(*args, **kwds)
        829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
        830       new_tracing_count = self.experimental_get_tracing_count()

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
        886         # Lifting succeeded, so variables are initialized and we can run the
        887         # stateless function.
    --> 888         return self._stateless_fn(*args, **kwds)
        889     else:
        890       _, _, _, filtered_flat_args = \

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
       2940       (graph_function,
       2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
    -> 2942     return graph_function._call_flat(
       2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
       2944 

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
       1916         and executing_eagerly):
       1917       # No tape is watching; skip to running the function.
    -> 1918       return self._build_call_outputs(self._inference_function.call(
       1919           ctx, args, cancellation_manager=cancellation_manager))
       1920     forward_backward = self._select_forward_and_backward_functions(

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
        553       with _InterpolateFunctionError(self):
        554         if cancellation_manager is None:
    --> 555           outputs = execute.execute(
        556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

    ~/anaconda3/envs/dl_learn/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
         57   try:
         58     ctx.ensure_initialized()
    ---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
             60                                         inputs, attrs, num_outputs)
             61   except core._NotOkStatusException as e:

    CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]
	 [[gradient_tape/sequential_1/embedding/embedding_lookup/Reshape/_38]] [Op:__inference_train_function_3172]

    Function call stack:
    train_function'

Error message on console:

    2021-03-29 05:37:44.895392: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED in tensorflow/stream_executor/cuda/cuda_dnn.cc(1859): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'

    2021-03-29 05:37:44.895639: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1521 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 30, 128, 1, 30, 128, 0] 


**Describe the expected behavior**

I expected the issue to be resolved by adding `import os ; os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""]=""true""`

**Standalone code to reproduce the issue**

Below is the code that produces the error messages:

    %config Completer.use_jedi = False
    import pandas as pd
    import numpy as np
    import urllib.request 
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from konlpy.tag import Okt
    import re
    import matplotlib.pyplot as plt
    import os
    os.environ[""TF_FORCE_ALLOW_GPU_GROWTH""] = 'true'
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

    urllib.request.urlretrieve(""https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"", filename = 'ratings_train.txt')
    urllib.request.urlretrieve(""https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"", filename = 'ratings_test.txt')

    train = pd.read_table('ratings_train.txt')
    train.drop_duplicates(subset = 'document', inplace = True)
    train.dropna(inplace = True)   
    train['document'] = train['document'].str.replace(r""[^ㄱ-ㅎㅏ-ㅣ가-힣 ]"", """")
    train[train['document'] == ''] = np.nan
    train.dropna(inplace = True)
    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
    okt = Okt()
    X_train = [[word for word in okt.morphs(sentence, stem = True) if word not in stopwords] for sentence in train['document']]
    vocab_size = 19417
    t = Tokenizer(vocab_size, oov_token = ""OOV"")
    t.fit_on_texts(X_train)
    X_train_sequences = t.texts_to_sequences(X_train)
    y_train = np.array(train['label'])
    empty_sample_idx = [index for index, sequence in enumerate(X_train_sequences) if len(sequence) < 1]
    X_train = np.delete(X_train_sequences, empty_sample_idx, axis = 0)
    y_train = np.delete(y_train, empty_sample_idx, axis = 0)
    maxlen = 30
    X_train = pad_sequences(X_train, maxlen = maxlen, padding = 'pre')

    model = Sequential()
    model.add(Embedding(vocab_size, 30))
    model.add(LSTM(128))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])
    es = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 4, verbose =1)
    mc = ModelCheckpoint('best_model_naver.h5', monitor = 'val_acc', mode = 'max', save_best_only = True)
    history = model.fit(X_train, y_train, batch_size = 128, callbacks = [es, mc], epochs = 15, validation_split = .2)





Below is the code I was able to run successfully after adding `import os ; os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""]=""true""` :

    import os
    os.environ[""TF_FORCE_GPU_ALLOW_GROWTH""]=""true""
    import numpy as np
    import matplotlib.pyplot as plt
    from tensorflow.keras.datasets import imdb
    import re
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, GRU, Dense, LSTM
    from tensorflow.keras.utils import to_categorical
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

    vocab_size = 10000
    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = 10000)

    maxlen = 500
    X_train = pad_sequences(X_train, maxlen = maxlen)
    X_test = pad_sequences(X_test, maxlen = maxlen)

    model = Sequential()
    model.add(Embedding(vocab_size, 100))
    model.add(LSTM(128))
    model.add(Dense(1, activation = 'sigmoid'))
    es = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 4, verbose = 1)
    mc = ModelCheckpoint('best_model_imdb.h5', monitor = 'val_acc', mode = 'max', save_best_only = True)
    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['acc'])

    history = model.fit(X_train, y_train, callbacks = [es, mc], epochs = 15, batch_size = 128, validation_split = .2)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48146,GPU Profiling: MemoryProfile do not contain memory events.,"**System information**
- Have I written custom code in: C++
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 2.3.2
- Python version: 3.6
- Bazel version: 3.1.0
- GCC/Compiler version: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: 10.2 /8
- GPU model and memory: GeForce GTX 1060 6GB 

**Describe the current behavior**

I'm trying to use the Tensorflow C++ profiling lib to extract the memory footprint of one network on my GPU.
Here is the code I use to trace my inference and extract the memory profile:
```
auto tracer = CreateGpuTracer();
tracer->Start(); 

// Inference
tensorflow::RunOptions run_options;
run_options.set_trace_level(tensorflow::RunOptions::FULL_TRACE);
tensorflow::RunMetadata run_metadata;
run_status = session->Run(run_options, input_tf_tensors, output_names, {}, &tf_outputs, &run_metadata);
tracer->Stop();

XSpace xspace;
status = tracer->CollectData(&xspace);

std::vector<const XPlane*> device_planes_gpu = FindPlanesWithPrefix(xspace, ""/device:GPU:"");
const XPlane* plane_gpu = device_planes_gpu[0];
MemoryProfile memory_profile = ConvertXPlaneToMemoryProfile(*plane_gpu);

std::string json_output;
google::protobuf::util::MessageToJsonString(memory_profile, &json_output);
LOG(INFO) << json_output;
```

Here the TF logs that shows how the GPU is found, Cuda libs are found and that some events are collected:
```
    2021-03-26 15:28:19.841126: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
    2021-03-26 15:28:19.841822: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.2
    2021-03-26 15:28:20.823429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
    2021-03-26 15:24:34.402625: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed
    2021-03-26 15:24:34.402648: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 562 callback api events and 562 activity events. 
```
 
Unfortunately no event either HostEventType::kMemoryDeallocation or HostEventType::kMemoryAllocation is collected in the XSpace and consequently the MemoryProfile is empty. The resulting memory summary is:
```
    {""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

**Describe the expected behavior**

I expect that the MemoryProfile contains at least one memoryProfilePerAllocator containing one MemoryProfileSummary.

From this object I should be able to get the information to print the Memory profile summary shown in the profiler guide:
https://www.tensorflow.org/guide/profiler#memory_profile_tool

**Other info / logs** Include any logs or source code that would be helpful to

My inference works as expected.
"
48145,Missing Add* function for Builtin operator FILL,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS
- TensorFlow installed from (source or binary): Source built locally
- Tensorflow version (commit SHA if source): 771c870a81c1025c4886a4fb60ca33971e98c577
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): nRF52840 DK

**Describe the problem**
I successfully converted my model to TF Lite for Microcontrollers and I am now trying to consume it from an nRF52840 DK target, but I get a failure when allocating the tensors:
```
[ERR] ./model/debug_log.cc:12: Didn't find op for builtin opcode 'FILL' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?
[ERR] ./model/debug_log.cc:12: 
[ERR] ./model/debug_log.cc:12: Failed to get registration from op code FILL
[ERR] ./model/debug_log.cc:12: 
[ERR] ./model/debug_log.cc:12: Failed starting model allocation.
[ERR] ./model/debug_log.cc:12: 
[ERR] ./model/debug_log.cc:12: AllocateTensors() failed
```

Being proprietary software, unfortunately, I cannot share more details about the model used.

I already have a proposed resolution of the issue and I'm opening this Issue ticket in order to open a PR.

**Please provide the exact sequence of commands/steps when you ran into the problem**
The first step is building TFLM locally:
```
cd tensorflow
make -f tensorflow/lite/micro/tools/make/Makefile \
    TARGET=cortex_m_generic \
    TARGET_ARCH=cortex-m4+fp \
    TARGET_TOOLCHAIN_ROOT=/opt/gcc-arm-none-eabi-9-2020-q2-update/bin/ \
    OPTIMIZED_KERNEL_DIR=cmsis_nn microlite
```
Once `libtensorflow-microlite.a` is built I can include it as a library in the Makefile of my project.

The second step is to consume my  TFLM model from th target nRF52840. The code fails within this function call:
```
void model_init(){
	// Set up logging
	static tflite::MicroErrorReporter micro_error_reporter;
	error_reporter = &micro_error_reporter;

	// Map the model into a usable data structure
	model = ::tflite::GetModel(g_model);
	if (model->version() != TFLITE_SCHEMA_VERSION) {
		TF_LITE_REPORT_ERROR(error_reporter,
				""Model provided is schema version %d not equal ""
				""to supported version %d.\n"",
				model->version(), TFLITE_SCHEMA_VERSION);
	}

	// This pulls in all the operation implementations we need
	static tflite::AllOpsResolver resolver;
	resolver.AddExpandDims();

	// Build an interpreter to run the model with.
	static tflite::MicroInterpreter static_interpreter(
			model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
	interpreter = &static_interpreter;

	// Allocate memory from the tensor_arena for the model's tensors.
	TfLiteStatus allocate_status = interpreter->AllocateTensors();
	if (allocate_status != kTfLiteOk){
		TF_LITE_REPORT_ERROR(error_reporter, ""AllocateTensors() failed"");
		return;
	}

	// Obtain pointers to the model's input and output tensors.
	input = interpreter->input(0);
	output = interpreter->output(0);
}
```"
48142,tensforflow stucks at the installation,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Family
- TensorFlow version:  2.4.1
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): No 
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: CUDA 11.2
- GPU model and memory: GTX 1660 Ti



I wanted to install tensorflow but the installation stucks.

I did a pip install. tensorflow had started downloading. And then this installation got stuck...
"
48140,Global barrier in TensorFlow,"
In paper https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf, there mentioned a concept **global barrier** in Tensorflow between successive iterations.
 _– the
global barrier waits for all communication operations to
finish before moving on to the next iteration_

But not found any discuss on this, want to make sure that is there really a **global barrier** in Tensorflow?
"
48138,"Error: Report to Tensorflow team, ""Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 60)""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 5.11.10-arch1-1
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.8
- CUDA/cuDNN version: 11.2
- GPU model and memory: 3060 Ti FE

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6218233/tf_env.txt)

**Describe the current behavior**
While using tensorflow and keras to create a LSTM model I get the error attached.

**Describe the expected behavior**
I expect to get no error and to be able to continue finalizing the model.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[Colab](https://colab.research.google.com/drive/1UKkZilpQj34lv6GvWOMC-g-pLarcb1dY?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[output.log](https://github.com/tensorflow/tensorflow/files/6218216/output.log)"
48137,Output size same as input size only if strides = 1,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D

## Description of issue (what needs changing):
In the description of the input argument ""padding"", it says:

_one of ""valid"" or ""same"" (case-insensitive). ""valid"" means no padding. ""same"" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input._

However, ""same"" would make the output have the same dimensions as the input only if strides is set to be 1. I was initially very confused by this typo.

"
48136,equations are terribly formatted,"hello, 
if I change the language (e.g. German) in the documentation, the equations are terribly formatted. This problem appears e.g. [here](https://www.tensorflow.org/agents/tutorials/intro_bandit). In English language, every looks fine. Nevertheless, as soon as I change the language, there seems to be a formatting issue.
Best Regards
"
48135,Removing dependency on full TF,"### 1. System information

- OS Platform and Distribution: aarch64 /Ubuntu 18.04
- TensorFlow installation:  Package
- TensorFlow library: 2.4.1

### 2. Code

I have taken first step to using TF Lite by using interpreter from full Tensorflow.   This code does the inferencing and there is separate code to do the modelling:

[https://gitlab.com/john---/xmit_processor/-/blob/tflite/xmit_processor/xmit_processor.py](url)

This code take around 2% less RAM (out of 4GB) than the full Tensorflow version.   However, I think tot get the full benefit I need to use the Tensorflow Lite runtime.   However, for inference I am using many tf.* functions.   A couple of them:

tf.audio.decode_wav
tf.signal.stft

From what I can see in the documentation these are not available in TF Lite.   Let me know:

-  To get the minimized memory footprint of TF Lite do I need to use TF Lite runtime?
- Do I need to find TF Lite (tfl.\*) alternates for all the tf.\* functions?
- Any other paths I can take to decrease memory footprint?

Note: the training currently works with full Tensorflow but I can make changes to that part as needed to improve memory consumption on the inferencing side.
"
48130,error: aliases are not supported on darwin In MacOS Big Sur,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.2.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.4.1/master branch
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.29) (XCode 12+)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Hi, I got this error while building TensorFlow from source using master branch/v2.4.1 branch from latest version of MacOS Big Sur (11+):

```
bazelisk build --config=mkl -c opt --copt=-march=native //tensorflow/tools/lib_package:libtensorflow

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=178
INFO: Reading rc options for 'build' from /Users/mania25/Downloads/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/mania25/Downloads/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /Users/mania25/Downloads/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/intel/oneapi/intelpython/latest/bin/python3 --action_env PYTHON_LIB_PATH=/opt/intel/oneapi/intelpython/latest/lib/python3.7/site-packages --python_path=/opt/intel/oneapi/intelpython/latest/bin/python3
INFO: Found applicable config definition build:short_logs in file /Users/mania25/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/mania25/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file /Users/mania25/Downloads/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:macos in file /Users/mania25/Downloads/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /Users/mania25/Downloads/tensorflow/WORKSPACE:23:14: in <toplevel>
  /Users/mania25/Downloads/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace
  /private/var/tmp/_bazel_mania25/2fa4938439e248c63b3c0f49ddc6c271/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /private/var/tmp/_bazel_mania25/2fa4938439e248c63b3c0f49ddc6c271/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (227 packages loaded, 20983 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /private/var/tmp/_bazel_mania25/2fa4938439e248c63b3c0f49ddc6c271/sandbox
ERROR: /private/var/tmp/_bazel_mania25/2fa4938439e248c63b3c0f49ddc6c271/external/llvm_openmp/BUILD.bazel:192:10: C++ compilation of rule '@llvm_openmp//:libiomp5.dylib' failed (Exit 1): wrapped_clang failed: error executing command external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 42 argument(s) skipped)
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1397:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_SET_NUM_THREADS, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1398:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_NUM_THREADS, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1399:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_MAX_THREADS, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1400:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_THREAD_NUM, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1401:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_NUM_PROCS, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1402:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_IN_PARALLEL, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1403:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_SET_DYNAMIC, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1404:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_DYNAMIC, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1405:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_SET_NESTED, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1406:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_GET_NESTED, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1407:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_INIT_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1408:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_INIT_NEST_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1409:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_DESTROY_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1410:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_DESTROY_NEST_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1411:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_SET_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1412:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_SET_NEST_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1413:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_UNSET_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1414:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_UNSET_NEST_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
In file included from external/llvm_openmp/runtime/src/kmp_ftn_cdecl.cpp:31:
external/llvm_openmp/runtime/src/kmp_ftn_entry.h:1415:1: error: aliases are not supported on darwin
KMP_VERSION_SYMBOL(FTN_TEST_LOCK, 10, ""OMP_1.0"");
^
external/llvm_openmp/runtime/src/kmp_os.h:359:3: note: expanded from macro 'KMP_VERSION_SYMBOL'
  _KMP_VERSION_SYMBOL(api_name, ver_num, ver_str, ""VERSION"")
  ^
external/llvm_openmp/runtime/src/kmp_os.h:362:22: note: expanded from macro '_KMP_VERSION_SYMBOL'
      __attribute__((alias(KMP_STR(__kmp_api_##api_name))));                    \
                     ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Target //tensorflow/tools/lib_package:libtensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1726.530s, Critical Path: 245.91s
INFO: 4331 processes: 148 internal, 4183 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Intel® oneAPI Base Toolkit (https://software.intel.com/content/www/us/en/develop/tools/oneapi/base-toolkit/download.html?operatingsystem=mac&distributions=webdownload&options=offline).
2. source /opt/intel/oneapi/setvars.sh
3. pip install pip numpy wheel
4. pip install keras_preprocessing --no-deps
5. git clone https://github.com/tensorflow/tensorflow.git
6. ./configure
7. bazelisk build --config=mkl -c opt --copt=-march=native //tensorflow/tools/lib_package:libtensorflow

**Any other info / logs**
None
"
48129,SparseCategoricalCrossentropy and sparse_categorical_crossentropy are different due to precision loss,"The following code demonstrates the issue:
```
from tensorflow.keras.losses import *
import numpy as np
import tensorflow as tf

loss_fn = SparseCategoricalCrossentropy(reduction='none')
y_true = np.random.randint(10, size = (4, 8))
y_pred = np.random.uniform(low=-10, high=10, size=(4,8,10))

l1 = loss_fn(y_true, y_pred).numpy()
l2 = sparse_categorical_crossentropy(y_true, y_pred).numpy()
print(np.array_equal(l1, l2), l1[0][0], l2[0][0])
```
According to TensorFlow documentation and expectations (see f.e https://stackoverflow.com/questions/59360263/tensorflow-2-0-what-is-the-difference-between-sparse-categorical-crossentropy-a) the two methods of computing the loss should always yield the exact same result. But that is not the case because the `SparseCategoricalCrossentropy` code path calls `compute_weighted_loss` and in that function the loss is converted to float32, losing precision. A comment in the function reads: ""TODO(psv): Handle casting here in a better way, eg. if losses is float64 # we do not want to lose precision."""
48128,name parameter not working in tf operations (colab),"System information
-google colab, standard free account, didn't touch any config, no accelerator selected

I was using the functional API because I have to write a network with two outputs, and I noticed that the the name parameter doesn't work in most of the tf operations. In this simple code I wrote below **b** should be the equal to **a** with the name changed, but it doesn't work, it simply ignores the parameter.

import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)
a=tf.keras.Input((1,))
b=tf.identity(a, name=""my_name"")
print(b.name)

OUTPUT:
v2.4.1-0-g85c8b2a817f 2.4.1
tf.identity/Identity:0


I also tried different operations, in al of them the name= parameter was ignored
here is the link to my colab notebook
https://colab.research.google.com/drive/1C9TR3sTgydES39LV8oPf5gz1zW9NgvJT?usp=sharing

It wouldn't be such a big deal, but when writing a network with multiple outputs (and custom losses) the documentation is not that good and only explains how to do it calling the layers by name, so now I'm pretty much stuck, cause the auto-assigned name changes based on the exectution."
48127,Conv2D is 20x slower using mixed_float16 in training process,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`
- Python version: `3.7.6`
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: `CUDA Version: 11.2`
- GPU model and memory: `Quadro RTX 8000, compute capability 7.5` and also `GeForce RTX 2080 Ti, compute capability 7.5`

**Describe the current behavior**
- `Conv2D` in training process is `~20x` slower if set `mixed_precission` `mixed_float16`. In my test, this happens in some conditions. In a short result, it's:
  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |
  | ----------- | ---------- | -------- | --------------------- | ---------------------- |
  | 112         | 512        | False    | 83                    | 67                     |
  | 112         | 512        | True     | 1969                  | 1857                   |

  Seems like `mixed_float16` is `~20x` times slower than `float32`.

**Describe the expected behavior**
- Expect `mixed_float16` performs better than `float32`.

**Standalone code to reproduce the issue**
- I just used a minimal python script to test this scenario, which seems easier for me:
  ```py
  import tensorflow as tf
  from tensorflow import keras

  gpus = tf.config.experimental.list_physical_devices(""GPU"")
  for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)

  def test_resnet(input_shape=None, classes=1000):
      img_input = keras.layers.Input(shape=input_shape)
      nn = img_input

      # nn = keras.layers.ZeroPadding2D(padding=((2, 2), (2, 2)), name='conv1_pad')(nn) # NOT helping
      # nn = keras.layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(nn) # NOT helping
      nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding='VALID', name=""conv1_conv"")(nn)
      # nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding='VALID', name=""conv1_conv"", data_format=""channels_first"")(nn)

      nn = keras.layers.Flatten()(nn)
      nn = keras.layers.Dense(classes, name=""predictions"")(nn)
      nn = keras.layers.Activation('softmax', dtype='float32')(nn)
      return keras.models.Model(img_input, nn)

  def load_cifar10(batch_size=1024, image_shape=(32, 32)):
      import tensorflow_datasets as tfds
      AUTOTUNE = tf.data.experimental.AUTOTUNE

      if image_shape[:2] == (32, 32):
          preprocessing = lambda data: (tf.cast(data[""image""], tf.float32) / 255.0, data[""label""])
      else:
          preprocessing = lambda data: (tf.image.resize(data[""image""], image_shape[:2]) / 255.0, data[""label""])
          # preprocessing = lambda data: (tf.transpose(tf.image.resize(data[""image""], image_shape[1:]) / 255.0, [2, 0, 1]), data[""label""])
      dataset = tfds.load(""cifar10"", split=""train"").map(preprocessing, num_parallel_calls=AUTOTUNE)
      dataset = dataset.cache().batch(batch_size).prefetch(buffer_size=AUTOTUNE)
      return dataset

  def run_test(input_shape=(32, 32, 3), batch_size=512, use_fp16=False, epochs=2):
      if use_fp16:
          keras.mixed_precision.set_global_policy('mixed_float16')
          # tf.config.optimizer.set_experimental_options({""auto_mixed_precision"": True})

      dataset = load_cifar10(batch_size=batch_size, image_shape=input_shape)

      # model = keras.applications.ResNet50(input_shape=input_shape, classes=10, weights=None)
      # model = keras.models.Model(model.inputs[0], keras.layers.Activation(""linear"", dtype=""float32"")(model.outputs[0]))
      model = test_resnet(classes=10, input_shape=input_shape)

      # optimizer = keras.mixed_precision.LossScaleOptimizer(keras.optimizers.Adam())
      optimizer = keras.optimizers.Adam()
      model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)
      history = model.fit(dataset, epochs=epochs)

  if __name__ == ""__main__"":
      import sys
      import argparse

      parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
      parser.add_argument(""-b"", ""--batch_size"", type=int, default=512, help=""Batch size"")
      parser.add_argument(""-i"", ""--input_shape"", type=int, default=32, help=""Input shape"")
      parser.add_argument(""-f"", ""--use_fp16"", action=""store_true"", help=""Use fp16"")
      parser.add_argument(""-e"", ""--epochs"", type=int, default=2, help=""Epochs"")

      args = parser.parse_known_args(sys.argv[1:])[0]
      run_test((args.input_shape, args.input_shape, 3), args.batch_size, args.use_fp16, args.epochs)
      # keras.backend.set_image_data_format('channels_first') # NOT helping
      # run_test((3, args.input_shape, args.input_shape), args.batch_size, args.use_fp16, args.epochs)
  ```

**Other info / logs**
- That `Conv2D` layer in `test_resnet` is actually the head layer in our `keras.applications.ResNet50`, and I found it's this layer slowing down the model training speed in `mixed_float16`.
- By changing the `input_shape` and `batch_size`, it seems `mixed_float16` working good in `input_shape == 224, batch_size == 512` or `input_shape == 112, batch_size == 256`.
  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |
  | ----------- | ---------- | -------- | --------------------- | ---------------------- |
  | 112         | 512        | False    | 83                    | 67                     |
  | 112         | 512        | True     | 1969                  | 1857                   |
  | 128         | 512        | False    | 96                    | 85                     |
  | 128         | 512        | True     | 96                    | 97                     |
  | 112         | 256        | False    | 43                    | 38                     |
  | 112         | 256        | True     | 46                    | 41                     |
  | 224         | 512        | False    | 233                   | 212                    |
  | 224         | 512        | True     | 228                   | 210                    |
  | 32          | 512        | False    | 8                     | 5                      |
  | 32          | 512        | True     | 38                    | 37                     |

- In some further tests in my environment:
  - First set `input_shape == 112`, and use different `batch_size`, seems this slowing happens if `batch_size` **>** `384`.
  - Then set `batch_size == 512`, and vary `input_shape`, seems it's happening if `input_shape` **<** `121`.

  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |
  | ----------- | ---------- | -------- | --------------------- | ---------------------- |
  | 112         | 384        | True     | 63                    | 45                     |
  | 112         | 385        | True     | 1407                  | 1353                   |
  | 120         | 512        | True     | 2306                  | 2183                   |
  | 121         | 512        | True     | 90                    | 84                     |
  | 122         | 512        | True     | 88                    | 66                     |

  Also, if we add a `nn = keras.layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(nn)` layer before `Conv2D`, it will be `input_shape < 115 == 121 - 6`.
- Then I set `input_shape == 112, batch_size == 512`, and change parameters in `Conv2D`, seems `padding=""SAME""` working good in `mixed_float16`:

  | kernel_size | padding | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |
  | ----------- | ------- | -------- | --------------------- | ---------------------- |
  | 7           | SAME    | True     | 77                    | 60                     |
  | 7           | VALID   | True     | 1969                  | 1857                   |
  | 5           | SAME    | True     | 1132                  | 1061                   |
  | 5           | VALID   | True     | 953                   | 948                    |
  | 3           | SAME    | True     | 351                   | 342                    |
  | 3           | VALID   | True     | 339                   | 330                    |

- In `ResNet50` case, using `padding=""VALID""` is `~4times slower` in `mixed_float16` training, but if we set `padding=""SAME""` only in the first `Conv2D` layer, it's faster:

  | padding | float16 | first epoch (ms/step) | second epoch (ms/step) |
  | ------- | ------- | --------------------- | ---------------------- |
  | VALID   | False   | 608                   | 570                    |
  | VALID   | True    | 2622                  | 2397                   |
  | SAME    | False   | 691                   | 656                    |
  | SAME    | True    | 438                   | 396                    |

- I also tested the forward time only, but it's not a difference for me:
  ```py
  from tensorflow import keras

  def test_conv(use_fp16=True, padding=""VALID""):
      if use_fp16:
          dtype = keras.mixed_precision.Policy('mixed_float16')
      else:
          dtype = ""float32""
      inputs = tf.ones([512, 112, 112, 3])
      nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding=padding, name=""conv1_conv"", dtype=dtype)
      print(nn(inputs).dtype)

      %timeit nn(inputs)

  test_conv(use_fp16=True, padding=""VALID"")  # 5.62 ms ± 82.2 µs
  test_conv(use_fp16=True, padding=""SAME"")   # 6.61 ms ± 109 µs
  test_conv(use_fp16=False, padding=""VALID"") # 4.77 ms ± 36.6 µs
  test_conv(use_fp16=False, padding=""SAME"")  # 5.14 ms ± 53.9 µs
  ```
- So what did I do wrong here?

**Related issue**
- I think this may related with issue [#39556](https://github.com/tensorflow/tensorflow/issues/39556) and [#41715](https://github.com/tensorflow/tensorflow/issues/41715). Here is my test results of `float32` / `float16` with `NHWC` / `NCHW`:
  ```py
  time for float32 NHWC: 0.11824584007263184
  time for float32 NCHW: 0.06493830680847168
  time for float16 NHWC: 0.1193842887878418
  time for float16 NCHW: 0.09624576568603516
  ```"
48126,tflite model is not running on webcam feed,"**System information**
- OS Platform and Distribution (Linux)
- TensorFlow version (2.4.1):


I used TensorFlow pre-trained faster rcnn inception v2 coco model and used my own images and trained them on that model. I exported the frozen inference graph successfully and check it on my webcam , I was working fine and using the code below , I converted the frozen inference graph into tflite model....

import logging
logging.getLogger(""tensorflow"").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib

#conversion of faster_rcnn_inception_v2_coco_model frozen_inference_graph.pbin to tflite model

import tensorflow as tf
import os 

os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'



converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(""frozen_inference_graph.pb"", [""image_tensor""], [""detection_classes"", ""detection_scores"", ""detection_boxes"", ""num_detections""])
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
model = converter.convert()

tflite_models_dir = pathlib.Path(""model/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_file = tflite_models_dir/""mnist_model.tflite""
tflite_model_file.write_bytes(model)


#To Quantize the model on Export, Set the optimizations flag to optimize for size

converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/""mnist_model_quant.tflite""
tflite_model_quant_file.write_bytes(tflite_quant_model)


 But I am not able to load my .tflite model in python for checking if my tflite model is working and using my webcam as input to the model ...For loading the tflite model ,I am using the code below : 

######## Webcam Object Detection Using Tensorflow-trained Classifier #########
#
# Author: Evan Juras
# Date: 10/27/19
# Description: 
# This program uses a TensorFlow Lite model to perform object detection on a live webcam
# feed. It draws boxes and scores around the objects of interest in each frame from the
# webcam. To improve FPS, the webcam object runs in a separate thread from the main program.
# This script will work with either a Picamera or regular USB webcam.
#
# This code is based off the TensorFlow Lite image classification example at:
# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py
#
# I added my own method of drawing boxes and labels using OpenCV.

# Import packages
import os
import argparse
import cv2
import numpy as np
import sys
import time
from threading import Thread
import importlib.util

# Define VideoStream class to handle streaming of video from webcam in separate processing thread
# Source - Adrian Rosebrock, PyImageSearch: https://www.pyimagesearch.com/2015/12/28/increasing-raspberry-pi-fps-with-python-and-opencv/
class VideoStream:
    """"""Camera object that controls video streaming from the Picamera""""""
    def __init__(self,resolution=(640,480),framerate=30):
        # Initialize the PiCamera and the camera image stream
        self.stream = cv2.VideoCapture(0)
        ret = self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
        ret = self.stream.set(3,resolution[0])
        ret = self.stream.set(4,resolution[1])
            
        # Read first frame from the stream
        (self.grabbed, self.frame) = self.stream.read()

	# Variable to control when the camera is stopped
        self.stopped = False

    def start(self):
	# Start the thread that reads frames from the video stream
        Thread(target=self.update,args=()).start()
        return self

    def update(self):
        # Keep looping indefinitely until the thread is stopped
        while True:
            # If the camera is stopped, stop the thread
            if self.stopped:
                # Close camera resources
                self.stream.release()
                return

            # Otherwise, grab the next frame from the stream
            (self.grabbed, self.frame) = self.stream.read()

    def read(self):
	# Return the most recent frame
        return self.frame

    def stop(self):
	# Indicate that the camera and thread should be stopped
        self.stopped = True

# Define and parse input arguments
parser = argparse.ArgumentParser()
parser.add_argument('--modeldir', help='Folder the .tflite file is located in',
                    required=True)
parser.add_argument('--graph', help='Name of the .tflite file, if different than detect.tflite',
                    default='mnist_model_quant.tflite')
parser.add_argument('--labels', help='Name of the labelmap file, if different than labelmap.txt',
                    default='labelmap.txt')
parser.add_argument('--threshold', help='Minimum confidence threshold for displaying detected objects',
                    default=0.5)
parser.add_argument('--resolution', help='Desired webcam resolution in WxH. If the webcam does not support the resolution entered, errors may occur.',
                    default='1280x720')
parser.add_argument('--edgetpu', help='Use Coral Edge TPU Accelerator to speed up detection',
                    action='store_true')

args = parser.parse_args()

MODEL_NAME = args.modeldir
GRAPH_NAME = args.graph
LABELMAP_NAME = args.labels
min_conf_threshold = float(args.threshold)
resW, resH = args.resolution.split('x')
imW, imH = int(resW), int(resH)
use_TPU = args.edgetpu

# Import TensorFlow libraries
# If tensorflow is not installed, import interpreter from tflite_runtime, else import from regular tensorflow
# If using Coral Edge TPU, import the load_delegate library
pkg = importlib.util.find_spec('tensorflow')
if pkg is None:
    from tflite_runtime.interpreter import Interpreter
    if use_TPU:
        from tflite_runtime.interpreter import load_delegate
else:
    from tensorflow.lite.python.interpreter import Interpreter
    if use_TPU:
        from tensorflow.lite.python.interpreter import load_delegate

# If using Edge TPU, assign filename for Edge TPU model
if use_TPU:
    # If user has specified the name of the .tflite file, use that name, otherwise use default 'edgetpu.tflite'
    if (GRAPH_NAME == 'mnist_model_quant.tflite'):
        GRAPH_NAME = 'mnist_model_quant.tflite'       

# Get path to current working directory
CWD_PATH = os.getcwd()

# Path to .tflite file, which contains the model that is used for object detection
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)

# Load the label map
with open(PATH_TO_LABELS, 'r') as f:
    labels = [line.strip() for line in f.readlines()]

# Have to do a weird fix for label map if using the COCO ""starter model"" from
# https://www.tensorflow.org/lite/models/object_detection/overview
# First label is '???', which has to be removed.
if labels[0] == '???':
    del(labels[0])

# Load the Tensorflow Lite model.
# If using Edge TPU, use special load_delegate argument
if use_TPU:
    interpreter = Interpreter(model_path=PATH_TO_CKPT,
                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
    print(PATH_TO_CKPT)
else:
    interpreter = Interpreter(model_path=PATH_TO_CKPT)

interpreter.allocate_tensors()

# Get model details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
height = input_details[0]['shape'][1]
width = input_details[0]['shape'][2]

floating_model = (input_details[0]['dtype'] == np.float32)

input_mean = 127.5
input_std = 127.5

# Initialize frame rate calculation
frame_rate_calc = 1
freq = cv2.getTickFrequency()

# Initialize video stream
videostream = VideoStream(resolution=(imW,imH),framerate=30).start()
time.sleep(1)

#for frame1 in camera.capture_continuous(rawCapture, format=""bgr"",use_video_port=True):
while True:

    # Start timer (for calculating frame rate)
    t1 = cv2.getTickCount()

    # Grab frame from video stream
    frame1 = videostream.read()

    # Acquire frame and resize to expected shape [1xHxWx3]
    frame = frame1.copy()
    print(frame.shape)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    print(frame_rgb.shape)
    frame_resized = cv2.resize(frame_rgb, (width, height))
    print(frame_resized.shape)
    input_data = np.expand_dims(frame_resized, axis=0)
    print(input_data.dtype)

    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)
    if floating_model:
        input_data = (np.float32(input_data) - input_mean) / input_std

    # Perform the actual detection by running the model with the image as input
    interpreter.set_tensor(input_details[0]['index'],input_data)
    interpreter.invoke()

    # Retrieve detection results
    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects
    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects
    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects
    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)

    # Loop over all detections and draw detection box if confidence is above minimum threshold
    for i in range(len(scores)):
        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):

            # Get bounding box coordinates and draw box
            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()
            ymin = int(max(1,(boxes[i][0] * imH)))
            xmin = int(max(1,(boxes[i][1] * imW)))
            ymax = int(min(imH,(boxes[i][2] * imH)))
            xmax = int(min(imW,(boxes[i][3] * imW)))
            
            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)

            # Draw label
            object_name = labels[int(classes[i])] # Look up object name from ""labels"" array using class index
            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'
            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size
            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window
            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in
            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text

    # Draw framerate in corner of frame
    cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)

    # All the results have been drawn on the frame, so it's time to display it.
    cv2.imshow('Object detector', frame)

    # Calculate framerate
    t2 = cv2.getTickCount()
    time1 = (t2-t1)/freq
    frame_rate_calc= 1/time1

    # Press 'q' to quit
    if cv2.waitKey(1) == ord('q'):
        break

# Clean up
cv2.destroyAllWindows()
videostream.stop()


the Error I am getting After running the code : 

Traceback (most recent call last):
  File ""TFLite_detection_webcam.py"", line 186, in <module>
    interpreter.invoke()
  File ""/root/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 540, in invoke
    self._interpreter.Invoke()
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)
	 (while executing 'TensorArrayScatterV3' via Eager)Node number 416 (TfLiteFlexDelegate) failed to invoke.

^CException ignored in: <module 'threading' from '/root/anaconda3/envs/tf_env/lib/python3.7/threading.py'>
Traceback (most recent call last):
  File ""/root/anaconda3/envs/tf_env/lib/python3.7/threading.py"", line 1307, in _shutdown
    lock.acquire()
KeyboardInterrupt
FATAL: exception not rethrown
Aborted

the error is in the screenshot attached 

![Screenshot at 2021-03-27 18-47-57](https://user-images.githubusercontent.com/65131911/112736937-8403e180-8f4e-11eb-9d6a-86f55af0de47.png)


"
48123,Tutorial Redirect for Time Series Forecasting gives 404. ,"
### Describe the problem
In the [Machine Learning Tutorial ](https://www.tensorflow.org/guide/data) the Time-series example hyperlink redirects to a [404 page](https://www.tensorflow.org/tutorials/text/time_series) instead of the proper tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series This needs to be updated, but I'm uncomfortable making the changes and then creating a pull request. 

Please nuke this comment when you can. I dont know another way to point this issue out, so I did this.
"
48120,tf.estimator.BoostedTreesEstimator center_bias=True breaks when label dimension > 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macos Big Sur && Google Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **na**
- TensorFlow installed from (source or binary): **pip (python 3.8)**
- TensorFlow version (use command below): **v2.4.0-49-g85c8b2a817f 2.4.1**
- Python version: **3.8**
- Bazel version (if compiling from source): **na**
- GCC/Compiler version (if compiling from source): **na**
- CUDA/cuDNN version: **na**
- GPU model and memory: **na**

**Describe the current behavior**
A BoostedTrees**Regressor** with center_bias=True  and label_dimension > 1, correctly raises `ValueError: center_bias not yet support with label_dimension > 1.`
When creating a BoostedTrees**Estimator** with center_bias=True and a custom head with label dimension > 1, this issue is not caught in time and only leads to a later more cryptic error.


**Describe the expected behavior**
Expected to raise `ValueError: center_bias not yet support with label_dimension > 1.`

**Standalone code to reproduce the issue**
[Colab gist](https://colab.research.google.com/gist/valkenburg/5404d404aca36d836659d292d2d811d2/boostedbiaslabdims.ipynb)
```python
import tensorflow as tf
import numpy as np

LABEL_DIM = 2
INPUT_DIMS = [1]

features = [
    tf.feature_column.numeric_column(f""random_input_{i}"",
                                     shape = (sh,),
                                     dtype=tf.float32
                                    ) 
    for i, sh in enumerate(INPUT_DIMS)
]

def input_fn(num_samples = 1000):
    inputs = {f.key : np.random.rand(num_samples, s) for f, s in zip(features, INPUT_DIMS)}
    targets = np.random.rand(num_samples, LABEL_DIM)
    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))
    ds = ds.shuffle(num_samples).repeat(10).batch(10) 
    return ds

def less_input_fn():
    return input_fn(10)


est = tf.estimator.BoostedTreesEstimator(
    feature_columns = features,
    head = tf.estimator.RegressionHead(
        label_dimension = LABEL_DIM, 
    ),
    n_batches_per_layer = 1,
    center_bias = True,
)

est.train(input_fn)
```
"
48119,tf.estimator.BoostedTreesEstimator returns all zero predictions when using custom loss_fn involving linear loss (zero second derivative),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macos Big Sur && Google Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **na**
- TensorFlow installed from (source or binary): **pip (python 3.8)**
- TensorFlow version (use command below): **v2.4.0-49-g85c8b2a817f 2.4.1**
- Python version: **3.8**
- Bazel version (if compiling from source): **na**
- GCC/Compiler version (if compiling from source): **na**
- CUDA/cuDNN version: **na**
- GPU model and memory: **na**

**Describe the current behavior**
When creating a BoostedTreesEstimator with a custom head in order to use various loss functions, the predictions are always zero if the loss function involves tf.math.abs (I have only tried tf.math.abs and tf.math.pow, tf.math.pow works as expected). 

**Describe the expected behavior**
non-zero predictions. See the example and use custom_loss_mse instead of custom_loss_mae. In this example of uniform data in [0,1], both mse and mae should return predictions close to 0.5. 

**Standalone code to reproduce the issue**
[Colab gist](https://colab.research.google.com/gist/valkenburg/e35ce97d89523413a3db378174db9a6b/bootsedzeros.ipynb)
```python
import tensorflow as tf
import numpy as np

LABEL_DIM = 1
INPUT_DIMS = [1]

features = [
    tf.feature_column.numeric_column(f""random_input_{i}"",
                                     shape = (sh,),
                                     dtype=tf.float32
                                    ) 
    for i, sh in enumerate(INPUT_DIMS)
]

def input_fn(num_samples = 1000):
    inputs = {f.key : np.random.rand(num_samples, s) for f, s in zip(features, INPUT_DIMS)}
    targets = np.random.rand(num_samples, LABEL_DIM)
    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))
    ds = ds.shuffle(num_samples).repeat(10).batch(10) 
    return ds

def less_input_fn():
    return input_fn(10)

def custom_loss_mse(labels, logits):
    return tf.math.pow(labels - logits, 2)

def custom_loss_mae(labels, logits):
    return tf.math.abs(labels - logits)

est = tf.estimator.BoostedTreesEstimator(
    feature_columns = features,
    head = tf.estimator.RegressionHead(
        label_dimension = LABEL_DIM, 
#         loss_fn = custom_loss_mse,
        loss_fn = custom_loss_mae,
    ),
    n_batches_per_layer = 1,
    center_bias = True,
)

est.train(input_fn)

list(est.predict(less_input_fn))
```

*I posted [this question](https://stackoverflow.com/questions/66831261/tf-estimator-boostedtreesestimator-returns-all-zero-predictions-when-using-custo) in stackoverflow, just in case the behaviour is expected and I'm misunderstanding things.*
"
48118,tensorflowlite.dll is huge (compared to other platforms),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 5d6cc7bf97a226c1e6a73ad4fc391c154dd622ac
- Python version: n/a
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): MSVC 2019 (cl: 19.28.29334)
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build -c opt //tensorflow/lite/tensorflowlite.dll
```

This yields a 15MB statically linked binary when compiled for 32-bit windows (i.e. the 64-bit version will be even larger). Compared to other platforms, this is a 5x increase that is difficult to explain, especially in lieu of tools like bloaty on Windows. Unfortunately I don't know where to start poking this at all - but I do believe that even accounting for platform differences, a 5x size difference is unexpected. The same issue was noted here: https://github.com/tensorflow/tensorflow/issues/33634#issuecomment-620645664 (with a suggestion to use the C API *instead*, but the C API is a binding that cannot be used independent from the C++ binary).

**Any other info / logs**
Let me know if I can include any more info.
"
48117,OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  Linux Ubuntu 18.04.2
- TensorFlow installed from: binary
- TensorFlow version: 2.4.1
- Keras version: 2.4.3
- Python version: 3.8.8
- Installed using: conda
- CUDA version: 11.1
- cuDNN version: 8.1.0
- GPU model and memory: two GeForce RTX 3080 10018MiB



**Describe the problem**
When i train the model from here [https://github.com/keras-team/keras-docs-zh/blob/master/sources/examples/mnist_cnn.md](https://github.com/keras-team/keras-docs-zh/blob/master/sources/examples/mnist_cnn.md) with GPU,  I get the following build error:
```text
2021-03-27 15:20:38.027337: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""mnist.py"", line 61, in <module>
    model.fit(x_train, y_train,
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 555, in call
    outputs = execute.execute(
  File ""/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!
         [[node sequential/conv2d/Conv2D (defined at mnist.py:61) ]] [Op:__inference_train_function_790]

Function call stack:
train_function
```
I don't think the code has any issues. It works fine when training with CPU.

**Any other info / logs**
I see #45044 but i still can't resolve this problem

I also tried to run it in `conda tensorflow`, But my code stopped at the following message.
```
2021-03-27 15:36:42.238717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2021-03-27 15:36:42.238747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1

**stop about 10 minutes**
```
here is the full log
```$ python mnist.py 
2021-03-27 15:36:40.615111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
2021-03-27 15:36:41.948740: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-27 15:36:41.949591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-27 15:36:42.018520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-03-27 15:36:42.019197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:3d:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-03-27 15:36:42.019216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-27 15:36:42.020691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-27 15:36:42.020739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-27 15:36:42.022260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-27 15:36:42.022487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-27 15:36:42.023973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-27 15:36:42.024853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-27 15:36:42.028187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-27 15:36:42.030707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2021-03-27 15:36:42.031200: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-27 15:36:42.235638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:1a:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-03-27 15:36:42.236273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:3d:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s
2021-03-27 15:36:42.236294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-27 15:36:42.236318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-27 15:36:42.236328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-27 15:36:42.236337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-27 15:36:42.236347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-27 15:36:42.236356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-27 15:36:42.236365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-27 15:36:42.236385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-27 15:36:42.238717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2021-03-27 15:36:42.238747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1


**here stop about 10 minutes**


2021-03-27 15:42:38.462326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-27 15:42:38.462363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 
2021-03-27 15:42:38.462369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 
2021-03-27 15:42:38.462373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 
2021-03-27 15:42:38.465253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9071 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6)
2021-03-27 15:42:38.466920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9071 MB memory) -> physical GPU (device: 1, name: GeForce RTX 3080, pci bus id: 0000:3d:00.0, compute capability: 8.6)
2021-03-27 15:42:38.467230: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-27 15:42:38.888017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-27 15:42:38.909002: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz
Epoch 1/12
2021-03-27 15:42:39.232928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10

**here stop about 2 minutes**

2021-03-27 15:44:14.199317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7

469/469 [==============================] - 1054s 5ms/step - loss: 2.3028 - accuracy: 0.1104 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 2/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1120 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 3/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1110 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 4/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1108 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 5/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1115 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 6/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1125 - val_loss: 2.3026 - val_accuracy: 0.1135
Epoch 7/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1121 - val_loss: 2.3025 - val_accuracy: 0.1135
Epoch 8/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1123 - val_loss: 2.3025 - val_accuracy: 0.1135
Epoch 9/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1124 - val_loss: 2.3025 - val_accuracy: 0.1135
Epoch 10/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1117 - val_loss: 2.3025 - val_accuracy: 0.1135
Epoch 11/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1128 - val_loss: 2.3025 - val_accuracy: 0.1135
Epoch 12/12
469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1122 - val_loss: 2.3025 - val_accuracy: 0.1135
Test loss: 2.3025200366973877
Test accuracy: 0.11349999904632568
```
It started training after waiting about 10 minutes

I think the problem may lie in the version of cuDNN

**nvidia-smi**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3080    Off  | 00000000:1A:00.0 Off |                  N/A |
|  0%   35C    P8    19W / 320W |      0MiB / 10018MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 3080    Off  | 00000000:3D:00.0 Off |                  N/A |
|  0%   30C    P8    19W / 320W |      0MiB / 10018MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

**nvcc -V**
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Tue_Sep_15_19:10:02_PDT_2020
Cuda compilation tools, release 11.1, V11.1.74
Build cuda_11.1.TC455_06.29069683_0
```

**cuDnn test**
```$ ./mnistCUDNN 
Executing: mnistCUDNN
cudnnGetVersion() : 8100 , CUDNN_VERSION from cudnn.h : 8100 (8.1.0)
Host compiler version : GCC 7.5.0

There are 2 CUDA capable devices on your machine :
device 0 : sms 68  Capabilities 8.6, SmClock 1740.0 Mhz, MemSize (Mb) 10018, MemClock 9501.0 Mhz, Ecc=0, boardGroupID=0
device 1 : sms 68  Capabilities 8.6, SmClock 1740.0 Mhz, MemSize (Mb) 10018, MemClock 9501.0 Mhz, Ecc=0, boardGroupID=1
Using device 0

Testing single precision
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 66640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.015264 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.027648 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.057344 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.160736 time requiring 66640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.179200 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.307200 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 128000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.032768 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054272 time requiring 128000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.060416 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.074752 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.086016 time requiring 1433120 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 66640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.013312 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.020480 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.036864 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.037888 time requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.112640 time requiring 66640 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 128000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.032768 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.052224 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.052224 time requiring 1433120 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054272 time requiring 128000 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!

Testing half precision (math in single precision)
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 100 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.015360 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.027648 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 100 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.040960 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.048128 time requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.063392 time requiring 184784 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 2000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.056352 time requiring 1433120 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.058304 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.060416 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.064512 time requiring 2000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.075776 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.092064 time requiring 64000 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 100 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.014336 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.020480 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.023552 time requiring 100 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.035840 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.037888 time requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.040960 time requiring 184784 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnGetConvolutionForwardAlgorithm_v7 ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 2000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 2450080 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.055296 time requiring 1433120 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.056320 time requiring 4656640 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.061440 time requiring 2000 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.075776 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.091136 time requiring 64000 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!

```"
48116,TransformedDistribution Documentation is not readable,"### Describe the problem
The documentation of tfp.TransformedDistribution is not readable in between, seen the attachement

### Source code / logs
Please see attachment
![Capture](https://user-images.githubusercontent.com/19791881/112711538-30ae7680-8eef-11eb-86e5-008263c64d6b.PNG)

"
48113,Please help Conda package tensorflow for OSX,"(I get that this is a weird thing to put in an issue)

Right now it's hard for the Anaconda people to make conda packages of Tensorflow for OSX because of gcc-versus-clang issues: 
https://github.com/ContinuumIO/anaconda-issues/issues/11697

This is impacting downstream packages that want to use tensorflow, as we can't specify conda environments that come with tensorflow included; installing it with pip afterwards severely limits our ability to sanely package it.

If you can spare the time to work this out with them, a lot of us who maintain downstream packages would really appreciate it."
48111,"""Graph is finalized and cannot be modified"" when loading saved Estimator","**System information**
- OS Platform and Distribution: MacOS 11.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.10
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): clang

**Describe the current behavior**

I have a DNNClassifier that I have created as follows:

```python

    estimator = tf.estimator.DNNClassifier(
        feature_columns=get_feature_columns(),
        hidden_units=[1024, 1024],
        model_dir=""./logs"",
        n_classes=n_classes,
        activation_fn=tf.nn.leaky_relu,
        dropout=0.5,
        optimizer=tf.compat.v1.train.RMSPropOptimizer(learning_rate=0.0001, momentum=0.9),
    )
```

where the `get_feature_columns()` function just creates a bunch of `feature_column`s and returns them as a set:

```python
def get_feature_columns():

    feature_columns = [tf.feature_column.numeric_column(col_name) for col_name in numeric_cols]
    ...
    return set(feature_columns)
```

The model fits and predicts just fine, and I am able to serialize it with:

```python
serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
    tf.feature_column.make_parse_example_spec(get_feature_columns()))

estimator_path = estimator.export_saved_model('from_estimator', serving_input_fn)
```

However, when I try to load the resulting saved model, it fails with a RuntimeError:

```python
imported = tf.saved_model.load_v2(estimator_path)
```

```

imported = tf.saved_model.load_v2(estimator_path)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
~/Yankees/nn_matchup/models/matchup_model_estimator.py in 
----> 554 imported = tf.saved_model.load_v2(estimator_path)

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)
    861     ValueError: If `tags` don't match a MetaGraph in the SavedModel.
    862   """"""
--> 863   return load_internal(export_dir, tags, options)[""root""]
    864 
    865 

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)
    911                        ""version) cannot be loaded with node filters."")
    912     with ops.init_scope():
--> 913       root = load_v1_in_v2.load(export_dir, tags)
    914       root.graph_debug_info = debug_info
    915 

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(export_dir, tags)
    276   """"""Load a v1-style SavedModel as an object.""""""
    277   loader = _EagerSavedModelLoader(export_dir)
--> 278   return loader.load(tags=tags)

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(self, tags)
    222     wrapped = wrap_function.wrap_function(
    223         functools.partial(self.load_graph, load_graph_returns, meta_graph_def),
--> 224         signature=[])
    225     saver, = load_graph_returns
    226     restore_from_saver = self._extract_saver_restore(wrapped, saver)

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)
    628           collections={}),
    629       variable_holder=holder,
--> 630       signature=signature)
    631 
    632 

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __init__(self, fn_graph, variable_holder, attrs, signature)
    223   def __init__(self, fn_graph, variable_holder, attrs=None, signature=None):
    224     self._variable_holder = variable_holder
--> 225     _lift_unlifted_variables(fn_graph, variable_holder)
    226     # We call __init__ after lifting variables so that the function's signature
    227     # properly reflects the new captured inputs.

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _lift_unlifted_variables(graph, variable_holder)
    176       if _should_lift_variable(old_variable):
    177         new_variable = _lift_single_variable(
--> 178             old_variable, graph, variable_holder)
    179         lifted_variables[id(old_variable)] = new_variable
    180         existing_captures.add(id(old_variable.handle))

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _lift_single_variable(old_variable, graph, variable_holder)
    128       name=old_variable.op.name,
    129       trainable=old_variable.trainable,
--> 130       extra_handle_data=old_variable.handle)
    131   new_variable._initializer_op = old_variable._initializer_op  # pylint: disable=protected-access
    132   graph.add_capture(new_variable.handle, old_variable.handle)

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    265 
    266 

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, trainable, caching_device, name, shape, dtype, constraint, synchronization, aggregation, extra_handle_data, distribute_strategy, **unused_kwargs)
   1956             name=name,
   1957             graph_mode=self._in_graph_mode,
-> 1958             initial_value=extra_handle_data)
   1959         if not context.executing_eagerly():
   1960           with ops.name_scope(""Read""):

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)
    163       shared_name=shared_name,
    164       name=name,
--> 165       container=container)
    166   if initial_value is None:
    167     initial_value = handle

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py in var_handle_op(dtype, shape, container, shared_name, allowed_devices, name)
   1204         ""VarHandleOp"", dtype=dtype, shape=shape, container=container,
   1205                        shared_name=shared_name,
-> 1206                        allowed_devices=allowed_devices, name=name)
   1207   _result = _outputs[:]
   1208   if _execute.must_record_gradient():

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    748       op = g._create_op_internal(op_type_name, inputs, dtypes=None,
    749                                  name=scope, input_types=input_types,
--> 750                                  attrs=attr_protos, op_def=op_def)
    751 
    752     # `outputs` is returned as a separate return value so that the output

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3516       An `Operation` object.
   3517     """"""
-> 3518     self._check_not_finalized()
   3519     if name is None:
   3520       name = op_type

~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _check_not_finalized(self)
   3106     """"""
   3107     if self._finalized:
-> 3108       raise RuntimeError(""Graph is finalized and cannot be modified."")
   3109 
   3110   def _add_op(self, op, op_name):

RuntimeError: Graph is finalized and cannot be modified.
```
"
48109,TF 2.4 still depends on GRPCIO v1.32.0,"GRPCIO was meant to be updated in TF 2.4 but TF 2.4.0 and 2.4.1 still depend on it: 
- https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/tools/pip_package/setup.py#L121
- https://github.com/tensorflow/tensorflow/issues/45785#issuecomment-806233485
"
48108,Can't load TFLite model on Android/iOS - NODE PAD failed to prepare ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): TF built on CentOS / current Docker version for Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung J2 Core 
- TensorFlow installed from (source or binary): Source (device)/Binary (conversion)
- TensorFlow version (use command below): 2.4.1/Nightly
- Python version: 3.8.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

[This model](https://github.com/tensorflow/tensorflow/files/6212646/vad.zip)  has been converted from Pytorch->ONNX->TFLite.

Loading the ONNX model, converting to a saved model, converting to TFLite and loading in the TFLite interpreter works fine in a notebook on **nightly** (da68297):
```
from onnx_tf.backend import prepare
import onnx
import tensorflow as tf

model_onnx = onnx.load('vad.onnx')
tf_rep = prepare(model_onnx)
tf_rep.export_graph('./tf_model')

converter = tf.lite.TFLiteConverter.from_saved_model(""./tf_model"")
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.allow_custom_ops=False
converter.experimental_new_converter =True

tflite_model = converter.convert()

# Save the model
with open(""vad.tflite"", 'wb') as f:
    f.write(tflite_model)
    
interpreter = tf.lite.Interpreter(model_path=""vad.tflite"")
interpreter.allocate_tensors()
    
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']

input_buf = np.ones((1, 64, 1),dtype=np.float32)

input_buf=np.array(input_buf,dtype=np.float32)

interpreter.set_tensor(input_details[0]['index'], input_buf)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data.shape)
```

However, take the same model and load on Android (with TFLite C++)

```
std::unique_ptr<tflite::FlatBufferModel> model;
tflite::ops::builtin::BuiltinOpResolver resolver;

model = tflite::FlatBufferModel::BuildFromFile(filepath_c);

auto builder = std::unique_ptr<tflite::InterpreterBuilder>(
new tflite::InterpreterBuilder(*model, resolver));

(*builder)(&interpreter);

const std::vector<int>& inputs = interpreter->inputs();
interpreter->AllocateTensors();
```

and this will either fail with ""NODE PAD failed to prepare"" or crash with:

```
F/libc    (31008): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x7e110870 in tid 31028 (1.ui), pid 31008 (example.example)
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'samsung/j2corey20ltecis/j2corey20lte:8.1.0/M1AJB/J260FUXXS1AUA1:user/release-keys'
Revision: '2'
ABI: 'arm'
pid: 31008, tid: 31028, name: 1.ui  >>> com.example.example <<<
signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x7e110870
    r0 00000003  r1 7e110870  r2 00000000  r3 00000002
    r8 a4edf754  r9 8eb6687c  sl a4edf740  fp 00000bc2
    ip 7db32ea8  sp 8eb66838  lr 7a3b538d  pc 7a3b5178  cpsr 60070030
backtrace:
    #00 pc 0051e178  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::ops::builtin::pad::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::pad::PadContext*)+51)
    #01 pc 0051e389  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::ops::builtin::pad::Prepare(TfLiteContext*, TfLiteNode*)+264)        
    #02 pc 005c366f  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::PrepareOpsStartingAt(int, std::__ndk1::vector<int, std::__ndk1::allocator<int>> const&, int*)+262)
    #03 pc 005c2df1  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::PrepareOpsAndTensors()+164)
    #04 pc 005c2c37  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::AllocateTensors()+202)
    #05 pc 005c694b  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Interpreter::AllocateTensors()+242)
    #06 pc 000391bf  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libvad.so (mfcc+294)
    #07 pc 000046a0  <anonymous:88080000>
```
Occasionally it seems to successfully move past this Pad operation, and will then fail with Node number 2 (SPLIT) failed to prepare.

This happens no matter whether TFLite is built via the official Docker release (current nightly), with select ops, or from source nightly or source/2.4.1.

Also, the ONNX model cannot be *converted* to TFLite with 2.4.1, giving the following error:

```<unknown>:0: error: loc(callsite(callsite(""Pad_1@__inference___call___8660"" at ""PartitionedCall@__inference_signature_wrapper_8735"") at ""PartitionedCall"")): operand #0 does not dominate this use
<unknown>:0: note: loc(""PartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""Pad_1@__inference___call___8660"" at ""PartitionedCall@__inference_signature_wrapper_8735"") at ""PartitionedCall"")): operand defined here
```

If I set `converter.experimental_new_converter =False`, then I get the following error during conversion:

```
ValueError: None is only supported in the 1st dimension. Tensor 'serving_default_audio_signal' has invalid shape '[None, 64, None]'.
```

I've tried manually setting the input shapes, and this then fails with other errors

Inspecting the original ONNX model via netron.app doesn't show anything unusual:

![vad onnx](https://user-images.githubusercontent.com/7238578/112647652-db497980-8e9c-11eb-9a0e-f1d39d26bfcc.png)

I think I did manage to successfully convert the model once (possibly with 2.3.1), but then experienced a similar ""NODE xx failed to prepare"" when running on Android.

The original model was from https://github.com/NVIDIA/NeMo/blob/ddd7e13cc0b81a377a55279eec7fe4ce0752f05e/tutorials/asr/07_Online_Offline_Microphone_VAD_Demo.ipynb, if that helps.

EDIT: on iOS with TFlite v2.4.1 built from source, the model converted with nightly errors with ""Node number 2 (SPLIT) failed to prepare"" and built with older version (2.3.1? not sure) errors with 
```
Pad value has to be greater than equal to 0. 
Node number 0 (PAD) failed to prepare
```

**Describe the expected behavior**

The model should load properly on TFLite Android/iOS.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48106,Call tf.lite.Interpreter.interpreter.invoke() makes program crash,"### 1. System information

- OS Platform and Distribution (Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip install 
- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow==2.3.2

### 2. Code
```
import numpy as np
import tensorflow as tf


class Tester(tf.Module):
    def __init__(self):
        super(Tester, self).__init__()

    @tf.function(input_signature=[tf.TensorSpec(shape=[100], dtype=tf.float32)])
    def test(self, x):
        # return tf.reshape(x, [10, -1])
        return tf.signal.rfft(x, [512])


model = Tester()
concrete_func = model.test.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='saved_models/pb/model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite = converter.convert()

TFLITE_FILE_PATH = 'content/tester.tflite'
with open(TFLITE_FILE_PATH, 'wb') as f:
    f.write(tflite)

# Load the TFLite model and allocate tensors.
# interpreter = tf.lite.Interpreter(model_path=""content/test_variable.tflite"")
interpreter = tf.lite.Interpreter(model_content=tflite)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

### 3. Failure after conversion
Model conversion successful, but invoke() makes program crash with error msg:
```
Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```
**In the above simple program, if test function return tf.signal.rfft(x, [512]), program crash, if test function return tf.reshape(x, [10, -1]), it works fine.**
The saved tflite model which return tf.signal.rfft(x, [512]) is sharing with url:
https://drive.google.com/file/d/1LY3h4MHIwGGi63TyWsNta6vNKluIdi38/view?usp=sharing

Can anyone help me?
"
48105,Incorrect ZeroPadding before MaxPool2D in keras' resnet,"Hi,

I've noticed that the implementation of resnet networks in keras introduces a ZeroPadding layer before the initial MaxPool2D 3x3 stride 2:

https://github.com/tensorflow/tensorflow/blob/204082475214b3f08d1301998780592b53076951/tensorflow/python/keras/applications/resnet.py#L161

I don't think this is correct. Zero is not a neutral element for a MaxPool2D operation. The input values at the edges could be negative.

I believe the intention of that zero padding layer is what would be correctly represented as SAME padding for the MaxPool2D directly. Note that the padding layer is also adding padding elements to the right and bottom edges of the input that the 3x3 stride 2 MaxPool2D operation won't ever use if the input is even size as I believe it commonly is."
48104,Tensorflow_xla model inference crash on Jetson AGX xavier,"
I met tensorflow_xla crash issue for model inference on Nvidia Jetson AGX Xavier aarch64 system.

**System information**
- Have I written custom code: Yes, I have some CPU computing custom ops, they are in different places in the middle of the network
- OS Platform and Distribution):  Linux Ubuntu18.04
- device: Nvidia Jetson AGX Xavier
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): tensorflow-2.4.1
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: cuda10.2, cudnn8.0
- GPU model and memory:  GPU model with cpu custom ops, device memory 32GB (including cpu mem and gpu mem)

[crash.log](https://github.com/tensorflow/tensorflow/files/6211738/crash.log)
[gdb.log](https://github.com/tensorflow/tensorflow/files/6211740/gdb.log)

Issues：
1, When do model inference with xla enable, this crash can be reproduced almost every time  (xavier aarch64 system)
2, When I turn off some of custom ops (CPU compute op), crash can happen about 7 times after 10 runs  (xavier aarch64 system)
3, When I run same code, same tensorflow-2.4.1 version on V100 GPU and x86 system, it can run successful without crash  (x86 + v100 gpu system)"
48103,EfficientNet official VS non-official implementation + Reproducibility ,"I've encountered weird phenomena with the official implementation of the `EfficientNet` model. The **validation score** doesn't improve as expected and far less than a non-official one. 

# Reproduce Issue

```
# Install non-official efficient network 
!pip install -U git+https://github.com/qubvel/efficientnet
```

```
import os
import numpy as np
import tensorflow as tf
print(tf.__version__)
import efficientnet.keras as efn 

batch_size = 32 
num_classes = 10
epochs = 3
official_efficient_net = False
```

```
## Model 
input_shape = (32,32,3)
if official_efficient_net:
    base_model  = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False,
                                                                    weights=""imagenet"", 
                                                                    input_shape=input_shape)
else:
    base_model = efn.EfficientNetB0(include_top=False,
                                        weights=""imagenet"", 
                                        input_shape=input_shape)

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
dense_layer = tf.keras.layers.Dense(10, activation='softmax')
Model  = tf.keras.Sequential([base_model, global_average_layer, dense_layer])
```

```
## Training Data
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

## Training
Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
cnn = Model.fit(x_train, y_train, batch_size=batch_size, 
                epochs=epochs, validation_data=(x_test,y_test))
```

Tested in `TensorFlow 2.4.1`. 
Training log from **Official EfficientNet**
>
Epoch 1/3
1563/1563 [==============================] - 128s 77ms/step - loss: 1.6419 - accuracy: 0.4368 - val_loss: 2.4821 - val_accuracy: 0.1817
Epoch 2/3
1563/1563 [==============================] - 119s 76ms/step - loss: 0.9287 - accuracy: 0.6800 - val_loss: 3.6128 - val_accuracy: 0.1184
Epoch 3/3
1563/1563 [==============================] - 119s 76ms/step - loss: 0.7524 - accuracy: 0.7478 - val_loss: 3.9361 - val_accuracy: 0.1000


Training log from **Non-official EfficientNet**
>
Epoch 1/3
1563/1563 [==============================] - 128s 77ms/step - loss: 1.3312 - accuracy: 0.5551 - val_loss: 0.7306 - val_accuracy: 0.7449
Epoch 2/3
1563/1563 [==============================] - 119s 76ms/step - loss: 0.7034 - accuracy: 0.7614 - val_loss: 0.6250 - val_accuracy: 0.7846
Epoch 3/3
1563/1563 [==============================] - 119s 76ms/step - loss: 0.5726 - accuracy: 0.8098 - val_loss: 0.6379 - val_accuracy: 0.7813

I've also tested with other ImageNet models (ie. `RestNet`, `DenseNet`). They all give a reasonable validation score. 


# Reproducibility 

Apart from the above issue, there is another issue about reproducibility from `EfficientNet`. It was asked before, here [Issue #47174](https://github.com/tensorflow/tensorflow/issues/47174). I've tested with `TF 2.4.1` and I also face the same problem. 


"
48102,TF2.4 ModelCheckpoint Cant Save Model,"
**System information**
- OS Platform and Distribution (e.g., Linux Redhat 7.4):
- TensorFlow installed from conda install
- TensorFlow version (use command below): tf 2.4.1
- Python version: 3.6.8
- CUDA/cuDNN version:  11.2/8.0.4
- GPU model and memory:   GTX3090   24G

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
checkpoint1 = ModelCheckpoint(best_model_filepath,monitor= 'loss',verbose=1,save_best_only= True,mode= 'min',period= 1)  
checkpoint2 = ModelCheckpoint(best_model_filepath,monitor= 'loss',verbose=1,save_best_only= True,
                                 save_weights_only=True,mode= 'min',period=1)
callbacks_list  = [checkpoint1,learning_rate_scheduler]
model.fit(train_generator.generator(),steps_per_epoch=1000,epochs=300,callbacks=callbacks_list)


The checkpoint1  will cause an error, otherwise checkpoint2 will run successly.
THE ERROR LOG  Part:

        312/312 [==============================] - 139s 398ms/step - loss: 94.4598 - CounterAttack_LL_loss: 93.7615 - CounterAttack_cls_out_loss: 0.6983

Epoch 00001: loss improved from inf to 93.23533, saving model to model/CounterAttack_best_model.h5
Traceback (most recent call last):
  File ""train_v2.py"", line 1213, in <module>
    train(continue_train=True)
  File ""train_v2.py"", line 1070, in train
    callbacks=callbacks_list)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1145, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 428, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1344, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1396, in _save_model
    self.model.save(filepath, overwrite=True, options=self._options)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2002, in save
    signatures, options, save_traces)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 154, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 115, in save_model_to_hdf5
    model_metadata = saving_utils.model_metadata(model, include_optimizer)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py"", line 155, in model_metadata
    model_config['config'] = model.get_config()
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py"", line 650, in get_config
    return copy.deepcopy(get_network_config(self))
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)

... ...  (many repeat lines)

  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/root/python_env/anaconda3/lib/python3.6/copy.py"", line 274, in _reconstruct
    y = func(*args)
  File ""/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 190, in __init__
    if value < 0:
RecursionError: maximum recursion depth exceeded in comparison
2021-03-26 15:34:35.680528: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]



"
48101,Add a  tf.keras.layers.WeightedAverage,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tensorflow 2.4.1
- Are you willing to contribute it (Yes/No): I will give example code

**Describe the feature and the current behavior/state.**
I notice there is a ```tf.keras.layers.Average``` , But it does't have a parameter like ```weights``` . 

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
All users who use ```tf.keras```

**Any Other info.**
I find the source code at [https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/merge.py#L327-L360](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/merge.py#L327-L360) . 
And I give an example code here : 
```
@keras_export('keras.layers.WeightedAverage')
class WeightedAverage(_Merge):
  """"""Layer that averages a list of inputs element-wise with weights.
  It takes as input a list of value tensors and a list of weight .  All of the value tensors are the same shape .  It returns
  a single tensor (also of the same shape as the value tensors ).
  Example:
  >>> x1 = np.ones((2, 2))
  >>> x2 = np.zeros((2, 2))
  >>> y = tf.keras.layers.WeightedAverage()([x1, x2],[2,3])
  >>> y.numpy().tolist()
  [[0.4, 0.4], [0.6, 0.6]]
  Usage in a functional model:
  >>> input1 = tf.keras.layers.Input(shape=(16,))
  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)
  >>> input2 = tf.keras.layers.Input(shape=(32,))
  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)
  >>> avg = tf.keras.layers.Average()([x1, x2],[1 for _ in range(16)])
  >>> out = tf.keras.layers.Dense(4)(avg)
  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)
  Raises:
    ValueError: If there is a shape mismatch between the inputs and the shapes
      cannot be broadcasted to match.
  """"""

  def _merge_function(self, inputs , weights):
    output = inputs[0] * weights[0]
    weight_sum = weights[0]
    for i in range(1, len(inputs)):
      output += inputs[i] * weights[i]
      if weights[i] < 0:
        raise
      weight_sum += weights[i]
    if weight_sum == 0:
        raise
    return output / weight_sum 

```






"
48100,How do you convert mobilenet v3 checkpoints to .h5 file?,"Hi, as pointed in https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L390-L429 :
```
The weights for all 6 models are obtained and translated from the Tensorflow
  checkpoints from TensorFlow checkpoints found [here]
  (https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet/README.md).
```

How do you convert the checkpoints to .h5 file? Is there any documentation about this?"
48098,Issue related to goldeneye DOS attack,"I have been seeing this below error when I run the final command, can anyone help me fix it.

Exception AttributeError: ""'NoneType' object has no attribute 'terminate'"" in <bound method Striker.__del__ of <Striker(Striker-2, initial)>> ignored

Shutting down GoldenEye
![image](https://user-images.githubusercontent.com/64961683/112573985-7c0a4b80-8e41-11eb-9432-477ae4a81714.png)
@jan Seidl @jseidl @giancluciano @samueloph"
48096,GPU device not found (Google colab),"I still got the same error after

```
pip install tf-nightly-gpu

%tensorflow_version 2.x  
import tensorflow as tf  
device_name = tf.test.gpu_device_name()  
if device_name != '/device:GPU:0':
   raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
```
in output:
_**SystemError Traceback (most recent call last)

in ()
3 device_name = tf.test.gpu_device_name()
4 if device_name != '/device:GPU:0':
----> 5 raise SystemError('GPU device not found')
6 print('Found GPU at: {}'.format(device_name))

SystemError: GPU device not found**_

but comand
`!nvidia-smi`
get me:
**_NVIDIA-SMI 460.56 Driver Version: 460.32.03 CUDA Version: 11.2_**"
48094,Eagerly calling a Keras Conv2D crashes with `floating point exception`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0.dev20210215
- Python version: 3.8.2
- CUDA/cuDNN version: n/a
- GPU model and memory: AMD Radeon Pro 5500M 8 GB

**Standalone code to reproduce the issue**

In a virtualenv with TensorFlow installed, run

```python
import tensorflow as tf
tf.keras.layers.Conv2D(100, 3)(tf.constant([[[[]]]]))
```

**Describe the current behavior**

Entire process dies, printing this error to the terminal: `floating point exception  python`

**Describe the expected behavior**

Process does not die.  I would probably expect a `ValueError` or similar exception."
48089,windows/subprocess.cc should log the command line for a failing invocations,"
**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Tensorflow fails on my system with an infinite loop of errors like: 
```
2021-03-25 17:18:35.568069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-25 17:18:37.778865: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.778941: W tensorflow/stream_executor/gpu/asm_compiler.cc:55] Couldn't invoke ptxas.exe --version
2021-03-25 17:18:37.781460: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.782300: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-03-25 17:18:37.785605: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.795062: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.798410: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.807112: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.810587: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.819411: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.822604: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.834319: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.837815: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.849892: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.853291: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.880399: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
2021-03-25 17:18:37.884144: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2
```

This is probably because my system (CUDA, cuDNN, python, whatever...) versions are messed up, but without knowing what command was invoked it's hard to figure out a fruitful next step. 

=> It would be most helpful to have `CreateProcess` log the actual command line it was trying to invoke. 

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
People running into weird errors. 

**Any Other info.**
Looking at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/subprocess.cc#L287 it seems like changing

```c++
    LOG(ERROR) << ""Call to CreateProcess failed. Error code: ""
               << GetLastError();
```

to something along the lines of: 

```c++
    LOG(ERROR) << ""Call to CreateProcess failed. Error code: ""
               << GetLastError() << "", command: '"" << command_line << ""'"";
```

would do the trick. 

(Sorry, I don't have a c++ env, and there may well be a bug or two in that code, but a dev should be able to get the jist)"
48086,tf.io.gfile.walk broken on Windows,"It seems that the path isn't split correctly. Filenames come with an extra `\` prefix. Minimal reproducible example:

```python3
import os
import tensorflow as tf


tf.io.gfile.makedirs(""ram://folder"")
with tf.io.gfile.GFile(""ram://folder/file.txt"", mode=""w"") as f:
    f.write(""data"")

for root, _, filenames in tf.io.gfile.walk(""ram://folder""):
    for filename in filenames:
        assert tf.io.gfile.exists(os.path.join(root, filename))
```

This passes on *nix but not on Windows. Here is a quick CI run in GitHub actions showing this: https://github.com/adriangb/tensorflow-test/actions/runs/688190284

ccing @mihaimaruseac @bhack "
48085,How to enable Multiple Object Tracking in tflite?,"Hello everyone, I'm new here. I would like to know how can I enable realtime object tracking in tflite example for android(object detection). I need to count objects in real time with a moving camera. I've read older versions tflite allowed tracking on android, but now I'm confused about that. Can you help me? please
"
48081,Performance of NNAPI Delegate on Snapdragon 888,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 7t and Xiaomi Mi 11
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly
- Python version: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

The two models attached were created by post-training quantization. I ran them with the android benchmark binary from the tensorflow web page. The results were:

sample1.tflite on Oneplus 7t (Snapdragon 855+):
on CPU, 8 threads: avg 36261 us
with NNAPI: avg 6734 us

sample1.tflite on Xiaomi Mi 11 (Snapdragon 888):
on CPU, 8 threads: avg 20330 us
with NNAPI: avg **61303** us

sample2.tflite on Oneplus 7t (Snapdragon 855+):
on CPU, 8 threads: avg 51679 us
with NNAPI: avg 15921 us

sample2.tflite on Xiaomi Mi 11 (Snapdragon 888):
on CPU, 8 threads: avg 39352 us
with NNAPI: avg **34375** us

So, with NNAPI sample1.tflite is about 9 times faster on the older hardware, sample2.tflite is more than twice as fast on the older hardware. It seems, that there is a performance bug.

**Describe the expected behavior**
The models should be faster on the newer hardware.

**Standalone code to reproduce the issue**
unzip sample1.zip      yields sample1.tflite
cat sample2_part1.zip sample2_part2.zip > s2.zip; unzip s2.zip     yields sample2.tflite
then run benchmark binary


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[sample2_part2.zip](https://github.com/tensorflow/tensorflow/files/6207391/sample2_part2.zip)
[sample2_part1.zip](https://github.com/tensorflow/tensorflow/files/6207392/sample2_part1.zip)
[sample1.zip](https://github.com/tensorflow/tensorflow/files/6207393/sample1.zip)


"
48078,Unable to get the prediction.,"I am using flutter tflite to run my custom model.
The project githubrepo - https://github.com/vijayshankarrealdeal/The-Network
and my model train code is -https://www.kaggle.com/vijayshankar756/notebookd234d927a6

Launching lib\main.dart on sdk gphone x86 arm in debug mode...
Invalid depfile: D:\GitHub\The-Network\.dart_tool\flutter_build\0574b688c553aea69c3fd41ddb320c5c\kernel_snapshot.d
Invalid depfile: D:\GitHub\The-Network\.dart_tool\flutter_build\0574b688c553aea69c3fd41ddb320c5c\kernel_snapshot.d
√ Built build\app\outputs\flutter-apk\app-debug.apk.
Connecting to VM Service at ws://127.0.0.1:52078/AsfCjkEa20Q=/ws
I/tflite  (15303): Initialized TensorFlow Lite runtime.
W/System  (15303): A resource failed to call close.
E/AndroidRuntime(15303): FATAL EXCEPTION: AsyncTask #1
E/AndroidRuntime(15303): Process: com.example.hex, PID: 15303
E/AndroidRuntime(15303): java.lang.RuntimeException: An error occurred while executing doInBackground()
E/AndroidRuntime(15303): 	at android.os.AsyncTask$4.done(AsyncTask.java:415)
E/AndroidRuntime(15303): 	at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:383)
E/AndroidRuntime(15303): 	at java.util.concurrent.FutureTask.setException(FutureTask.java:252)
E/AndroidRuntime(15303): 	at java.util.concurrent.FutureTask.run(FutureTask.java:271)
E/AndroidRuntime(15303): 	at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:305)
E/AndroidRuntime(15303): 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
E/AndroidRuntime(15303): 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
E/AndroidRuntime(15303): 	at java.lang.Thread.run(Thread.java:923)
E/AndroidRuntime(15303): Caused by: java.lang.IllegalArgumentException: Cannot copy from a TensorFlowLite tensor (Identity) with shape [1, 1] to a Java object with shape [1, 2].
E/AndroidRuntime(15303): 	at org.tensorflow.lite.Tensor.throwIfDstShapeIsIncompatible(Tensor.java:482)
E/AndroidRuntime(15303): 	at org.tensorflow.lite.Tensor.copyTo(Tensor.java:252)
E/AndroidRuntime(15303): 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:175)
E/AndroidRuntime(15303): 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
E/AndroidRuntime(15303): 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)
E/AndroidRuntime(15303): 	at sq.flutter.tflite.TflitePlugin$RunModelOnImage.runTflite(TflitePlugin.java:481)
E/AndroidRuntime(15303): 	at sq.flutter.tflite.TflitePlugin$TfliteTask.doInBackground(TflitePlugin.java:448)
E/AndroidRuntime(15303): 	at sq.flutter.tflite.TflitePlugin$TfliteTask.doInBackground(TflitePlugin.java:422)
E/AndroidRuntime(15303): 	at android.os.AsyncTask$3.call(AsyncTask.java:394)
E/AndroidRuntime(15303): 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
E/AndroidRuntime(15303): 	... 4 more
I/Process (15303): Sending signal. PID: 15303 SIG: 9
Lost connection to device.
Exited (sigterm)
"
48076,Tensorflow / TensorFlow Lite issue when compiling,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS, Big Sur, version 11.2.2
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.4.1
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: pip under venv
- GPU model and memory: AMD Radeon Pro 5300M 4 Go ; Intel UHD Graphics 630 1536 Mo



**Describe the problem**
I am trying to run a code provided by a user on Github, that deals with .tflite models. I asked to other people to try it (same os, same version of python / tensorflow) and have been able to compile it and to get the output, while I endlessly get an error. I also tested the code on another Mac (Catalina + python 3.8) and that works. I did not find any source of relevant information on the internet to solve the issue. I'm pretty sure it has something to do with tensorflow.
Here's the error : 
  File ""/Users/username/Desktop/Face_mask_detector/my3.8/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py"", line 423, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
ValueError: Cannot set tensor: Got value of type STRING but expected type FLOAT32 for input 0, name: serving_default_input:0 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```git clone https://github.com/tanhouren/Face_mask_detector```
```cd Face_mask_detector``` 
```python3.8  --version```
```python3.8 -m venv my3.8```
```source my3.8/bin/activate```
``` pip install tensorflow opencv-python``` 
```python --version```
```python main.py```

**Any other info / logs**

"
48075,pip install tensorflow==1.15 with Python 2.7 issue,"**System information**
 
 On `CentOS Linux 7 (Core)` I'm trying to install `tensorflow==1.15` in a Python `2.7.3` venv (in order to run [this repo](https://github.com/ShafeenTejani/fast-style-transfer)).
 
 However, I get the error that there is `No matching distribution found for tensorflow==1.15`?
 
 
 ```
 $ python --version
Python 2.7.13
$ virtualenv tensorflow

$ virtualenv venv
$ source venv/bin/activate



(venv) $ pip install --upgrade pip
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already up-to-date: pip<=18.1 in ./venv/lib/python2.7/site-packages (from -c /cvmfs/soft.computecanada.ca/config/python/constraints.txt (line 1)) (18.1)

(venv) $ pip install tensorflow==1.15
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Collecting tensorflow==1.15
  Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: )
No matching distribution found for tensorflow==1.15
```

## `$ pip freeze`

```
(venv) $ pip freeze
-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/avx2
-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/nix/generic
-f /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
```"
48074,Pow layer is not available when fallback quantized conversion to tflite,"I used tfliteConverter to convert DPED network to tflite and it has the Pow layer inside.
When I tried tflite quantization conversion with default optimization with fallback,
pow layer is not available to run because the input of Pow layer is dequantized but the parameter of pow layer is not dequantized and tflite return error about the two of them have different data type (float != uint).
Thank you."
48073,No code size difference between MicroInterpreter and AllOpsResolver ,"@tensorflow/micro
Hi,

I do not see any code size reduction while using microInterpreter with a selection of usefull operators versus the full ops list.
For a quite simple network the code size for TFlite runtime is 160KBytes. Which is quite huge compared to what it claims (16KB for CM3!).
Can you explain this behavior?
Thank you,
OC

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Unbuntu 1804
- TensorFlow installed from (source or binary): Anaconda
- Tensorflow version (commit SHA if source): 2.4.1
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): tflite micro

**Description of the problem**

My TFLITE model is a 12 layers network of 52000 bytes obtained with full integer (int8) quantization.
All application code, except TFLite stuff is around 8KBytes. TFLite code is around 160KB.

- ""Full"" operators version is done with:
_static tflite::AllOpsResolver resolver;
static tflite::MicroInterpreter static_interpreter( model, resolver, tensor_arena, kTensorArenaSize, error_reporter);_

=> Code size:
.text                **164818**   
.rodata              124764    

- ""Reduced"" version
_static tflite::MicroMutableOpResolver<5> micro_op_resolver;
  micro_op_resolver.AddConv2D();
  micro_op_resolver.AddDepthwiseConv2D();
  micro_op_resolver.AddAveragePool2D();
  micro_op_resolver.AddReshape();
  micro_op_resolver.AddFullyConnected();
  static tflite::MicroInterpreter static_interpreter(model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);_

=> Code size:
.text                **165442**     (Even bigger!)
.rodata              124816    

** Compilation **
Compilation of code is done with linux gcc
Flags: 
-Os 
-DTF_LITE_STATIC_MEMORY 
-DNDEBUG 
-DTF_LITE_DISABLE_X86_NEON 

"
48072,Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.2
- Are you willing to contribute it (Yes/No):Wish I was smart enough



**Describe the feature and the current behavior/state.**
Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Easy switch and tf wll become as competant as pytorch
**Any Other info.**
Nope, just that"
48071,Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):conda
- TensorFlow version (use command below):2.2
- Python version:3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow
**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48070,3D Non Max Suppression,"**System information**

- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

- Tensorflow incorporates a native Non Max Suppression algorithm for 2D bounding boxes.

**Will this change the current api? How?**

- The feature would add a Non Max Suppression algorithm to TF for 3D bounding boxes on the model of the 2D case algorithm.

**Who will benefit with this feature?**

- This addon will greatly benefit to the increasing community of 3D object detection or instance segmentation.

**Any Other info.**
"
48069,Slow startup and model loading time,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux Ubuntu 18.04, 5.4.0-66-generic
- Device: MPG B550I GAMING EDGE MAX WIFI (MS-7C92). AMD Ryzen 9 5950X 16-Core Processor
- TensorFlow installed from (source or binary): binary
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.6.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 3090, 24265MiB
- NVIDIA Driver: 460.39

**Describe the current behavior**
I was experiencing very slow speed when loading models into the GPU memory (about 1MiB/s when watching with nvidia-smi) and investigated this by using tensorflow/tensorflow:2.1.0-gpu-py3 docker image to run this command:
`time python -c ""import tensorflow as tf; tf.test.is_gpu_available()""`

and it prints the following logs:
```
2021-03-25 07:39:11.387959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2021-03-25 07:39:11.388827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
WARNING:tensorflow:From <string>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-03-25 07:39:11.747442: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-03-25 07:39:11.771337: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399945000 Hz
2021-03-25 07:39:11.772645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47819f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-25 07:39:11.772667: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-03-25 07:39:11.774848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-03-25 07:39:11.863115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:39:11.863658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4783c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-25 07:39:11.863670: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2021-03-25 07:39:11.863767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:39:11.864237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:2b:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.785GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-25 07:39:11.864264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-03-25 07:39:11.864281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-03-25 07:39:11.865203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-03-25 07:39:11.865342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-03-25 07:39:11.866162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-03-25 07:39:11.866588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-03-25 07:39:11.866606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-03-25 07:39:11.866644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:39:11.867128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:39:11.867588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-03-25 07:39:11.867606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
```

after which it doesn't print anything and after about 2.5 minutes prints this:
```
2021-03-25 07:41:58.768624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 07:41:58.768648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-03-25 07:41:58.768653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-03-25 07:41:58.768811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:41:58.769347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-25 07:41:58.769954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 22243 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:2b:00.0, compute capability: 8.6)

real	2m48.205s
user	2m47.844s
sys	0m2.368s
```

Running the same command again finishes execution in 2 seconds. 

**Describe the expected behavior**
There should not be such a long startup and model loading time. I am not entirely sure if this issue lies with tensorflow, the driver or the hardware itself. I would greatly appreciate any help I can get.

**Standalone code to reproduce the issue**
Within tensorflow/tensorflow:2.1.0-gpu-py3 docker container, run
`time python -c ""import tensorflow as tf; tf.test.is_gpu_available()""`"
48068,how to reduce libtensorflowlite.so size,"
**System information**
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow (source)
- TensorFlow version (master)
- Bazel version (3.7.2):
- ndk (r21)
- android sdk(android-30)

**Describe the problem**
when I use bazel to build libtensorflowlite.so, I got library size about 3.0M
when I use bazel to build tensorflow-lite.aar, I got aar size about 1.1M
question: How to reduce libtensorflowltie.so size, it's too big for me when i use c++ to inference model

so build command:
bazel build -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11"" --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite/java:tensorflow-lite

aar build command:
bazel build -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11"" --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite/java:tensorflow-lite"
48067,Tensorflow 2.0: where is tensorflow.contrib.layers.apply_regularization and tensorflow.contrib.layers.l2_regularizer?,"<from tensorflow.contrib.layers import apply_regularization, l2_regularizer>
I am trying to run the example of VAE which uses above code. Need help how to update this for latest tensorflow version.

below is snippet of code where it is getting used

<def build_graph(self):

        self.construct_weights()

        saver, logits = self.forward_pass()
        log_softmax_var = tf.nn.log_softmax(logits)

        # per-user average negative log-likelihood
        neg_ll = -tf.reduce_mean(tf.reduce_sum(
            log_softmax_var * self.input_ph, axis=1))
        **# apply regularization to weights
        reg = l2_regularizer(self.lam)
        reg_var = apply_regularization(reg, self.weights)**
        # tensorflow l2 regularization multiply 0.5 to the l2 norm
        # multiply 2 so that it is back in the same scale
        loss = neg_ll + 2 * reg_var
        
        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)

        # add summary statistics
        tf.summary.scalar('negative_multi_ll', neg_ll)
        tf.summary.scalar('loss', loss)
        merged = tf.summary.merge_all()
        return saver, logits, loss, train_op, merged>




**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-gcp x86_64)
- TensorFlow installed from (source or binary): using pip
- TensorFlow version: 2.4.1
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: pip

"
48066,Evaluate mobilenet v3 keras model performance on ImageNet,"Hi, I've tested the keras mobilenetv3_large_1.0 model accuracy, the model is created by calling the official API and using the official h5 model:
`model = tf.keras.applications.MobileNetV3Large(alpha=1.0, minimalistic=False, weights='imagenet')`

I do the following preprocess on ImageNet:
```
1. cropp the image of size [0.875*original height, 0.875*original width] from the center of original image, 
2. resize the image to 224x224
3. each channel subtract 127.5, multiply 2 (as the .h5 model has a rescaling layer, *1/255)
4. convert BGR to RGB format
```
Finally, I call model.evaluate() to do evaluation. But I can only get the accuracy:
>  sparse_categorical_accuracy: 0.7208 - sparse_top_k_categorical_accuracy: 0.9122

which is 3% less than reported 75%. Could you help me some advice on how the get the right accuracy? 
"
48064,Build failure: Undefined symbol error. ,"**System information**
- Linux Ubuntu 18.04:
- TensorFlow installed from source from the main
- TensorFlow version: 2.3, 2.4, 2.5
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version: 3.7.2 (installed using bazelisk release)
- GCC/Compiler version: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609

**Describe the problem**
I'm trying to build a CPU optimized Tensorflow version. I've been trying different flags in the building process to test performance for weeks. I was able to build yesterday, but today it started to give an undefined symbol error and failed in build. I tried tensorflow with branches r2.3, r2.4 and the main branch, they all failed. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
bazel build --config=mkl --config=noaws --config=nogcp --config=nohdfs //tensorflow/tools/lib_package:libtensorflow --verbose_failures

Note that I also tried removing the options from the last bazel command such as, 
bazel build //tensorflow/tools/lib_package:libtensorflow --verbose_failures
and
bazel build --config=opt //tensorflow/tools/lib_package:libtensorflow --verbose_failures
They all failed. 

**configure output**
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/dogac/anaconda3/envs/MnM/bin/python3]: 

Found possible Python library paths:
  /home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN support for Aarch64.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.

**bazel output and error**
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=179
INFO: Reading rc options for 'build' from /home/dogac/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/dogac/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/dogac/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/dogac/anaconda3/envs/MnM/bin/python3 --action_env PYTHON_LIB_PATH=/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages --python_path=/home/dogac/anaconda3/envs/MnM/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/dogac/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/dogac/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file /home/dogac/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:noaws in file /home/dogac/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /home/dogac/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /home/dogac/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:linux in file /home/dogac/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/dogac/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (218 packages loaded, 20353 targets configured).
INFO: Found 1 target...
ERROR: /home/dogac/tensorflow/tensorflow/compiler/tf2xla/cc/BUILD:31:21: Executing genrule //tensorflow/compiler/tf2xla/cc:xla_jit_op_gen_genrule failed (Exit 127): bash failed: error executing command 
  (cd /home/dogac/.cache/bazel/_bazel_dogac/e57be43ab57df3f949d2ecc82ad15737/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/lib \
    PATH=/home/dogac/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/dogac/bin:/home/dogac/.local/bin:/home/dogac/anaconda3/envs/MnM/bin:/home/dogac/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/bin:/home/dogac/.go/bin \
    PYTHON_BIN_PATH=/home/dogac/anaconda3/envs/MnM/bin/python3 \
    PYTHON_LIB_PATH=/home/dogac/anaconda3/envs/MnM/lib/python3.6/site-packages \
    TF2_BEHAVIOR=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc bazel-out/k8-opt/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops.h bazel-out/k8-opt/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops.cc 1 ,')
Execution platform: @local_execution_config_platform//:platform
bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc: undefined symbol: _ZN4absl14lts_2020_09_239ByAnyCharC1ENS0_11string_viewE
Target //tensorflow/tools/lib_package:libtensorflow failed to build
INFO: Elapsed time: 688.756s, Critical Path: 228.18s
INFO: 5362 processes: 180 internal, 5182 local.
FAILED: Build did NOT complete successfully
"
48062,"Tensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() | error: Not JSON Serializable:', MirroredVariable","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: 11.0 ( Cuda compilation tools, release 11.2, V11.2.152, Build cuda_11.2.r11.2/compiler.29618528_0)
- GPU model and memory: , 12Gb




**Describe the current behavior**
Point 1:
WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', MirroredVariable:{
  0: <tf.Variable 'conv2d_flipout/kl_loss_weight:0' shape=() dtype=float32, numpy=0.0>

Point 2:
Error details shared below. 


**Describe the expected behavior**
Tensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() should work without any problem

**Standalone code to reproduce the issue**
_Please confirm if the combination of Tensorflow probability +tensorflow 2.4 and tf.distribute.MirroredStrategy() and share the key points to be taken care of for this case_. Since the code is big, If I share the code, the question will be to share a small code to reproduce!

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

`File ""............_vevn/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1145, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""............_vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 432, in on_epoch_end
    callback.on_epoch_end(epoch, numpy_logs)
  File ""/home/rr/Sensor_fusion_ws/RR_SF_net_tf2_ws_Good_CRL_vgg_21Mar2021_res_trial/rrsfnet/../rrsfnet/callbacks/common.py"", line 31, in on_epoch_end
    self.callback.on_epoch_end(epoch, logs=logs)
  File ""...................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1344, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""......................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1396, in _save_model
    self.model.save(filepath, overwrite=True, options=self._options)
  File ""......................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2002, in save
    signatures, options, save_traces)
  File ""........................_vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 154, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""............................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 119, in save_model_to_hdf5
    v, default=json_utils.get_json_type).encode('utf8')
  File ""/usr/lib/python3.6/json/__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""/usr/lib/python3.6/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.6/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""..............................._vevn/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py"", line 134, in get_json_type
    raise TypeError('Not JSON Serializable:', obj)
TypeError: ('Not JSON Serializable:', MirroredVariable:{
  0: <tf.Variable 'conv2d_flipout/kl_loss_weight:0' shape=() dtype=float32, numpy=0.0>
})`
"
48059,incorrect number of nodes in graph - TFLite C++,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5
- TensorFlow installed from (source or binary): Source. Using Bazel and with [libcoral C++](https://github.com/google-coral/libcoral)
- Tensorflow version (commit SHA if source): installed from Bazel
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Coral edge TPU (USB accelerator)

**Describe the problem**
I'm using TF lite C++ working with coral edge TPU. I want to output intermediate results of the graph, so I use `lldb` debugger to examine the `interpreter.cc` and `subgraph.cc` file during `Invoke()`.

Although I'm running a model `mobilenet_v1_1.0_224_quant_edgetpu.tflite`, the command `primary_subgraph().nodes_size()` only gives a value of 2, meaning that there are only two nodes in the graph. Also, input and output sizes (and `tensors_size`) show the same. From my understanding, the number of nodes should be similar to the number of operations in the model (i.e. the mobilenet). Is there anything I missed or misunderstood? Could you please help to point it to me?

**Please provide the exact sequence of commands/steps when you ran into the problem**
1- set `COMPILATION_MODE ?= dbg`, and compile [libcoral C++](https://github.com/google-coral/libcoral) with bazel. `make CPU=k8 examples`
2- debug classify_image example with `lldb .out/k8-debug/classify_image`
3- set breakpoint `b interpreter.cc:180`
4- run `p primary_subgraph().nodes_size()` which then gives a `2`
"
48058,Resource lookup errors when using shared libraries,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): bug occurs with both
- TensorFlow version (use command below): 2.3.2 CPU
- Python version: 3.10
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
Using the C API (`libtensorflow.so` and `libtensorflow_framework.so`) in conjunction with tensorflow-text (or any library loading custom ops) throws errors like:
`Invalid argument: Trying to access resource using the wrong type. Expected N10tensorflow6lookup15LookupInterfaceE got N10tensorflow6lookup15LookupInterfaceE`
This is caused by the same bug as issue #44209.  In `tensorflow/core/framework/type_index.h`, the address of a static variable is used as a hash, leading to a false mismatch when called from a shared library.  That poster suggested that the bug did not occur on Linux because of the GNU unique symbol extension.  In my case, it still happens even on Linux.

Looking at the shared libraries, I think I can see why:
```
$ nm -C /usr/local/lib/libtensorflow.so | grep LookupInterface.*hash_bit
0000000008e091ac b tensorflow::TypeIndex::Make<tensorflow::lookup::LookupInterface>()::hash_bit
                 ^ BSS symbol

$ nm -C /usr/local/lib/libtensorflow_framework.so | grep LookupInterface.*hash_bit
0000000001c656ec u tensorflow::TypeIndex::Make<tensorflow::lookup::LookupInterface>()::hash_bit
                 ^ UNIQUE symbol
```
In libtensorflow_framework.so, the hash_bit is a unique symbol, but in libtensorflow.so, it is not.  I wondered why this problem was not happening when running Tensorflow from Python, so I checked the Python shared library:
```
$ nm -C ./_pywrap_tensorflow_internal.so | grep LookupInterface.*hash_bit
000000001de2842c u tensorflow::TypeIndex::Make<tensorflow::lookup::LookupInterface>()::hash_bit
                 ^ UNIQUE symbol
```
Sure enough, unique symbol.  So the question is why does libtensorflow.so get this symbol in BSS when every other library has it as unique?  I don't know enough about Bazel to inspect how these libraries are being linked, so I'm hoping someone here can help.


**Describe the expected behavior**
This error should not be thrown.
"
48057,Crash when using tf.nn.local_response_normalization across multiple GPUs,"Model using `tf.nn.local_response_normalization` trains OK but crashes upon evaluation when parallelizing over multiple GPUs. Same model does not crash when running on a single GPU. 

Replacing `tf.nn.local_response_normalization` with `keras.layers.BatchNormalization` model trains OK and evaluates OK on single or multiple GPUs. So it seems to me the problem is with `tf.nn.local_response_normalization` over multiple GPUs.

Number of GPUs allocated via Slurm, _e.g._,

    #SBATCH --nodes=1
    #SBATCH --ntasks=2
    #SBATCH --gres=gpu:2    << Here I allocate 2 GPUs
    #SBATCH --mem=16G

**System information**

* Red Hat Enterprise Linux Server 7.6 (Maipo)
* Slurm 19.05.4
* TensorFlow 2.4.1, installed via Conda
* Python 3.7.10
* Cuda compilation tools, release 10.1, V10.1.168
* Multiple NVIDIA Tesla V100 with 32GB RAM

**Describe the current behavior**

When training on two GPUs, code below trains OK, but crashes when evaluating and dumps core. 

`50/51 [============================>.] - ETA: 0s - loss: 1.6090 - accuracy: 0.21062021-03-24 17:11:41.834444: F tensorflow/stream_executor/cuda/cuda_dnn.cc:535] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, dims.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)batch_descriptor: {count: 0 feature_map_count: 1 spatial: 224 224  value_min: 0.000000 value_max: 0.000000 layout: BatchYXDepth}` Aborting (core dumped)

If I train on only one GPU, code below trains OK and evaluates OK.

**Describe the expected behavior**

Trains and evaluates OK on two GPUs.

**Standalone code to reproduce the issue**

    def create_model():
        inputs = keras.Input(shape=(224, 224, 3))
        x = tf.cast(inputs, tf.float32)
        x = keras.layers.Conv2D(1, (2, 2), strides=(1, 1), padding='same')(x)
        x = keras.layers.Lambda(tf.nn.local_response_normalization)(x)
        # if I use x = keras.layers.BatchNormalization()(x) then OK
        x = keras.layers.Activation('relu')(x)
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(5, activation='softmax')(x)
        return keras.Model(inputs=inputs, outputs=x, name=""toy"")
    
    if __name__ == '__main__':
        training_data, validation_data, testing_data = \
            load_img_datasets(""path/to/data"", (224, 224))
        # These ^ are tensorflow.python.data.ops.dataset_ops.BatchDataset
    
        strategy = tf.distribute.MirroredStrategy()
        with strategy.scope():
            model = create_model()
            optimizer = tf.keras.optimizers.Adam()
            metrics = ['accuracy']
            model.compile(loss='categorical_crossentropy',
                          optimizer=optimizer,
                          metrics=metrics)
    
        history = model.fit(training_data, epochs=1,
                            validation_data=validation_data)
    
        loss, accuracy = model.evaluate(testing_data)
        # Crashes here ^"
48047,only two nodes in graph - TFLite C++,"Hi Team,

I'm using TF lite C++ working with coral edge TPU. I want to output intermediate results of the graph, so I use `lldb` debugger to examine the `interpreter.cc` and `subgraph.cc` file during `Invoke()`.

Although I'm running a model `mobilenet_v1_1.0_224_quant_edgetpu.tflite`, the command `primary_subgraph().nodes_size()` only gives a value of 2, meaning that there are only two nodes in the graph. Also, input and output sizes show the same. From my understanding, the number of nodes should be similar to the number of operations in the model. Is there anything I missed or misunderstood? Could you please help to point it to me?

Thank you in advance!
Best,
Fan"
48046,issue: tflite size same as frozen_inference graph and not working on android app.,"### 1. System information

- OS Platform and Distribution =linux
- TensorFlow installation : TF 2.4.1


### 2. Code

import tensorflow as tf
import os 

os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'



converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(""frozen_inference_graph.pb"", [""image_tensor""], [""detection_classes"", ""detection_scores"", ""detection_boxes"", ""num_detections""])
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
model = converter.convert()

tflite_model_name=""TF.tflite""
open(tflite_model_name,""wb"").write(model)

### 3. Failure after conversion
I am getting the tflite model size same as my inference graph of faster rcnn inception v2.pet model which is 54MB ...and when I am trying to use the same tflite model on my android app ,it doesn't run...."
48044,"[Tensorflow Lite] GPU delegate problem, SSD-mobilenetV2 converted from object detection API","

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: i used object detection demo app, some change for gpu delegate
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 64 bit
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: emulator : Google Pixel 4  (android 10.0 / 11.0)
    and real device : Samsung Galuxy Note 10+ / Android 11.0
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:11.0 / 8.0
-   **GPU model and memory**: Nvidia 1070 8gb, 32gb
-   **Exact command to reproduce**:


### Describe the problem
I use custom model from Object Detection API Model Zoo, its SSD-Moblienet-v2-fpnlite-640x640.
my trained model run Object detection demo app successfully, but i want to run demo app on GPU with my trained model.

So, I change some demo app code, and run it on real device.
with some error message, the application is shutdown.

I Just wonder PostProcessing operation, in my model is not supported?
or something other problem?

### Source code / logs
demo app : https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

some code change, lib_interpreter/TFLiteObjectDetecionAPIModel.java
like this
```
    CompatibilityList compatList = new CompatibilityList();
    try {
      Interpreter.Options options = new Interpreter.Options();
      if (compatList.isDelegateSupportedOnThisDevice()){
        System.out.println(""This walking on GPU!"");
        GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
        GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
        options.addDelegate(gpuDelegate);
      } else {
        System.out.println(""This walking on CPU!"");
      }

      options.setNumThreads(NUM_THREADS);
      options.setUseXNNPACK(true);
      d.tfLite = new Interpreter(modelFile, options);
      d.tfLiteModel = modelFile;
      d.tfLiteOptions = options;
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
```

error logs
```
E/libEGL: call to OpenGL ES API with no current context (logged once per thread)
I/System.out: This walking on GPU!
I/tflite: Created TensorFlow Lite delegate for GPU.
I/tflite: Initialized TensorFlow Lite runtime.
I/tflite: Created 0 GPU delegate kernels.
D/AndroidRuntime: Shutting down VM
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.tensorflow.lite.examples.detection, PID: 12798
    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:
    CUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess
    154 operations will run on the GPU, and the remaining 1 operations will run on the CPU.
    TfLiteGpuDelegate Init: PACK: Tensor ""tfl.pack"" has bad input dims size: 5.
    TfLiteGpuDelegate Prepare: delegate is not initialized
    Node number 155 (TfLiteGpuDelegateV2) failed to prepare.
    
    Restored original execution plan after delegate application failure.
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:162)
        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:168)
        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:453)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)
        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)
        at android.view.TextureView.getTextureLayer(TextureView.java:402)
and more...
```

Searching very hard, but I can't found solution."
48043,Accuracy of resnet50/vgg16 after tfmot.quantization.keras.quantize_model is really low,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): v2.4
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Accuracy of resnet50/vgg16 after tfmot.quantization.keras.quantize_model is really low
I tested them with imagenet2012_subset validation portion. For resnet50, the pretrained model imported from keras.applications has top-1 accuracy: 70% before quantization and 0.1% after quantization. 

**Describe the expected behavior**

I expected the accuracy drop only slightly.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1nSG7fqYidwBSbeMTyDEQ_Sr6NATov2ik?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
48041,Failed to create cuSolverDN instance on GPU with 4GB,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- CUDA/cuDNN version: 11.0 / 8.0.4.30-1+cuda11.0
- GPU model and memory: RTX 1650 / T2000 with 4GB

**Describe the current behavior**

The following linear algebra operations:
- tf.linalg.eigh
- tf.linalg.eigvalsh
- tf.linalg.expm

when applied to a tensor with tf.complex* type in a RTX1650/T2000 GPUs with 4GB produce:
```bash
F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.
Aborted (core dumped)
```
This issue is not visible when setting a GPU memory limit or `set_memory_growth`.

**Describe the expected behavior**

No crash.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
rho = tf.constant([[ 9.14909492+1.14491749e-16j,  0.27933987-1.27970271e-01j, 
    0.25616772+1.12442776e-01j, -0.05876706-3.76165554e-01j],
    [ 0.27933987+1.27970271e-01j,  9.62506895+2.77555756e-17j,
    -0.19640032-1.72149754e-01j,  0.28515435+2.58716412e-01j],
    [ 0.25616772-1.12442776e-01j, -0.19640032+1.72149754e-01j,
    9.13383471-1.38777878e-17j,  0.18600413-2.68212068e-01j],
    [-0.05876706+3.76165554e-01j,  0.28515435-2.58716412e-01j,
    0.18600413+2.68212068e-01j,  9.58168325-5.20417043e-17j]])
tf.linalg.eigvalsh(rho)
```"
48040,"win10 build tensorflow2.4.1,ERROR: C:/tensorflow/tensorflow/python/util/BUILD:609:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): python.exe failed: error executing command","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary):  source 
- TensorFlow version:2.4.1
- Python version:3.8.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):MSVC2019
- CUDA/cuDNN version:11.0 / 8.0.5
- GPU model and memory: GeForce RTX 2080 ti



**Describe the problem**
ERROR: C:/tensorflow/tensorflow/python/util/BUILD:609:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): python.exe failed: error executing command
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel clean
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
cd C:/users/zn58887/_bazel_zn58887/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python38/python.exe
    SET PYTHON_LIB_PATH=C:/Python38/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\zn58887\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TMP=C:\Users\zn58887\AppData\Local\Temp
  C:/Python38/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc
Execution platform: @local_execution_config_platform//:platform
cl: 命令行 warning D9035 :“experimental:preprocessor”选项已否决，并将在将来的版本中移除
cl: 命令行 warning D9036 :使用“Zc:preprocessor”而不使用“experimental:preprocessor”
cl: 命令行 warning D9002 :忽略未知选项“-fno-strict-aliasing”
cl: 命令行 warning D9002 :忽略未知选项“-fexceptions”
注意: 包含文件:  bazel-out/x64_windows-opt/bin/external/local_config_python/python_include\Python.h
注意: 包含文件:   C:\users\zn58887\_bazel_zn58887\xv6zejqw\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\external\local_config_python\python_include\patchlevel.h
注意: 包含文件:   C:\users\zn58887\_bazel_zn58887\xv6zejqw\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\external\local_config_python\python_include\pyconfig.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\io.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_io.h
注意: 包含文件:      C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_share.h
注意: 包含文件:       C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt.h
注意: 包含文件:        C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\vcruntime.h
注意: 包含文件:         C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\sal.h
注意: 包含文件:          C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\concurrencysal.h
注意: 包含文件:         C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\vadefs.h
注意: 包含文件:      C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_wio.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\float.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared\basetsd.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\stdio.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_wstdio.h
注意: 包含文件:      C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_stdio_config.h
注意: 包含文件:   C:\users\zn58887\_bazel_zn58887\xv6zejqw\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\external\local_config_python\python_include\pymacconfig.h
注意: 包含文件:   C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\limits.h
注意: 包含文件:   C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\string.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_memory.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_memcpy_s.h
注意: 包含文件:      C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\errno.h
注意: 包含文件:      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\vcruntime_string.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_wstring.h
注意: 包含文件:   C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\stdlib.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_malloc.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_search.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\stddef.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_wstdlib.h
注意: 包含文件:   C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\assert.h
注意: 包含文件:   C:\users\zn58887\_bazel_zn58887\xv6zejqw\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\external\local_config_python\python_include\pyport.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\inttypes.h
注意: 包含文件:     C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include\stdint.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\math.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_math.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_math_defines.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\time.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\corecrt_wtime.h
注意: 包含文件:    C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\sys/stat.h
注意: 包含文件:     C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt\sys/types.h"
48039,Expression issue of BatchNormalization layer,"A issue about the expression in the BatchNormalization layer.

## URL(s) with the issue:
https://keras.io/api/layers/normalization_layers/batch_normalization/

## Description of issue (what needs changing):
In the provided URL, during inference, this layer returns:
(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.

But, I think a square operation is missing. It should return:
(batch - self.moving_mean) / **sqrt**(self.moving_var + epsilon) * gamma + beta.

Similarly, the square operation should be add in the equation during training, as:
(batch - mean(batch)) / **sqrt**(var(batch) + epsilon) * gamma + beta.

Please refer to Algorithm 1 in the paper: 
_Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift_"
48038,Named dictionary outputs in tf.keras.Model do not work with defining seperate metrics per output,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux-4.9.0-4-amd64-x86_64-with-Ubuntu-18.04-bionic
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 2.3.0 also tested in 2.4.1 
-   **Python version**: 3.6.9

### Describe the problem
Using a custom model with named outputs doesn't work when defining seperate metrics per output
There was already a bug for which dict outputs did not work here: [#34199
](https://github.com/tensorflow/tensorflow/issues/34199) but this was closed due to it being resolved apparently

### Describe the expected behavior
When using a list or tuple everything works fine but when using a dictionary as outputs it fails. Would expect it to work the same as when using a list or tuple

### Source code / logs
This code is based on the [this example](https://towardsdatascience.com/building-a-multi-output-convolutional-neural-network-with-keras-ed24c7bc1178) on how to make a multi-output network

```
from tensorflow.keras.models import Model
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Lambda
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
import tensorflow as tf

class UtkMultiOutputModel():
    def make_default_hidden_layers(self, inputs):
        x = Conv2D(16, (3, 3), padding=""same"")(inputs)
        x = Activation(""relu"")(x)
        x = BatchNormalization(axis=-1)(x)
        x = MaxPooling2D(pool_size=(3, 3))(x)
        x = Dropout(0.25)(x)
        x = Conv2D(32, (3, 3), padding=""same"")(x)
        x = Activation(""relu"")(x)
        x = BatchNormalization(axis=-1)(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        x = Dropout(0.25)(x)
        x = Conv2D(32, (3, 3), padding=""same"")(x)
        x = Activation(""relu"")(x)
        x = BatchNormalization(axis=-1)(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        x = Dropout(0.25)(x)
        return x

    def build_race_branch(self, inputs, num_races):
        x = self.make_default_hidden_layers(inputs)
        x = Flatten()(x)
        x = Dense(128)(x)
        x = Activation(""relu"")(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)
        x = Dense(num_races)(x)
        x = Activation(""softmax"", name=""race_output"")(x)
        return x

    def build_gender_branch(self, inputs, num_genders=2):
        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)
        x = self.make_default_hidden_layers(inputs)
        x = Flatten()(x)
        x = Dense(128)(x)
        x = Activation(""relu"")(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)
        x = Dense(num_genders)(x)
        x = Activation(""sigmoid"", name=""gender_output"")(x)
        return x

    def build_age_branch(self, inputs):   
        x = self.make_default_hidden_layers(inputs)
        x = Flatten()(x)
        x = Dense(128)(x)
        x = Activation(""relu"")(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)
        x = Dense(1)(x)
        x = Activation(""linear"", name=""age_output"")(x)
        return x

    def assemble_full_model(self, width, height, num_races):
        input_shape = (height, width, 3)
        inputs = Input(shape=input_shape)
        age_branch = self.build_age_branch(inputs)
        race_branch = self.build_race_branch(inputs, num_races)
        gender_branch = self.build_gender_branch(inputs)
        model = Model(inputs=inputs,
                     outputs = {'A' : age_branch, 'B' : race_branch, 'C' : gender_branch},
                     name=""face_net"")
        return model
    
model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))

model.compile(optimizer=opt, 
              loss={
                  'age_output': 'mse', 
                  'race_output': 'categorical_crossentropy', 
                  'gender_output': 'binary_crossentropy'},
              loss_weights={
                  'age_output': 4., 
                  'race_output': 1.5, 
                  'gender_output': 0.1},

              # this will not work when outputs is a dict, but will work when outputs is a list or tuple
              metrics={
                  'age_output': ['mae', 'accuracy'], 
                  'race_output': ['accuracy', 'mae'],
                  'gender_output': ['accuracy', 'mae']})

             # this will work with a dict, list and tuple but is not what I want
             # metrics=['accuracy']

history = model.fit_generator(train_gen,
                    steps_per_epoch=len(train_idx)//batch_size,
                    epochs=epochs,
                    validation_data=valid_gen,
                    validation_steps=len(valid_idx)//valid_batch_size)
```

This is the traceback from the above code:
```
Epoch 1/4
WARNING:tensorflow:Gradients do not exist for variables ['conv2d_15/kernel:0', 'conv2d_15/bias:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_16/kernel:0', 'conv2d_16/bias:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'dense_11/kernel:0', 'dense_11/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['conv2d_15/kernel:0', 'conv2d_15/bias:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_16/kernel:0', 'conv2d_16/bias:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'dense_11/kernel:0', 'dense_11/bias:0'] when minimizing the loss.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-a8fc86d7fb42> in <module>
     21                     callbacks=[checkpoint_cb, tensorboard_cb],
     22                     validation_data=valid_gen,
---> 23                     validation_steps=len(valid_idx)//valid_batch_size)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--> 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1827         use_multiprocessing=use_multiprocessing,
   1828         shuffle=shuffle,
-> 1829         initial_epoch=initial_epoch)
   1830 
   1831   @deprecation.deprecated(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:759 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:388 update_state
        self.build(y_pred, y_true)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:319 build
        self._metrics, y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1139 map_structure_up_to
        **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1221 map_structure_with_tuple_paths_up_to
        expand_composites=expand_composites)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:854 assert_shallow_structure
        input_length=len(input_tree), shallow_length=len(shallow_tree)))

    ValueError: The two structures don't have the same sequence length. Input structure has length 6, while shallow structure has length 3.
```
"
48037,"How to use Allgather, Allreduce","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
48036,Is tensorflow Java interface support Mac M1 slices ? Problematic frame: C [libtensorflow_framework.2.dylib+0x14c15] tensorflow::monitoring::MetricDef,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOs Big Sur ver11.0.1,  M1 slices** 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  Maven
- TensorFlow version (use command below): Old tensorflow version(1.4.0/1.5.0), and new  tensorflow Java Version 0.2.0
- JVM version:  1.8.0_162
-  No GPU

**Describe the current behavior**
I just try to run tensorflow java offical example, and get tensorflow version for test. but It dosen't work. 
I have test different versions of tensorflow java interface, and only ver 1.13.1 works well. 
And all other versions can not work,  for example old tensorflow version(1.4.0/1.5.0), and new  tensorflow Java Version 0.2.0/0.3.0(tensorflow ver2.3.1/2.4.1) .

The Error shows below:

> A fatal error has been detected by the Java Runtime Environment:
> 
> SIGILL (0x4) at pc=0x00000001290edc15, pid=6333, tid=0x0000000000001a03
> 
> JRE version: Java(TM) SE Runtime Environment (8.0_162-b12) (build 1.8.0_162-b12)
> Java VM: Java HotSpot(TM) 64-Bit Server VM (25.162-b12 mixed mode bsd-amd64 compressed oops)
> Problematic frame:
> **C  [libtensorflow_framework.2.dylib+0x14c15]  tensorflow::monitoring::MetricDef<(tensorflow::monitoring::MetricKind)1, long long, 2>::MetricDef<char [11], char [7]>(absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, char const (&) [11], char const (&) [7])+0x125**
> 
> Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
> 
> An error report file with more information is saved as:
>  /***/tf_test/hs_err_pid6333.log
> 
> If you would like to submit a bug report, please visit:
> http://bugreport.java.com/bugreport/crash.jsp
> The crash happened outside the Java Virtual Machine in native code.
> See problematic frame for where to report the bug.
> 

**Code to reproduce the issue**
Java code:
```

import org.tensorflow.TensorFlow;
public class HelloTensorFlow {
    public static void main(String[] args) throws Exception {
        System.out.println(""Hello TensorFlow "" +TensorFlow.version());
    }
}
```

pom.xml
```
<project>
    <modelVersion>4.0.0</modelVersion>
    <groupId>org.myorg</groupId>
    <artifactId>hellotensorflow</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <exec.mainClass>HelloTensorFlow</exec.mainClass>
    <!-- Minimal version for compiling TensorFlow Java is JDK 8 -->
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <dependencies>
    <!-- Include TensorFlow (pure CPU only) for all supported platforms -->
        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>tensorflow-core-platform</artifactId>
            <version>0.2.0</version>
        </dependency>
    </dependencies>
</project>
```"
48034,Installing tensorflow nightly,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- windows 10:
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tf-nightly

- TensorFlow version:2.5.0
- Python version:3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 6 gb ram and gt710



**Describe the problem**
it just threw a lot of exception errors , i'm 17 y/o noobie, really aint into this .....
total noobie

**Provide the exact sequence of commands / steps that you executed before running into the problem**
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 171, in _merge_into_criterion
    crit = self.state.criteria[name]
KeyError: 'tf-nightly-gpu'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\urllib3\response.py"", line 438, in _error_catcher        
    yield
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\urllib3\response.py"", line 519, in read
    data = self._fp.read(amt) if not fp_closed else b""""
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\cachecontrol\filewrapper.py"", line 62, in read
    data = self.__fp.read(amt)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\http\client.py"", line 458, in read
    n = self.readinto(b)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\http\client.py"", line 502, in readinto
    n = self.fp.readinto(b)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\socket.py"", line 704, in readinto
    return self._sock.recv_into(b)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\cli\base_command.py"", line 189, in _main
    status = self.run(options, args)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\cli\req_command.py"", line 178, in wrapper
    return func(self, options, args)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\commands\install.py"", line 316, in run
    requirement_set = resolver.resolve(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\resolver.py"", line 121, in resolve
    self._result = resolver.resolve(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 453, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 318, in resolve
    name, crit = self._merge_into_criterion(r, parent=None)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 173, in _merge_into_criterion
    crit = Criterion.from_requirement(self._p, requirement, parent)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py"", line 82, in from_requirement   
    if not cands:
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\resolvelib\structs.py"", line 124, in __bool__
    return bool(self._sequence)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py"", line 143, in __bool__
    return any(self)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py"", line 38, in _iter_built
    candidate = func()
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\factory.py"", line 167, in _make_candidate_from_link
    self._link_candidate_cache[link] = LinkCandidate(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 300, in __in    super().__init__(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 144, in __init__
    self.dist = self._prepare()
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 226, in _pre    dist = self._prepare_distribution()
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\resolution\resolvelib\candidates.py"", line 311, in _prepare_distribution
    return self._factory.preparer.prepare_linked_requirement(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\operations\prepare.py"", line 457, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\operations\prepare.py"", line 480, in _prepare_linked_re    local_file = unpack_url(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\operations\prepare.py"", line 230, in unpack_url        
    file = get_http_url(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\operations\prepare.py"", line 108, in get_http_url      
    from_path, content_type = download(link, temp_dir.path)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\network\download.py"", line 163, in __call__
    for chunk in chunks:
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\cli\progress_bars.py"", line 159, in iter
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_internal\network\utils.py"", line 64, in response_chunks
    for chunk in response.raw.stream(
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\urllib3\response.py"", line 576, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\urllib3\response.py"", line 541, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\contextlib.py"", line 135, in __exit__
  File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip\_vendor\urllib3\response.py"", line 443, in _error_catcher        
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48033,tf2.4.0 and 2.4.1 keras history.history is different from each epoch end ,"i run the code on colab
the code is :[https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/guide/keras/train_and_evaluate.ipynb?hl=zh_CN#scrollTo=mGYBLPAR13g3](url)


**and below is epcoh end** 

Fit model on training data
Epoch 1/2
782/782 [==============================] - 3s 3ms/step - loss: 0.5712 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.2025 - val_sparse_categorical_accuracy: 0.9416
Epoch 2/2
782/782 [==============================] - 2s 3ms/step - loss: 0.1734 - sparse_categorical_accuracy: 0.9493 - val_loss: 0.1453 - val_sparse_categorical_accuracy: 0.9571

**and keras history.history below**

{'loss': [0.33999016880989075, 0.16297979652881622],
 'sparse_categorical_accuracy': [0.9027799963951111, 0.9526399970054626],
 'val_loss': [0.20245662331581116, 0.14533831179141998],
 'val_sparse_categorical_accuracy': [0.9416000247001648, 0.957099974155426]}

**i find the two is different.
for example train sparse_categorical_accuracy of epoch 1 is 0.8411 but in history.history is 0.9027799963951111.**

**but when i set verbose=2 in model.fit have no this problem**


**i try tf 2.3.2 and other low version have no question。**

**what changes in tf2.4.0 。i could not find the change log in ：https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md**



"
48032,NotImplementedError: Conversion for TF op 'Bucketize' not implemented.,"Error encountered while converting the TensorFlow model to CoreML model:
NotImplementedError: Conversion for TF op 'Bucketize' not implemented.

TensorFlow uses the following code for training:

numeric_column = tf. Feature_column. Numeric_column (col)
buket_col = tf. Feature_column. Bucketized_column (numeric_column, boundary_dict [col])
embedding_column = tf.feature_column. Embedding_column (buket_col, K + 1)

How do I skip support for bucketized_column?
"
48031,How does tf.nn.erosion2d work? its output seems unmatched with the formular shown in doc,"output[b, y, x, c] =
   min_{dy, dx} value[b,
                      strides[1] * y - rates[1] * dy,
                      strides[2] * x - rates[2] * dx,
                      c] -
                kernel[dy, dx, c]"
48027,Float16 TFLite models with multiple interpreters doesn't share weights but bloats memory,"**System information**
- Linux 5.11.4-arch1-1 #1 SMP PREEMPT Sun, 07 Mar 2021 18:00:49 +0000 x86_64 GNU/Linux
- TensorFlow 2.4.1 from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)

**Describe the current behavior**

When we create multiple interpreters per single model (e.g. for thread-safety reasons) the expected behavior is that all interpreters share model weights. And that's statement is true when the model is `float32` model:

```
$ ./minimal mem-bloat-float32.tflite        
initial mem 1 MB
interpreter #1, mem 139 MB
interpreter #2, mem 141 MB
interpreter #3, mem 144 MB
interpreter #4, mem 146 MB
interpreter #5, mem 148 MB
interpreter #6, mem 150 MB
interpreter #7, mem 152 MB
interpreter #8, mem 154 MB
interpreter #9, mem 156 MB
```

But when the same model had been converted to be `float16` the weights are not shared anymore and memory is bloated so that each interpreter has its own copy of model weights:

```
$ ./minimal mem-bloat-float16.tflite
initial mem 1 MB
interpreter #1, mem 206 MB
interpreter #2, mem 341 MB
interpreter #3, mem 476 MB
interpreter #4, mem 610 MB
interpreter #5, mem 745 MB
interpreter #6, mem 880 MB
interpreter #7, mem 1015 MB
interpreter #8, mem 1149 MB
interpreter #9, mem 1284 MB
```

**Describe the expected behavior**

The expected behavior is that multiple interpreters share weights for the same model, whenever it's `float16` or `float32` model.

**Standalone code to reproduce the issue**
- unzip TFLite models: [mem-bloat-tflite.zip](https://github.com/tensorflow/tensorflow/files/6192575/mem-bloat-tflite.zip)
- build TFLite `minimal` example with the following code within `tensorflow/tensorflow/lite/examples/minimal.cc`:
```c++
#include <cstdio>

#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""

#if defined(_WIN32)
#include <windows.h>
#include <psapi.h>

#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) && defined(__MACH__))
#include <unistd.h>
#include <sys/resource.h>

#if defined(__APPLE__) && defined(__MACH__)
#include <mach/mach.h>

#elif (defined(_AIX) || defined(__TOS__AIX__)) || (defined(__sun__) || defined(__sun) || defined(sun) && (defined(__SVR4) || defined(__svr4__)))
#include <fcntl.h>
#include <procfs.h>

#elif defined(__linux__) || defined(__linux) || defined(linux) || defined(__gnu_linux__)
#include <stdio.h>

#endif

#else
#error ""Cannot define getCurrentRSS( ) for an unknown OS.""
#endif


/**
 * Returns the current resident set size (physical memory use) measured
 * in bytes, or zero if the value cannot be determined on this OS.
 */
size_t getCurrentRSS( )
{
#if defined(_WIN32)
    /* Windows -------------------------------------------------- */
    PROCESS_MEMORY_COUNTERS info;
    GetProcessMemoryInfo( GetCurrentProcess( ), &info, sizeof(info) );
    return (size_t)info.WorkingSetSize;

#elif defined(__APPLE__) && defined(__MACH__)
    /* OSX ------------------------------------------------------ */
    struct mach_task_basic_info info;
    mach_msg_type_number_t infoCount = MACH_TASK_BASIC_INFO_COUNT;
    if ( task_info( mach_task_self( ), MACH_TASK_BASIC_INFO,
        (task_info_t)&info, &infoCount ) != KERN_SUCCESS )
        return (size_t)0L;      /* Can't access? */
    return (size_t)info.resident_size;

#elif defined(__linux__) || defined(__linux) || defined(linux) || defined(__gnu_linux__)
    /* Linux ---------------------------------------------------- */
    long rss = 0L;
    FILE* fp = NULL;
    if ( (fp = fopen( ""/proc/self/statm"", ""r"" )) == NULL )
        return (size_t)0L;      /* Can't open? */
    if ( fscanf( fp, ""%*s%ld"", &rss ) != 1 )
    {
        fclose( fp );
        return (size_t)0L;      /* Can't read? */
    }
    fclose( fp );
    return (size_t)rss * (size_t)sysconf( _SC_PAGESIZE);

#else
    /* AIX, BSD, Solaris, and Unknown OS ------------------------ */
    return (size_t)0L;          /* Unsupported. */
#endif
}


#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[])
{
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  printf(""initial mem %d MB\n"", getCurrentRSS() >> 20);

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);

  std::unique_ptr<tflite::Interpreter> interpreter_list[9];
  for (auto &interpreter : interpreter_list) {

    builder(&interpreter, 1);
    TFLITE_MINIMAL_CHECK(interpreter != nullptr);
    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
    interpreter->Invoke();

    printf(""interpreter #%d, mem %d MB\n"", &interpreter - &interpreter_list[0] + 1, getCurrentRSS() >> 20);
  }

  return 0;
}
```

**Other info / logs**
Both versions of model had been converted from this protobuf file: [mem-bloat-protobuf.zip](https://github.com/tensorflow/tensorflow/files/6192621/mem-bloat-protobuf.zip)
"
48026,Unexpected rank of mask computed by tf.keras.layers.Masking,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.8

**Describe the current behavior**
I have batch tensors of the shape `(batch_size, n_time_steps, n_features, n_channels)`. They arise from tensors of the shape `(n_time_steps, n_features, n_channels)`, where `n_time_steps` is *not constant*. When constructing the batches, the tensors are padded to the maximum value of `n_time_steps`.

These tensors should be fed into a neural network of the following architecture:
- The inputs are masked due to the padding.
- The tensors for each timestep are fed to a time distributed convolutional block. The mask is propagated.
- The extracted features are fed to an RNN layer.

In the last layer, I run into an error due to the fact that the mask has the shape `(batch_size, n_time_steps, n_features)`, but the RNN layer expects it to have shape `(batch_size, n_time_steps)`.

**Describe the expected behavior**
According to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking):
> For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).

So I expect the mask to have shape `(batch_size, n_time_steps)` and, in particular, tensor rank 2, regardless of the tensor rank of the input tensor.

**Standalone code to reproduce the issue**

Here is a minimal example:
```python
import tensorflow as tf

class TimeDistributedMaskPropagating(tf.keras.layers.TimeDistributed):
    """"""TimeDistributed layer that propagates mask.""""""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.supports_masking = True
        
    def compute_mask(self, inputs, mask=None):
        return mask

n_features = 3
n_channels = 1

cnn_block = tf.keras.layers.Flatten()
estimator = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(None, n_features, n_channels)),
    tf.keras.layers.Masking(),
    TimeDistributedMaskPropagating(cnn_block),
    # tf.keras.layers.LSTM(10)
    # yields ValueError: Dimensions must be equal, but are 3 and 10
])

x1 = tf.random.uniform((4, 3, 1))       # shape: 4, 3, 1
x2 = tf.random.uniform((3, 3, 1))       # shape: 3, 3, 1

paddings = tf.constant([[0, 1], [0, 0], [0, 0]])
padded_x2 = tf.pad(x2, paddings)        # shape: 4, 3, 1
mini_batch = tf.stack((x1, padded_x2))  # shape: 2, 4, 3, 1

logits = estimator(mini_batch)          # shape: 2, 4, 3
print(logits._keras_mask)               # shape: 2, 4, 3
# mask has shape 2, 4, 3 with values
# [[[ True  True  True]
#   [ True  True  True]
#   [ True  True  True]
#   [ True  True  True]]
# 
#  [[ True  True  True]
#   [ True  True  True]
#   [ True  True  True]
#   [False False False]]]

# mask should have shape 2, 4 with values
# [[ True  True  True  True]
#  [ True  True  True False]]
```

**Other info / logs**
I have solved the problem for my use case by defining a custom masking layer, where `axis=-1` from the [original code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py) in the methods `compute_mask` and `call` is replaced by `axis=[2, 3]` (in my case) or, more generally, by `axis=list(range(2, len(inputs.shape)))` ([full code](https://stackoverflow.com/a/66769283/10816965)).

If the TensorFlow team agrees with me that the current behavior of the `Masking` layer is unexpected and unintended, I would love to contribute with a pull request."
48025,"Converted TensorFlow Lite MaskRCNN model, conversion was successful but outputs were wrong","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): '2.5.0-dev20210318'

### 2. Code

This [folder](https://drive.google.com/drive/folders/1Ot7GzI69t0ywdAhwvwaOj1VQFQfloqT4?usp=sharing) has the converted model, and also the code used to convert the model.

### 3. Failure after conversion
The conversion is successful, but the output tensors don't look right. I expect the MaskRCNN model to have four output tensors, however, this model seems to have 7 as shown in the image below. Beside, I tried inference with one image on Android, but the output tensors are either all zeros, or some meaningless numbers.
 
![Capture](https://user-images.githubusercontent.com/5137261/112209456-d90ec300-8bef-11eb-8239-27d55e6360aa.PNG)

"
48023,How to convert conv3d layer ?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10  + anaconda
- TensorFlow installation (pip package or built from source): 2.4.1
- TensorFlow library (version, if pip package or github SHA, if built from source): pip package

### 2. Describe the problem
I'm using a model containing conv3d layers. I've got an error when converting the model to 
tflite saying that conv3d op is neither a custom op nor a flex op. 

**Question 1 : how to quantize a conv3d layers ?**

I tried to add ""f.lite.OpsSet.SELECT_TF_OPS"" to the converter supported ops. The code worked
without errors but I don't know if the conv3d layers were correctly quantized or not (to int8).

**Question2 : does adding ""Select TensorFlow operators"" affect  quantization ? if yes how ?**


### 3. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
model_path = os.path.join(os.getcwd(), ""models"", ""best-model.hdf5"")
    # load best weights
loaded_model = tf.keras.models.load_model(model_path)


no_quant_converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)

no_quant_tflite_model = no_quant_converter.convert()

# write the model to a tflite file as binary file
tflite_no_quant_file = os.path.join(os.getcwd(), ""models"", ""tflite_models"", ""best-model_no_quant.tflite"") 
with open(tflite_no_quant_file, ""wb"") as f:
    f.write(no_quant_tflite_model)
```

### 4. Failure after conversion
error : 
![image](https://user-images.githubusercontent.com/33780694/112195173-ef297d00-8c09-11eb-823d-49ab2942a97d.png)



"
48022,TensorFlow Lite allocation error after successful conversion: tfl.Slice node failed to prepare,"### 1. System information

- Linux 5.11.4-arch1-1 #1 SMP PREEMPT Sun, 07 Mar 2021 18:00:49 +0000 x86_64 GNU/Linux
- TensorFlow 2.4.1 from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)

### 2. Code

Please unzip this protobuf file before running the code: [allocate-fail-protobuf.zip](https://github.com/tensorflow/tensorflow/files/6191821/allocate-fail-protobuf.zip)

```python
# use tensorflow API v1
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

PROTOBUF_PATH = ""allocate-fail.pb""

with tf.gfile.GFile(PROTOBUF_PATH, ""rb"") as fprotobuf:
    graphDef = tf.GraphDef()
    graphDef.ParseFromString(fprotobuf.read())

X = tf.placeholder(dtype=tf.float32, shape=[1, 309], name=""X"")
P = tf.placeholder(dtype=tf.float32, shape=[1], name=""P"")

Yhat, = tf.import_graph_def(
    graphDef,

    input_map={
        ""runtime/X:0"": X,
        ""runtime/P:0"": P,
    },
    name=""model"",
    return_elements=[f""runtime/Y/fast/32:0""]
)

with tf.Session() as session:
    converter = tf.lite.TFLiteConverter.from_session(
        session,
        input_tensors=[X, P],
        output_tensors=[Yhat]
    )
    flatbuffer = converter.convert()

interpreter = tf.lite.Interpreter(model_content=flatbuffer)
interpreter.allocate_tensors()
```


### 3. Failure after conversion
The computational graph converts without an error, but when I try to allocate tensors there's internal error within TensorFlow Lite C modules:
```
Traceback (most recent call last):
  File ""/home/barabanus/work/4a-games/pfnn-tools/test/tflite-allocate-fail.py"", line 34, in <module>
    interpreter.allocate_tensors()
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py"", line 259, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Invalid begin and size.Node number 20 (SLICE) failed to prepare.
```
The error refers to `tfl.Slice` operation within the graph.

### 5. Any other info / logs

TFLite model after conversion: [allocate-fail-flatbuffer.zip](https://github.com/tensorflow/tensorflow/files/6191844/allocate-fail-flatbuffer.zip)

The full log:
```
2021-03-23 19:48:41.585258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:From /usr/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-03-23 19:48:42.626809: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:48:42.627325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-23 19:48:42.739393: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-23 19:48:42.739422: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rb15): /proc/driver/nvidia/version does not exist
2021-03-23 19:48:42.739809: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-23 19:48:42.740475: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:48:42.740514: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-03-23 19:48:42.742921: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-03-23 19:48:42.743008: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-03-23 19:48:42.743161: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:48:42.757234: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200660000 Hz
2021-03-23 19:48:42.758795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.004ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.

2021-03-23 19:48:42.764013: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-03-23 19:48:42.764171: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-03-23 19:48:42.764396: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:48:42.766009: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.002ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.

2021-03-23 19:48:42.776385: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.
2021-03-23 19:48:42.776410: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.
2021-03-23 19:48:42.786790: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:48:42.786850: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/home/barabanus/work/4a-games/pfnn-tools/test/tflite-allocate-fail.py"", line 34, in <module>
    interpreter.allocate_tensors()
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py"", line 259, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Invalid begin and size.Node number 20 (SLICE) failed to prepare.
```"
48021,CMake config for TensorFlow Lite C API for Windows builds debug target (not release target as expected),"**System information**
- Microsoft Windows 10 Pro Insider Preview 10.0.21337 Build 21337
- Microsoft Visual Studio Community 2019 Version 16.9.1
- TensorFlow master 42d9939623f7792ef01b9750df4cd91e5a4b26fa

**Describe the problem**
When I try to build TensorFlow Lite C API dynamic library for Windows using MSVC 2019 compiler with CMake config I get debug target compiled (not release target as expected).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- install Microsoft Visual Studio Community 2019 with CMake, git and command prompt tools components
- run `x86 Native Tools Command Prompt for VS 2019`
- run the following commands within the terminal:
```
git clone --depth 1 https://github.com/tensorflow/tensorflow.git
cmake tensorflow/tensorflow/lite/c -DTFLITE_C_BUILD_SHARED_LIBS=ON -Bbuild
cmake --build build -j 8
```
After the compilation there's `build/Debug/tensorflowlite_c.dll` which is built as debug target. It doesn't help to provide `-DCMAKE_BUILD_TYPE=Release` - CMake still builds debug target.

Expected behavior is that CMake builds release target.

**Any other info / logs**
Here's full log from the commands above: [cmake-msvc-win.txt](https://github.com/tensorflow/tensorflow/files/6191359/cmake-msvc-win.txt)
"
48019,steps_per_epoch documentation not in accordance with the tutorials on the tf website,"The documentation for the `steps_per_epoch` argument to the `tf.keras.Model.fit()` function, located [here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/engine/training.py#L942), specifies that:

> If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted.

Also, the default value of `steps_per_epoch` is **None**.

In the [Load images tutorial](https://www.tensorflow.org/tutorials/load_data/images#train_a_model), the `train_ds` variable is passed to `Model.fit()` for training, and is a `tf.data.Dataset` on which several `.map()`s have been applied before:

![image](https://user-images.githubusercontent.com/25421460/112180016-74f1fc00-8bfb-11eb-8549-1dd9f00937bd.png)

Following the documentation, how can the steps (underlined in red in the picture) be displayed?

Please feel free to ask if you need any more detail!"
48018,TensorFlow Lite conversion error when using the slice semantics to expand the last dimension,"### 1. System information

- Linux 5.11.4-arch1-1 #1 SMP PREEMPT Sun, 07 Mar 2021 18:00:49 +0000 x86_64 GNU/Linux
- TensorFlow 2.4.1 from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)

### 2. Code

Please unzip this protobuf file before running the code: [tflite-strided-slice-fail.zip](https://github.com/tensorflow/tensorflow/files/6191669/tflite-strided-slice-fail.zip)


```python
# use tensorflow API v1
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

PROTOBUF_PATH = ""tflite-strided-slice-fail.pb""

with tf.gfile.GFile(PROTOBUF_PATH, ""rb"") as fprotobuf:
    graphDef = tf.GraphDef()
    graphDef.ParseFromString(fprotobuf.read())

X = tf.placeholder(dtype=tf.float32, shape=[1, 309], name=""X"")
P = tf.placeholder(dtype=tf.float32, shape=[1], name=""P"")

Yhat, = tf.import_graph_def(
    graphDef,

    input_map={
        ""runtime/X:0"": X,
        ""runtime/P:0"": P,
    },
    name=""model"",
    return_elements=[f""runtime/Y/fast/32:0""]
)

with tf.Session() as session:
    converter = tf.lite.TFLiteConverter.from_session(
        session,
        input_tensors=[X, P],
        output_tensors=[Yhat]
    )
    flatbuffer = converter.convert()
```

### 3. Failure after conversion
```
ConverterError: /usr/lib/python3.9/site-packages/tensorflow/python/framework/importer.py:400:0: error: 'tf.StridedSlice' op is neither a custom op nor a flex op
/usr/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:538:0: note: called from
/home/barabanus/work/4a-games/pfnn-tools/test/tflite-fail-1.py:14:0: note: called from
/usr/lib/python3.9/site-packages/IPython/utils/py3compat.py:168:0: note: called from
/usr/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2740:0: note: called from
/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:377:0: note: called from
/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:452:0: note: called from
/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:328:0: note: called from
/usr/lib/python3.9/site-packages/IPython/terminal/ipapp.py:323:0: note: called from
/usr/lib/python3.9/site-packages/traitlets/config/application.py:87:0: note: called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.StridedSlice {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
```

### 5. Any other info / logs
The problem has to do with the slice semantics when I try to expand the last dimension of a tensor like `tensor[..., None]` to match matrix multiplication dimensions requirement. Usually it works, but I found an example of computational graph which fails to convert with TFLite.

The problem has been solved by replacing the slice semantics `X[..., None]` with `tf.expand_dims(X, axis=-1)` and `Y[0]` with `tf.squeeze(Y, axis=0)`, but it should have been worked with the slice semantics.

The full log:
```
2021-03-23 19:32:32.072655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:From /usr/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-03-23 19:32:33.177145: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:32:33.177749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-23 19:32:33.290409: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-23 19:32:33.290438: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rb15): /proc/driver/nvidia/version does not exist
2021-03-23 19:32:33.290815: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-23 19:32:33.291632: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:32:33.291674: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-03-23 19:32:33.294267: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-03-23 19:32:33.294368: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-03-23 19:32:33.294574: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:32:33.310613: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200660000 Hz
2021-03-23 19:32:33.312516: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.005ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.

2021-03-23 19:32:33.318140: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2021-03-23 19:32:33.318275: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-03-23 19:32:33.318494: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:32:33.320979: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.003ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.

2021-03-23 19:32:33.331507: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.
2021-03-23 19:32:33.331542: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.
2021-03-23 19:32:33.341942: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 19:32:33.341994: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
loc(callsite(""model/runtime/strided_slice_26""(""/usr/lib/python3.9/site-packages/tensorflow/python/framework/importer.py"":400:0) at callsite(""/usr/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py"":538:0 at ""/home/barabanus/work/4a-games/pfnn-tools/test/tflite-fail-1.py"":14:0))): error: 'tf.StridedSlice' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.StridedSlice {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
Traceback (most recent call last):
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 210, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: /usr/lib/python3.9/site-packages/tensorflow/python/framework/importer.py:400:0: error: 'tf.StridedSlice' op is neither a custom op nor a flex op
/usr/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:538:0: note: called from
/home/barabanus/work/4a-games/pfnn-tools/test/tflite-fail-1.py:14:0: note: called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.StridedSlice {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/barabanus/work/4a-games/pfnn-tools/test/tflite-fail-1.py"", line 31, in <module>
    flatbuffer = converter.convert()
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1947, in convert
    return super(TFLiteConverter, self).convert()
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1300, in convert
    result = _toco_convert_impl(
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 608, in toco_convert_impl
    data = toco_convert_protos(
  File ""/usr/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: /usr/lib/python3.9/site-packages/tensorflow/python/framework/importer.py:400:0: error: 'tf.StridedSlice' op is neither a custom op nor a flex op
/usr/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:538:0: note: called from
/home/barabanus/work/4a-games/pfnn-tools/test/tflite-fail-1.py:14:0: note: called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.StridedSlice {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
```"
48017,Can't install Tensorflow for Golang,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>



**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): go get
- TensorFlow version: 2.4.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip, conda
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version:-
- GPU model and memory: virtgl



**Describe the problem**

Can't install Tensorflow for Golang

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
~$ sudo go get github.com/tensorflow/tensorflow/tensorflow/go
[sudo] heslo pre používateľa gg187on: 
go: downloading github.com/tensorflow/tensorflow v1.15.5
go: downloading github.com/tensorflow/tensorflow v2.4.1+incompatible
go: downloading github.com/golang/protobuf v1.5.1
go: downloading google.golang.org/protobuf v1.26.0
github.com/tensorflow/tensorflow/tensorflow/go imports
    github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find module providing package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
48016,XLA CPU CNN compilation SegFault,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 / macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 3da7b09556355b54d324238ea99c376c9713a414
- Python version: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

We're working on XLA bindings over at [Nx](https://github.com/elixir-nx/nx). We're currently working on a high-level API for writing Neural networks. We have a CNN that basically looks like:

```elixir
input({32, 3, 32, 32})
|> conv(32, kernel_size: {3, 3}, activation: :relu)
|> batch_norm()
|> avg_pool(kernel_size: {2, 2})
|> conv(64, kernel_size: {3, 3}, activation: :relu)
|> batch_norm()
|> avg_pool(kernel_size: {2, 2})
|> conv(64, kernel_size: {3, 3}, activation: :relu)
|> batch_norm()
|> flatten()
|> dense(64, activation: :relu)
|> dropout()
|> dense(10, activation: :log_softmax)
```

Unfortunately, the program SegFaults during compilation when using XLA CPU. We have previously successfully compiled and run the same network using XLA GPU. GDB Backtrace indicates this happens somewhere in LLVM. I can provide HLO Dumps as well.

**Other info / logs**

<details><summary>GDB Backtrace</summary>
<pre>
#0  0x00007f68bb04ee09 in llvm::MemorySSA::getOrCreateAccessList(llvm::BasicBlock const*) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#1  0x00007f68bb04f4af in llvm::MemorySSA::insertIntoListsForBlock(llvm::MemoryAccess*, llvm::BasicBlock const*, llvm::MemorySSA::InsertionPlace) ()
   from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#2  0x00007f68bb050033 in llvm::MemorySSA::createMemoryPhi(llvm::BasicBlock*) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#3  0x00007f68bb061876 in llvm::MemorySSAUpdater::getPreviousDefRecursive(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#4  0x00007f68bb0628f6 in llvm::MemorySSAUpdater::getPreviousDefFromEnd(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#5  0x00007f68bb0617db in llvm::MemorySSAUpdater::getPreviousDefRecursive(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#6  0x00007f68bb0628f6 in llvm::MemorySSAUpdater::getPreviousDefFromEnd(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#7  0x00007f68bb0617db in llvm::MemorySSAUpdater::getPreviousDefRecursive(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#8  0x00007f68bb0628f6 in llvm::MemorySSAUpdater::getPreviousDefFromEnd(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#9  0x00007f68bb0619ab in llvm::MemorySSAUpdater::getPreviousDefRecursive(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
#10 0x00007f68bb0628f6 in llvm::MemorySSAUpdater::getPreviousDefFromEnd(llvm::BasicBlock*, llvm::DenseMap<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess>, llvm::DenseMapInfo<llvm::BasicBlock*>, llvm::detail::DenseMapPair<llvm::BasicBlock*, llvm::TrackingVH<llvm::MemoryAccess> > >&) () from /home/sean/projects/axon/_build/dev/lib/exla/priv/libexla.so
</pre>
</details>
"
48015,Tensorflow C++ get_visible_devices,"### System information

-   I wrote custom code
-   Windows 10 64-bit
-   Tensorflow compiled from source
-   Tensorflow v2.4
-   Python v3.8
-   Bazel v3.1
-   Visual Studio Build Tools 2019
-   CUDA 11.0

### Describe the problem
Hello, I am trying to get the visible GPU devices in my C++ program that integrates Tensorflow compiled from source. I can't find the C++ equivalent function of the Python tf.config.get_visible_devices() documented here https://www.tensorflow.org/api_docs/python/tf/config/get_visible_devices .
Is this function or a similar one implemented in C++?
I found a similar function in the GPUOptions but I got a linking error when try to building the program and I am not sure that the function is meant to read the available GPU devices.

### Source code / logs
ConfigProto* config = &options.config;
auto gpu_options = config->gpu_options();
string pi = gpu_options.visible_device_list();

Thank you,
Mauro
"
48014,Couldn't build android C++ .so libs with bazel and tensorflow as a remote dependency.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Source, current master branch (23.03.2021)
- TensorFlow version: Current master branch(23.03.2021), latest tag v2.4.1
- Python version: 3.8
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: Not Used
- GPU model and memory: Not Used



**Describe the problem**
I am trying to build on C++ .so on top of tensorflow with bazel. I am trying to add tensorflow as a remote repository just like tensorflow_serving ( https://github.com/tensorflow/serving ).
I can build for native targets(linux x86_64) all fine, when I tried to build for **--config android** I got errors that android is not defined, so I copy/pasted the .bazelrc file from tensorflow, and updated .tf_configure.bazelrc. Than the build started but it is failing 
for this target: 

``` 
ERROR: /home/flamur/.cache/bazel/_bazel_flamur/8aff49c09e177f51e03fa24b3bf8aaa8/external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/BUILD:486:22: Generating flatbuffer files for serialization_cc_fbs_srcs: @org_tensorflow//tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs failed: (Exit 1): bash failed: error executing command 
  (cd /home/flamur/.cache/bazel/_bazel_flamur/8aff49c09e177f51e03fa24b3bf8aaa8/execroot/this_repo && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=30.0.0-rc4 \
    ANDROID_NDK_API_LEVEL=24 \
    ANDROID_NDK_HOME=/home/flamur/Android/Sdk/ndk-bundle \
    ANDROID_SDK_API_LEVEL=30 \
    ANDROID_SDK_HOME=/home/flamur/Android/Sdk \
    PATH=/home/flamur/anaconda3/envs/flamxi/bin:/home/flamur/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/flamur/bin:/home/flamur/.cargo/bin:/snap/bin:/home/flamur/anaconda3/envs/flamxi/bin:/home/flamur/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/flamur/bin:/snap/bin:/home/flamur/Documents/CLion-2020.2.4/clion-2020.2.4/bin:/home/flamur/Documents/CLion-2020.2.4/clion-2020.2.4/bin \
    PYTHON_BIN_PATH=/home/flamur/anaconda3/envs/flamxi/bin/python3 \
    PYTHON_LIB_PATH=/home/flamur/anaconda3/envs/flamxi/lib/python3.7/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/serialization.fbs; do bazel-out/host/bin/external/flatbuffers/flatc --scoped-enums -I ./  -c -o bazel-out/armeabi-v7a-opt/bin/external/org_tensorflow/tensorflow/lite/delegates/gpu/cl $f; done')
Execution platform: @local_execution_config_platform//:platform
error: /home/flamur/.cache/bazel/_bazel_flamur/8aff49c09e177f51e03fa24b3bf8aaa8/external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/serialization.fbs:15: 75: error: unable to load include file: tensorflow/lite/delegates/gpu/common/task/serialization_base.fbs
Target //skeleton:centernet failed to build
INFO: Elapsed time: 0.131s, Critical Path: 0.01s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully

```
I checked that file **serialization.fbs** and it seems like having a relative path for this include:
```
include ""tensorflow/lite/delegates/gpu/common/task/serialization_base.fbs"";
```
I am guessing this could be the problem! As the error says that cannot find this file.

If you want more files like WORKSPACE or .bazelrc please mention that, I don't know if they are required at this state! 

For the target that I get this error I tested it with tensorflow as local dependency it works all fine, also with the remote dependency the non android targets are builded succesfully! 
"
48013,RuntimeError: Input pipelines based on Queues are not supported when eager execution is enabled. Please use tf.data to ingest data into your model instead.,"OS Platform and Distribution (e.g., Linux Ubuntu 20.04): RHEL 7.7
TensorFlow version: 2.4.1
Python version: 3.8.5
CPU
NVIDIA Corporation GP102
I try to run this code
python -m CycleGAN_TensorFlow.create_cyclegan_dataset --image_path_a=folder_a --image_path_b=folder_b --dataset_name=""horse2zebra_train"" --do_shuffle=0

I have this error
RuntimeError: Input pipelines based on Queues are not supported when eager execution is enabled. Please use tf.data to ingest data into your model instead."
48012,How to pass -add_postprocessing_op=true while using tflite_convert in tensorflow2.x,"How we can pass -add_postprocessing_op=true while creating PB file for tflite_convert 

python object_detection/export_tflite_ssd_graph.py --pipeline_config_path=pipeline.config --trained_checkpoint_prefix=model.ckpt-4000 --output_directory=saved_model --add_postprocessing_op=true

Above scripts should output two files In saved_model directory: tflite_graph.pb and tflite_graph.pbtxt

Above script i used in tensorflow1.13 if it need to be changed or modify please share the solution, thanks in advance

Regards
Vivek"
48011,How to generate tflite file for object detection on android in tensorflow 2.3,"I am using SSD pretrained model ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8, I trained it on tensorflow 2.3, after checkpoint has been generated i have downloaded it on my windows machine.

Can someone please share scripts require to generate tflite file for android, also following scripts works for me in tensorflow 1.13 which is not supported in 2.x

python object_detection/export_tflite_ssd_graph.py --pipeline_config_path=pipeline.config --trained_checkpoint_prefix=model.ckpt-4000 --output_directory=saved_model --add_postprocessing_op=true
	
tflite_convert --graph_def_file=tflite_graph.pb --output_file=saved_model.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --default_ranges_min=0 --default_ranges_max=255 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops
"
48010,"Getting error in each second step in model training in gcloud tensorflow2.3 python 3.7, error - model_lib_v2.py:651","Hi while running a training job in gcloud, continuously getting following error in each step, why this is occurring and how to resolve it

I 2021-03-23T10:34:36.455145835Z master-replica-0 Step 20900 per-step time 11.124s loss=1.705 master-replica-0 
E 2021-03-23T10:34:36.455868005Z master-replica-0 I0323 10:34:36.455145 140138105964352 model_lib_v2.py:651] Step 20900 per-step time 11.124s loss=1.705 master-replica-0 
{
 insertId: ""1m02xqjg250xpg0""  
 jsonPayload: {
  created: 1616495676.455868   
  levelname: ""**ERROR**""   
  lineno: 328   
  message: ""I0323 10:34:36.455145 140138105964352 model_lib_v2.py:651] Step 20900 per-step time 11.124s loss=1.705""   
  pathname: ""/runcloudml.py""   
 }
 labels: {…}  
 logName: ""projects/tuberculosis-240217/logs/master-replica-0""  
 receiveTimestamp: ""2021-03-23T10:34:39.125659068Z""  
 resource: {…}  
 severity: ""ERROR""  
 timestamp: ""2021-03-23T10:34:36.455868005Z""  
}

"
48009,Allow Server-Side Encryption KMS option when uploading objects to S3 with Tensorflow,"
**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Productive systems frequently impose a requirement to use server-side encryption (SSE) to protect all data stored on S3, e.g. via [AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html).

Tensorflow (and tensorboard) has an integration with S3, but it does not seem to support specifying SSE options for write operations. 

Is there a way in the current tensorflow code to specify SSE options for file uploads to S3?

If not, I'd suggest to add such a feature, e.g. by exposing an environment variable for extra arguments to S3 file upload operations (S3_UPLOAD_EXTRA_ARGS).

Current behaviour in this scenario is to issue an  error:
`tensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location [Op:CreateSummaryFileWriter]`

when following this stock example but using an S3 bucket with SSE enabled: [link](https://www.tensorflow.org/tensorboard/get_started#using_tensorboard_with_keras_modelfit)


**Will this change the current api? How?**

This would not change the current api if the function that uploads data to S3 reads an environment variable and passes relevant information on to AWS SDK.


**Who will benefit with this feature?**

Any users of Tensorflow in production systems as they will typically use server-side encryption (e.g. kms) with S3.

"
48007,Converted Object Detection model trained with q-aware training still has fake quantization layers,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 1.15

### 2. Code

I Trained a MobileNetV2SSD using the Object Detection API including this block:
```
graph_rewriter {
  quantization {
    delay: 0
    weight_bits: 8
    activation_bits: 8
  }
}
```

Then I exported the model for tflite using the command `python object_detection/export_tflite_ssd_graph.py 
`.
Finally, I converted to tflite using this code:
```
import tensorflow as tf 
from PIL import Image
import numpy as np 
import os
import sys

def _representative_dataset_gen(): 
    images_path = ""/work/data/repr_dataset""
    if images_path is None: 
        raise Exception( 
             ""Image directory is None, full integer quantization requires images directory!"" 
        ) 
    imagePaths = os.listdir((images_path)) 
    for p in imagePaths: 
        image = Image.open(os.path.join(images_path, p))
        image = image.resize((300, 300)) 
        image = np.asarray(image)
        image = np.expand_dims(image, axis=1) 
        image = image.reshape(1, 300, 300, 3)
        yield [image.astype(""float32"")]

saved_model_path = sys.argv[1] 
input_arrays = [""normalized_input_image_tensor""] 
output_arrays = ['TFLite_Detection_PostProcess', 
    'TFLite_Detection_PostProcess:1', 
    'TFLite_Detection_PostProcess:2', 
    'TFLite_Detection_PostProcess:3']
input_shapes = {""normalized_input_image_tensor"" : [1, 300, 300, 3]} 
converter = tf.lite.TFLiteConverter.from_frozen_graph(saved_model_path,input_arrays, 
     output_arrays, input_shapes) 
converter.allow_custom_ops = True 
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.representative_dataset = _representative_dataset_gen 


tflite_model_quant = converter.convert() 
with open('quantized.tflite', ""wb"") as tflite_file: 
     tflite_file.write(tflite_model_quant)
```

### 3. Failure after conversion
The converted model includes quantization/dequantization/fake quantization layers:
![quantized tflite](https://user-images.githubusercontent.com/8984671/112117585-cbd7e100-8bbb-11eb-8b20-325fb7f98365.png)
As a result, the inference time on an embedded system equipped with a NPU+NNAPI delegate is very high. I don't think this is normal. Is there a way to fix it?"
48005,gradient wrt input not working after applying tfmot.quantization.keras.quantize_model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
After applying tfmot.quantization.keras.quantize_model to VGG16 model imported from keras:
1. If I do tf.gradient(loss, input_tensor) directly, it will give me None which implies no relationship between input and the loss which is not expected.
2. After I do  tf.gradient(loss, q_model.get_layer('quant_block1_conv1').input), things seem working fine. But after I pass an input with valid shape and dtype. it will give me an error complaining about the input.

**Describe the expected behavior**

All of these mentioned above did not happen to the original model.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1_UAEoZDamfmtnyiMEkMVlbdzwf9qCbwm?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

: ---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-11-dac3dfc084ae> in <module>
----> 1 _,l,grads_value = my_func2(img)

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3822       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
   3823 
-> 3824     fetched = self._callable_fn(*array_vals,
   3825                                 run_metadata=self.run_metadata)
   3826     self._call_fetch_callbacks(fetched[-len(self._fetches):])

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1468       try:
   1469         run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
-> 1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
   1472                                                run_metadata_ptr)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: You must feed a value for placeholder tensor 'input_1_5' with dtype float and shape [?,224,224,3]
	 [[{{node input_1_5}}]]
	 [[Func/gradients/quant_block2_conv1/cond_1_grad/StatelessIf/then/_1388/input/_3903/_1395]]
  (1) Invalid argument: You must feed a value for placeholder tensor 'input_1_5' with dtype float and shape [?,224,224,3]
	 [[{{node input_1_5}}]]
"
48004,MLIR binaries build fails due to missing dependency declaration,"With the TF git version HEAD at 

commit b725e835c68bb50bea582cac0aceeb828d3d6ea9 (HEAD -> master, origin/master, origin/HEAD)
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Mar 22 22:00:12 2021 -0700

$ bazel build --copt=-UNDEBUG --linkopt='-fuse-ld=lld'  tensorflow/compiler/mlir:all

yields:

```
INFO: Analyzed 15 targets (0 packages loaded, 0 targets configured).
INFO: Found 15 targets...
ERROR: /home/uday/tensorflow-upstream/tensorflow/compiler/xla/service/gpu/BUILD:437:16: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/service/gpu:nccl_collective_thunks':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/service/gpu/nccl_all_gather_thunk.cc':
  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'
INFO: Elapsed time: 39.406s, Critical Path: 36.14s
INFO: 329 processes: 67 internal, 262 local.
```

System information
Ubuntu Linux 20.04.02 LTS
bazel 3.7.2
gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
TF configured with GPU/CUDA support

CC: @jpienaar @joker-eph 
"
48003,Multiple inputs with diffrent shapes,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

When working with a graph we usually provide as inputs to the model a feature matrix (Batch, nb_nodes, nb_features) or (Batch, time, nb_nodes, nb_features) and adjacency matrix (batch, nb_nodes, nb_nodes) or (nb_nodes, nb_nodes)

1. differences of input shapes are different something that isn't allowed in model.fit(), model.predict() or model.evaluate() (for this we can use a custom training and evaluation step that allow us to handle the difference in shape, however, is it possible to still use model.fit() in this kind of situation?)
2. if using tf.data.Dataset.from_tensor_slices it returns `tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match:` (is it possible to create tf.data.Dataset with x as multiple inputs with diffrent shapes?)

**Will this change the current API? How?**

Flexibility in input shapes

**Who will benefit from this feature?**

Creating support for GNN

**Any Other info.**
"
48002,CUDA_ERROR_UNKNOWN in Docker Rootless Mode.,"## Summary
Tensorflow docker image fails to use GPU

## Configuration
1. Debian 10
2. Nvidia driver 460.32.03
3. 3 RTX 2080, 128 GB of Memory

## Explanation
I was trying to set up a Tensorflow container in rootless-mode. I had to jump through a few hoops, like disabling cgroups and manually installing slirp4netns (v1.1.4).

I tried the docker containers tensorflow/tensorflow:latest-gpu and tensorflow/tensorflow:2.4.1-gpu both seem to not work.

I run
```
docker run -it --rm --gpus all tensorflow/tensorflow:latest-gpu python
```
and everything starts normally. nvidia-smi reports all three of my gpu's. If I try to test functionality I get unknown error that I don't know how to track down in the container.

```
>>> import tensorflow as tf
2021-03-23 05:58:32.641242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0

>>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
2021-03-23 05:59:00.670935: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-23 05:59:00.673743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-23 05:59:00.705979: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-03-23 05:59:00.706237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 45f73818aefd
2021-03-23 05:59:00.706366: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 45f73818aefd
2021-03-23 05:59:00.706791: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3
2021-03-23 05:59:00.706968: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3
2021-03-23 05:59:00.707088: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3
Num GPUs Available:  0
```

## Extra Information
If it helps I tried to use these resources.

https://docs.docker.com/engine/install/debian/.
https://docs.docker.com/engine/security/rootless/
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker

"
48001,No OpenCL for GPU delegate on Android 12,"* Device: Google Pixel 4
* OS: Android 12 developer preview 2
* TensorFlow Lite: 2.4.0
* TensorFlow Lite GPU delegate: 2.4.0
* TensorFlow Lite support: 0.1.0

Calling `Interpreter.runForMultipleInputsOutputs` with pre-configured GPU delegate results in the following message:

```
CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
DEQUANTIZE: 
310 operations will run on the GPU, and the remaining 2 operations will run on the CPU.
Can not open OpenCL library on this device - dlopen failed: library ""libOpenCL.so"" not found
Falling back to OpenGL
```

There is no such message on Android 11 (and the same device). As a result, the executing time is higher than it was before (with OpenCL available)."
47999,TensorFlow Lite file Conversion ,"
I have trained a Tfrecord file and converted into tflite file. I have used roboflow instructions as I don't have any knowledge on this domain. As per instructions https://gist.github.com/kishorekethineni/7b170822f27019d2f142be117f9855af I had run all the mentioned scripts exactly and generted tflite file successfully also checked tfilte file for input and output arrays worked fine there. required files: https://drive.google.com/drive/folders/1JnOp1ZXQCKR4RyNbg3BMuogJMOd9YmHg?usp=sharing



### 2. Code

https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing (GoogleColab)
(can check code here..)


!tflite_convert \
  --input_shape=1,300,300,3 \
  --input_arrays=normalized_input_image_tensor \
--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 \
  --allow_custom_ops \
  --graph_def_file=/content/models/research/fine_tuned_model/tflite/tflite_graph.pb \
  --output_file=""/content/models/research/fine_tuned_model/final_model.tflite""

https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing (GoogleColab)

https://drive.google.com/drive/folders/1JnOp1ZXQCKR4RyNbg3BMuogJMOd9YmHg?usp=sharing (TensorFlow Lite file)


```

### 3. Failure after conversion
Model was not producing any detection


"
47998,attr.s decorated keras Layers broken in keras.Sequential,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.1  (installed with miniconda)
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX1660, 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

If a custom layer implementation uses the `@attr.s` decorated (e.g. for parameter validation), it will break its use in `Sequential`. It otherwise seems to function correctly in other usages.

Within `Sequential`, the decorated custom layer is flattened into its attributes used for creation instead of a Layer object.

**Describe the expected behavior**

Using the `@attr.s` should not break its use in `Sequential`.
`nest.flatten` in utils should keep the custom layer as a `Layer` object.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import attr

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model, Sequential

@attr.s(eq=False)
class TestLayer(layers.Layer):
    _n_out: int = attr.ib(validator=lambda i, a, x: x>=8)
    
    def __attrs_post_init__(self):
        super().__init__()
        self.fn = layers.Conv2D(self._n_out, 3, activation='relu')
    
    def call(self, x, **kwargs):
        return self.fn(x, **kwargs)
    

m = Sequential([TestLayer(8), layers.Dense(1)])

m.build([None, 128, 128, 3])
```

Possible cause of issue from within `Sequential` implementation:
```
from tensorflow.python.util import nest

nest.flatten(TestLayer(8)) # returns [8], should be [ TestLayer at <0x???????>]
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-9c0118781b91> in <module>
     13 m = Sequential([TestLayer(8), layers.GlobalMaxPooling2D(), layers.Dense(1)])
     14 
---> 15 m.build([None, 128, 128, 3])

~/miniconda3/envs/genesis_dev/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in build(self, input_shape)
    345       if input_shape is None:
    346         raise ValueError('You must provide an `input_shape` argument.')
--> 347       self._build_graph_network_for_inferred_shape(input_shape)
    348       if not self.built:
    349         input_shape = tuple(input_shape)

~/miniconda3/envs/genesis_dev/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/miniconda3/envs/genesis_dev/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in _build_graph_network_for_inferred_shape(self, input_shape, input_dtype)
    320               raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)
    321             # Keep track of nodes just created above
--> 322             track_nodes_created_by_last_call(layer, created_nodes)
    323             layer_input = layer_output
    324             outputs = layer_output

~/miniconda3/envs/genesis_dev/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in track_nodes_created_by_last_call(layer, created_nodes)
    566   prev_layers = layer._inbound_nodes[-1].inbound_layers
    567   for prev_layer in nest.flatten(prev_layers):
--> 568     if prev_layer._outbound_nodes:
    569       created_nodes.add(prev_layer._outbound_nodes[-1])

AttributeError: 'int' object has no attribute '_outbound_nodes'
```
"
47996,Segmentation fault for tflite Interpreter,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): nightly pypi
- TensorFlow version (use command below): v1.12.1-53437-g58a2aa4a353 2.5.0-dev20210322
- Python version: 3.7.6

**Describe the current behavior**

Getting `Segmentation fault (core dumped)`

**Describe the expected behavior**

No segmentation faults so I can figure out what's wrong with my script, e.g. how do I pass it a base64 image, like this with serving:

```python
import json
from base64 import b64encode

# encode img
with img.open(""rb"") as image_file:
    img_data = b64encode(image_file.read())
    data = json.dumps({
        ""signature_name"": ""serving_default"",
        ""inputs"": {
            ""input"": {
                ""b64"": img_data.decode('utf-8')
            }
        }
    })
# make request
res = requests.post(url, data=data)
response = res.json()
```

**Standalone code to reproduce the issue**
```python
import argparse
import time
from io import BytesIO
from pprint import pprint
from pathlib import Path
from base64 import b64encode

import numpy as np
import tensorflow as tf # TF2


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '-i',
      '--image',
      default='captchas/captcha-0b81fccb-85e3-4ae4-8da5-f090b9dc7ada.jpg',
      help='image to be classified')
  parser.add_argument(
      '-m',
      '--model_file',
      default='model.tflite',
      help='.tflite model to be executed')
  parser.add_argument(
      '--num_threads', default=1, type=int, help='number of threads')
  args = parser.parse_args()

  interpreter = tf.lite.Interpreter(
      model_path=args.model_file
  )
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  with Path(args.image).open(""rb"") as image_file:
      img_data = b64encode(image_file.read())
      img_tensor = img_data.decode('utf-8')

  interpreter.set_tensor(input_details[0]['index'], img_tensor)

  start_time = time.time()
  interpreter.invoke()
  stop_time = time.time()

  output_data = interpreter.get_tensor(output_details[0]['index'])
  results = np.squeeze(output_data)

  print('time: {:.3f}ms'.format((stop_time - start_time) * 1000))
```

**Other info / logs**

Script output:

```
INFO: Created TensorFlow Lite delegate for select TF ops.
2021-03-23 03:00:54.956373: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO: TfLiteFlexDelegate delegate: 17 nodes delegated out of 1614 nodes with 10 partitions.

INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 2 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 4 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 4 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 1 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 14 nodes delegated out of 30 nodes with 3 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 6 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 7 nodes with 2 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 6 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 7 nodes with 2 partitions.

Segmentation fault (core dumped)
```
"
47994,tf.data.experimental.snapshot should allow providing custom hash,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Ues



**Describe the feature and the current behavior/state.**
Users may want to specify a custom hash_code to control the behavior of when snapshot enters read vs. write mode.
As an example: If snapshot is generated outside of training, and the dataset call-chain/graph is guaranteed to be unchanged,
then a simple hash function can checksum all the input files and generate a hash_code.

As long as the input dataset remains the same during training, the hash function can generate the same hash_code, and the
generated snapshots would be read. If the input dataset is different, the hash function would generate a new hash_code,
and therefore the SnapshotDataset op would generate new snapshots.
**Will this change the current api? How?**
It will expose a new optional parameter `hash_code` in the current `tf.data.experimental.snapshot` api.
**Who will benefit with this feature?**
All users of `tf.data.experimental.snapshot` who
**Any Other info.**
This extends the capability exposed in the C++ API: https://github.com/tensorflow/tensorflow/commit/cbc7f31b7d6aac81158be084201d3b3e8e346907
"
47991,"Python 3.6.12-debug is not supported, but documentation says it is","This works:
```
pyenv shell 3.6.12
pip install --upgrade pip
pip install tensorflow
```
This also works:
```
pyenv shell 3.8.5-debug
pip install --upgrade pip
pip install tensorflow
```

But this:
```
pyenv shell 3.6.12-debug
pip install --upgrade pip
pip install tensorflow
```
says 
```
ERROR: Could not find a version that satisfies the requirement tensorflow
ERROR: No matching distribution found for tensorflow
```
And, afaik, this is nowhere documented

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): All versions
- Python version: 3.6.12-debug
- CUDA/cuDNN version: Different versions
- GPU model and memory: Different versions

"
47989,DOC: CONTRIBUTING.md Sanity Check guide not working,"If I try running the [instructions in CONTRIBUTING.md](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#running-sanity-check):

```bash
tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh
```

I get:

```
#10 1.118 /install/install_pip_packages.sh: line 21: python3.6: command not found
```

However, if I instead use the `cpu-py3.6` image, I get a bit further:

```bash
addgroup: Please enter a username matching the regular expression configured via the NAME_REGEX[_SYSTEM] configuration variable.
 Use the `--force-badname' option to relax this check or reconfigure NAME_REGEX.
```

I don't see any way for users to fix this without diving into the plethora of different Dockerfiles and scripts-calling scripts that exist in the repo."
47985,Invalid format in tf.math.log doc page,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/math/log

## Description of issue (what needs changing):

### Clear description

<img width=""1039"" alt=""Screen Shot 2021-03-23 at 5 12 34 AM"" src=""https://user-images.githubusercontent.com/8815362/112052367-6291a700-8b96-11eb-8535-c874920d66b4.png"">

The example section is broken.

https://github.com/tensorflow/tensorflow/blob/e13f434909c7007b14f7930370472f26fba06f4e/tensorflow/core/api_def/python_api/api_def_Log.pbtxt#L13-L22

Maybe the above lines are invalid and markdown-style codeblock lines(```) should be removed like [tensorflow/core/api_def/python_api/api_def_Log1p.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/python_api/api_def_Log1p.pbtxt)."
47983,ERROR: cannot compute CombinedNonMaxSuppression as input #0(zero-based) was expected to be a float tensor but is a uint8 tensor 	 (while executing 'CombinedNonMaxSuppression' via Eager) ERROR: Node number 644 (TfLiteFlexDelegate) failed to invoke.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04/Android 10
- Mobile device if the issue happens on a mobile device: Xiaomi Redmi Note 7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf-nightly-gpu==2.5.0.dev20210322
- Python version: 3.8

**Describe the current behavior**
I am currently running a benchmark on android and the following error is raised:
```
ERROR: cannot compute CombinedNonMaxSuppression as input #0(zero-based) was expected to be a float tensor but is a uint8 tensor (while executing 'CombinedNonMaxSuppression' via Eager)
ERROR: Node number 644 (TfLiteFlexDelegate) failed to invoke.
```

The thing is I do not have control over the inner operations in the TFLite model. Moreover, I am already casting it into `tf.float32` when building the `tf.keras.Model`:

```
boxes_to_nms = tf.expand_dims(bboxes, axis=2)
boxes = tf.cast(boxes_to_nms, tf.float32)
scores = tf.cast(scores_to_nms, tf.float32)

nms_output = tf.image.combined_non_max_suppression(
    boxes=boxes,
    scores=scores,
    ...
    )
```

Does someone know how to deal with this ERROR, because I do not find any relevant information online and I feel quite powerless here. Thanks in advance!

**Standalone code to reproduce the issue**
I am running the benchmark using the native tool which includes TFOps and Flex Delegate that can be downloaded [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_arm_benchmark_model_plus_flex).

My model can be downloaded [removed].
I run the benchmark on an android divide using the following lines:
```
adb push android_arm_benchmark_model_plus_flex /data/local/tmp/benchmark
adb shell chmod +x /data/local/tmp/benchmark
adb push model.tflite /data/local/tmp/model.tflite
adb shell ""/data/local/tmp/benchmark""  --graph=""/data/local/tmp/model.tflite"" --input_layer=input --input_layer_shape=1,800,1024,3
```

**Other informations**
The node number in the error is 644 but when I visualize the model using [netron](https://netron.app/), the CombinedNonMaxSuppression has the number 643 which is unexpected."
47981,Infinite loop when using reduce() inside tf.data.Dataset.map(),"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: Binary
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.5
- CUDA/cuDNN version: None
- GPU model and memory: None


**Describe the current behavior**
I'm trying to create a 3D image dataset using my previous dataset of sorted 2D images. To do so, I use map() method of tf.data.Dataset along with a reduce() function that takes some 2D images from the same dataset (or a copy, from what I tried it doesn't matter) and adds them together. Nevertheless, when I use functools.reduce() or tf.foldl() functions inside of a tf.data.Dataset.map(), even in a simpler case, it enters an infinite loop.
For the example, I've simplified the case to a dummy dataset of 2 elements with shape (2, 3). And I'm using reduce() instead of tf.fold() to avoid using a generator in the input of the function.
The result is also an infinite loop.

**Describe the expected behavior**
In my understanding, I should get a new dataset with one element of shape (1, 2, 3). But if this transformation is not possible, I think it should raise an error.

**Standalone code to reproduce the issue**
import tensorflow as tf
from functools import reduce

ds = tf.data.Dataset.from_tensor_slices([[[1,2,3],[4,5,6]],[[7,8,9], [10,11,12]]])
new_ds = ds.map(lambda _: reduce(lambda a, b: tf.concat([tf.expand_dims(a[0], axis=0),tf.expand_dims(b[0], axis=0)], 0), ds))
"
47979,ImportError: DLL load failed while importing _pywrap_tensorflow_internal,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): idk
- TensorFlow version: 2.4.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): idk
- GCC/Compiler version (if compiling from source): idk
- CUDA/cuDNN version: idk
- GPU model and memory: n/a



**Describe the problem**
I am trying to install and use DeepLabCut on my CPU, which requires Tensorflow. Tensorflow 2.4.1 seems to be installed but whenever I try to import it I get error messages saying 

""ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

Failed to load the native TensorFlow runtime.""

To address common fixes I see for this error - I have the most recent update of Visual C++, I am running 64-bit Python on a 64 bit machine, and my machine supports AVX2 (here are my specs https://ark.intel.com/content/www/us/en/ark/products/78937/intel-core-i7-4810mq-processor-6m-cache-up-to-3-80-ghz.html)

Some people have apparently solved this problem by creating a separate environment, but I am new to Python and have no idea what that means or how to do it.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf
Traceback (most recent call last):

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-39-64156d691fe5>"", line 1, in <module>
    import tensorflow as tf

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.


**Any other info / logs**
This is what happens when I try to import DeepLabCut.

import deeplabcut
Traceback (most recent call last):

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-19-cfa4f159dfc5>"", line 1, in <module>
    import deeplabcut

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\deeplabcut\__init__.py"", line 14, in <module>
    import tensorflow as tf

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\pc\anaconda3\envs\DLC-CPU\Lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\pc\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
"
47978,How to Build TensorFlowLiteGPUExperimental for iOS x86 + others,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur
- TensorFlow version: 2.5.0
- Python version:  3.8.3
- Bazel version (if compiling from source):  3.7.2
- GCC/Compiler version (if compiling from source): 12


**Describe the problem**
There is a great pod called 'TensorFlowLiteGpuExperimental' that has C++ support for GPU delegates on iOS devices. Until now I've been using it as a pod and using the C++ API. It's been working great.

The contents of the Pod is a .framework file. Contents of the framework file are attached as screenshots at the end of this post. Most importantly it seems to include the flatbuffers and tensorflow header files, as well as libmetal_delegate.a

I want to build this framework locally so I can add x86 support so I can use this framework in an iOS simulator. Currently it runs great on iOS devices but is lacking x86 support.

I think the solution is writing some custom build instructions for Bazel, but after many many attempts I'm still pretty lost.

I think the solution lies somewhere in this BUILD.apple file
https://github.com/tensorflow/tensorflow/blob/b04e4ecfea73a460ecaf80997dc1ca4a01ceeccd/tensorflow/lite/ios/BUILD.apple#L102

My current understanding is that I need to come up with my own build section here and then run that? Is that right?

My own version of this somehow? `bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. I've tried  these to see if it would work out of the box, but unfortunately it's not working for C++ support. 
https://github.com/tensorflow/tensorflow/blob/b04e4ecfea73a460ecaf80997dc1ca4a01ceeccd/tensorflow/lite/ios/BUILD.apple
`bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework
`and then separately also doing
`bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteCMetal_framework
`

The good news is I see this header that I use `metal_delegate.h`, but I think I want to expose all the headers like the TensorFlowLiteGPUExperimental does (see screenshots at end).

In my Objective C code on the device itself, I include the headers as follows:
```
#include <tensorflow_lite_gpu/tensorflow/lite/interpreter.h>
#include <tensorflow_lite_gpu/tensorflow/lite/kernels/register.h>
#include <tensorflow_lite_gpu/tensorflow/lite/model.h>
#include <tensorflow_lite_gpu/tensorflow/lite/op_resolver.h>
#include <tensorflow_lite_gpu/tensorflow/lite/string_util.h>
#include <tensorflow_lite_gpu/tensorflow/lite/delegates/gpu/metal_delegate.h>
```

edit:
@thaink 
Here are the contents of the original TensorFlowLite GPU pod that I am attempting to replicate by building
![image](https://user-images.githubusercontent.com/2061485/112009196-8e604e80-8afc-11eb-9896-a5e9b5dfc503.png)
![image](https://user-images.githubusercontent.com/2061485/112009215-928c6c00-8afc-11eb-8abf-6a2c8e4589f6.png)
"
47977,"Will Lamba.compute_output_shape change ""inbound_nodes"" of Lambda layer?","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.4.1
- Python version:3.8.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.1
- GPU model and memory:Titan V  12G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I run x.compute_output_shape((5, 5, 5)),the inbound_nodes will become two.(x is a keras.layers.lambda)


**Describe the expected behavior**
compute_output_shape  should not change  inbound_layers

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1p1YhDb3TDXXVuf6tRFQsTIwyNLeCW8rB?usp=sharing




**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47976,"WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002051844F0D0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 'arguments' object has no attribute 'posonlyargs' To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert","I am trying to train my model and then get the Mean Attribute Error between predicted and true values.
But the predicted value column is empty due to this error."
47975,No visible @interface for 'TFLInterpreter' declares the selector 'copyData:toInputTensorAtIndex:error:',"Hello, I have a little problem with the tensorflow documentation.

I would like to use tensorflow lite with Objective C by following this [documentation](https://www.tensorflow.org/lite/guide/inference)

## URL(s) with the issue:

https://www.tensorflow.org/lite/guide/inference

## Description of issue (what needs changing):

Apparently this function is not declared:

`[interpreter copyData:inputData toInputTensorAtIndex:0 error:&error];`

I got this following error:

`No visible @interface for 'TFLInterpreter' declares the selector 'copyData:toInputTensorAtIndex:error:'`

Am I missing something, how can I fill a tensor with objective C ?
"
47973,Mask RCNN tflite detection speed on android too slow,"**System information**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  Android mobile
- TensorFlow installed from (source or binary):
   Binary
- TensorFlow version (use command below):
   2.4.0
- Python version:
  3.7

**Describe the current behavior**

We have converted Mask RCNN to TFLite using this link: 

https://wathek.medium.com/convert-mask-r-cnn-model-to-tflite-with-tensorflow-2-3-57160d3be18d and Tensorflow 2.4.0

We have used byte buffers for the input and output tensors and have the GPU delegate, multithreading enabled. However it is taking around 35 seconds for detection, which is really not feasible in terms of latency. 

**Describe the expected behavior**

We expected the inference time to be around 3-5 seconds. How can we reduce the time taken for detection? Are there any changes we can make to the model other than those done in the link above? Any help would be appreciated, thank you.

"
47971,Training becomes increasingly slower when creating models in a loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): BINARY
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.6
- CUDA/cuDNN version: NO GPU
- GPU model and memory: NO GPU

**Describe the current behavior**
When creating and training a model multiple times in a loop, the training process gets increasingly slower with each model.

**Describe the expected behavior**
I expected the training times to be roughly the same.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
import time

n_samples = 300000
n_features = 100
n_targets = 5
batch_size = 100
n_inducing_points = 100
x = np.array(range(n_samples * n_features), dtype=np.float64).reshape((n_samples, n_features))
y = np.array(range(n_samples * n_targets), dtype=np.float64).reshape((n_samples, n_targets))
for t_idx in range(10):
    tf.keras.backend.clear_session()
    dataset = [x, y]
    dataset = tf.data.Dataset.from_tensor_slices(tuple(dataset)).shuffle(n_samples).repeat().batch(batch_size=batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    data_iterator = iter(dataset)

    inputs = tf.keras.Input(shape=(n_features,), name='input')
    outputs = tf.keras.layers.Dense(n_features, name='dense_1', activation=tf.keras.activations.relu)(inputs)
    outputs = tf.keras.layers.Dense(n_features, name='dense_2', activation=tf.keras.activations.relu)(outputs)
    outputs = tf.keras.layers.Dense(n_features, name='dense_3', activation=tf.keras.activations.relu)(outputs)
    outputs = tf.keras.layers.Dense(n_features, name='dense_4', activation=tf.keras.activations.relu)(outputs)
    outputs = tf.keras.layers.Dense(n_targets, name='output', activation=tf.keras.activations.linear)(outputs)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    trainable_variables = list(model.trainable_variables)

    adam_opt = tf.optimizers.Adam(learning_rate=0.001)


    @tf.function
    def loss(batch):
        x_, y_ = batch
        y_pred_ = model(x_)
        return tf.keras.losses.MSE(y_pred_, y_)


    @tf.function
    def optimization_step():
        batch = next(data_iterator)
        def f(): return loss(batch)
        adam_opt.minimize(f, var_list=trainable_variables)

    iterations = 50000
    loop_start = time.time()
    optimization_times = []
    for idx in range(iterations):
        optimization_step()

    loop_end = time.time()
    print(f'Elapsed: {loop_end - loop_start}')
```

Using `model.fit()` is not an option, since this code was obtained by stripping down a much more complicated code, where the call to `model.fit()` is not possible.

**Other info / logs**
Output obtained with the script above:
```
Elapsed: 49.798316955566406
Elapsed: 55.18571472167969
Elapsed: 58.57510209083557
Elapsed: 64.41855955123901
Elapsed: 66.76858448982239
Elapsed: 68.3305652141571
Elapsed: 67.73438382148743
Elapsed: 69.73751258850098
Elapsed: 73.59102845191956
Elapsed: 73.14124798774719
```
As you can see, training times have a clear upward trend. Moreover, this does not happen on my Linux machine (same python version, same tensorflow version, on a clean virtualenv; Ubuntu 18.04)."
47970,tf.math.floordiv (//) is broken when inside @tf.function if second argument is 1.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: 10.1
- GPU model and memory: Titan Xp

**Describe the current behavior**
`tf.math.floordiv` produces different values if compiled with `experimental_compile=True` vs not

**Describe the expected behavior**
`tf.math.floordiv` should return the same values regardless of compilation method.

**Standalone code to reproduce the issue**
[colab](https://colab.research.google.com/drive/1RgVH3RaLmfcjdzKA5pDtpWhK-23tHmrX#scrollTo=2Iw5kpILGneG)
```python
import tensorflow as tf

def floordiv(x, y):
    # x // y
    return tf.math.floordiv(x, y)

@tf.function
def floordiv_tffn(x, y):
    # x // y
    return tf.math.floordiv(x, y)

@tf.function(experimental_compile=True)
def floordiv_compiled(x, y):
    # x // y
    return tf.math.floordiv(x, y)

x, y = tf.constant([0., 0.1, 0.9]), 1.
print(floordiv(x, y))
print(floordiv_tffn(x, y))
print(floordiv_compiled(x, y))
```
```
tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
tf.Tensor([0.  0.1 0.9], shape=(3,), dtype=float32)
tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
```"
47968,sample_weights not passed to loss from dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1
- Python version: 3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
I use tf.data.Dataset to feed data into model. Besides the features and labels I need to use weights for each sample (image pixel). I've tried to pass that weighs according to https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=en#fit but it didn't work.

**Describe the expected behavior**
Third value returned from Dataset should be passed to loss `sample_weights` argument.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/153csG3Qv5GiXduEERHkG1Np3pMM958dX?usp=sharing"
47966,how to build  tensorflow-lite-gpu-x.x.x.aar,"
**System information**
-  Linux Ubuntu 20.04:
- Mobile device : android
- TensorFlow installed from:source
- TensorFlow version:2.4.1
- Python version:3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):Bazel 3.1.0

**Describe the problem**
Build success with https://www.tensorflow.org/lite/guide/build_android but no gpu support
Any information on 'how to build tensorflow-lite-gpu-x.x.x.aar'
"
47965,"Multiple inputs api does NOT work correctly, concatenates first input and null to second","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47964,CUDA Issue: Initialization of a toy model takes 5GB graphical memory,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
conda
- TensorFlow version (use command below):
tf.version.VERSION = 2.3.0
tf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087
tf.version.COMPILER_VERSION = 7.3.1 20180303
- Python version:
== check python ===================================================
python version: 3.8.5
python branch: 
python build version: ('default', 'Sep  4 2020 07:30:14')
python compiler version: GCC 7.3.0
python implementation: CPython

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
== compiler =====================================================
c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version:
CUDA Version: 11.0
- GPU model and memory:
```python
class MyModel(tf.keras.Model):

  def __init__(self):
    pdb.set_trace()
    super(MyModel, self).__init__()
    pdb.set_trace()

    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    pdb.set_trace()
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    pdb.set_trace()

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)
```
This chosen model is tiny, but it still takes 5GB graphical memory when I run it.

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I have a personal workstation with Nvidia RTX 2060s card. I tried to run my ml model using cuda while the graphical memory immediately ran out once the model is initiated. In order to eliminate the error from my model, I tried with the following tiny model, and the memory still ran out once it's initiated.

**Describe the expected behavior**
model should only takes a few MB graphical memory as it's indeed tiny.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import pdb
import tensorflow as tf


class MyModel(tf.keras.Model):

  def __init__(self):
    pdb.set_trace() # trace1
    super(MyModel, self).__init__()
    pdb.set_trace() # trace2

    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    pdb.set_trace()
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    pdb.set_trace()

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
```
According to the nvidia-smi, the graphical memory increases from 2767MB to 7755MB after running the object initiation between trace1 and trace2.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.






```bash
$python runnet.py 
2021-03-21 20:30:55.223213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Working dir: /home/shuqi/Documents/2step-glm
manual save dir: /home/shuqi/Documents/2step-glm/manualsave/1616355056
2021-03-21 20:30:56.200025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-03-21 20:30:56.238946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.239299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2021-03-21 20:30:56.239314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-21 20:30:56.240455: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-21 20:30:56.241618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-21 20:30:56.241779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-21 20:30:56.242891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-21 20:30:56.243485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-21 20:30:56.245888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-21 20:30:56.246000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.246377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.246673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-03-21 20:30:56.246919: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-21 20:30:56.251829: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600030000 Hz
2021-03-21 20:30:56.252237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55872d90cd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-21 20:30:56.252250: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-03-21 20:30:56.339116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.339497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55872d909050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-21 20:30:56.339519: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5
2021-03-21 20:30:56.339804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.340317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5
coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2021-03-21 20:30:56.340346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-21 20:30:56.340379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-21 20:30:56.340396: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-21 20:30:56.340412: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-21 20:30:56.340428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-21 20:30:56.340444: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-21 20:30:56.340460: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-21 20:30:56.340541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.341052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.341503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-03-21 20:30:56.341532: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-21 20:30:56.643133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-21 20:30:56.643167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2021-03-21 20:30:56.643176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2021-03-21 20:30:56.643404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.643752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-21 20:30:56.644045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4658 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5)
2021-03-21 20:30:56.645082: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.687899: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688103: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688291: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688487: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688718: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688740: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688791: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688863: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.688908: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.689016: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.689036: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.689082: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.689153: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:30:56.689196: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
> /home/shuqi/Documents/2step-glm/src/buildnet.py(345)__init__()
-> self.conv2a = tf.keras.layers.Conv1D(FLAGS.conv1, FLAGS.conv1size, strides=1, padding='same',
(Pdb) c
> /home/shuqi/Documents/2step-glm/src/buildnet.py(349)__init__()
-> self.maxpool2a = tf.keras.layers.MaxPool1D(FLAGS.nk1, strides=FLAGS.nstride1, padding='same')
(Pdb) c
> /home/shuqi/Documents/2step-glm/src/buildnet.py(353)__init__()
-> self.conv2b = tf.keras.layers.Conv1D(FLAGS.conv2, FLAGS.conv2size, strides=1, padding='same',
(Pdb) c
> /home/shuqi/Documents/2step-glm/src/buildnet.py(357)__init__()
-> self.maxpool2b = tf.keras.layers.MaxPool1D(FLAGS.nk2, strides=FLAGS.nstride2, padding='same')
(Pdb) c
> /home/shuqi/Documents/2step-glm/src/buildnet.py(363)__init__()
-> self.flatten = tf.keras.layers.Flatten()
(Pdb) c
> /home/shuqi/Documents/2step-glm/src/buildnet.py(387)build_model()
-> inputnet(np.random.rand(FLAGS.batch_size, 31, FLAGS.numconsframes))  # dummy call
(Pdb) c
WARNING:tensorflow:Layer conv2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

2021-03-21 20:31:02.226508: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.227279: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.227682: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228007: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228116: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228343: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228386: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228489: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228620: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.228799: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.229711: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.229888: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.229926: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.229993: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.230095: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.230188: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.230797: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.230876: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.230913: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.231055: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.231237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-21 20:31:02.589000: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2021-03-21 20:31:02.590836: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""runnet.py"", line 95, in <module>
    inputnet, rnn, cell = build_model(ncell, FLAGS) # thr=0.4 if no buffer
  File ""/home/shuqi/Documents/2step-glm/src/buildnet.py"", line 387, in build_model
    inputnet(np.random.rand(FLAGS.batch_size, 31, FLAGS.numconsframes))  # dummy call
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 985, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/shuqi/Documents/2step-glm/src/buildnet.py"", line 366, in call
    x = self.conv2a(input_tensor)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 985, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 247, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1011, in convolution_v2
    return convolution_internal(
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1141, in convolution_internal
    return op(
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 574, in new_func
    return func(*args, **kwargs)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 574, in new_func
    return func(*args, **kwargs)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1881, in conv1d
    result = gen_nn_ops.conv2d(
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 938, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]
2021-03-21 20:31:02.714717: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.714808: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.714833: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.714856: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-03-21 20:31:02.714883: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0

```


"
47963,self-deleted,self-deleted
47961,[TFLM] Person detection benchmark build broken when generating Makefile projects,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): Nightly, source
- Tensorflow version (commit SHA if source): commit id 9711a668d8538ac2a3ebe93a90226b89683d0206
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**
When generating a Makefiles project for the person detection benchmark build fails due to some missing headers.
Command used: make -f tensorflow/lite/micro/tools/make/Makefile  generate_person_detection_benchmark_make_project

I will open a PR shortly with a fix.
"
47959,ValueError: Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled.,"
**System information**
- Windows 10
- TensorFlow version 2.4.1
- Keras 2.4.1
- Python version: 3.7
- CUDA/cuDNN version: v11.2/11.2
- GPU model and memory: MSI geforce rtx 3060 12GB

**Describe the current behavior**
The program fails to start. 

**Describe the expected behavior**
It should work. 

**Standalone code to reproduce the issue**
I have deep Q learning class: 
```
class DQNAgent:
    def __init__(self):
        self.model = self.create_model()
        self.target_model = self.create_model()
        self.target_model.set_weights(self.model.get_weights())

        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)

        self.tensorboard = ModifiedTensorBoard(log_dir=f""logs/{MODEL_NAME}-{int(time.time())}"")

        self.target_update_counter = 0
        self.graph = tf.compat.v1.get_default_graph()

        self.terminate = False
        self.last_log_episode = 0
        self.training_initialized = False

    def create_model(self):
        base_model = Xception(weights=None, include_top=False, input_shape=(IM_HEIGHT, IM_WIDTH, 3))

        x = base_model.output
        x = GlobalAveragePooling2D()(x)

        predictions = Dense(3, activation='linear')(x)
        model = Model(inputs=base_model.input, outputs=predictions)
        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])
        return model

    def update_replay_memory(self, transition):
        # transition = (current_state, action, reward, new_state, done)
        self.replay_memory.append(transition)

    def train(self):
        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:
            return

        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)

        current_states = np.array([transition[0] for transition in minibatch])/255
        with self.graph.as_default():
            current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)

        new_current_states = np.array([transition[3] for transition in minibatch]) / 255
        with self.graph.as_default():
            future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)

        X = []
        y = []

        # we want to update q table only when there exists a future state. If a car crashed just save the last q values
        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):
            if not done:
                max_future_q = np.max(future_qs_list[index])
                new_q = reward + DISCOUNT * max_future_q
            else:
                new_q = reward

            current_qs = current_qs_list[index]
            current_qs[action] = new_q

            X.append(current_state)
            y.append(current_qs)

        log_this_step = False
        if self.tensorboard.step > self.last_logged_episode:
            log_this_step = True
            self.last_logged_episode = self.tensorboard.step

        with self.graph.as_default():
            self.model.fit(np.array(X)/255, np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if log_this_step else None)

        if log_this_step:
            self.target_update_counter += 1

        if self.target_update_counter > UPDATE_TARGET_EVERY:
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0

    def get_qs(self, state):
        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]

    def train_in_loop(self):
        X = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)
        y = np.random.uniform(size=(1, 3)).astype(np.float32)

        with self.graph.as_default():
            self.model.fit(X, y, verbose=False, batch_size=1)

        self.training_initialized = True

        while True:
            if self.terminate:
                return
            self.train()
            time.sleep(0.01)
```

When I run the program: 
```
2021-03-21 18:50:05.877519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
Using TensorFlow backend.
WARNING:tensorflow:From C:/Users/Michał/Desktop/Magisterka/Project/Carla_0.9.11_env/sentex_carla/sentex_full_tut.py:291: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

2021-03-21 18:50:07.803967: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-21 18:50:07.806461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-21 18:50:07.827618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6
coreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s
2021-03-21 18:50:07.827846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-21 18:50:07.834034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-21 18:50:07.834164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-21 18:50:07.837381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-21 18:50:07.838887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-21 18:50:07.843668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-21 18:50:07.846965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-21 18:50:07.848560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-21 18:50:07.848732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-21 18:50:08.633848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-21 18:50:08.634021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-21 18:50:08.634128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-21 18:50:08.634412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4915 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-03-21 18:50:08.635416: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-21 18:50:08.660884: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-21 18:50:08.661049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6
coreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s
2021-03-21 18:50:08.661286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-21 18:50:08.661415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-21 18:50:08.661545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-21 18:50:08.661688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-21 18:50:08.662177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-21 18:50:08.662341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-21 18:50:08.662515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-21 18:50:08.662726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-21 18:50:08.663120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-21 18:50:08.663570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6
coreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s
2021-03-21 18:50:08.663900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-21 18:50:08.664065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-21 18:50:08.664222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-21 18:50:08.664385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-21 18:50:08.664546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-21 18:50:08.664710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-21 18:50:08.664866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-21 18:50:08.665026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-21 18:50:08.665225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-21 18:50:08.665395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-21 18:50:08.665567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-21 18:50:08.665683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-21 18:50:08.665901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4915 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-03-21 18:50:08.666224: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-21 18:50:10.410494: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-03-21 18:50:10.410594: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-03-21 18:50:10.410692: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2021-03-21 18:50:10.411657: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found
2021-03-21 18:50:10.412499: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2021-03-21 18:50:10.412642: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-03-21 18:50:10.412928: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-03-21 18:50:10.413062: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Users\Michał\AppData\Local\Programs\Python\Python37\lib\threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""C:\Users\Michał\AppData\Local\Programs\Python\Python37\lib\threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""C:/Users/Michał/Desktop/Magisterka/Project/Carla_0.9.11_env/sentex_carla/sentex_full_tut.py"", line 268, in train_in_loop
    self.model.fit(X, y, verbose=False, batch_size=1)
  File ""C:\Users\Michał\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1031, in fit
    version_utils.disallow_legacy_graph('Model', 'fit')
  File ""C:\Users\Michał\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\keras\utils\version_utils.py"", line 122, in disallow_legacy_graph
    raise ValueError(error_msg)
ValueError: Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled.
```

I have tried adding to model.compile an argument: , run_eagerly equals: True, None, False - the result was still the same.

The whole code is here: https://pastebin.pl/view/10e9e350

Due to the fact that I am working with the Carla environment, it will be hard to reproduce exactly my setup.
The code is from: [sentex](https://pythonprogramming.net/reinforcement-learning-self-driving-autonomous-cars-carla-python/?completed=/reinforcement-learning-agent-self-driving-autonomous-cars-carla-python/)
For him that code worked perfectly. I guess that he was using a significantly older version of TensorFlow., so the error is most likely because I am using 2.4.1 version of TF.
"
47958,""" ValueError: assignment destination is read-only"" when using Pandas DataFrame","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code I suppose (see below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: Using CPU
- GPU model and memory: Using CPU


**Describe the current behavior**
This code
```python
import pandas as pd
import tensorflow as tf

df = pd.DataFrame()
df[""x""] = tf.linspace(0, 100, 20)
df[""x""] /= 2
```
produces the following error
```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-796d4d4e000c> in <module>
      4 df = pd.DataFrame()
      5 df[""x""] = tf.linspace(0, 100, 20)
----> 6 df[""x""] /= 2

~/.local/lib/python3.8/site-packages/pandas/core/generic.py in __itruediv__(self, other)
  11338 
  11339     def __itruediv__(self, other):
> 11340         return self._inplace_method(
  11341             other, type(self).__truediv__  # type: ignore[operator]
  11342         )

~/.local/lib/python3.8/site-packages/pandas/core/generic.py in _inplace_method(self, other, op)
  11315         ):
  11316             # GH#36498 this inplace op can _actually_ be inplace.
> 11317             self._values[:] = result._values
  11318             return self
  11319 

ValueError: assignment destination is read-only
```

**Describe the expected behavior**
Normal assignment of the operation, like with Numpy.
```python
import pandas as pd
import numpy as np

df = pd.DataFrame()
df[""x""] = np.linspace(0, 100, 20)
df[""x""] /= 2
```

**Workaround**
```python
import pandas as pd
import tensorflow as tf

df = pd.DataFrame()
df[""x""] = pd.Series(tf.linspace(0, 100, 20))
df[""x""] /= 2
```"
47956,WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer,"In my training setup, I've customized the `.fit` method by overriding the `train_step` function. 

```
class CustomFit(tf.keras.Model()):
	def train_step():
    	# do some modification 

    def test_step():
		# no modification

model = CustomFit()
model.compile(loss, optimizer, metrics)
model.fit()
```

Training start at this point as it should be but a warning message appears, though until now I didn't find any other issue except this annoying warning message. 

> WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:
  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)

In my code, by myself, I didn't write anywhere this statement `opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)`. What is this all about? Do I need to pass optimizer like this way before setting in `.compile`? Like 

```
opt = Adam()
opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)
model.compile(loss, metrics, optimizer=opt)
```
Or, I can just ignore this warning? When I should use it and not?"
47955,WARNING:tensorflow:multiprocessing can interact badly with TensorFlow,"I've a custom custom `tf.keras` data generator using `tf.keras.utils.Sequence`. Whenever I set `use_multiprocessing` as True, it gives a warning message at the very beginning. The custom data generator (in my case) read the image data, do some augmentation and return batch-wise samples. Nothing special, just a conventional approach. The full warning statement is 


> WARNING:tensorflow: multiprocessing can interact badly with TensorFlow, causing non-deterministic deadlocks. For high-performance data pipelines tf. data is recommended.

Just about to start training but freeze everything anyway after showing this message. The issue is, **Is it BAD to use multiprocessing, or Is it NOT POSSIBLE to use?** 


"
47954,TextVectorization inconsistency depending on output_sequence_length,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.4.1
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
`TextVectorization`'s behavior changes if `output_sequence_length` is set.

**Describe the expected behavior**
I would expend the behavior to remain the same (up to the effect of `output_sequence_length`, of course). However, same that runs perfectly in one case, causes a runtime error in the other. The reason is that the former accepts scalar inputs, whereas the latter requires input rank > 0.

P. S.
I can fix it, the question is whether we want to accept scalar inputs or not.


**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow_datasets as tfds

ds_train, ds_test = tfds.load(""imdb_reviews"", split=[""train"", ""test""], as_supervised=True)


encoder1 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000)
encoder2 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000, output_sequence_length=200)

encoder1.adapt(ds_train.map(lambda x, y: x))
encoder2.adapt(ds_train.map(lambda x, y: x))

for x, y in ds_train:
  print(x)
  print(encoder1(x)) # This one works
  print(encoder2(x)) # This one fails



tf.Tensor(b""This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it."", shape=(), dtype=string)
tf.Tensor(
[  11   14   34  412  384   18   90   28    1    8   33 1322 3560   42
  487    1  191   24   85  152   19   11  217  316   28   65  240  214
    8  489   54   65   85  112   96   22 5596   11   93  642  743   11
   18    7   34  394 9522  170 2464  408    2   88 1216  137   66  144
   51    2    1 7558   66  245   65 2870   16    1 2860    1    1 1426
 5050    3   40    1 1579   17 3560   14  158   19    4 1216  891 8040
    8    4   18   12   14 4059    5   99  146 1241   10  237  704   12
   48   24   93   39   11 7339  152   39 1322    1   50  398   10   96
 1155  851  141    9], shape=(116,), dtype=int64)

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-3-fe1d367865bf> in <module>()
      2   print(x)
      3   print(encoder1(x))
----> 4   print(encoder2(x))

8 frames

/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: text_vectorization_1/strided_slice/

```"
47951,Cant install tflite_runtime while deploying on web application online,"**System information**
- OS Platform and Distribution (windows 10)

- TensorFlow installed from (source or binary):
- TensorFlow version: Uninstalled(As I cannot use such a huge library)
- Python version:  3.8
- Installed using virtualenv and pip3 on local
[requirements.txt](https://github.com/tensorflow/tensorflow/files/6177015/requirements.txt)

- GPU 8Gb memory:

I am new to this but I will try to explain my situation.
So I am using tflite.interpreter to load the saved model(converted to tflite) using tflite. interpreter.
Everything works fine on the local system but when I deploy my Django app to Heroku or azure. I cant install tflite_runtime from Requirements.py during git push to remote.
I used this to install it on my local
pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime

How do I include tflite_runtime in requirements.txt so that it automatically installs using pip while pushing to GIT?

`remote: Collecting scipy==1.6.1 remote: Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB) remote: Collecting six==1.15.0 remote: Downloading six-1.15.0-py2.py3-none-any.whl (10 kB) remote: Collecting sqlparse==0.4.1 remote: Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB) remote: ERROR: Could not find a version that satisfies the requirement tflite-runtime==2.5.0 (from -r /tmp/build_1599e83a/requirements.txt (line 22)) (from versions: none) remote: ERROR: No matching distribution found for tflite-runtime==2.5.0 (from -r /tmp/build_1599e83a/requirements.txt (line 22)) remote: ! Push rejected, failed to compile Python app. remote: remote: ! Push failed remote: Verifying deploy... remote: remote: ! Push rejected to predictivemaintenance2021. remote: To https://git.heroku.com/pred2021.git ! [remote rejected] master -> master (pre-receive hook declined)
`
"
47949,[TF2.3.0] Failed to Build libtensorflow_cc.so for C++ APIs with conda environment ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (CentOS Linux 7 (Core)):
- TensorFlow installed from (source):
- TensorFlow version: 2.3.0
- Python version: 3.7.10
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.1.243 (cuda), 7.6.5(cuDNN)
- GPU model and memory: v100, 32g



**Describe the problem**
I'am tring to build TensorFlow’s C++ interface by this document (https://deepmd.readthedocs.io/en/latest/install-tf.2.3.html).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
First, I create environment by 

> conda install bazel=3.1.0 cudatoolkit=10.1.243 cudnn=7.6.0=cuda10.1_0 python=3.8.8
> conda install -c conda-forge cudatoolkit-dev=10.1.243 openmpi
> conda install numpy 

After configuration, I try to build by 'bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so' 
The error is

> ERROR: /mnt/gs18/scratch/users/gepei/tensorflow/tensorflow/cc/BUILD:629:1: Linking of rule '//tensorflow/cc:ops/functional_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
>   (cd /mnt/ufs18/home-183/gepei/.cache/bazel/_bazel_gepei/515900c424239e830302083d92d41b3a/execroot/org_tensorflow && \
>   exec env - \
>     LD_LIBRARY_PATH=/opt/software/binutils/2.30-GCCcore-7.3.0/lib:/opt/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/software/GCCcore/7.3.0/lib64:/opt/software/GCCcore/7.3.0/lib \
>     PATH=/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/bin:/opt/software/binutils/2.30-GCCcore-7.3.0/bin:/opt/software/GCCcore/7.3.0/bin:/mnt/home/gepei/anaconda3/condabin:/usr/lib64/qt-3.3/bin:/opt/software/core/lua/lua/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/hpcc/bin:/usr/lpp/mmfs/bin:/opt/ibutils/bin:/opt/puppetlabs/bin:/opt/dell/srvadmin/bin \
>     PWD=/proc/self/cwd \
>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/cc/ops/functional_ops_gen_cc-2.params)
> Execution platform: @local_execution_config_platform//:platform
> bazel-out/k8-opt-exec-50AE0418/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<long long>::allocator()'
> collect2: error: ld returned 1 exit status
> Target //tensorflow:libtensorflow_cc.so failed to build
> INFO: Elapsed time: 1427.425s, Critical Path: 388.90s
> INFO: 5225 processes: 5225 local.
> FAILED: Build did NOT complete successfully

 
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The configuration is here.
> You have bazel 3.1.0- (@non-git) installed.
> Please specify the location of python. [Default is /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/bin/python3]: 
> 
> 
> 
> Found possible Python library paths:
>   /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/lib/python3.8/site-packages
> Please input the desired Python library path to use.  Default is [/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/lib/python3.8/site-packages]
> 
> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
> No OpenCL SYCL support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with ROCm support? [y/N]: 
> No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: y
> CUDA support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with TensorRT support? [y/N]: 
> No TensorRT support will be enabled for TensorFlow.
> 
> Could not find any cuda.h matching version '' in any subdirectory:
>         ''
>         'include'
>         'include/cuda'
>         'include/*-linux-gnu'
>         'extras/CUPTI/include'
>         'include/cuda/CUPTI'
> of:
>         '/lib'
>         '/lib64'
>         '/opt/dell/srvadmin/lib64'
>         '/opt/dell/srvadmin/lib64/openmanage'
>         '/opt/ibutils/lib64'
>         '/opt/mellanox/hcoll/lib'
>         '/opt/mellanox/mxm/lib'
>         '/opt/mellanox/sharp/lib'
>         '/usr'
>         '/usr/lib64//bind9-export'
>         '/usr/lib64/atlas'
>         '/usr/lib64/dyninst'
>         '/usr/lib64/mysql'
>         '/usr/lib64/qt-3.3/lib'
> Asking for detailed CUDA configuration...
> 
> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 
> 
> 
> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 
> 
> 
> Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 
> 
> 
> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/,/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/
> 
> 
> Found CUDA 10.1 in:
>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/lib64
>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/include
> Found cuDNN 7 in:
>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/lib
>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/include
> 
> 
> Please specify a list of comma-separated CUDA compute capabilities you want to build with.
> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0,7.0,7.0,7.0]: 
> 
> 
> Do you want to use clang as CUDA compiler? [y/N]: 
> nvcc will be used as CUDA compiler.
> 
> Please specify which gcc should be used by nvcc as the host compiler. [Default is /opt/software/GCCcore/7.3.0/bin/gcc]: 
> 
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
> 
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
> Not configuring the WORKSPACE for Android builds.
> 
> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
> 	--config=mkl         	# Build with MKL support.
> 	--config=monolithic  	# Config for mostly static monolithic build.
> 	--config=ngraph      	# Build with Intel nGraph support.
> 	--config=numa        	# Build with NUMA support.
> 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
> 	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
> Preconfigured Bazel build configs to DISABLE default on features:
> 	--config=noaws       	# Disable AWS S3 filesystem support.
> 	--config=nogcp       	# Disable GCP support.
> 	--config=nohdfs      	# Disable HDFS support.
> 	--config=nonccl      	# Disable NVIDIA NCCL support.
> Configuration finished

Here is the line 629 of /mnt/gs18/scratch/users/gepei/tensorflow/tensorflow/cc/BUILD 
```
tf_gen_op_wrappers_cc(
    name = ""functional_ops"",
    include_internal_ops = 1,
    op_lib_names = [
        ""functional_ops"",
    ],
    pkg = ""//tensorflow/core"",
    visibility = [""//tensorflow:internal""],
)
```


Can someone help me with that? Thanks a lot."
47947,Optimize unique_op by repetition rate,"My test about unique op:

> Input: size:20\*1000k, uniqued size:5\*1000k.

> cmp test:

>> Origin: The initial size of the hash table in unique_op is 2*input_size

>> My test: The initial size of the hash table in unique_op is 1*input_size(change source code""tensorflow/core/kernels/unique_op.cc"").

> cmp result:
>> Origin: spend: 2.517s
>> My test: spend: 1.709s(↑32.1%)

**As the size of the hash table increases, memory bind will increase.**
I think unique_op should be added a default params of repetition rate, default value is non-repeating(hash table size is 2*N)。Repetition rate will help us do some prediction, like: Adjust the initial size of the hash table to improve performance。
If the user does not know the repetition rate of his data, he should use default value.
If the user knows the repetition rate of his data, he will adjust this args to get a better performance.
Do you think this view makes sense? If it makes sense, I would be happy to discuss & implement this part of the code. ^v^

My test code:
```
from __future__ import print_function
import random
import tensorflow as tf
import time

size = 20 * 1000000
sum=0
keys, idx = tf.unique(tf.random_uniform(shape=[int(size)], dtype=tf.int64, maxval=int(size/5)), version='v2')

sess = tf.Session()
with tf.Session() as sess:
    for i in range(0,10):
       start = time.time()
       _ = sess.run(keys)
       sum+=(time.time() - start)
    print(""avg time:"", sum / 10);
```"
47946,pip and conda install tensorflow problems in anaconda environment,"After creating an environment in anaconda, I try to install the tensorflow by pip and conda. 


**System information of my computer:** 
- OS Platform and Distribution **win10 64bit**
- Python version: **3.8.8 64 bit**
- pip version: **21.0.1**
- Installed using virtualenv: **pip and conda (both have problems)**
- CUDA/cuDNN version: **11.0 for CUDA/ 8.0.5 for cuDNN**

**Describe the problem**

**1. For the pip method**
```
conda create --name tf python=3.8
pip install tensorflow
```
**The results are as followings:**
```
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/
Could not fetch URL https://pypi.org/simple/tensorflow/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/tensorflow/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))) - skipping
ERROR: Could not find a version that satisfies the requirement tensorflow
ERROR: No matching distribution found for tensorflow
Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))) - skipping
```

**2. For the conda method**
```
conda create --name tf python=3.8
conda install tensorflow
```
It successes. **But the tensorflow version is 2.3.0, not the recent version 2.4.0.** So that I cannot use GPU.
"
47945,TensorFlow-Keras Code Example ,"In keras official [doc](https://keras.io/examples/), there are some nice code examples to get started on something. Last time I visited this amazing write-up about [object detection](https://keras.io/examples/vision/retinanet/). However, what I mostly found in the computer vision examples are kinda randomly placed code examples with different degrees of complexities. 

So, I think it would be clear and convenient to **make them sort in some order**. In computer vision, there are plenty of subcategories like classification, detection, segmentation, etc. Additionally, there are some general code examples like augmentation, different training approaches like knowledge distillation, etc. Because of contribution, the content gets enrich from time to time, sooner or later some kind of organizing strategies will be required. "
47944,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
47943,"When in ""webgl"" backend and image width is odd, the second run of tf.conv2d() may be wrong.","
**System information**
- TensorFlow.js version: 3.2.0

**Describe the current behavior**

When tf.gtBackend() is ""webgl"".
  - If the width of the input is even, the result of tf.conv2d() is correct.
  - If the width of the input is odd, the result of first time tf.conv2d() is correct. But the second time result may be wrong.
 
When tf.gtBackend() is ""wasm"" or ""cpu"".
  - The above problem seems not existed.


**Describe the expected behavior**
No matter which backend, what kinds image width (even or odd), how many times to run tf.conv2d(), the result should be same and corrrect.

**Standalone code to reproduce the issue**
https://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02
(The code coulde be paste into any example console of Tensorflow.js API (https://js.tensorflow.org/api/3.2.0/) weg page to run.)
"
47942,Bug Issues Testing,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47941,Add additive/multiplicative angluar margin loss,"
**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Additive angular margin loss can create highly discriminative embedding appropriate for re-identification and cross-domain instance matching tasks. Such a layer is currently accessible only via third-party implementations.

**Will this change the current api? How?**
Will add a new layer that allows for angular penalties.

**Who will benefit with this feature?**
Anyone working in cross domain instance matching/ identification problems. Additionally, can be used as a stepping stone to implement certain SOTA instance matching architectures.
**Any Other info.**
N/A"
47938,`model.fit()` ends before all epochs are ran,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (in a Docker container)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 11.0 with cuDNN 8.0.5.39
- GPU model and memory: GeForce RTX 2080 Ti with 11019 MiB

**Describe the current behavior**
I have been trying to implement [FlowNet 2.0](https://arxiv.org/abs/1612.01925) using Tensorflow 2.0 as an exercise. I've managed to implement a single FlowNetS module, but have been unable to train it. My `model.fit()` function is ran with 480 epochs, with 2500 steps per epoch. For some reason, `model.fit()` always ends after 2 epochs.

**Describe the expected behavior**
The code runs for all 480 epochs (this will take, by my estimation, 200 days as the dataset loading is slow).

**Standalone code to reproduce the issue**
This issue probably cannot be reproduced in Colab due to the large dataset and long runtime.

Here's some details of my implementation:
- subclassed from `tf.keras.Model`, which has a custom implementation of `__init__()`, `call()` and `test_step()`. The only thing I changed in `test_step()` is the metrics that get returned by `test_step()`
- A custom loss class and a custom metric class
- Adam optimizer with PiecewiseConstantDecay schedule
- A `tf.data.Dataset` that dynamically loads the images and flows as necessary, and passes the data to the network in batches of 8, with prefetching. I cannot load the full Flying Chairs dataset into memory, since it is 30 GB
- Learning done with the standard calls of `model.compile()` and `model.fit()`. The model returns multiple outputs during training, so loss_weights are provided during compilation. The `fit()` command is as below, with 2 `ModelCheckpoint` callbacks to store the full model of the best and last checkpoint (using this [temporary workaround](https://github.com/tensorflow/tensorflow/issues/42741#issuecomment-706534711)):
```
history = net.fit(x=train_set, epochs=480, verbose=1, validation_data=val_set,
                      steps_per_epoch=2500, callbacks=[best_checkpoint, last_checkpoint])
```

I'm not sure how I'm supposed to include the source code, since the project source code is split into multiple files across subdirectories. Should I compile all the relevant code into a single source file?

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The output I get is as follows:
```
root@4d7382461e21:/workspace/flownet2# python3 main.py
2021-03-19 15:54:31.910652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcudart.so.11.0
2021-03-19 15:54:33.351618: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_device
s not set
2021-03-19 15:54:33.352532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcuda.so.1
2021-03-19 15:54:33.438470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-19 15:54:33.439233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-19 15:54:33.439303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcudart.so.11.0
2021-03-19 15:54:33.439397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcublas.so.11
2021-03-19 15:54:33.439440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcublasLt.so.11
2021-03-19 15:54:33.439484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar
y libcufft.so.10
2021-03-19 15:54:33.439527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 15:54:33.443278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 15:54:33.444066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 15:54:33.444126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 15:54:33.446050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2021-03-19 15:54:33.446391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-19 15:54:33.447649: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 15:54:33.614936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-19 15:54:33.615494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-19 15:54:33.615560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 15:54:33.615574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 15:54:33.615609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-19 15:54:33.615623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-19 15:54:33.615635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 15:54:33.615723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 15:54:33.615762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 15:54:33.615778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 15:54:33.617531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
2021-03-19 15:54:33.617607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 15:54:34.312786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-19 15:54:34.312840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1
2021-03-19 15:54:34.312848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N
2021-03-19 15:54:34.312853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N
2021-03-19 15:54:34.314939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10069 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5)
2021-03-19 15:54:34.316283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9825 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)
2021-03-19 15:54:34.754017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-19 15:54:34.772736: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz
Epoch 1/480
2021-03-19 15:55:54.178077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 15:55:56.009098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 15:55:56.406221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2500/2500 [==============================] - 31685s 13s/step - loss: 965251.6302 - flow2_loss: 2211748.8726 - flow3_loss: 2211866.4072 - flow4_loss: 2213292.5525 - flow5_loss: 2219776.0292 - flow6_loss: 2219457.2435 - flow2_aae: 273742796.5962 - flow3_aae: 273851546.8446 - flow4_aae: 274638881.0683 - flow5_aae: 275303256.6630 - flow6_aae: 275404682.2226 - val_epe: 2226169.7500 - val_aae: 17670296.0000

Epoch 00001: loss improved from inf to 964092.00000, saving model to /workspace/flownet2/milestone1/best/
2021-03-20 00:42:41.302179: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.

Epoch 00001: saving model to /workspace/flownet2/milestone1/last/
Epoch 2/480
2500/2500 [==============================] - 4285s 2s/step - loss: 919111.6756 - flow2_loss: 2113024.2584 - flow3_loss: 2112958.6253 - flow4_loss: 2113410.2004 - flow5_loss: 2113135.0545 - flow6_loss: 2112805.9978 - flow2_aae: 56491300.6014 - flow3_aae: 56474786.1364 - flow4_aae: 56593601.0892 - flow5_aae: 56521067.3974 - flow6_aae: 56437117.0523 - val_epe: 2226564.2500 - val_aae: 17701936.0000

Epoch 00002: loss improved from 964092.00000 to 921045.31250, saving model to /workspace/flownet2/milestone1/best/

Epoch 00002: saving model to /workspace/flownet2/milestone1/last/
{'loss': [964092.0, 921045.3125], 'flow2_loss': [2194216.25, 2117453.25], 'flow3_loss': [2195233.75, 2117392.75], 'flow4_loss': [2199682.75, 2117850.75], 'flow5_loss': [2219717.75, 2117574.25], 'flow6_loss': [2217494.0, 2117253.5], 'flow2_aae': [545963968.0, 59821748.0], 'flow3_aae': [546967424.0, 59804596.0], 'flow4_aae': [550095296.0, 59930716.0], 'flow5_aae': [552018432.0, 59854408.0], 'flow6_aae': [551568128.0, 59766340.0], 'val_epe': [2226169.75, 2226564.25], 'val_aae': [17670296.0, 17701936.0]}
root@4d7382461e21:/workspace/flownet2#
```"
47934,Process finished with exit code -1073740791 (0xC0000409) when training on GPU,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [copied code from tensorflow transfer learning documentation](https://www.tensorflow.org/guide/keras/transfer_learning#an_end-to-end_example_fine-tuning_an_image_classification_model_on_a_cats_vs_dogs_dataset)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 - 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): ``pip install tensorflow``
- TensorFlow version (use command below): ``v2.4.0-rc4-71-g582c8d236cb 2.4.0``
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: cuda:  cuda_11.2.r11.2/ cuDNN: v8.1.1.33
- GPU model and memory: Nvidia 1080 8GB

tf_env script output:

```

== check python ===================================================

== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== check pips ===================================================
numpy                  1.19.5
protobuf               3.14.0
tensorflow             2.4.0
tensorflow-estimator   2.4.0

== check for virtualenv =========================================

== tensorflow import ============================================
bash: /c/Users/Faraz: No such file or directory

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Mar 19 23:41:09 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.89       Driver Version: 460.89       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080   WDDM  | 00000000:01:00.0  On |                  N/A |
| N/A   36C    P8     9W /  N/A |    963MiB /  8192MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       532    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A      1276    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A      2784    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A      4564    C+G   ...t\Teams\current\Teams.exe    N/A      |
|    0   N/A  N/A      7408    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A      7928    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      8532    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |
|    0   N/A  N/A      8732    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      9116    C+G   ...w5n1h2txyewy\SearchUI.exe    N/A      |
|    0   N/A  N/A      9996    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |
|    0   N/A  N/A     11768    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A     11836    C+G   ...m Files (x86)\SCM\SCM.exe    N/A      |
|    0   N/A  N/A     11936    C+G   ...b3d8bbwe\WinStore.App.exe    N/A      |
|    0   N/A  N/A     12800    C+G   ...t\Teams\current\Teams.exe    N/A      |
|    0   N/A  N/A     13164    C+G   ...a\74.0.3911.160\opera.exe    N/A      |
|    0   N/A  N/A     14716    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A     15196    C+G   ...in7x64\steamwebhelper.exe    N/A      |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.4.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\faraz khan\appdata\local\programs\python\python36\lib\site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 8, 'final', 0)

== bazel version  ===============================================

```


**Describe the current behavior**

When running model training on GPU, I get the following output:

```
2021-03-19 23:43:27.889416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-19 23:43:31.028195: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 23:43:31.029096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-03-19 23:43:31.052703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.771GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-19 23:43:31.053038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-19 23:43:31.080499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-19 23:43:31.080660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-19 23:43:31.097601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-19 23:43:31.101390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-19 23:43:31.150363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-19 23:43:31.165564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-19 23:43:31.166410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-19 23:43:31.166617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 23:43:31.167034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-19 23:43:31.167949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.771GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2021-03-19 23:43:31.168488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-03-19 23:43:31.168746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-19 23:43:31.169257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-19 23:43:31.169475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-03-19 23:43:31.169697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-03-19 23:43:31.169887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-03-19 23:43:31.170027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-03-19 23:43:31.170168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-03-19 23:43:31.170350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 23:43:31.796539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-19 23:43:31.796758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-19 23:43:31.796853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-19 23:43:31.797098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6274 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-03-19 23:43:31.798090: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Number of training samples: 9305
Number of validation samples: 2326
Number of test samples: 2326
2021-03-19 23:43:32.077770: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-19 23:43:33.184162: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Epoch 1/20
2021-03-19 23:43:37.097103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-03-19 23:43:37.446836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-03-19 23:43:37.484678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll

Process finished with exit code -1073740791 (0xC0000409)
```
The code I am using is mentioned from the official documentation for transfer learning as linked above. But every time I get this error. 

Also, no matter what batch size I use, the GPU memory always shoots up to almost maximum capacity as can be seen from screenshot below, but no training happens as it fails at ``epoch 1``.

![Capture221](https://user-images.githubusercontent.com/33456896/111851982-4202ec00-890d-11eb-9cf8-70a1540d6675.JPG)

When I re-run this code on cpu using the lines:

```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
```

The training runs fine without any problems but runs on the cpu. :(

**Describe the expected behavior**

It should be able to train on the GPU.

**Standalone code to reproduce the issue**

As mentioned above, I'm using the transfer learning tensorflow example code:

```python
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# tfds.disable_progress_bar()

train_ds, validation_ds, test_ds = tfds.load(
    ""cats_vs_dogs"",
    # Reserve 10% for validation and 10% for test
    split=[""train[:40%]"", ""train[40%:50%]"", ""train[50%:60%]""],
    as_supervised=True,  # Include labels
)

print(""Number of training samples: %d"" % tf.data.experimental.cardinality(train_ds))
print(
    ""Number of validation samples: %d"" % tf.data.experimental.cardinality(validation_ds)
)
print(""Number of test samples: %d"" % tf.data.experimental.cardinality(test_ds))

plt.figure(figsize=(10, 10))
for i, (image, label) in enumerate(train_ds.take(9)):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(image)
    plt.title(int(label))
    plt.axis(""off"")

size = (150, 150)

train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))
validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))
test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))

batch_size = 32

train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)
validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)
test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)

data_augmentation = keras.Sequential(
    [
        layers.experimental.preprocessing.RandomFlip(""horizontal""),
        layers.experimental.preprocessing.RandomRotation(0.1),
    ]
)

for images, labels in train_ds.take(1):
    plt.figure(figsize=(10, 10))
    first_image = images[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(
            tf.expand_dims(first_image, 0), training=True
        )
        plt.imshow(augmented_image[0].numpy().astype(""int32""))
        plt.title(int(labels[0]))
        plt.axis(""off"")


base_model = keras.applications.Xception(
    weights=""imagenet"",  # Load weights pre-trained on ImageNet.
    input_shape=(150, 150, 3),
    include_top=False,
)  # Do not include the ImageNet classifier at the top.

# Freeze the base_model
base_model.trainable = False

# Create new model on top
inputs = keras.Input(shape=(150, 150, 3))
x = data_augmentation(inputs)  # Apply random data augmentation

# Pre-trained Xception weights requires that input be normalized
# from (0, 255) to a range (-1., +1.), the normalization layer
# does the following, outputs = (inputs - mean) / sqrt(var)
norm_layer = keras.layers.experimental.preprocessing.Normalization()
mean = np.array([127.5] * 3)
var = mean ** 2
# Scale inputs to [-1, +1]
x = norm_layer(x)
norm_layer.set_weights([mean, var])

# The base model contains batchnorm layers. We want to keep them in inference mode
# when we unfreeze the base model for fine-tuning, so we make sure that the
# base_model is running in inference mode here.
x = base_model(x, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

# model.summary()

model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

epochs = 20
model.fit(train_ds, epochs=epochs, validation_data=validation_ds)

```
"
47933,model.save() outputs unexpected files in SavedModel directory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): '2.5.0-dev20210318'
- Python version: python 3.8
- CUDA/cuDNN version: cuda11.2; cudnn 8.x

**Describe the current behavior**

model.save() outputs an extra ""keras_metadata.pb"" file.

**Standalone code to reproduce the issue**
I wrote some code to load my maskRCNN model, and save it to a SavedModel.

this [link](https://drive.google.com/drive/folders/1qf3LaEhu16o6s7fu_X-6qf6M-Dg8Zq3W?usp=sharing) includes: 

1. Model weights: ""mask_rcnn_nucleus_0040_thick_Pf.h5""
2. My code to load and save the model: ""load_SavedModel.py""
3. Source code needed to load the maskRCNN model: Folder ""mrcnn""
4. The SavedModel folder, which is the output of my code: ""thick_Pf_model""
"
47932,TensorflowLiteObj duplicate symbol,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Catalina v.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: TensorFlowLiteObjC
- Python version:
- Installed using virtualenv? pip? conda?: cocoapods
- Bazel version (if compiling from source): Not used


**Describe the problem**

I've tried to follow this [tutorial ](https://www.tensorflow.org/lite/guide/ios)to use tensorflowLite with Objective-C on my mac

I've inserted the line in my podfile like this:

` pod 'TensorFlowLiteObjC'`

Then I run pod install and got the following line:

```
Analyzing dependencies
Downloading dependencies
Installing TensorFlowLiteC (2.4.0)
Installing TensorFlowLiteObjC (2.4.0)
```

I build my code with xCode but I have the following error:

```
duplicate symbol '_TfLiteDelegateCreate' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayCopy' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayCreate' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayEqual' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayEqualsArray' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayFree' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteIntArrayGetSizeInBytes' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteTensorDataFree' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteTensorFree' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteTensorRealloc' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteTensorReset' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
duplicate symbol '_TfLiteTypeGetName' in:
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)
    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC
ld: 12 duplicate symbols for architecture x86_64
```

I've remove the installation line in my podfile and `pod deintegrate`, I try to build and it's working.
I've try to reinstall the lib but same problem again...



"
47931,Failed to convert SparseTensor to Tensor,"My code raises this error message:

`TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""DeserializeSparse:0"", shape=(None, 2), dtype=int64), values=Tensor(""DeserializeSparse:1"", shape=(None,), dtype=float32), dense_shape=Tensor(""stack:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.`

It is based almost 1:1 on this [Google guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification) for an ngram-model.

It works if I add `.todense()` like this:
```
x_train = vectorizer.fit_transform(train_texts).todense()
x_val = vectorizer.transform(val_texts).todense()
```

It seems to be very slow, though. That's why I reduced the number of texts and labels in the example.

I'm running it in a pipenv environment with Python 3.8.6, TensorFlow 2.4.1, MacOS Big Sur 11.2.3.

_Originally posted by @DanielMH in https://github.com/tensorflow/tensorflow/issues/42916#issuecomment-802169046_"
47930,Share variable across devices in Tensorflow 2 cluster,"Hello,
I would like to use Tensorflow 2.4 in order to share a variable across devices on a cluster. In my use case, I want to share a queue stored on a parameter server that can be filled by all the workers. I know this was possible with tensorflow 1 but I do not understand how to do this in tf2 with the strategy API and the documentation does not seem to mention it. 
Here is my code to do this with a simple variable:

```python
# run.py
import tensorflow as tf
import os
import json
import argparse
import time

parser = argparse.ArgumentParser()
parser.add_argument('--type', type=str, default='ps', required=False)
parser.add_argument('--index', type=int, default=0, required=False)
args = parser.parse_args()

cluster_dict = {
    ""cluster"": {
        ""worker"": ['localhost:50848'],
        ""ps"": ['localhost:50849'],
    },
    ""task"": {""type"": args.type, ""index"": args.index}
}


os.environ[""GRPC_FAIL_FAST""] = ""use_caller""
os.environ[""TF_CONFIG""] = json.dumps(cluster_dict)
cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()

server = tf.distribute.Server(
    cluster_resolver.cluster_spec(), job_name=cluster_resolver.task_type,
    task_index=cluster_resolver.task_id, protocol=""grpc""
)
# Initializes tf server internally.
strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)


with tf.device('/job:ps/task:0'):
    v = tf.Variable(
        1, name='test')

if cluster_resolver.task_type == ""worker"":
    while True:
        with tf.device('/job:ps/task:0'):
            # <tf.Variable 'test:0' shape=() dtype=int32, numpy=>: ok
            print(v.device)
            # Increments variable on the worker
            v.assign_add(1)
            # v is incremented: ok
            print('v:', v)

            time.sleep(1)
elif cluster_resolver.task_type == ""ps"":
    with tf.device('/job:ps/task:0'):
        while True:
            # <tf.Variable 'test:0' shape=() dtype=int32, numpy=>: ok
            print(v.device)
            # v should be incremented by the worker
            print(v)
            # v=1: not working
            time.sleep(1)

else:
    pass
```

I then launch the worker and the parameter server using:
```
python run.py --type ps
```
and
```
python run.py --type worker
```

However the code in its current form does not work: the variable stored in the parameter server is updated by the worker but not visible by the parameter server. Note that I use the PS strategy but I have also tried the Mirrored Strategy. 

Is this behavior (i.e. sharing a custom variable over different workers of the cluster) supported in tf2? If so, could someone help me with the synthax, I would be very grateful! 

Thanks! 


"
47929,tfcompile for arm,"I'm looking for a step-by-step procedure or example of how to use a compile (XLA/AOT) for the arm64 device. 
Is it possible to cross-compile the TensorFlow graph into executable code? (e.g. Linux/windows as a host machine for Linux-based arm64 device)
Please, if you can link me, I would very much appreciate it.

Thank you."
47927,TFLiteSavedModelConverterV2 breaks ordering of multi output models,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installation (pip package or built from source): binary and source 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0-dev20210319, 5abe2b515db51d3aa35f6acf5869c1ce51a6b09c

**Describe the current behavior**

`tf.lite.TFLiteConverter.from_saved_model` doesn't support multi output models and outputs non-deterministic ordering which breaks users of the generated flatbuffers.

This is due to the fact that TFLite converter relies on the output ordering, but the saved model import function purposefully breaks this assumption by scrambling the inputs and outputs:
https://github.com/tensorflow/tensorflow/blob/c36991b0367a12bc81bf37dfdc5cf4793c7a8bde/tensorflow/compiler/mlir/tensorflow/translate/import_model.cc#L3708-L3721

**Describe the expected behavior**

`tf.lite.TFLiteConverter.from_saved_model` should behave the same way as `tf.lite.TFLiteConverter.from_keras_model` and preserve the output ordering. This issue is especially problematic when relying on models using `@tf.function(experimental_implements=True)` which are only supported by the saved model converter.

**Standalone code to reproduce the issue**
I opened PR #47928 which includes a failing test case (093f927aa9c217bfbecf7350c3c3f651964587af) that reproduces this issue when running the unittests with `bazelisk test tensorflow/lite/python:lite_v2_test --test_output=all`.
"
47924,Gpu memory issue,"**System information**
- I am using keras finetuned vgg model with 2 dense layers
-  Linux Ubuntu 18.04 (Colab)
- TensorFlow version (use command below): 2.4.3
- Python version: 3.7
- CUDA Version: 11.2   
- GPU model and memory: Tesla P100 with 16 gb

**Describe the current behavior**

here is the log after loading model
Fri Mar 19 17:02:34 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   36C    P0    33W / 250W |   **5703MiB / 16280MiB** |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

it's using 5gb gpu memory.

After making one prediction.

`Fri Mar 19 17:02:55 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   36C    P0    38W / 250W |   **9969MiB / 16280MiB** |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
Gpu usage is increased almost duoble.


Can you tell me it's a normal usage or there is something wrong with my code."
47923,Shared svdf OpData should be renamed to OpDataSvdf,"@tensorflow/micro

Noticed this while reviewing #47911

Similar to 
https://github.com/tensorflow/tensorflow/blob/d73ba75a4c1b26b91cc20cb86866d775c02dc479/tensorflow/lite/micro/kernels/conv.h#L27-L28

the svdf struct should be OpDataSvdf instead of
https://github.com/tensorflow/tensorflow/blob/d73ba75a4c1b26b91cc20cb86866d775c02dc479/tensorflow/lite/micro/kernels/svdf.h#L23-L24

This renaming will likely happen after the CEVA optimized kernel is merged so we should include reference, cmsis_nn, xtensa and ceva in the renaming."
47918,Installation error: Tensorflow having some conflict with rocm-3.9.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10 with kernel 5.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.2 with tensorflow-rocm2.3.4
- Python version: 3.8.6
- GPU model and memory: AMD Radeon RX580 4g

**Describe the problem**
```
Traceback (most recent call last):
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Any other info / logs**
-
"
47917,Tensorflow ROCM getting wrong version,"Didn't find this error on errors list: https://www.tensorflow.org/install/errors

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10 with kernel 5.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.2 with tensorflow-rocm2.3.4
- Python version: 3.8.6
- GPU model and memory: AMD Radeon RX580

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

```
Traceback (most recent call last):
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Describe the current behavior** I have rocm 3.9.1 (I downgraded from 4), but it is still trying to get libamdhip64.so.4 instead of libamdhip64.so.3.

**Describe the expected behavior** look for correct version.

**Standalone code to reproduce the issue**
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
"
47916,Tensorflow Training crashes when using tf dataset on large amount of data,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes, using tf dataset pipelines
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: 
-   **TensorFlow version (use command below)**: 2.2.0
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**: 
-   **CUDA/cuDNN version**: 11.0
-   **GPU model and memory**: 
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Get Error "" IVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.
2021-03-08 08:25:35.572927: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2247026860 exceeds 10% of free system memory.
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:523] CHECK failed: (value.size()) <= (kint32max):
[1]    4943 segmentation fault  sudo python3 msynth-train-script.py""

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
47915,Build fails on Arch Linux,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, LSB version 1.4, Linux 5.10.23-1-lts, x86-64 
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.4.1
- Python version: 3.9
- Installed using virtualenv? pip? conda?: virtualenv, pip
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: CUDA 11.2.0-3, cuDNN 8.0.5.39-1
- GPU model and memory: Nvidia GeForce RTX 280 Ti 11016MiB

**Describe the problem**
I have two problems. These problems are related to two different types of situations. However, I have opened just one issue because, even if the problems could not be related themselves, they are probably related to building/installation problems.

1. Manual compilation of TensorFlow (2.4.1 version) using the instruction related to Arch Linux ([see the PKGBUILD file here](https://github.com/archlinux/svntogit-community/blob/packages/tensorflow/trunk/PKGBUILD)) fails. The error is the following (I'll log the entire output, error at the end). Note: The compilation has been started outside any virtualenv.
```bash
patching file tensorflow/tools/ci_build/install/install_centos_pip_packages.sh
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_centos_pip_packages.sh.rej
patching file tensorflow/tools/ci_build/install/install_pip_packages.sh
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_pip_packages.sh.rej
patching file tensorflow/tools/ci_build/install/install_python3.5_pip_packages.sh
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_python3.5_pip_packages.sh.rej
patching file tensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh.rej
patching file tensorflow/tools/ci_build/release/common_win.bat
Reversed (or previously applied) patch detected!  Skipping patch.
2 out of 2 hunks ignored -- saving rejects to file tensorflow/tools/ci_build/release/common_win.bat.rej
patching file tensorflow/tools/pip_package/setup.py
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/pip_package/setup.py.rej
patching file tensorflow/python/keras/saving/hdf5_format.py
Reversed (or previously applied) patch detected!  Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej
patching file tensorflow/python/keras/saving/hdf5_format.py
Reversed (or previously applied) patch detected!  Skipping patch.
3 out of 3 hunks ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej
patching file tensorflow/python/keras/saving/hdf5_format.py
Reversed (or previously applied) patch detected!  Skipping patch.
2 out of 2 hunks ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej
patching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py
Hunk #2 FAILED at 80.
1 out of 2 hunks FAILED -- saving rejects to file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py.rej
patching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py
Hunk #2 FAILED at 79.
1 out of 2 hunks FAILED -- saving rejects to file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py.rej
patching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py
patching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py
patching file tensorflow/python/pywrap_mlir.py
Hunk #1 succeeded at 38 (offset 1 line).
patching file tensorflow/python/pywrap_mlir.py
Hunk #1 succeeded at 38 (offset 1 line).
You have bazel 4.0.0 installed.
Found CUDA 11.2 in:
    /opt/cuda/targets/x86_64-linux/lib
    /opt/cuda/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib
    /usr/include
Found NCCL 2 in:
    /usr/lib
    /usr/include


Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=198
INFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python --config=xla --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_NCCL_VERSION=2.8 --action_env TF_CUDA_PATHS=/opt/cuda,/usr/lib,/usr,/usr/include,/usr/local/lib --action_env CUDA_TOOLKIT_PATH=/opt/cuda --action_env NCCL_INSTALL_PATH=/usr --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5 --action_env LD_LIBRARY_PATH=/opt/cuda/extras/CUPTI/lib64: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --action_env TF_SYSTEM_LIBS=boringssl,curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,pcre,png,pybind11,zlib --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /run/media/federico/XData/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /run/media/federico/XData/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /run/media/federico/XData/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file /run/media/federico/XData/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /run/media/federico/XData/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:avx2_linux in file /run/media/federico/XData/tensorflow/.bazelrc: --copt=-mavx2
INFO: Found applicable config definition build:mkl in file /run/media/federico/XData/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:linux in file /run/media/federico/XData/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /run/media/federico/XData/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  /run/media/federico/XData/tensorflow/WORKSPACE:37:30: in <toplevel>
  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories
Repository rule git_repository defined at:
  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
DEBUG: /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /run/media/federico/XData/tensorflow/WORKSPACE:37:30: in <toplevel>
  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed 4 targets (418 packages loaded, 31513 targets configured).
INFO: Found 4 targets...
INFO: Deleting stale sandbox base /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/sandbox
ERROR: /run/media/federico/XData/tensorflow/tensorflow/python/keras/api/BUILD:111:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
2021-03-18 20:25:35.715885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 48, in <module>
    from tensorflow.python import keras
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py"", line 27, in <module>
    from tensorflow.python.keras import models
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/models.py"", line 26, in <module>
    from tensorflow.python.keras.engine import functional
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/functional.py"", line 38, in <module>
    from tensorflow.python.keras.engine import training as training_lib
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py"", line 52, in <module>
    from tensorflow.python.keras.engine import data_adapter
  File ""/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/data_adapter.py"", line 65, in <module>
    import pandas as pd  # pylint: disable=g-import-not-at-top
  File ""/usr/lib/python3.9/site-packages/pandas/__init__.py"", line 29, in <module>
    from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib
  File ""/usr/lib/python3.9/site-packages/pandas/_libs/__init__.py"", line 13, in <module>
    from pandas._libs.interval import Interval
  File ""pandas/_libs/interval.pyx"", line 1, in init pandas._libs.interval
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject
ERROR: /run/media/federico/XData/tensorflow/tensorflow/python/tools/BUILD:282:10 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
INFO: Elapsed time: 7946.935s, Critical Path: 567.22s
INFO: 28075 processes: 5830 internal, 22245 local.
FAILED: Build did NOT complete successfully

```
2. Manual compilation of TensorFlow (git cloned and checked out at r2.4/v2.4.1) following the instruction [here](https://www.tensorflow.org/install/source) seems to work. The `pip install` of the wheel produced correctly works too. However, trying to create a new CustoOps, in particular a basic one `zero_out.cc` as proposed [here](https://www.tensorflow.org/guide/create_op#define_the_op_interface), after having compiled with bazel, when I try to test it as described [here](https://www.tensorflow.org/guide/create_op#use_the_op_in_python) I receive this error (demangled):
`tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext const&, bool) const`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Regarding the first problem, I have tried to compile TensorFlow following a subset of the instruction in this [PKGBUILD](https://github.com/archlinux/svntogit-community/blob/packages/tensorflow/trunk/PKGBUILD) file (see below for the exact subset of commands I have used). In particular:

* I've used the `tensorflow-2.4.1-cuda-opt` extracted from the source indicated in the `PKGBUILD` (`source=(""$pkgname-$pkgver.tar.gz::https://github.com/tensorflow/tensorflow/archive/v${_pkgver}.tar.gz""`)
* Renamed the folder into `tensorflow`
* Put the `fix-h5py3.0.patch` into the above `tensorflow` folder
* From the mentioned `PKGBUILD` file, I've used this subset of instructions:
```bash
#!/bin/bash          

# Set the bazel version
echo ""4.0.0"" > .bazelversion

# Patch the h5py
patch -Np1 -d . -i fix-h5py3.0.patch

# Get rid of hardcoded versions. Not like we ever cared about what upstream
# thinks about which versions should be used anyway. ;) (FS#68772) 
sed -i -E ""s/'([0-9a-z_-]+) .= [0-9].+[0-9]'/'\1'/"" tensorflow/tools/pip_package/setup.py

# Set the variables
export PYTHON_BIN_PATH=/usr/bin/python
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_KAFKA=1
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_AWS=1
export TF_NEED_GCP=1
export TF_NEED_HDFS=1
export TF_NEED_S3=1
export TF_ENABLE_XLA=1
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_MPI=0
export TF_NEED_TENSORRT=0
export TF_NEED_NGRAPH=0
export TF_NEED_IGNITE=0
export TF_NEED_ROCM=0
# See https://github.com/tensorflow/tensorflow/blob/master/third_party/systemlibs/syslibs_configure.bzl
export TF_SYSTEM_LIBS=""boringssl,curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,pcre,png,pybind11,zlib""
export TF_SET_ANDROID_WORKSPACE=0
export TF_DOWNLOAD_CLANG=0
export TF_NCCL_VERSION=2.8
export TF_IGNORE_MAX_BAZEL_VERSION=1
export TF_MKL_ROOT=/opt/intel/mkl
export NCCL_INSTALL_PATH=/usr
export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
export HOST_C_COMPILER=/usr/bin/gcc
export HOST_CXX_COMPILER=/usr/bin/g++
export TF_CUDA_CLANG=0  # Clang currently disabled because it's not compatible at the moment.
export CLANG_CUDA_COMPILER_PATH=/usr/bin/clang
export TF_CUDA_PATHS=/opt/cuda,/usr/lib,/usr,/usr/include,/usr/local/lib
export TF_CUDA_VERSION=$(/opt/cuda/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')
export TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' /usr/include/cudnn_version.h)
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5
export CC=gcc
export CXX=g++
export BAZEL_ARGS=""--config=mkl -c opt --copt=-I/usr/include/openssl-1.0 --host_copt=-I/usr/include/openssl-1.0 --linkopt=-l:libssl.so.1.0.0 --linkopt=-l:libcrypto.so.1.0.0 --host_linkopt=-l:libssl.so.1.0.0 --host_linkopt=-l:libcrypto.so.1.0.0""
export BAZEL_ARGS=""$BAZEL_ARGS --host_copt=-Wno-stringop-truncation""


echo ""Building with cuda and with non-x86-64 optimizations"" 
export CC_OPT_FLAGS=""-march=haswell -O3""
export TF_NEED_CUDA=1
./configure
bazel build --config=avx2_linux ${BAZEL_ARGS[@]} //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so //tensorflow:install_headers //tensorflow/tools/pip_package:build_pip_package
```
2. For the second problem, related to the successful build and installation, I reproduced the steps indicated in the official build guide [here](https://www.tensorflow.org/install/source), and then I have installed the wheel inside a virtualenv with python 3.9. The only thing I had to do to succeed in the installation process has been to downgrade the  `hdf5` library installed on my system from the `1.12.0-2` to the `1.10.5-1`.

Any hint will be appreciated. Thank you."
47913,Tenorflow Traceback Errors ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
This is what I do 


!git clone https://github.com/tensorflow/models.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/models/research/
!protoc object_detection/protos/*.proto --python_out=.
# Install TensorFlow Object Detection API.
!cp object_detection/packages/tf2/setup.py .
!python -m pip install .

!python /content/models/research/object_detection/builders/model_builder_tf2_test.py

!pip install -q kaggle
!pip install -q kaggle-cli
!pip install lxml

import os
os.environ['KAGGLE_USERNAME'] = 'callumfawcett'
os.environ['KAGGLE_KEY'] = '4e2c04f4c1e23da1796a889a29104c55'

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# mkdir /content/dataset
# cd /content/dataset
# kaggle datasets download -d callumfawcett/graphs --unzip

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/train.record -i dataset/images -csv dataset/train_labels.csv
!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/test.record -i dataset/images -csv dataset/test_labels.csv

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz
!tar -xvf mobilenet_v2.tar.gz
!rm mobilenet_v2.tar.gz

!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config
!mv ssd_mobilenet_v2_320x320_coco17_tpu-8.config mobilenet_v2.config

num_classes = 5
batch_size = 96
num_steps = 7500
num_eval_steps = 1000

train_record_path = '/content/dataset/train.record'
test_record_path = '/content/dataset/test.record'
model_dir = '/content/training/'
labelmap_path = '/content/labelmap.pbtxt'

pipeline_config_path = 'mobilenet_v2.config'
fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'

import re

with open(pipeline_config_path) as f:
    config = f.read()

with open(pipeline_config_path, 'w') as f:

  # Set labelmap path
  config = re.sub('label_map_path: "".*?""', 
             'label_map_path: ""{}""'.format(labelmap_path), config)
  
  # Set fine_tune_checkpoint path
  config = re.sub('fine_tune_checkpoint: "".*?""',
                  'fine_tune_checkpoint: ""{}""'.format(fine_tune_checkpoint), config)
  
  # Set train tf-record file path
  config = re.sub('(input_path: "".*?)(PATH_TO_BE_CONFIGURED/train)(.*?"")', 
                  'input_path: ""{}""'.format(train_record_path), config)
  
  # Set test tf-record file path
  config = re.sub('(input_path: "".*?)(PATH_TO_BE_CONFIGURED/val)(.*?"")', 
                  'input_path: ""{}""'.format(test_record_path), config)
  
  # Set number of classes.
  config = re.sub('num_classes: [0-9]+',
                  'num_classes: {}'.format(num_classes), config)
  
  # Set batch size
  config = re.sub('batch_size: [0-9]+',
                  'batch_size: {}'.format(batch_size), config)
  
  # Set training steps
  config = re.sub('num_steps: [0-9]+',
                  'num_steps: {}'.format(num_steps), config)
  
  f.write(config)

!python /content/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_config_path} \
    --model_dir={model_dir} \
    --alsologtostderr \
    --num_train_steps={num_steps} \
    --sample_1_of_n_eval_examples=1 \
    --num_eval_steps={num_eval_steps}
 error given
2021-03-19 13:10:29.033663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:10:31.900190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 13:10:31.901266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-19 13:10:31.923864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.924658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-03-19 13:10:31.924704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:10:31.927664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:10:31.927752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-19 13:10:31.929670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-19 13:10:31.930074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 13:10:31.932218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 13:10:31.933137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 13:10:31.933384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:10:31.933500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.934304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.935034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 13:10:31.935512: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 13:10:31.935645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.936414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-03-19 13:10:31.936454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:10:31.936504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:10:31.936555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-19 13:10:31.936605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-19 13:10:31.936647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 13:10:31.936693: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 13:10:31.936739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 13:10:31.936785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:10:31.936893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.937699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:31.938434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 13:10:31.938494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:10:32.376021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-19 13:10:32.376092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-19 13:10:32.376119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-19 13:10:32.376392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:32.377213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:32.378024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:10:32.378728: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-03-19 13:10:32.378806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0319 13:10:32.380789 140196829169536 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: 7500
I0319 13:10:32.386101 140196829169536 config_util.py:552] Maybe overwriting train_steps: 7500
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0319 13:10:32.386274 140196829169536 config_util.py:552] Maybe overwriting use_bfloat16: False
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.565161 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.566583 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.568816 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.569891 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.624391 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.628402 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.655039 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.656113 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.657880 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0319 13:10:32.658880 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2021-03-19 13:10:34.452403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:10:35.371642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:10:35.587620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.040649 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.041203 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.041422 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.041633 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.041833 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:10:36.042120 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:540: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0319 13:10:36.096746 140196829169536 deprecation.py:339] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:540: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 110, in main
    record_summaries=FLAGS.record_summaries)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py"", line 540, in train_loop
    train_dataset_fn)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py"", line 340, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1143, in experimental_distribute_datasets_from_function
    return self.distribute_datasets_from_function(dataset_fn, options)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1135, in distribute_datasets_from_function
    dataset_fn, options)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 547, in _distribute_datasets_from_function
    options)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 162, in get_distributed_datasets_from_function
    input_contexts, strategy, options)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1273, in __init__
    self._input_contexts, self._input_workers, dataset_fn))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1936, in _create_datasets_from_function_with_input_context
    dataset = dataset_fn(ctx)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py"", line 535, in train_dataset_fn
    input_context=input_context)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/inputs.py"", line 898, in train_input
    reduce_to_frame_fn=reduce_to_frame_fn)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py"", line 210, in build
    decoder = decoder_builder.build(input_reader_config)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/builders/decoder_builder.py"", line 64, in build
    load_keypoint_depth_features=input_reader_config
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py"", line 416, in __init__
    default_value=''),
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py"", line 103, in __init__
    default_value=-1)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 314, in __init__
    super(StaticHashTable, self).__init__(default_value, initializer)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 179, in __init__
    self._resource_handle = self._create_resource()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 322, in _create_resource
    name=self._name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py"", line 121, in hash_table_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node HashTableV2}} = HashTableV2[container="""", key_dtype=DT_FLOAT, shared_name=""8716"", use_node_name_sharing=false, value_dtype=DT_INT64]
All kernels registered for op HashTableV2:
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_DOUBLE]
 [Op:HashTableV2] name: hash_table

!python /content/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_config_path} \
    --model_dir={model_dir} \
    --checkpoint_dir={model_dir}
2021-03-19 13:11:15.148998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
W0319 13:11:18.025036 139725362648960 model_lib_v2.py:1051] Forced number of epochs for all eval validations to be 1.
INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None
I0319 13:11:18.025284 139725362648960 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0319 13:11:18.025420 139725362648960 config_util.py:552] Maybe overwriting use_bfloat16: False
INFO:tensorflow:Maybe overwriting eval_num_epochs: 1
I0319 13:11:18.025554 139725362648960 config_util.py:552] Maybe overwriting eval_num_epochs: 1
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
W0319 13:11:18.025730 139725362648960 model_lib_v2.py:1072] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
2021-03-19 13:11:18.033286: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 13:11:18.034218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-19 13:11:18.058184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.059025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-03-19 13:11:18.059071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:11:18.062061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:11:18.062148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-19 13:11:18.063991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-19 13:11:18.064394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 13:11:18.066400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 13:11:18.067184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 13:11:18.067424: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:11:18.067565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.068417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.069163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 13:11:18.069685: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-19 13:11:18.069832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.070598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-03-19 13:11:18.070640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:11:18.070686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:11:18.070729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-19 13:11:18.070776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-19 13:11:18.070818: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-19 13:11:18.070879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-19 13:11:18.070957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-19 13:11:18.071001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:11:18.071108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.071985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.072687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-19 13:11:18.072745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-19 13:11:18.508368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-19 13:11:18.508446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-19 13:11:18.508469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-19 13:11:18.508680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.509505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.510275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-19 13:11:18.511030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-03-19 13:11:18.511089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2021-03-19 13:11:19.738225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-19 13:11:20.644943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-19 13:11:20.845673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.081701 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.082200 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.082447 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.082655 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.082916 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0319 13:11:21.083143 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 88, in main
    wait_interval=300, timeout=FLAGS.eval_timeout)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py"", line 1096, in eval_continuously
    model=detection_model))
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/inputs.py"", line 1056, in eval_input
    reduce_to_frame_fn=reduce_to_frame_fn)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py"", line 210, in build
    decoder = decoder_builder.build(input_reader_config)
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/builders/decoder_builder.py"", line 64, in build
    load_keypoint_depth_features=input_reader_config
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py"", line 416, in __init__
    default_value=''),
  File ""/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py"", line 103, in __init__
    default_value=-1)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 314, in __init__
    super(StaticHashTable, self).__init__(default_value, initializer)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 179, in __init__
    self._resource_handle = self._create_resource()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py"", line 322, in _create_resource
    name=self._name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py"", line 121, in hash_table_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node HashTableV2}} = HashTableV2[container="""", key_dtype=DT_FLOAT, shared_name=""5060"", use_node_name_sharing=false, value_dtype=DT_INT64]
All kernels registered for op HashTableV2:
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_DOUBLE]
 [Op:HashTableV2] name: hash_table


# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir '/content/training/'"
47912,Weights in tf.estimator.BoostedTreesClassifier(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): PIP
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.2
- GPU model and memory: NVIDIA Quadro T2000 with Max Q-design, 4GB. 64GB RAM

**Describe the current behavior**
Error when setting the weight_column in ```tf.estimator.BoostedTreesClassifier()```
Encoded as a feature using tf.feature_column.numeric_column() and appended to list feature_columns.

It works perftectly with ```tf.estimator.DNNClassifier()```

**Describe the expected behavior**
No error :)

**Standalone code to reproduce the issue**
```
tf.estimator.BoostedTreesClassifier(feature_columns,
                                    n_batches_per_layer = 1,
                                    weight_column = 'weeks_to_complete_weights',
                                    n_trees = 100)
```
**Other info / logs**
TypeError: Expected float32 passed to parameter 'y' of op 'Mul', got 'weeks_to_complete_weights' of type 'str' instead. Error: Expected float32, got 'weeks_to_complete_weights' of type 'str' instead."
47910,Tracking python lists (or ListWrapper) in keras custom preprocessing layers,"### System information

-   **Custom code**
-   **OS Platform and Distribution**:

> - PRETTY_NAME=""Debian GNU/Linux 9 (stretch)""
> - NAME=""Debian GNU/Linux""
> - VERSION_ID=""9""
> - VERSION=""9 (stretch)""
> - ID=debian

-   **TensorFlow installed from binaries**:
-   **TensorFlow version**: 2.4.0
-   **Python version**: 3.6.8

### Problem
Hi,
My problem is the following: I try to create a custom keras preprocessing layer in which I absolutely need an attribute which is a regular python list computed via adapt method. When I include my layer in model and I save it (via SavedModel), my list attribute becomes empty at load time. Is there anyway to make this list (or any other python object) trackable so that it is properly loaded?

Some additional points:
- It can be made trackable if I use a variable instead of a list but in my case a list is necessary.
- I still can use a variable and transform it to a list at call time, but this will work only in eager mode and I need it to work at graph mode also

### Source code / logs
Here a simple example to reproduce my issue:
```python
import numpy as np
import tensorflow as tf

from tensorflow.python.ops import math_ops


@tf.keras.utils.register_keras_serializable()
class BucketizeLayer(tf.keras.layers.experimental.preprocessing.PreprocessingLayer):
    
    def __init__(self, quantiles, **kwargs):
        self.quantiles = quantiles
        super(BucketizeLayer, self).__init__(**kwargs)
        self._boundaries = None
        
    def adapt(self, data):
        if isinstance(data, tf.Tensor):
            data = data.numpy()
            
        self._boundaries = np.nanquantile(data, self.quantiles).tolist()
        
    def call(self, data):
        return math_ops.bucketize(data, self._boundaries)
    
    def get_config(self):
        config = {'quantiles': self.quantiles}
        base_config = super(BucketizeLayer, self).get_config()
        return dict(**config, **base_config)


data = 3 * np.random.randn(100) + 5
bucketize = BucketizeLayer(quantiles=[0.1, 0.5, 0.9])
bucketize.adapt(data)
print(bucketize(data))


inp = tf.keras.Input(shape=(1,), dtype=tf.float32)
model = tf.keras.models.Model(inp, bucketize(inp))
model.save('bucketize/')
del model
model = tf.keras.models.load_model('bucketize/')
model.layers[1]._boundaries
# output: ListWrapper([])
```
"
47909,TF.keras in TF2.4,What are the differences in tf.keras in TF2.4 with respect to TF1.15 version? 
47908,TF2 + GH,"![image](https://user-images.githubusercontent.com/24195074/111758231-24de0700-8894-11eb-9778-1716c8fdb9dd.png)

I am getting list of error as I am trying to change TF2 BERT from using tfhub bert to hugging face bert.

The accuracy and results are not similar to using tfhub bert besides I am getting all the error above.

I am using this:
from transformers import BertConfig, AutoTokenizer, TFAutoModel
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
config = BertConfig.from_pretrained(model_name, num_labels=2, return_dict=True,output_hidden_states=True,use_cache=True,output_attentions=True)
bert = TFAutoModel.from_pretrained(model_name, config=config)



"
47906,Using custom callback to set trainable=true doesn't work when use model.fit(),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.2.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tensorflow==2.3.0
- Python version: 3.6.10
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None



Hi, I want to train the model with transfer learning. First, I load the pretrain model, then I freeze some of layer at front of model. After several epoch, I want to unfreeze the weight for training. I use callback to write some  snippet to test it. But I found that when I set the trainable to True, the model still not training...

Code:

[Colab link](https://colab.research.google.com/drive/1suX_4qMZ0yyBGaIqmDGFLa6KN0hMK5Jo?usp=sharing)

```
import tensorflow as tf
import tensorflow_datasets as tfds
import os
from tensorflow.python.keras.callbacks import ModelCheckpoint

class UnFreezeWeight(tf.keras.callbacks.Callback):
    def __init__(self, freeze_before_epoch):
        super().__init__()
        self.freeze_before_epoch = freeze_before_epoch

    def on_epoch_begin(self, epoch, logs=None):
        if self.freeze_before_epoch != epoch:
            return

        # Unfreeze all weight.
        print('set trainable to True.')
        for layer in self.model.layers:
            layer.trainable = True


def _normalize_img(img, label):
    img = tf.cast(img, tf.float32) / 255.
    return img, label


def main():
    train_ds = tfds.load('mnist', split='train', as_supervised=True)
    train_ds = train_ds.batch(32)
    train_ds = train_ds.map(_normalize_img)

    valid_ds = tfds.load('mnist', split='test', as_supervised=True)
    valid_ds = valid_ds.batch(32)
    valid_ds = valid_ds.map(_normalize_img)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10)
    ])
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # Freeze all weight.
    for layer in model.layers:
        layer.trainable = False
        
    # If I unfreeze at epoch index 0, the model will learning.
    # But if I un freeze after epoch index 1, the model won't training.
    freeze = UnFreezeWeight(1)

    model.compile(optimizer='adam',
                  loss=loss_fn,
                  metrics=['accuracy'])

    model.fit(train_ds,
              validation_data=valid_ds,
              callbacks=[freeze],
              epochs=5)


if __name__ == '__main__':
    main()
```

Output:
When unfreeze weight at epoch 2
```
Epoch 1/5
1875/1875 [==============================] - 9s 5ms/step - loss: 2.3907 - accuracy: 0.1362 - val_loss: 2.3740 - val_accuracy: 0.1488
Epoch 2/5
set trainable to True.
1875/1875 [==============================] - 3s 2ms/step - loss: 2.3922 - accuracy: 0.1349 - val_loss: 2.3740 - val_accuracy: 0.1488
Epoch 3/5
1875/1875 [==============================] - 3s 2ms/step - loss: 2.3910 - accuracy: 0.1361 - val_loss: 2.3740 - val_accuracy: 0.1488
Epoch 4/5
1875/1875 [==============================] - 3s 2ms/step - loss: 2.3914 - accuracy: 0.1378 - val_loss: 2.3740 - val_accuracy: 0.1488
Epoch 5/5
1875/1875 [==============================] - 3s 2ms/step - loss: 2.3926 - accuracy: 0.1346 - val_loss: 2.3740 - val_accuracy: 0.1488
```

When unfreeze weight at epoch 1
```
Epoch 1/5
set trainable to True.
1875/1875 [==============================] - 12s 6ms/step - loss: 0.4833 - accuracy: 0.8568 - val_loss: 0.1364 - val_accuracy: 0.9607
Epoch 2/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.1502 - accuracy: 0.9567 - val_loss: 0.0991 - val_accuracy: 0.9711
Epoch 3/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.1100 - accuracy: 0.9668 - val_loss: 0.0870 - val_accuracy: 0.9751
Epoch 4/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0910 - accuracy: 0.9734 - val_loss: 0.0776 - val_accuracy: 0.9744
Epoch 5/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0768 - accuracy: 0.9759 - val_loss: 0.0790 - val_accuracy: 0.9751
```
"
47905,"tflite runtime API + NNAPI delegate refusing layers from a MobileNetV2SSD trained with Object Detection API, with high inference time","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): training on Ubuntu 18.04, inference on Yocto
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15 (tflite runtime 2.3)
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11
- GPU model and memory: Titan-v 12GB


**Describe the current behavior**
I trained a Object detection model (MobileNetV2SSD) from scratch using the Object Detection API. I used TF version 1.15 as I needed to perform quantization-aware training. My pipeline config file includes the following block:

```
graph_rewriter {
  quantization { 
  delay: 48000
  weight_bits: 8
  activation_bits: 8
  } 
}
```
The model was then exported using the `object_detection/export_tflite_ssd_graph.py` script and converted to tflite format as described [here](https://neuralet.com/article/quantization-of-tensorflow-object-detection-api-models/) (full integer quantization). I then tested the model with the tflite runtime API + NNAPI on a board running YOCTO and equipped with a NPU. The inference time was very high (0.5 s) and I had many warnings from the NNAPI delegate which refused quantization operators (see below). Did I do something wrong?



**Standalone code to reproduce the issue**
Inference code:
```
import tflite_runtime.interpreter as tflite
import sys
import cv2
import numpy as np
import time
import os

model_path =""model_path""
img_path = ""img_path""

interpreter = tflite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

height = input_details[0]['shape'][1]
width = input_details[0]['shape'][2]

img = cv2.imread(img_path)
img = cv2.resize(img, (300,300))

for  i in range(100):
       
        input_data = np.expand_dims(img, axis=0)
        input_data = (input_data/255.0).astype(np.float32)

        interpreter.set_tensor(input_details[0]['index'], input_data)
        t1 = time.time()
        interpreter.invoke()
        t2 = time.time()

        output_data = interpreter.get_tensor(output_details[0]['index'])
        print(t2-t1)

```
**Other info / logs** Include any logs or source code that would be helpful to
**INFO: Created TensorFlow Lite delegate for NNAPI.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1
WARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.
WARNING: Operator CUSTOM (v1) refused by NNAPI delegate: Unsupported operation type.
Applied NNAPI delegate.**

"
47904,ASL Gesture Recognition SSD Model not showing labels upon running,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): version 2, ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

The image below shows how the label map was created for the ssd model.
![ssd_model_label_code](https://user-images.githubusercontent.com/80737339/111729623-d86bda80-8845-11eb-8799-b2595f4be735.jpeg)


### 3. Failure after conversion
The code is for ASL gesture recognition and translates ASL to text. The conversion is successful, however, when running on android studio, the labels of the ASL signs do not show but the accuracy does. What shows up on the android app is a '}' followed by the accuracy. We are assuming it is because of how the labels are defined in the label map file. We used the same object detection Android Studio project as the one in the link below. We just used our ssd tflite model instead of theirs. The labels show for their example and tflite model, but it does not show the labels for our model. 

Example Android Studio project used: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

Any suggestions on how to get the labels to show when running on Android Studio will be appreciated."
47903,Train.py error Blas GEMM launch failed,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I used nmt code https://github.com/daniel-kukiela/nmt-chatbot
- OS Platform and Distribution: Windows 10
- TensorFlow installed from binary
- TensorFlow version (use command below): 1.15.0
- Python version: 3.6.5
- CUDA/cuDNN version: Cuda 10, cuDNN v7.6.5
- GPU model and memory: Nvidia Geforce 3070 8gb



**Im getting a large error**

``
Traceback (most recent call last):                                                                                                                File ""train.py"", line 17, in <module>                                                                                                             tf.app.run(main=nmt.main, argv=[os.getcwd() + '\nmt\nmt\nmt.py'] + unparsed)                                                                  File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run          _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)                                                                          File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py"", line 303, in run                                    _run_main(main, args)                                                                                                                         File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py"", line 251, in _run_main                              sys.exit(main(argv))                                                                                                                          File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\nmt.py"", line 539, in main                                                                run_main(FLAGS, default_hparams, train_fn, inference_fn)                                                                                      File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\nmt.py"", line 532, in run_main                                                            train_fn(hparams, target_session=target_session)                                                                                              File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\train.py"", line 266, in train                                                             sample_tgt_data)                                                                                                                              File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\train.py"", line 140, in run_full_eval                                                     eval_model, eval_sess, model_dir, hparams, summary_writer)                                                                                    File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\train.py"", line 71, in run_internal_eval                                                  summary_writer, ""dev"")                                                                                                                        File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\train.py"", line 420, in _internal_eval                                                    ppl = model_helper.compute_perplexity(model, sess, label)                                                                                     File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model_helper.py"", line 453, in compute_perplexity                                         loss, predict_count, batch_size = model.eval(sess)                                                                                            File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model.py"", line 251, in eval                                                              self.batch_size])                                                                                                                             File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\client\session.py"", line 956, in run       run_metadata_ptr)                                                                                                                             File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\client\session.py"", line 1180, in _run     feed_dict_tensor, options, run_metadata)                                                                                                      File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\client\session.py"", line 1359, in _do_run                                                                                                                                                  run_metadata)                                                                                                                                 File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\client\session.py"", line 1384, in _do_call                                                                                                                                                 raise type(e)(node_def, op, message)                                                                                                        tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.                                                                     (0) Internal: Blas GEMM launch failed : a.shape=(135, 512), b.shape=(512, 15003), m=135, n=15003, k=512                                                [[node dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul (defined at C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]                                                                                        [[dynamic_seq2seq/truediv/_195]]                                                                                                         (1) Internal: Blas GEMM launch failed : a.shape=(135, 512), b.shape=(512, 15003), m=135, n=15003, k=512                                                [[node dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul (defined at C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]                                                                               0 successful operations.                                                                                                                        0 derived errors ignored.                                                                                                                                                                                                                                                                       Original stack trace for 'dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul':                                                            File ""train.py"", line 17, in <module>                                                                                                             tf.app.run(main=nmt.main, argv=[os.getcwd() + '\nmt\nmt\nmt.py'] + unparsed)                                                                  File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run          _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)                                                                          File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py"", line 303, in run                                    _run_main(main, args)                                                                                                                         File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py"", line 251, in _run_main                              sys.exit(main(argv))                                                                                                                          File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\nmt.py"", line 539, in main                                                                run_main(FLAGS, default_hparams, train_fn, inference_fn)                                                                                      File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\nmt.py"", line 532, in run_main                                                            train_fn(hparams, target_session=target_session)                                                                                              File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\train.py"", line 223, in train                                                             eval_model = model_helper.create_eval_model(model_creator, hparams, scope)                                                                    File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model_helper.py"", line 157, in create_eval_model                                          extra_args=extra_args)                                                                                                                        File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\attention_model.py"", line 61, in __init__                                                 extra_args=extra_args)                                                                                                                        File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model.py"", line 97, in __init__                                                           res = self.build_graph(hparams, scope=scope)                                                                                                  File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model.py"", line 284, in build_graph                                                       encoder_outputs, encoder_state, hparams)                                                                                                      File ""C:\Users\Harjyot\Desktop\code\nmt-chatbot/nmt\nmt\model.py"", line 407, in _build_decoder                                                    logits = self.output_layer(outputs.rnn_output)                                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\layers\base.py"", line 548, in __call__     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 854, in __call__                                                                                                                                         outputs = call_fn(cast_inputs, *args, **kwargs)                                                                                               File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 234, in wrapper                                                                                                                                               return converted_call(f, options, args, kwargs)                                                                                               File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 439, in converted_call                                                                                                                                        return _call_unconverted(f, args, kwargs, options)                                                                                            File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 330, in _call_unconverted                                                                                                                                     return f(*args, **kwargs)                                                                                                                     File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\layers\core.py"", line 1039, in call                                                                                                                                                  outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]])                                                                      File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 4071, in tensordot                                                                                                                                                  ab_matmul = matmul(a_reshape, b_reshape)                                                                                                      File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\util\dispatch.py"", line 180, in wrapper                                                                                                                                                    return target(*args, **kwargs)                                                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 2754, in matmul     a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)                                                                            File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 6136, in mat_mul                                                                                                                                                name=name)                                                                                                                                    File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 794, in _apply_op_helper                                                                                                                                op_def=op_def)                                                                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func                                                                                                                                                return func(*args, **kwargs)                                                                                                                  File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3357, in create_op                                                                                                                                                 attrs, op_def, compute_device)                                                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3426, in _create_op_internal                                                                                                                                       op_def=op_def)                                                                                                                                File ""C:\Users\Harjyot\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__                                                                                                                                                  self._traceback = tf_stack.extract_stack()
``

this error is when i run train.py
"
47902,Unable to link c++ project due to missing header files,"**System information**
- Linux Ubuntu 18.04
- Tensorflow 2.4.1 (from source)
- Bazel 4.0.0
- g++ 7.5.0

**Describe the problem**
I am trying to run some c++ source examples that I've found around, but they require the following header files:
```
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""
```
I ran into a compile issue:
```
/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/public/session.h:22:10: fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory
```
I found this file `/tensorflow-2.4.1/bazel-out/host/bin/tensorflow/core/framework/device_attributes.pb.h` and moved it to `tensorflow/core/framework/`, same with `graph.pb.h`. That seemed to work. However, now I'm running into this issue:

```
/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/framework/device_attributes.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory
 #include <google/protobuf/port_def.inc>
```
I think I found the protobuf that was downloaded for tensorflow at `tensorflow-2.4.1/bazel-bin/external/com_google_protobuf`, but it doesn't contain this file. I think the version is different? I tried independently downloading protobuf but this file still didn't exist. Here's my protobuf version:
```
protoc --version
libprotoc 3.0.0
```

So I'm kind of stuck at this point. Am I doing this right? Is there an easier way to access the c++ api? Is there a different example I need to be following?

EDIT: I tried cloning protobuf and building from source, which seems to be version 3.15.6, but now I'm missing yet another file
```
protoc --version
libprotoc 3.15.6

/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/framework/device_attributes.pb.h:28:10: fatal error: google/protobuf/inlined_string_field.h: No such file or directory
 #include <google/protobuf/inlined_string_field.h>
```"
47901,Feature Request for N-Dimensional Operations,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes. I would prefer that.



**Describe the feature and the current behavior/state.**
Currently tensorflow has 1D, 2D and 3D layers for various operations. I would like to request N-dimensional convolutional layers (i.e. ConvND, MaxPoolingND, etc). I can see that there is some reference to convND here (https://github.com/tensorflow/tensorflow/blob/8e985babf8c8ed248551aa42dc9b21fd757739d1/tensorflow/core/grappler/optimizers/constant_folding.cc). 


**Will this change the current api? How?**
Yes. 
- ConvND, MaxPoolND, AveragePoolingND, CroppingND, GlobalAveragePoolingND, ZeroPaddingND would be added to the already existing API. 
- Other convolutional Layers can be a specific form of the more generalized N dimensional layers.
**Who will benefit with this feature?**
- Anyone willing to explore these operations on more than 3 dimensions
**Any Other info.**
"
47898,WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image nvidia/cuda:10.2-devel-ubuntu18.04 (running Kebernetes on GCP)
- TensorFlow installed from (source or binary): 2.4.0
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: Python 3.8.0
- CUDA/cuDNN version: cuda:10.2
- GPU model and memory:  NVIDIA Tesla T4 16 GB GDDR6

Came across this message while debugging a problem in our processing pipeline, with the request to file a bug report with export AUTOGRAPH_VERBOSITY=10
...
TypeError: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
Error transforming entity <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>>
WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
...

Full output is rather large:
https://pastebin.com/raw/G9jA8fXm
"
47895,using ragged tensors with custom loss and metric ,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): NA
- TensorFlow version (use command below): TF 2.4.1
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11/10
- GPU model and memory: NVIDIA Quadro P2000

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

TF 2.4.1

**Describe the current behavior**

Please see below link to my code. I am working with ragged tensors. My data is (N, length, features) where length is variable. I do not understand how each epoch takes around 6000 seconds to complete. Is it because of the @tf.autograph.experimental.do_not_convert decorator? Is it because of ragged tensors? Is it because of something else. I have implemented the same but padding each data observation to maximum length (therefore, did not need the custom loss and metric I implemented) and an epoch runs for 200 seconds.

**Describe the expected behavior**

Understand why using ragged tensors takes so long vs padding to max length.

Understand 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

My issue is not reproducible because I am working with ragged tensors and had to manually change some of the .py files that come with tensorflow. See issue https://github.com/tensorflow/tensorflow/pull/45015 from @pedro-r-marques and many others referencing ragged tensors. If whoever reviews this can manually make the changes then my code should work. 

Anyways here is my code so it helps -- also the data is fictitious. I am dealing with roughly 10 GB of data but the format is the same. A list of 2d lists with unequal sequence length (same feature length).

https://colab.research.google.com/drive/1bXmCKuABlGwjoeTxwk-mBfKA_z7BXWYA?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I had to add the decorator @tf.autograph.experimental.do_not_convert on essentially all functions as otherwise I would get an WARNING as such:

```
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000001A3835F4040> and will run it as-is.
Cause: could not parse the source code of <function <lambda> at 0x000001A3835F4040>: no matching AST found
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
"
47894,"Last batch always pad up to the batch size and cause error ""logits and labels must be broadcastable""","I'm using tensorflow 2.4.1 on Win 10 and Python 3.8.6.

When I use data generator and reach the last batch of training, the model will always pad up to the full batch size even if the real size is less than that. 

This will cause an error ""logits and labels must be broadcastable"", which is basically a mismatch of my label size (13x24) and predication size (32x24), which 32 is my batch size and 24 is number of classes. 

```
BATCH_SIZE = 32
NUM_EPOCHS = 1
NUM_CLASSES = 24
TIME_STEPS = 1000

def build_model():
    total = ((len(tr_idx) + BATCH_SIZE - 1) // BATCH_SIZE) * NUM_EPOCHS

    inp1 = layers.Input(shape=(TIME_STEPS, 12))
    # inp2 = layers.Input(shape=2)

    out = layers.BatchNormalization(axis=1)(inp1)
    out = layers.Conv1D(filters=64, kernel_size=50, strides=10, activation='relu')(out)
    out = layers.Conv1D(filters=64, kernel_size=50, strides=10, activation='relu')(out)
    out = layers.MaxPool1D(pool_size=2)(out)
    out = layers.Flatten()(out)
    out = layers.Dense(NUM_CLASSES*2, activation='relu')(out)

    out1 = layers.Dense(NUM_CLASSES, activation='softmax')(out)
    out2 = layers.Dense(NUM_CLASSES, activation='softmax')(out)
    out3 = layers.Dense(NUM_CLASSES, activation='softmax')(out)

    model = models.Model(inp1, [out1, out2, out3])
    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=keras.experimental.LinearCosineDecay(5e-4, total)))
    # model.summary()
    return model

model = build_model()
```

Here is my data generator
```
class datagen:
    def __init__(self, data, batch_size = BATCH_SIZE, branch='train'):
        self.data = data.reset_index(drop=True)
        self.batch_size = batch_size
        self.branch = branch
        self.steps = len(self.data) // self.batch_size
        if len(self.data) % self.batch_size !=0:
            self.steps += 1

    def __len__(self):
        return self.steps

    def __iter__(self):
        while True:
            if self.branch == 'train':
                self.data = self.data.sample(frac=1).reset_index(drop=True)
            for i in range(self.steps):
                batch_data = self.data[i * self.batch_size: (i+1) * self.batch_size]
                batch_image_ids = batch_data['ID'].tolist()
                with zipfile.ZipFile('record_step1k.zip', 'r') as zipf:
                    tmp = np.zeros((self.batch_size, TIME_STEPS, 12))
                    for cnt, id in enumerate(batch_image_ids):
                        x = zipf.read(id)
                        tmp[cnt] = np.frombuffer(x).reshape((12, -1)).transpose()
                if self.branch == 'train':
                    yield tmp, [keras.utils.to_categorical(batch_data['Dx_1'], NUM_CLASSES), 
                                keras.utils.to_categorical(batch_data['Dx_2'], NUM_CLASSES),
                                keras.utils.to_categorical(batch_data['Dx_3'], NUM_CLASSES)]
                else:
                    yield tmp
```"
47893,TFLite Documentation Request: How do per-axis quantizations work with subsequent layers?,"## URL(s) with the issue:

https://www.tensorflow.org/lite/performance/quantization_spec

## Description of issue (what needs changing):
Documentation on how per-axis quantization works with subsequent layers

### Clear description

I am working on implementing the algorithms for TFLite layers in FPGA and I'm uncertain how the quantization spec expects subsequent layers to deal with per-axis quantizations. For example, following the tutorial [here](https://www.tensorflow.org/lite/performance/post_training_integer_quant) there is a 12-channel convolution which is pooled and flattened before being fed to a fully connected layer. The output of the convolution op has different quantization scales for each of the 12 channels. But then flattening the tensor it would seem this information is lost? The two options I can see are either rescaling the tensor so that the reshaped tensor has the same quantization uniformly. That would seemingly eliminate any benefits of per-axis quantization if it's just being undone in the next step. If the reshape does not re-scale them to the same scheme, wouldn't that then introduce a massive difference in the distribution of data going to the fully connected layer?

The logical explanation to me is that per-axis quantizations are only useful when feeding one convolutional layer to another convolutional layer. If the output of a convolution operation goes to anything other than a convolution, the output is re-scaled to match the same quantization scheme for all channels. But I'm unsure if that's what actually happens

### Correct links

N/A

### Parameters defined

N/A

### Returns defined

N/A

### Raises listed and defined

N/A

### Usage example

N/A

### Submit a pull request?

I do not know enough to submit a PR but if someone with knowledge can answer my question I can contribute it to the docs.
"
47891,OSError: Cannot understand given URI: <tf.Tensor 'filepath:0' shape=() dtype=string>.,"Dear All,
I am trying to implement a deep learning model using .tiff files
To read them I use skimage but I have difficulties to create a tensor flow dataset:
I did:
dataset_train = tf.data.Dataset.from_tensor_slices((X_train_path, y_train_path))
dataset_train = dataset_train.map(lambda x, y : [load_image(x), load_image(y)], num_parallel_calls=-1).batch(32)
Where:
def load_image(filepath):
    # read file
    im = skimage.io.imread(filepath)
    return im

and X_train_path is an object containing the list of the tif files

I have the following error message:
    OSError: Cannot understand given URI: <tf.Tensor 'filepath:0' shape=() dtype=string>.

Images are encoded on 4 channels.
Can you help me ?"
47890,Latest Windows 10 update - Import tensorflow - Kernel dies,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 19042.867
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): binary (conda)
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.1 (anaconda / spyder 4.1.4)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.2 / cuDNN 8.1.0.77
- GPU model and memory: Nvidia Quadro M620

**Describe the current behavior**
Since the last Windows 10 upgrade, it is impossible to import tensorflow. The kernel dies and restarts.
Non TF python code runs fine

**Describe the expected behavior**
Import tensorflow as usual :)

**Standalone code to reproduce the issue**
`import tensorflow as tf`

2021-03-18 17:10:01.890125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll

Kernel died, restarting


 
2021-03-18 17:10:01.890125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
[SpyderKernelApp] WARNING | No such comm: 12ee0da4880411ebac01b6d88ae4f6b5


"
47889,Rename tf.keras.optimizers to tf.keras.optims for more aesthetic import statements,"TensorFlow imports are often tedious, and i think it would be nice if some of the most common imports from `tensorflow.keras` aligned better with one another. For this reason, I propose that `tf.keras.optimizers` be renamed to `tf.keras.optims`.

```python
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.losses import *
from tensorflow.keras.optims import *
```

rather than

```python
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.losses import *
from tensorflow.keras.optimizers import *
```"
47888,Tensorflow lite micro 2.4 Alpha version missing tflite::ops::micro::Register_FULLY_CONNECTED(),"**System information**
- Arduino Nano BLE 33 SENSE 
- Arudino Pro IDE Alpha 0.1.4 macos 


**Provide the text output from tflite_convert**

```
#tensorflow lite micro missing Register_FULLY_CONNECTED() layer in version 2.4 alpha when upload the Arduino IDE recommend with ""Register_UNPACK();"" but somehow it still fixing the issue during upload the problem is solved when i return back to TFlite micro version 2.1 !!!
is there any replacement layer here ????? '

please helps thank you so much for who answer this question 
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
47886,Segmentation Fault when Benchmarking TFLite EfficientNetB0 ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.02/Android 10
- Mobile devices: Xiaomi Redmi Note 7 & Huawei P30
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8

**Describe the current behavior**
When running the TFLite benchmark of a converted `EfficientNetB0` model on a mobile phone, I get a `Segmentation fault` error. I would be interested in any ideas of what could be causing this, if it is coming from the `.tflite` file or when running the benchmark. I am quite lost and have no idea where to look. Thanks in advance!

**Describe the expected behavior**
Get the profiling of the model.

**Standalone code to reproduce the issue**
I am generating the `tflite` file with the following script:
```
import tensorflow as tf
import tensorflow_datasets as tfds

# Create model
model = tf.keras.applications.EfficientNetB0(include_top=False)


# Create MNIST dataset
ds = tfds.load(
    name=""coco/2017"",
    split=""train"",
    data_dir=""/data/datasets/tensorflow_datasets/"",
)
ds = ds.map(lambda obj: tf.image.resize(obj[""image""], (224, 224)))

# Transform the dataset into a representative dataset as in the TF guide
def representative_data_gen():
    for input_value in ds.batch(1).take(100):
        # Model has only one input so each data point has one element.
        yield [input_value]


# Converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Set the representative dataset in order to quantize the activations
converter.representative_dataset = representative_data_gen

# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]

# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

# Additional tricks
converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
]

tf_lite_quant_model = converter.convert()

# saving converted model in TFLite file
with open(""EfficientNetB0.tflite"", ""wb"") as tf_file:
    tf_file.write(tf_lite_quant_model)

print(""Converted"")
```
I use the TFLite benchmarking tool that can be downloaded [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_arm_benchmark_model)

I am running the benchmark with the successive commands : 
```
adb push android_arm_benchmark_model /data/local/tmp/benchmark
adb shell chmod +x /data/local/tmp/benchmark
adb push EfficientNetB0.tflite /data/local/tmp/model.tflite
adb shell /data/local/tmp/benchmark --graph=/data/local/tmp/model.tflite --use_gpu=false
adb shell rm -f /data/local/tmp/benchmark
adb shell rm -f /data/local/tmp/model.tflite
```"
47885,I hope that the tfjs-models project will increase the configuration resource link method,"Hi guys, I used tfjs-models project on my vue project for the first time today, and I also successfully launched posenet and handpose. Thank you for your contribution, but due to some special reasons, I cannot open tfhub.dev Just like google.com, I found that tfhub.dev will redirect to storage.googleapis.com, I can access storage.googleapis.com normally, and there is no problem running posenet, but handpose will fail to load mode, so I hope it can Provide a url method to configure the model, easy to replace https://github.com/tensorflow/tfjs-models/blob/master/handpose/src/index.ts 28 lines 38 lines 48 lines links.

My English is not very good, this is machine translation, thank you again for your contribution"
47884,Return gradient 0 in mixed_precison setting,"**System information**

OS Platform and Distribution: linux
TensorFlow version (use command below): tf2.4.1

**Reproducing code**

```

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import mixed_precision
import numpy as np
from tqdm import tqdm

gpus = tf.config.experimental.list_physical_devices('GPU')

for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)


mixed_precision.set_global_policy('mixed_float16')


def forward_conv(x, filters, kernels, name='forward', padding='same'):
    i = 0
    for flt, kernel in zip(filters, kernels):
        x = layers.Conv3D(flt, kernel, activation='relu', padding=padding, dilation_rate=(1, 1, 1),
                          use_bias=False, name=str(i) + '_' + name)(x)
        x = layers.BatchNormalization(name=str(i) + '_bn_' + name)(x)
        i += 1
    return x


def part_one(ipt):
    l1 = forward_conv(ipt, (4, 4), (3, 3), name='enc1')
    d2 = layers.MaxPool3D(pool_size=(2, 2, 2))(l1)
    l2 = forward_conv(d2, (4, 4), (3, 3), name='enc2')
    return l1, l2


def part_inner(ipt1, ipt2):
    l1 = forward_conv(ipt1, (4, 4), (3, 3), name='enc1')
    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='enc2')
    return l1, l2


def part_two(ipt1, ipt2):
    l2 = forward_conv(ipt2, (4, 4), (3, 3), name='dec2')
    u1 = layers.UpSampling3D(size=(2, 2, 2))(l2)
    r1 = forward_conv(ipt1 + u1, (4, 4), (3, 3), name='dec1')
    return r1


initial = tf.ones([1, 256, 368, 368, 1], dtype=tf.float16)

tf.random.set_seed(1)

with tf.GradientTape() as g:
    g.watch(initial)
    l1_, l2_ = part_one(initial)
    for _ in range(2):
        l1_, l2_ = part_inner(l1_, l2_)
    opt_ = part_two(l1_, l2_)
    loss = tf.reduce_mean(l1_) + tf.reduce_mean(opt_)
    gd = g.gradient(loss, initial)
    print('-' * 100)
    print(f'loss is {loss} and grad is {np.sum(gd)} with ckpt= {ckpt}')
```

**Behavior description**

When using tf.float32 setting, the result of gradient is reasonable with value around 0.6, however, when shift to tf.float16 with mixed_precision, the gradient is constantly 0. Should we expect that the computed gradient is so different between normal float32 mode and mixed_precision float16 mode? Thank you!

"
47883,"Unable to install Tensorflow due to one of the file named ""getObjectTorrentRequest.h"" file , Torrent named file are blocked in various system across organisations.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform: Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:latest
- Python version:3.8
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

Unable to install Tensorflow due to one of the file named ""getObjectTorrentRequest.h"" file , ""Torrent"" named file are blocked in systems across various organisations.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://user-images.githubusercontent.com/42913143/111647376-6f299e80-8828-11eb-9426-e5c7f8fca466.png)
"
47882,"How Is Padding Calculated for Conv1D,2D,....","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D]()
[https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/convolutional.py#L375-L512]()
[https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/layers/convolutional.py#L52]()

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):
Add clear description of how the amount of convolutional padding is computed as well as conv transpose padding.

### Clear description
I'm looking for the padding algorithm used to calculate the amount of padding for ""SAME,"" ""VALID,"" etc, but I cannot find it. I've looked at the above links along with some other links to find the algorithm used, but I've only seen class attributes being assigned strings of what ""mode"" to use. I could not find anything that looked like an associated function call or a dict of functions which makes it difficult to read the algorithm or even view pre-made documentation. I have looked elsewhere on the web to find methods of computing the padding size. Those methods do work for some if not most cases, but they aren't universal as many of them fail for an input dimension of 9 (9x1 1D-Conv) with a filter size of 3. In this special case, the amount of padding necessary is 3 (the same size as the filters), and the methods I have found have maxed out at a padding of 1 for both sides of the input which will fail. Also, most posts on StackOverflow are in regards to computing the output dimensions of convolutional layers which isn't what I'm looking for. I have also looked at tf.pad(), but there is no translation from ""SAME"" to an accepted function argument.

This is necessary to be able to port the Tensorflow model to a system where Tensorflow cannot run.

For example, why should someone use this method? How is it useful?
This is necessary to be able to port the Tensorflow model to a system where Tensorflow cannot run. It would also be necessary for any porting to other Deep Learning packages.

### Correct links

Is the link to the source code correct?
As far as I know they are.

### Parameters defined

N/A

### Returns defined

N/A

### Raises listed and defined

N/A

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

N/A

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

N/A

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
47881,Extracting tf.Variable() in numpy or tensor format for further processing from a ListWrapper(),"Hello there,

I have a simple question, that I don't seem to find any documentation on regarding it. I am using costume code, and during one of my training loops a ListWrapper Object is created containing some tensor variable.

Initially when using tf.print() I was able to see the numerical contents of the tensor but not assign to a variable for further processing. 

After using tf.compat.v1.flags.tf_decorator.unwrap(x), where x is the initial ListWrapper, I am indeed able to see the tensor in tf.Variable format like that : 


<bound method ListWrapper.append of ListWrapper([<tf.Variable 'Variable:0' shape=(10, 10) dtype=float32, numpy=                                                                                                                array([[ 0.05768297,  0.        ,  0.        ,  0.        ,  0.        ,                                                                                                                                                                0.        ,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.00587903,  0.01599882,  0.        ,  0.        ,  0.        ,                                                                                                                                                                0.        ,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.0069333 ,  0.00599516,  0.05650787,  0.        ,  0.        ,                                                                                                                                                                0.        ,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.00601203,  0.0059952 , -0.0027298 ,  0.05753344,  0.        ,                                                                                                                                                                0.        ,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [ 0.00507043, -0.00599517, -0.00196748,  0.00593348,  0.03158937,                                                                                                                                                                0.        ,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.00458831,  0.00599515,  0.00015493, -0.00596476, -0.00691961,                                                                                                                                                                0.03968148,  0.        ,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.00672386,  0.00599529,  0.00553486, -0.00642598,  0.00682343,                                                                                                                                                               -0.00884094,  0.05674272,  0.        ,  0.        ,  0.        ],                                                                                                                                                             [-0.00481304,  0.00599513,  0.00011724, -0.00551202, -0.0070322 ,                                                                                                                                                                0.00368161, -0.00388715,  0.05674548,  0.        ,  0.        ],                                                                                                                                                             [-0.00566915,  0.00599517, -0.00349799, -0.00581874, -0.00713581,                                                                                                                                                               -0.0043654 , -0.00537512, -0.00558803,  0.05750472,  0.        ],                                                                                                                                                             [-0.00441868,  0.00599512,  0.00767725, -0.00547412, -0.00668069,                                                                                                                                                                0.00388396, -0.00506157, -0.00536125, -0.00621854,  0.05662818]],                                                                                                                                                           dtype=float32)>])>   


I am not able to subscript or use attributes to recover the variable and store it in numpy format, which would be the easier way for me to recover it in my training loop. 

Any suggesting are highly appreciated.



"
47879,Keras model functional build of a dynamic lambda layer fails with `RecursionError`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8

**Describe the current behavior**
Building a model with dynamic lambda layer fails with exception `RecursionError`

**Describe the expected behavior**
The model should build without error.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

input = tf.keras.Input(shape=())
output = tf.keras.layers.Lambda(lambda x: x, dynamic=True)(input)
tf.keras.Model(inputs=input, outputs=output)
```
Output:
`RecursionError: maximum recursion depth exceeded while calling a Python object
`
"
47878,tensorflow 2.4.1 incompatibility with numpy 1.20.1,"Hi, 

pip install reports that tensorflow 2.4.1 is incompatible with numpy 1.20.1 as it requires numpy 1.19.x

Is there any workaround for this right now or are there plans to upgrade requirements to numpy 1.20.x?

Thanks in advance!

"
47877, module 'tensorflow.keras.layers' has no attribute 'MulitiHeadAttention',"When i use the newest version2.4.1 when i use 

head_attention1 = tf.keras.layers.MulitiHeadAttention(num_heads = 1,key_dim=1)(conv1)

it will has a AttributeError : module 'tensorflow.keras.layers' has no attribute 'MulitiHeadAttention'"
47876,Tensorflow not using GPU eventhough it is detecting it.,"**System information**
- I have custom code
- Linux, Ubuntu 20.04
- Not on a mobile device
- Binary
- Tensorflow: v2.4.1
- Python version: python3.8
- CUDA/cuDNN version: v11.0, v8.0.4
- GPU model and memory: Nvidia GTX 1080 Ti, 16 GB

Tensorflow detects my GPU. When I do` tf.config.list_physical_devices()`
I get


```

2021-03-18 13:21:44.097589: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-18 13:21:44.113342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-18 13:21:44.230424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-03-18 13:21:44.230498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-18 13:21:44.248080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-18 13:21:44.248189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-18 13:21:44.257026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-18 13:21:44.261437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-18 13:21:44.277956: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-18 13:21:44.285094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-18 13:21:44.288191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-18 13:21:44.295201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```


But when I'm training `nvidia-smi`
returns 


```
Thu Mar 18 13:22:35 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:0A:00.0  On |                  N/A |
| 32%   57C    P0    65W / 250W |    381MiB / 11177MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3413      G   /usr/lib/xorg/Xorg                 35MiB |
|    0   N/A  N/A      5526      G   /usr/lib/xorg/Xorg                116MiB |
|    0   N/A  N/A      5669      G   /usr/bin/gnome-shell               96MiB |
|    0   N/A  N/A      7173      G   ...AAAAAAAAA= --shared-files       75MiB |
|    0   N/A  N/A      7628      G   ...AAAAAAAAA= --shared-files       14MiB |
|    0   N/A  N/A      7636      G   ...AAAAAAAA== --shared-files       15MiB |
|    0   N/A  N/A    277380      G   ...AAAAAAAAA= --shared-files       11MiB |
+-----------------------------------------------------------------------------+

```
It doesn't look like my GPU is being used.

I expect the GPU to be used

I'm just import tensorflow and expecting it to use the GPU, tell me if there's another way.
I remember setting an environment variable `CUDA_VISIBLE_DEVICES` a few versions ago. Is it still necessary to set this?"
47875,Nvidia RTX 3090 is slower than Nvidia GTX 1080Ti for single image prediction,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1 (2.2.5 for Keras version)
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2/8.1.0
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I tried five CNN models (ResNet50, ResNet101, ResNet152, DenseNet121, and MobileNet) to measure their inference time with  batch size one. The CNN models running on 3090 GPU is slower than running on 1080Ti GPU, and both of the devices are slow in eager execution.
| | 3090 GPU (enable eager model) | 3090 GPU (disable eager model) | 1080Ti GPU (enable eager model) | 1080Ti GPU (disable eager model) |
| :---: | :---: | :---: | :---: | :---: |
| ResNet50 | 39.643 ms | 23.588 ms | 25.103 ms | 11.846 ms |
| ResNet101  | 51.097 ms | 36.063 ms | 29.640 ms | 17.598 ms |
| ResNet152  | 50.035 ms | 48.3 ms | 34.169 ms | 22.586 ms |
| DenseNet121  | 52.822 ms | 43.287 ms | 29.565 ms | 18.306 ms | 
| MobileNet  | 28.205 ms | 10.332 ms | 21.362 ms | 4.294 ms | 

**Describe the expected behavior**
3090 GPU is faster than 1080Ti GPU.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

import tensorflow
import numpy
import time
import cv2
 
tensorflow.compat.v1.disable_eager_execution()
keras_resnet50 = tensorflow.keras.applications.resnet50.ResNet50()
image = numpy.asarray(cv2.resize(cv2.imread('cat.png'), (224, 224)))
data = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
keras_resnet50.predict(data)
t = time.time()
keras_resnet50.predict(data)
print((time.time() - t) * 1e6)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Test image: https://raw.githubusercontent.com/dmlc/mxnet.js/main/data/cat.png
"
47874,Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64,"I'm having issues with lots of stuff and I've a limited amount of knowledge in nvidia drivers. So, I wanted to use my GPU for training in keras. For that, I had to make tensorflow use my GPU(correct me if I'm wrong). I have a supported GPU. I followed [this][1] tutorial. I have a Geforce GTX 1080 Ti. I installed Nvidia driver 455 successfully. I've installed Nvidia Cuda Toolkit 11.2.1 and have tried all three methods of installing. I think CUPTI comes with Nvidia Cuda Toolkit, so, I did nothing for that. and installed cuDNN v8.1.1.33.


When I import tensorflow and list devices, I get this:

    >>> tf.config.list_physical_devices()   
    2021-03-18 10:56:30.410381: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
    2021-03-18 10:56:30.411387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
    2021-03-18 10:56:30.451723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
    pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
    coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
    2021-03-18 10:56:30.451771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
    2021-03-18 10:56:30.454059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
    2021-03-18 10:56:30.454124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
    2021-03-18 10:56:30.454855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
    2021-03-18 10:56:30.455023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
    2021-03-18 10:56:30.455150: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64
    2021-03-18 10:56:30.455675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
    2021-03-18 10:56:30.455764: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
    2021-03-18 10:56:30.455776: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
    Skipping registering GPU devices...
    [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]


This is my LD_LIBRARY_PATH, All these directories exist.

    export LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64



I know this issue has been reported before, but the solution there seemed to be reinstalling and I've removed and installed all of the required software multiple times. I'm at this for 4 days now. Also, I installed an outdated Nvidia Cuda toolkit v10 from `apt` once and later removed it.


Please help. Thank you.

  [1]: https://www.tensorflow.org/install/gpu#software_requirements

"
47872,Multi input/output model works correctly in TF 1.15; produces error in TF 2.4.1,"- TensorFlow version: 2.4.1
- Python version: 3.8.3


**Describe the current behavior** 
Coded a multi input/output model
model.fit fails with the following error:

ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {""<class 'numpy.ndarray'>""}), (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'float\'>""})'})


**Describe the expected behavior**
Code runs properly as expected in TF 1.15 (with Python 3.6). 

**Standalone code to reproduce the issue**
% Runs properly in TF 1.15
model.fit([x1, x2], y.T.tolist(), 
                  batch_size=8,
                  epochs=50, verbose=1, shuffle=True,
                  callbacks=[earlystop])

% where type(x1) is numpy.ndarray
% where x1.shape is (7869, 1, 68)
% where type(x2) is numpy.ndarray
% where x2.shape is (7869, 500)
% where type(y.T.tolist()) is list
% where len(y) is 3
% where type(y) is numpy.ndarray
% where y.shape is (7869, 3)

**Addendum**

Note: I referred to [#42175](https://github.com/tensorflow/tensorflow/issues/42175) and ensured x1, x2, and y are np.array. 

model.fit([np.asarray(x1), np.asarray(x2)], y, 
                  batch_size=8,
                  epochs=50, verbose=1, shuffle=True,
                  callbacks=[earlystop])

This produced this error:
TypeError: 'NoneType' object is not callable
"
47871,Unnecessary Device to Host copies,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8
- Bazel version (if compiling from source):3.0.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.1
- GPU model and memory:8.1

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

There are issues regarding unnecessary DtoH copies. When The tensor is originated on the device using any (GPU-friendly) dataset subclass or any other type of pipeline (one particular example is CuPy and DLPack), there are unnecessary DtoH copies happening. As a more explicit example when CuPy and keras are used DLPack invokes unexpected DtoH copies, which makes DLPack meaningless

**Describe the expected behavior**

Should eliminate the unnecessary DtoH and HtoD copies.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

`
!pip install cupy-cuda111
import os
print(os.getpid())
import cupy as cp
import tensorflow as tf
from tensorflow.keras.layers import Input, Lambda
from tensorflow.keras.models import Model

n = 10000000

a = Input(shape=(n))
output = Lambda(lambda x: x ** 2)(a)
model = Model(a, output)

x = cp.arange(n, dtype=cp.float32).reshape(1, n)
dltensor = x.toDlpack()
x2 = tf.experimental.dlpack.from_dlpack(dltensor)

cp.cuda.nvtx.RangePush('model.predict')
y = model.predict(x2)
cp.cuda.nvtx.RangePop()
print('done')
`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
47870,Confine Tensorflow v2.3.2 C API to use only one thread.,"

**System information**
- Have I written custom code: No
- OS Platform and Distribution: Linux CentOS 8.3.2011
- TensorFlow installed from: source (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md)
- TensorFlow version: Tensorflow C API v2.3.2 (v2.3.2-0-g9edbe5075f7)
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.1
- CUDA/cuDNN version: N/A
- GPU model and memory: No GPU, 16 GB RAM




**Describe the current behavior**
Tensorflow C API generates multiple threads during inference, with at least one thread on each available CPU.

**Describe the expected behavior**
There should be one and only one thread created regardless of hardware setup (E.g. number of CPUs, number of cores etc.)

**Standalone code to reproduce the issue**
N/A

I am interested in what source code would change in order to make TensorFlow C API use only 1 thread.

Some related issues : 
- https://github.com/tensorflow/tensorflow/issues/43671
- https://github.com/tensorflow/tensorflow/issues/42510
- https://github.com/usnistgov/frvt/issues/12

Other issues on Github deal with TensorFlow 1.x API, but I am specifically interested in TensorFlow v2.3.2 C API.
"
47866,Incompatibility between `set_visible_devices()` and `from_dlpack()`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX 1080Ti 11GB

**Describe the current behavior**

After selecting which GPU to use via `tf.config.experimental.set_visible_devices()`, converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` only works for device 0. Using other devices results in the error `InvalidArgumentError: GPU:1 unknown device.`

**Describe the expected behavior**

Converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` should work for all devices.

**Standalone code to reproduce the issue**
```python
import cupy
import random

import tensorflow as tf

# gpu_to_use = 0      # Works
gpu_to_use = 1        # Errors

gpus = tf.config.experimental.list_physical_devices(""GPU"")
if gpus:
    tf.config.experimental.set_visible_devices(gpus[gpu_to_use], ""GPU"")

# Converting from TF to CuPy with dlpack works for both devices
tensor = tf.random.uniform((10,))

dltensor = tf.experimental.dlpack.to_dlpack(tensor)
array1 = cupy.fromDlpack(dltensor)

# Converting from CuPy to TF with dlpack only works for device 0
array1 = cupy.array([random.uniform(0.0, 1.0) for i in range(10)], dtype=cupy.float32)
dltensor = array1.toDlpack()
x = tf.experimental.dlpack.from_dlpack(dltensor)

# Using device 1 results in the following error

# Traceback (most recent call last):
#   File ""examples/multi-gpu/tf-dlpack-repro.py"", line 22, in <module>
#     x = tf.experimental.dlpack.from_dlpack(dltensor)
#   File ""/home/karl/miniconda3/envs/nvtabular_dev_11.0/lib/python3.8/site-packages/tensorflow/python/dlpack/dlpack.py"", line 66, in from_dlpack
#     return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)
# tensorflow.python.framework.errors_impl.InvalidArgumentError: GPU:1 unknown device.
```
"
47863,OOM errors running multiprocessing inference,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.65
- GPU model and memory: NVIDIA GTX 1070 8 GB

**Describe the current behavior**
I have a set of images that I need to run an inference on. What I am trying to do is spawn 4 workers and each worker has 1/4 access to GPU's memory. I am getting OOM even if I restrict the memory to 1/8 per worker. Below is some of the code:

```
def process_image(doc_paths):
    model = models.Model(my_model)
    for path in doc_paths:
        ...do some work...
        model.do_predict()
        return 0 or 1


if __name__ == ""__main__"":
    num_processes = 4
    process_pool_executer = concurrent.futures.ProcessPoolExecutor(num_processes)

    # Split documents into `num_processes` equal parts, so in our case -> 4 equal parts
    for i, paths in enumerate(sub_file_paths):
        print(f'==== Processing path {i} ====')
        f = process_pool_executer.submit(process_image, paths)
        futures.append(f)

        for future in concurrent.futures.as_completed(futures):
            r = future.result()
            results.append(r)
```

Within the `do_predict()` function, I import tensorflow and have tried both these memory allocation options:

```
# OPTION 1
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_virtual_device_configuration(gpus[0], [
            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
    except RuntimeError as e:
        print(e)


# OPTION 2
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
```

Both give me OOM errors. According to nvidia-smi my GPU has 8081 MiB free so allocating 1024 MB for each process should be fine. So I must be doing something or thinking of something in the wrong way. Any thoughts?


**Describe the expected behavior**
Expected behavior would be that I could run the above without OOM errors.


**EDIT:**
When running this script with one worker it works fine. And looking in task manager, my GPU usage does not get above 10%.

**EDIT2:**
I am actually using TF version 2.2 - not 2.3."
47862,TensorFlow-Lite: need for ruy library,"Hello

For an embedded software project, we are trying to integrate TensorFlow-Lite via buildroot (via CMake). In the CMakeLists.txt we find following option:

option(TFLITE_ENABLE_RUY ""Enable experimental RUY integration"" OFF)

However, even with this option disabled, the ruy library is still needed in order to compile TensorFlow-Lite. My question is if this is really necessary, or that with the right adaptions to the code it should in principle work without this libary. We want to minimize the amount of libraries we are taking in, also because ruy in its turn depends for example on the pyTorch cpuinfo library.

We are compiling for an ARM64 architecture.

Kind regards,

Wannes

"
47861,Cannot limit GPU memory with MultiWorkerMirroredStrategy,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: pypi
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: Python 3.8.5
- CUDA/cuDNN version: 11.1/8.0.5
- GPU model and memory: RTX2080Ti

**Describe the current behavior**

When running trainings with `OneDeviceStrategy` or `MirroredStrategy` on a single machine, it is possible to set a limit on the used GPU memory:
```python
limit_mb = ...
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_virtual_device_configuration(
            gpu,
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit_mb)])
strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())
...
with strategy.scope():
   ...
   model.fit(...)
```
However running a distributed training with `MultiWorkerMirroredStrategy` on multiple machines does not work as both the `set_virtual_device_configuration` and `MultiWorkerMirroredStrategy` must run before anything else.

The code is the same with the strategy created as follows:

```python
    cluster_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver(
        jobs=None, port_base=8888, gpus_per_node=None, gpus_per_task=None,
        tasks_per_node=None, auto_set_gpu=True, rpc_layer='grpc'
    )
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
        communication=tf.distribute.experimental.CollectiveCommunication.NCCL,
        cluster_resolver=cluster_resolver
    )
```

Calling `set_virtual_device_configuration` before creating the `MultiWorkerMirroredStrategy` (as for the other strategies) leads to:
```
File ""venv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 340, in new_func
  return func(*args, **kwargs)
File ""venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 253, in __init__
  super(_CollectiveAllReduceStrategyExperimental,
File ""venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 185, in __init__
  CollectiveAllReduceExtended(
File ""venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 327, in __init__
  self._initialize_strategy(self._cluster_resolver)
File ""venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 335, in _initialize_strategy
  self._initialize_multi_worker(cluster_resolver)
File ""venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 434, in _initialize_multi_worker
  context.context().configure_collective_ops(
File ""venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 729, in configure_collective_ops
  raise RuntimeError(""Collective ops must be configured at program startup"")
```

Calling `MultiWorkerMirroredStrategy` before `set_virtual_device_configuration` leads to:
```
File ""venv/lib/python3.8/site-packages/tensorflow/python/framework/config.py"", line 748, in set_logical_device_configuration
  context.context().set_logical_device_configuration(device, logical_devices)
File ""venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 1490, in set_logical_device_configuration
  raise RuntimeError(
RuntimeError: Virtual devices cannot be modified after being initialized
```

**Describe the expected behavior**

It is possible to limit the GPU memory for `MultiWorkerMirroredStrategy` as for the other strategies.

If this is expected to not work by design, it would be useful to have it clearly stated in the docs.
"
47859,Android: TFLite GPU delegate has very little gain of performance over CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 33 Silverblue
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi redmi note 5 pro (android 10)
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below): 2.4
- Python version: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:qualcom 509

**Describe the current behavior**
I'm trying to use TFLite on a yolo v5 neural network exported using [this repo](https://github.com/zldrobit/yolov5/tree/tf-android-tfl-detect). 
What I do is export the pretrained coco model from .pt file to .tflite in FP16 using the tf.py from repo. Then I make 50 inferences on an image in the android app and I get the avarage of the times. I take single inference time using `getLastNativeInferenceDurationNanoseconds()` function.
I've made some test with CPU (4 thread and xnnpack) and GPU delegate both on FP16.  On GPU I set in `GpuDelegate.Options()`  the following two options
```
gpu_options.setPrecisionLossAllowed(true);
gpu_options.setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_SUSTAINED_SPEED); 
```
What i've seen is that there's no much time gain using GPU over CPU.
On an immage 256x256 I've got
248.499 ms on CPU and 218.727 ms on GPU

With a bigger immage 800x640 the gain is a bit more but stil not a lot:
1671.821 ms on CPU and 1445.127 ms on GPU

(all times are the avarage time of 50 inferences)

**Describe the expected behavior**
What I expected was a greater improvement in time using GPU delegate. 

Is it normal or there's something I can do to improve GPU performance?

"
47858,Quantization Aware Training does not seem to perform per-channel quantization with AllValuesQuantizer,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
Custom code

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10

-   **TensorFlow installed from (source or binary)**:
Anaconda Navigator

-   **TensorFlow version (use command below)**:
tensorflow-gpu 2.3.0

-   **Python version**:
3.7.9

-   **CUDA/cuDNN version**:
cudatoolkit 10.1.243
cudnn 7.6.5

-   **GPU model and memory**:
GTX 1070 8GB


**Describe the current behavior**
I have quantized two CNN models with QAT (AllValuesQuantizer), one with per-tensor and one with per-axis (per-channel) quantization. When saving the models in .h5 format and inspecting them i Netron, I note that each QuantizeWrapper layer's parameters both have scalar kernel_min and kernel_max.

**Describe the expected behavior**
As I have understood from [this paper](https://arxiv.org/abs/1712.05877), the min/max values of the kernel are what defines the scale and zero-point quantization parameters. For per-tensor quantization it is reasonable that the model only has a single min and max value, as the whole tensor has the same scale and zero-point. HOWEVER, for per-channel quantization (where each channel has its own scale and zero-point) I believe that kernel_min and kernel_max should be vectors? Why aren't they?

[Here is one of my per-tensor models (AllValues)](https://i.stack.imgur.com/HsbTF.png)

[Here is one of my per-axis models (AllValues)](https://i.stack.imgur.com/uWMRI.png)

In [this github issue](https://github.com/tensorflow/tensorflow/issues/34299) someone mentions that QAT automatically uses per-tensor quantization (as of march 2020), but that this is subject to change. To me it looks like QAT (at least AllValuesQuantizer) still only uses per-tensor quantization? If that's the case, why is there a parameter that I can set to enable per-tensor quantization (See AllValuesQuantizer's per-axis boolean)?

I also noted in the [source code for the AllValuesQuantizer](https://github.com/tensorflow/model-optimization/blob/v0.5.0/tensorflow_model_optimization/python/core/quantization/keras/quantizers.py#L291-L366) that self.per_axis is never passed to the next function, so what is that even variable used for?

So; does QAT even perform per-channel quantization? Doesn't seem like it to me. How can I use per-channel quantization with the AllValuesQuantizer?

**Standalone code to reproduce my models**

```
import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

# Possible quantization aware quantizers:
QAT_ALL_VALUES = tfmot.quantization.keras.quantizers.AllValuesQuantizer
QAT_LAST_VALUE = tfmot.quantization.keras.quantizers.LastValueQuantizer
QAT_MA = tfmot.quantization.keras.quantizers.MovingAverageQuantizer


def quantization_aware_training(model, save, w_bits, a_bits, symmetric, per_axis, narrow_range, quantizer, batch_size=64, epochs=2):

    # Create quantized model's name string
    name = model.name + '_'
    name = name + str(w_bits) + 'wbits_' + str(a_bits) + 'abits_'

    if symmetric:
        name = name + 'sym_'
    else:
        name = name + 'asym_'

    if narrow_range:
        name = name + 'narr_'
    else:
        name = name + 'full_'

    if per_axis:
        name = name + 'perch_'
    else:
        name = name + 'perten_'

    if quantizer == QAT_ALL_VALUES:
        name = name + 'AV'
    elif quantizer == QAT_LAST_VALUE:
        name = name + 'LV'
    elif quantizer == QAT_MA:
        name = name + 'MA'

    # Quantization
    # *****
    quantize_apply = tfmot.quantization.keras.quantize_apply
    quantize_model = tfmot.quantization.keras.quantize_model
    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer
    clone_model = tf.keras.models.clone_model
    quantize_scope = tfmot.quantization.keras.quantize_scope

    supported_layers = [
        tf.keras.layers.Conv2D,
    ]

    class Quantizer(tfmot.quantization.keras.QuantizeConfig):
        # Configure how to quantize weights.
        def get_weights_and_quantizers(self, layer):
            return [(layer.kernel, tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]

        # Configure how to quantize activations.
        def get_activations_and_quantizers(self, layer):
            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]

        def set_quantize_weights(self, layer, quantize_weights):
            # Add this line for each item returned in `get_weights_and_quantizers`
            # , in the same order
            layer.kernel = quantize_weights[0]

        def set_quantize_activations(self, layer, quantize_activations):
            # Add this line for each item returned in `get_activations_and_quantizers`
            # , in the same order.
            layer.activation = quantize_activations[0]

        # Configure how to quantize outputs (may be equivalent to activations).
        def get_output_quantizers(self, layer):
            return []

        def get_config(self):
            return {}

    class ConvQuantizer(Quantizer):
        # Configure weights to quantize with 4-bit instead of 8-bits.
        def get_weights_and_quantizers(self, layer):
            return [(layer.kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]

        # Configure how to quantize activations.
        def get_activations_and_quantizers(self, layer):
            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]

    class DepthwiseQuantizer(Quantizer):
        # Configure weights to quantize with 4-bit instead of 8-bits.
        def get_weights_and_quantizers(self, layer):
            return [(layer.depthwise_kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]

        # Configure how to quantize activations.
        def get_activations_and_quantizers(self, layer):
            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]

    # Instead of simply using quantize_annotate_model or quantize_model we must use
    # quantize_annotate_layer since it's the only one with a quantize_config argument
    def quantize_all_layers(layer):
        if isinstance(layer, tf.keras.layers.DepthwiseConv2D):
            return quantize_annotate_layer(layer, quantize_config=DepthwiseQuantizer())
        elif isinstance(layer, tf.keras.layers.Conv2D):
            return quantize_annotate_layer(layer, quantize_config=ConvQuantizer())
        return layer

    annotated_model = clone_model(
        model,
        clone_function=quantize_all_layers
    )

    with quantize_scope(
        {'Quantizer': Quantizer},
        {'ConvQuantizer': ConvQuantizer},
            {'DepthwiseQuantizer': DepthwiseQuantizer}):
        q_aware_model = quantize_apply(annotated_model)

    # *****

    # Compile and train model
    optimizer = keras.optimizers.Adam(
        learning_rate=0.001)
    q_aware_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True),
        optimizer=optimizer, metrics=['sparse_categorical_accuracy'])

    (train_images, train_labels),_ = keras.datasets.cifar10.load_data()

    q_aware_model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, verbose=1,
                      validation_split=0.1)

    if save:
        save_path = 'models/temp/' + name
        q_aware_model.save(save_path + '.h5')

    return q_aware_model


def temp_net():
    dropout = 0.1

    model = keras.Sequential()
    model.add(keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.Activation('relu'))

    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(10, activation='softmax'))

    model._name = ""temp_net""

    return model


if __name__ == ""__main__"":
    q_model = quantization_aware_training(model=temp_net(), save=True,
                                          w_bits=8, a_bits=8, symmetric=False, narrow_range=False, per_axis=False, quantizer=QAT_ALL_VALUES, batch_size=64, epochs=1)
```
"
47857,Are you using old TFLite binary with newer model?Registration failed.,"### 1. System information

- OS Platform and Distribution 
NAME=""Raspbian GNU/Linux""
VERSION_ID=""10""

- TensorFlow installation with this code : 
echo ""deb https://packages.cloud.google.com/apt coral-edgetpu-stable main"" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-get update
sudo apt-get install python3-tflite-runtime

### 2. Code to reproduce
- Code

import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path_lite) # path to lite model

- Error raised
Traceback (most recent call last):
    interpreter = tflite.Interpreter(model_path=model_path_lite)
  File ""/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py"", line 207, in __init__
    custom_op_registerers_by_func))
ValueError: Op builtin_code out of range: 132. Are you using old TFLite binary with newer model?Registration failed.


### 5. Other infos
- Conversion of keras model to .tflite model
I converted the model from a keras model with this code :
converter = tf.lite.TFLiteConverter.from_saved_model(model_path_h5) # model to keras model
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
with open(model_path_lite, 'wb') as f:
   f.write(tflite_model)
   
To be able to convert, I had to use tf-nighlty  because the keras model contained Conv3DBackpropInputV2 operations. 

- Related issue
I don't manage to install tf-nighlty on the raspberry pi. 


"
47856,"After training the model , how I can save it? or from which folder I can download my trained model?","hello

after training the model , how i can save it? or from which folder I can download my trained model?

I just have trained ""ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz"" and use object detections API's

my folder structure shown in below image:

![2021-03-17](https://user-images.githubusercontent.com/72650269/111440953-461de680-8720-11eb-87d7-940f27d963fc.png)
"
47855,Update CMSIS kernels,"@tensorflow/micro

The CMSIS kernels in `lite/micro/kernels/cmsis_nn/` need to be updated to match the latest changes in the corresponding micro kernels.

"
47854,training_a_model.md problem,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform
- TensorFlow installed from (source or binary): 
- Tensorflow version (commit SHA if source): tensorflow-1.15
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32-CAM

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**

I am working on custom object detection for use in Arduino board (ESP32-CAM) 
ESP32-CAM working stand alone (no wifi mode)
I am practicing github page.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md

1.  PYTHONPATH problem
    ---------------------------------------------------------------------------------
    echo 'export PYTHONPATH=$PYTHONPATH:models/research/slim' >> ~/.bashrc
    source ~/.bashrc
    --- result ---------------------------------------------------------------------------------
      File ""<ipython-input-30-ddee1c530575>"", line 1
        export PYTHONPATH=$PYTHONPATH:models/research/slim >> ~/.bashrc
                        ^
    SyntaxError: invalid syntax

2. python: can't open file 'models/research/slim/datasets/build_visualwakewords_data.py': [Errno 2] No such file or directory
""build_visualwakewords_data.py"" not exist in the datasets folder
-------------------------------------------------------------------------------------------
  ! python models/research/slim/datasets/build_visualwakewords_data.py
  --logtostderr \
  --train_image_dir=coco/raw-data/train2014 \
  --val_image_dir=coco/raw-data/val2014 \
  --train_annotations_file=coco/raw-data/annotations/instances_train2014.json \
  --val_annotations_file=coco/raw-data/annotations/instances_val2014.json \
  --output_dir=coco/processed \
  --small_object_area_threshold=0.005 \
  --foreground_class_of_interest='person'

3. build_visualwakewords_data.py file not exist in the datasets folder
 ! python models/research/slim/datasets/build_visualwakewords_data.py"
47853,Creating ragged tensors is incredibly slow,"I'm running the following code on a Google Colab instance with GPU support enabled:
```
tf.ragged.constant(np.random.randint(10, size = 10_000_000))
```
The code takes 15 seconds to finish. In comparison, `tf.constant(np.random.randint(10, size = 10_000_000))` takes only 50 milliseconds to finish."
47852,tf.train.Server session_config parameter cannot be configured by Estimator,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**

session_config=tf.ConfigProto(...)
run_config=tf.estimator.RunConfig(session_config=session_config)
estimator = tf.estimator.Estimator(model_fn=..., config=run_config)
tf.estimator.train_and_evaluator(estimator, train_spec, eval_spec)

ps/worker tf.Server is not configured by session_config

**Describe the expected behavior**

ps/worker tf.Server can be configured by Estimator

PR: https://github.com/tensorflow/estimator/pull/66
"
47849,C API Binary Release for 2.4.0 Has Invalid Symbol Table,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (also affects other Ubuntu releases, see below)
- TensorFlow installed from (source or binary): C API binary from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz
- TensorFlow version: 2.4.0
- GCC/Compiler version (if compiling from source): 9.3.0-17ubuntu1~20.04
 
**Describe the problem:**

The binary release of the C API distributed under the link above appears to have a malformed symbol table, which prevents `ld` from being able to link against it when using the versions distributed in Ubuntu 16.04, 18.04, and 20.04. It seems likely that this affects other platforms as well, but I haven't tested those myself.

From https://sourceware.org/bugzilla/show_bug.cgi?id=24857, suggests that newer versions of binutils may have fixed this, but I still see this failure on every version of gcc/ld that I've tried, including ones that should have the fix identified in that issue. That issue also notes that:

> The real bug is in whatever linker set .dynsym sh_info too low for the shared lib.

which suggests that the root issue here might be in whatever build toolchain is producing the C API binaries that tensorflow is publishing.

This has been reported previously in https://github.com/tensorflow/tensorflow/issues/41382, but the resolution there seems to have been ""Use a different linker that doesn't complain about this issue"", which isn't really a solution to the root of the problem. Switching linkers isn't always possible for an existing project, or at least it can require a nontrivial amount of work. It would be better to fix whatever is building this malformed symbol table.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Create a directory and put the following in `Dockerfile` in that directory.

```dockerfile
FROM ubuntu:20.04

RUN apt-get update && apt-get install -y g++ wget

RUN mkdir /build
RUN cd /build && wget -qO- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz | tar -xvz

COPY repro.cc repro.cc
```

with the following file in `repro.cc` next to the Dockerfile:

```c++
#include <stdio.h>
#include <tensorflow/c/c_api.h>

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}
```

2. Build and run the resulting image, and attempt to compile the repro file.

```bash
$ docker build . -t repro:20.04
$ docker run -it repro:20.04
root@fc196abb4665:/# gcc -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 857 (>= sh_info of 2)
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 1944 (>= sh_info of 2)
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2314 (>= sh_info of 2)
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2502 (>= sh_info of 2)
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2724 (>= sh_info of 2)
/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2725 (>= sh_info of 2)
/usr/bin/ld: /tmp/ccomNmut.o: in function `main':
repro.cc:(.text+0x9): undefined reference to `TF_Version'
collect2: error: ld returned 1 exit status
```

**Additional Info:**

I can repro this issue on all recent Ubuntu versions I've tried (it's easy to test different versions by changing the tag of the base image in the Dockerfile posted above).

As noted in the issue linked above, the easiest workaround for this issue seems to be to use a different linker. `gold` happily accepts the release binary without complaint:

```bash
root@fc196abb4665:/ gcc -fuse-ld=gold -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc
root@fc196abb4665:/ ./a.out
Hello from TensorFlow C library version 2.4.0
```

`lld` builds successfully, but warns (and provides more info about the specific symbols that seem to be misplaced):

```bash
root@fc196abb4665:/ gcc -fuse-ld=lld -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc
ld.lld: warning: found local symbol '_ZN4absl14lts_2020_02_2518container_internal18global_next_sampleE' in global part of symbol table in file /build/lib/libtensorflow.so
ld.lld: warning: found local symbol 'tl_thread_handler_context' in global part of symbol table in file /build/lib/libtensorflow.so
ld.lld: warning: found local symbol '_ZN10tensorflow27ScopedMemoryDebugAnnotation11annotation_E' in global part of symbol table in file /build/lib/libtensorflow.so
ld.lld: warning: found local symbol 'tl_logging_thread_id' in global part of symbol table in file /build/lib/libtensorflow.so
ld.lld: warning: found local symbol '_ZN9grpc_core7ExecCtx9exec_ctx_E' in global part of symbol table in file /build/lib/libtensorflow.so
ld.lld: warning: found local symbol '_ZN9grpc_core26ApplicationCallbackExecCtx18callback_exec_ctx_E' in global part of symbol table in file /build/lib/libtensorflow.so
```

However, as noted above, not everyone can easily swap out the linker in their project for another one, and the root issue here seems to be the actual release file, rather than the linker that's rejecting it."
47848,Keras tuner - Missing validation split for model training,"URL with the issue:
https://www.tensorflow.org/tutorials/keras/keras_tuner#instantiate_the_tuner_and_perform_hypertuning

Description of issue (what needs changing):

On the second to last code cell where the hypermodel is re-instantiated and trained with the optimal number of epochs, a validation split parameter on the training set is not defined,as per best practises when fitting a model.

Is the link to the source code correct?

Yes

Are all parameters defined and formatted correctly?

Yes

Are return values defined?

Yes

Are you planning to also submit a pull request to fix the issue?

Yes, here: https://github.com/tensorflow/docs/pull/1844"
47846,Training BERT for QNLI task - Fitting the model stops after the first Epoch without crash. No warning or error given to understand why it is happening.,"my code follows the tensorflow example below very closely:

https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu

My data is in a csv and in total 14000. It has a 'question', 'answer' and 'label' fields. labels are binary 0/1.
The questions and answers are strings. In the code below they are uploaded onto colab via pandas using read_csv and are referred to as train_df, val_df and test_df.

After the running the code below, the model refuses to go past the first Epoch. 
<img width=""1083"" alt=""tfqnli"" src=""https://user-images.githubusercontent.com/47080256/111368034-bf0f3680-866b-11eb-86a8-b281f53b49dc.PNG"">

```
!pip install -q -U tensorflow-text
!pip install -q -U tf-models-official
!pip install -U tfds-nightly
```

```
import os
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds
import tensorflow_text as text 
import tensorflow_addons as tfa
from official.nlp import optimization
import numpy as np

tf.get_logger().setLevel('ERROR')
```
```
bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'

tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

bert_preprocess = hub.load(tfhub_handle_preprocess)
```
```
def make_bert_preprocess_model(sentence_features, seq_length=128):
  """"""Returns Model mapping string features to BERT inputs.

  Args:
    sentence_features: a list with the names of string-valued features.
    seq_length: an integer that defines the sequence length of BERT inputs.

  Returns:
    A Keras Model that can be called on a list or dict of string Tensors
    (with the order or names, resp., given by sentence_features) and
    returns a dict of tensors for input to BERT.
  """"""

  input_segments = [
      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)
      for ft in sentence_features]

  # Tokenize the text to word pieces.
  bert_preprocess = hub.load(tfhub_handle_preprocess)
  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')
  segments = [tokenizer(s) for s in input_segments]

  truncated_segments = segments

  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,
                          arguments=dict(seq_length=seq_length),
                          name='packer')
  model_inputs = packer(truncated_segments)
  return tf.keras.Model(input_segments, model_inputs)
```

```
def load_dataset_from_tfds(dataset, batch_size, bert_preprocess_model):

  num_examples = len(list(dataset))

  dataset = dataset.batch(batch_size)
  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))
  dataset = dataset.prefetch(1)
  return dataset, num_examples
```

```
sentence_features = ['question', 'answer']
num_classes = 2

bert_preprocess_model = make_bert_preprocess_model(sentence_features)
```

```
train_dataset = tf.data.Dataset.from_tensor_slices({'idx':train_df.index.values, 'question':train_df.question.values, 'answer':train_df.answer.values,  'label':train_df.label.values})

val_dataset = tf.data.Dataset.from_tensor_slices({'idx':val_df.index.values, 'question':val_df.question.values, 'answer':val_df.answer.values, 'label':val_df.label.values})

test_dataset = tf.data.Dataset.from_tensor_slices({'idx':test_df.index.values, 'question':test_df.question.values, 'answer':test_df.answer.values, 'label':test_df.label.values})
```
```
def build_classifier_model(num_classes, activation=None):
  inputs = dict(
      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids'),
      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask'),
      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids'),
  )

  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')
  net = encoder(inputs)['pooled_output']
  net = tf.keras.layers.Dropout(rate=0.1)(net)
  net = tf.keras.layers.Dense(num_classes, activation=activation, name='classifier')(net)
  return tf.keras.Model(inputs, net, name='prediction')
```
```
strategy = tf.distribute.MirroredStrategy()

batch_size = 32
epochs = 5
init_lr = 2e-5

with strategy.scope():
  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)

  train_dataset_, train_data_size= load_dataset_from_tfds(train_dataset, batch_size, bert_preprocess_model)
  steps_per_epoch = train_data_size // batch_size
  num_train_steps = steps_per_epoch * epochs
  num_warmup_steps = num_train_steps // 10

  val_dataset_, val_data_size= load_dataset_from_tfds(val_dataset, batch_size, bert_preprocess_model)
  validation_steps = val_data_size // batch_size

  classifier_model = build_classifier_model(num_classes)


  optimizer = optimization.create_optimizer(init_lr =init_lr , num_train_steps = num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')

  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])

  classifier_model.fit(x = train_dataset_, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data = val_dataset_, validation_steps=validation_steps )
```"
47845,"@amahendrakar, I did check this link (before raising the support request), some changes are made, but still, there are errors. I have attached the ipynb file. Please support.","@amahendrakar, I did check https://www.tensorflow.org/guide/migrate  (before raising the support request), some changes are made, but still, there are errors. I have attached the ipynb file. Please support.

_Originally posted by @rrklearn2020 in https://github.com/tensorflow/tensorflow/issues/47742#issuecomment-797540614_"
47843,<Removed>,<removed>
47841,Renode download fails from time to time,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
In our CI we have seen that the Renode download fails sometimes. There are no clues to why. Not a big deal as it usually works when retrying.

It turns out that it is because of API rate limiting. So it would be nice to check for this and possible do a few retries.
**Please provide the exact sequence of commands/steps when you ran into the problem**

Call ./tensorflow/lite/micro/tools/make/renode_download.sh until it fails."
47838,Compressed pruned model is the same size as compressed baseline model,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
Code from pruning documentation:
https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10

-   **TensorFlow installed from (source or binary)**:
Anaconda Navigator

-   **TensorFlow version (use command below)**:
tensorflow-gpu 2.3.0

-   **Python version**:
3.7.9

-   **CUDA/cuDNN version**:
cudatoolkit 10.1.243
cudnn 7.6.5

-   **GPU model and memory**:
GTX 1070 8GB


### Describe the problem
I followed the code provided in the documentation and pruned a model. I expected the compressed pruned model to be of smaller size than the baseline. [Here](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#create_3x_smaller_models_from_pruning), the baseline is compared to the pruned model, and is definitely smaller.
In my case, they are both the same size.

### Source code / logs
```
import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import numpy as np
import tempfile
import os
import zipfile


def get_gzipped_model_size(model):
  # Returns size of gzipped model, in bytes.
  import os
  import zipfile

  _, keras_file = tempfile.mkstemp('.h5')
  model.save(keras_file, include_optimizer=False)

  _, zipped_file = tempfile.mkstemp('.zip')
  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:
    f.write(keras_file)

  return os.path.getsize(zipped_file)


input_shape = [20]
x_train = np.random.randn(1, 20).astype(np.float32)
y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)


def setup_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(20, input_shape=input_shape),
      tf.keras.layers.Flatten()
  ])
  return model

def setup_pretrained_weights():
  model = setup_model()

  model.compile(
      loss=tf.keras.losses.categorical_crossentropy,
      optimizer='adam',
      metrics=['accuracy']
  )

  model.fit(x_train, y_train)

  _, pretrained_weights = tempfile.mkstemp('.tf')

  model.save_weights(pretrained_weights)

  return pretrained_weights


pretrained_weights = setup_pretrained_weights()


def test():
    base_model = setup_model()
    base_model.load_weights(pretrained_weights)
    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(base_model)

    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)

    print(""Size of gzipped baseline model: %.2f bytes"" % (get_gzipped_model_size(base_model)))
    print(""Size of gzipped pruned model without stripping: %.2f bytes"" % (get_gzipped_model_size(model_for_pruning)))
    print(""Size of gzipped pruned model with stripping: %.2f bytes"" % (get_gzipped_model_size(model_for_export)))


if __name__ == ""__main__"":
    test()
```


**Output:**

_Size of gzipped baseline model: 2935.00 bytes_
_Size of gzipped pruned model without stripping: 3360.00 bytes_
_Size of gzipped pruned model with stripping: 2935.00 bytes_"
47837,constants.SAVED_MODEL_FILENAME_PB)) OSError: SavedModel file does not exist at: /home/pi/Downloads/tf_lite_model.h5/{saved_model.pbtxt|saved_model.pb},"I faced this problem when I wanted to import the pre-trained model to my rpi 3b+. I saw there are some similar issues but neither of them works for my case. Pls help me out!!

constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: /home/pi/Downloads/tf_lite_model.h5/{saved_model.pbtxt|saved_model.pb}  

^ this is the error.





"
47836,issue with multi objects detection (using ssd_resnet50_v1_fpn_640x640_coco17_tpu-8),"**System information**

- Colab (GPU runtime)



**Describe the current behavior**
I'm trying to modified  this [notebook ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb)to detect multi objects(2 objects) instead of one object, I have modified  the ""gt_boxes_list"" and ""gt_classes_list"" so both have (2,4) and (2,2) shapes respectively, however when I train the model I get the below error: 
*******
ValueError: in user code:

    <ipython-input-17-9f168971fcaf>:55 train_step_fn  *
        losses_dict = model.loss(prediction_dict, shapes)
    /usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py:842 loss  *
        (batch_cls_targets, batch_cls_weights, batch_reg_targets,
    /usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py:1083 _assign_targets  *
        groundtruth_weights_list)
    /usr/local/lib/python3.7/dist-packages/object_detection/core/target_assigner.py:512 batch_assign  *
        (cls_targets, cls_weights,
    /usr/local/lib/python3.7/dist-packages/object_detection/core/target_assigner.py:177 assign  *
        unmatched_shape_assert = shape_utils.assert_shape_equal(
    /usr/local/lib/python3.7/dist-packages/object_detection/utils/shape_utils.py:321 assert_shape_equal  *
        raise ValueError('Unequal shapes {}, {}'.format(shape_a, shape_b))

    ValueError: Unequal shapes [3], [2]
*****
[hint] I have checked ""prediction_dict['class_predictions_with_background'] and its shape is (1, 51150, 2) instead of (1, 51150, **3**) as num_classes is 2 so prediction_dict['class_predictions_with_background'].shape[2] should be (num_classes + 1) as per the below quoted comment from ""ssd_meta_arch.py"" file:
""3) class_predictions_with_background: 3-D float tensor of shape
          [batch_size, num_anchors, num_classes+1] containing class predictions
          (logits) for each of the anchors.  Note that this tensor *includes*""



**Describe the expected behavior**
model should train and predict the 2 classes with their corresponding boxes


**Standalone code to reproduce the issue**
please find the link to the modified colab [notebook:](https://colab.research.google.com/drive/1l8e2LcPmnDAlf8r54ltC7kkTb70WbpyP?usp=sharing)

for training images, I used only 5 images (they are uploaded under this path:
'/content/models/research/object_detection/test_images/bike'
![bike_1](https://user-images.githubusercontent.com/72650269/111270399-3da6ac80-8649-11eb-9d7b-79b741159dc4.jpg)
![bike_2](https://user-images.githubusercontent.com/72650269/111270415-426b6080-8649-11eb-846a-fcf8af843220.jpg)
![bike_3](https://user-images.githubusercontent.com/72650269/111270436-49926e80-8649-11eb-839b-1425b7cb7fab.jpg)
![bike_4](https://user-images.githubusercontent.com/72650269/111270450-4e572280-8649-11eb-8bcd-60eb20be28be.jpg)
![bike_5](https://user-images.githubusercontent.com/72650269/111270468-54e59a00-8649-11eb-9c04-a62bd04a2291.jpg)

you need to change the pictures extension to ""jpg"" in image_path
```
train_image_dir = 'models/research/object_detection/test_images/bike/'
train_images_np = []
for i in range(1, 6):
  image_path = os.path.join(train_image_dir, 'bike_' + str(i) + '.jfif')
```

Best Regards

"
47834,error about Kernel8bitNeonDotprodOutOfOrder  occurs  when running int8 CPU inference by tflite,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: on mobile
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:  under-release yet, the platform is Qualcom SM4350
-   **TensorFlow installed from (source or binary)**: on mobile
-   **TensorFlow version (use command below)**: v2.0.0
-   **Python version**: on mobile
-   **Bazel version (if compiling from source)**: on mobile
-   **GCC/Compiler version (if compiling from source)**:  on mobile
-   **CUDA/cuDNN version**: on mobile
-   **GPU model and memory**: on mobile
-   **Exact command to reproduce**:tflite->Invoke

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
The problem  occurs when doing stress testing for camera of mobile phone. In detail, we do cpu inference for a int8 quantilized model and the problem occurs occasionally, causing the camera crash.

### Source code / logs
03-16 04:58:16.798467 28412 28412 F DEBUG   : backtrace:
03-16 04:58:16.798525 28412 28412 F DEBUG   :       #00 pc 00000000002247dc  /vendor/lib64/libtensorflowLite.so (ruy::Kernel8bitNeonDotprodOutOfOrder(ruy::KernelParams8bit<8, 8> const&)+1364)
03-16 04:58:16.798548 28412 28412 F DEBUG   :       #01 pc 0000000000114774  /vendor/lib64/libtensorflowLite.so (void ruy::RunKernelTyped<(ruy::Path)8, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Tuning, ruy::PackedMatrix<signed char> const&, ruy::PackedMatrix<signed char> const&, ruy::BasicSpec<int, signed char> const&, int, int, int, int, ruy::Matrix<signed char>*)+508)
03-16 04:58:16.798564 28412 28412 F DEBUG   :       #02 pc 0000000000113c74  /vendor/lib64/libtensorflowLite.so (void ruy::RunKernel<(ruy::Path)8, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Tuning, ruy::SidePair<ruy::PMatrix> const&, void*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::DMatrix*)+160)
03-16 04:58:16.798573 28412 28412 F DEBUG   :       #03 pc 00000000002297b4  /vendor/lib64/libtensorflowLite.so
03-16 04:58:16.798583 28412 28412 F DEBUG   :       #04 pc 0000000000228dc4  /vendor/lib64/libtensorflowLite.so (ruy::TrMul(ruy::TrMulParams*, ruy::Context*)+2292)
03-16 04:58:16.798596 28412 28412 F DEBUG   :       #05 pc 00000000001131d0  /vendor/lib64/libtensorflowLite.so (void ruy::DispatchMul<(ruy::Path)15, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::BasicSpec<int, signed char> const&, ruy::Context*, ruy::Matrix<signed char>*)+384)
03-16 04:58:16.798610 28412 28412 F DEBUG   :       #06 pc 0000000000112410  /vendor/lib64/libtensorflowLite.so (tflite::optimized_integer_ops::ConvPerChannel(tflite::ConvParams const&, int const*, int const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, signed char*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)+1164)
03-16 04:58:16.798622 28412 28412 F DEBUG   :       #07 pc 000000000011109c  /vendor/lib64/libtensorflowLite.so (void tflite::ops::builtin::conv::EvalQuantizedPerChannel<(tflite::ops::builtin::conv::KernelType)1>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)+648)
03-16 04:58:16.798636 28412 28412 F DEBUG   :       #08 pc 0000000000105ed8  /vendor/lib64/libtensorflowLite.so (TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)1>(TfLiteContext*, TfLiteNode*)+280)
03-16 04:58:16.798647 28412 28412 F DEBUG   :       #09 pc 000000000023079c  /vendor/lib64/libtensorflowLite.so (tflite::Subgraph::Invoke()+740)
03-16 04:58:16.798657 28412 28412 F DEBUG   :       #10 pc 00000000002342dc  /vendor/lib64/libtensorflowLite.so (tflite::Interpreter::Invoke()+32)
"
47832,Bazel build fails downloading sqlite-amalgamation-3340100.zip,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubunto 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2
- Python version: 3.8
- Installed using: pip
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc-14
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the problem**
Building from sources from git clone https://github.com/tensorflow/tensorflow.git, using bazel fails
    
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Command

    bazel build //tensorflow/tools/pip_package:build_pip_package

fails with error:

    Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip, https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip] to /home/attardi/.cache/bazel/_bazel_attardi/9e2174a9a29f23641a2f691abf17e76f/external/org_sqlite/temp4950223412785952933/sqlite-amalgamation-3340100.zip: connect timed out

Indeed:

    https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip
is missing.

**Any other info / logs**
Strangely, the second alternative URL to download, does exists:

    ""https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip""

but bazel ignores it.
"
47831,'Windows fatal exception: access violation' with tensor flow object detection,"Hi, so I am trying to create a custom object detector for myself and am using this guide:https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#training-the-model. As I am new to this I have followed all instructions to a T. However when I try to train my model with 
`python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config 
`
I get the following error:


```
2021-03-15 23:49:57.441253: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-15 23:50:08.636642: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 49 of 2048
2021-03-15 23:50:18.699069: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 149 of 2048
2021-03-15 23:50:28.835509: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 234 of 2048
2021-03-15 23:50:39.771722: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 297 of 2048
2021-03-15 23:50:49.312747: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 320 of 2048
2021-03-15 23:50:58.806089: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 360 of 2048
2021-03-15 23:51:09.810531: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 400 of 2048
2021-03-15 23:51:18.791169: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 406 of 2048
2021-03-15 23:51:30.375699: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 442 of 2048
2021-03-15 23:51:38.994588: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 466 of 2048
2021-03-15 23:51:49.395020: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 506 of 2048
2021-03-15 23:51:59.317995: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 531 of 2048
2021-03-15 23:52:09.465026: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 570 of 2048
2021-03-15 23:52:19.124648: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 594 of 2048
2021-03-15 23:52:28.983142: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 634 of 2048
2021-03-15 23:52:38.615317: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 673 of 2048
2021-03-15 23:52:49.003972: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 698 of 2048
2021-03-15 23:52:59.077313: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 738 of 2048
2021-03-15 23:53:09.490022: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 763 of 2048
2021-03-15 23:53:18.937036: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 800 of 2048
2021-03-15 23:53:29.407881: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 825 of 2048
2021-03-15 23:53:39.302728: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 863 of 2048
2021-03-15 23:53:48.757507: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 886 of 2048
2021-03-15 23:53:58.636679: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 910 of 2048
2021-03-15 23:54:09.077197: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 949 of 2048
2021-03-15 23:54:18.762039: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 988 of 2048
Windows fatal exception: access violation

Thread 0x00000890 (most recent call first):
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 2573 in iterator_get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 730 in _next_internal
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 800 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py"", line 585 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1619 in get_next_as_list_static_shapes
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 663 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 632 in __next__
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 628 in next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\object_detection\model_lib_v2.py"", line 352 in load_fine_tune_checkpoint
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\object_detection\model_lib_v2.py"", line 580 in train_loop
  File ""model_main_tf2.py"", line 104 in main
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\absl\app.py"", line 300 in run
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""model_main_tf2.py"", line 113 in <module>
Windows fatal exception: access violation

Thread 0x00000890 (most recent call first):
  File ""C:\Users\agnip\anaconda3\eWindows fatal exception: access violation

nvs\tensorflow4\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 2573 in iterator_get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 730 in _next_internal
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 800 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py"", line 585 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1619 in get_next_as_list_static_shapes
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 663 in get_next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 632 in __next__
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 628 in next
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\object_detection\model_lib_v2.py"", line 352 in load_fine_tune_checkpoint
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\object_detection\model_lib_v2.py"", line 580 in train_loop
  File ""model_main_tf2.py"", line 104 in main
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\absl\app.py"", line 300 in run
  File ""C:\Users\agnip\anaconda3\envs\tensorflow4\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""model_main_tf2.py"", line 113 in <module>
```

I can't figure out what the error is. Please help me solve this issue so I can create my custom object detector.
Also I am using Windows 10, 16GB ram and all the packages are listed below:

(tensorflow4) D:\Tensorflow\workspace\training_demo>conda list
# packages in environment at C:\Users\agnip\anaconda3\envs\tensorflow4:
#
# Name                    Version                   Build  Channel
absl-py                   0.10.0                   pypi_0    pypi
apache-beam               2.28.0                   pypi_0    pypi
astunparse                1.6.3                    pypi_0    pypi
attrs                     20.3.0                   pypi_0    pypi
avro-python3              1.9.2.1                  pypi_0    pypi
blas                      1.0                         mkl
ca-certificates           2021.1.19            haa95532_1
cachetools                4.2.1                    pypi_0    pypi
certifi                   2020.12.5                pypi_0    pypi
cffi                      1.14.5                   pypi_0    pypi
chardet                   4.0.0                    pypi_0    pypi
contextlib2               0.6.0.post1              pypi_0    pypi
crcmod                    1.7                      pypi_0    pypi
cycler                    0.10.0                   pypi_0    pypi
cython                    0.29.22                  pypi_0    pypi
dataclasses               0.6                      pypi_0    pypi
dill                      0.3.1.1                  pypi_0    pypi
dm-tree                   0.1.5                    pypi_0    pypi
docopt                    0.6.2                    pypi_0    pypi
fastavro                  1.3.3                    pypi_0    pypi
flatbuffers               1.12                     pypi_0    pypi
future                    0.18.2                   pypi_0    pypi
gast                      0.3.3                    pypi_0    pypi
gin-config                0.4.0                    pypi_0    pypi
google-api-core           1.26.1                   pypi_0    pypi
google-api-python-client  2.0.2                    pypi_0    pypi
google-auth               1.27.1                   pypi_0    pypi
google-auth-httplib2      0.1.0                    pypi_0    pypi
google-auth-oauthlib      0.4.3                    pypi_0    pypi
google-cloud-bigquery     2.11.0                   pypi_0    pypi
google-cloud-core         1.6.0                    pypi_0    pypi
google-crc32c             1.1.2                    pypi_0    pypi
google-pasta              0.2.0                    pypi_0    pypi
google-resumable-media    1.2.0                    pypi_0    pypi
googleapis-common-protos  1.53.0                   pypi_0    pypi
grpcio                    1.32.0                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
hdfs                      2.6.0                    pypi_0    pypi
httplib2                  0.17.4                   pypi_0    pypi
idna                      2.10                     pypi_0    pypi
importlib-resources       5.1.2                    pypi_0    pypi
intel-openmp              2020.2                      254
joblib                    1.0.1                    pypi_0    pypi
kaggle                    1.5.12                   pypi_0    pypi
keras-preprocessing       1.1.2                    pypi_0    pypi
kiwisolver                1.3.1                    pypi_0    pypi
labelimg                  1.8.3                    pypi_0    pypi
lvis                      0.5.3                    pypi_0    pypi
lxml                      4.6.2                    pypi_0    pypi
markdown                  3.3.4                    pypi_0    pypi
matplotlib                3.3.4                    pypi_0    pypi
mkl                       2020.2                      256
mkl-service               2.3.0            py38h196d8e1_0
mkl_fft                   1.3.0            py38h46781fe_0
mkl_random                1.1.1            py38h47e9c7a_0
mock                      2.0.0                    pypi_0    pypi
numpy                     1.20.1                   pypi_0    pypi
oauth2client              4.1.3                    pypi_0    pypi
oauthlib                  3.1.0                    pypi_0    pypi
object-detection          0.1                      pypi_0    pypi
opencv-python             4.5.1.48                 pypi_0    pypi
opencv-python-headless    4.5.1.48                 pypi_0    pypi
openssl                   1.1.1j               h2bbff1b_0
opt-einsum                3.3.0                    pypi_0    pypi
packaging                 20.9                     pypi_0    pypi
pandas                    1.2.3            py38hf11a4ad_0
pbr                       5.5.1                    pypi_0    pypi
pillow                    8.1.2                    pypi_0    pypi
pip                       21.0.1           py38haa95532_0
promise                   2.3                      pypi_0    pypi
proto-plus                1.17.0                   pypi_0    pypi
protobuf                  3.15.6                   pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
py-cpuinfo                7.0.0                    pypi_0    pypi
pyarrow                   2.0.0                    pypi_0    pypi
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pycocotools               2.0                      pypi_0    pypi
pycparser                 2.20                     pypi_0    pypi
pydot                     1.4.2                    pypi_0    pypi
pymongo                   3.11.3                   pypi_0    pypi
pyparsing                 2.4.7                    pypi_0    pypi
pyqt5                     5.15.4                   pypi_0    pypi
pyqt5-qt5                 5.15.2                   pypi_0    pypi
pyqt5-sip                 12.8.1                   pypi_0    pypi
python                    3.8.8                hdbf39b2_4
python-dateutil           2.8.1              pyhd3eb1b0_0
python-slugify            4.0.1                    pypi_0    pypi
pytz                      2021.1             pyhd3eb1b0_0
pyyaml                    5.4.1                    pypi_0    pypi
requests                  2.25.1                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
scikit-learn              0.24.1                   pypi_0    pypi
scipy                     1.4.1                    pypi_0    pypi
sentencepiece             0.1.95                   pypi_0    pypi
seqeval                   1.2.2                    pypi_0    pypi
setuptools                54.1.2                   pypi_0    pypi
six                       1.15.0           py38haa95532_0
sqlite                    3.33.0               h2a8f88b_0
tensorboard               2.4.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
tensorflow                2.4.1                    pypi_0    pypi
tensorflow-addons         0.12.1                   pypi_0    pypi
tensorflow-datasets       4.2.0                    pypi_0    pypi
tensorflow-estimator      2.4.0                    pypi_0    pypi
tensorflow-hub            0.11.0                   pypi_0    pypi
tensorflow-metadata       0.28.0                   pypi_0    pypi
tensorflow-model-optimization 0.5.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
text-unidecode            1.3                      pypi_0    pypi
tf-models-official        2.4.0                    pypi_0    pypi
tf-slim                   1.1.0                    pypi_0    pypi
threadpoolctl             2.1.0                    pypi_0    pypi
tqdm                      4.59.0                   pypi_0    pypi
typeguard                 2.11.1                   pypi_0    pypi
typing-extensions         3.7.4.3                  pypi_0    pypi
uritemplate               3.0.1                    pypi_0    pypi
urllib3                   1.26.3                   pypi_0    pypi
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.36.2                   pypi_0    pypi
wincertstore              0.2                      py38_0
wrapt                     1.12.1                   pypi_0    pypi
zlib                      1.2.11               h62dcd97_4


Thanks in advance!"
47829,Conv2D output not compatible with ZeroPadding3D input,"I need to zero pad outputs of multiple CNN layers in order to concatenate them together as input to another layer. 

A simple way to demonstrate this is:

```
x = Input(shape=(256, 256, 3))
x = Conv2D(32, (2,2), (2,2))(x)     # 1st CNN layer
x = ZeroPadding3D(padding=(2, 2, 2))(x)
```

This fails on the output of the CNN layer, irrespective of what values I provide for padding.

The error I get is:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 862, in _infer_output_signature
    self._maybe_build(inputs)
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 2684, in _maybe_build
    input_spec.assert_input_compatibility(
  File ""C:\Users\Mark\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\keras\engine\input_spec.py"", line 219, in assert_input_compatibility
    raise ValueError('Input ' + str(input_index) + ' of layer ' +
ValueError: Input 0 of layer zero_padding3d_4 is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, 128, 128, 32)
```
I have looked at the example of ZeroPadding2D and this appears to work OK [in the Pix2Pix example](https://www.tensorflow.org/tutorials/generative/pix2pix), but only works on the first 2 dimensions. I need to pad 3 dimensions.

The documentation for [ZeroPadding3D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding3D) also does not show how this layer can be used in a network.


Output of `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`:

`v2.4.0-49-g85c8b2a817f 2.4.1`
Python version = 3.8.5
OS = Windows 10 (Latest developer release)
NVIDIA-SMI 465.51       
Driver Version: 465.51       
CUDA Version: 11.3
GPU = RTX3070 (8GB)
Tensorflow installed from binary (pip)

The expected behaviour is that I can use ZeroPadding3D as a layer within my CNN to pad the output of a convolutional layer in all 3 dimensions.

"
47827,"ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'   ","I am using faster_rcnn_inception_v2_coco model and want to convert it into tflite...   
import tensorflow as tf  

tflite_model_name=""TF.lite""

saved_model_dir=""exported_output_graph/saved_model/saved_model.pb""  
#Convert the model 
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory 
tflite_model = converter.convert()  
with open(tflite_model_name, 'wb') as f:   
f.write(tflite_model)   
I don't what I am missing here


### 1. System information

- OS Platform and Distribution =Linux 
- TensorFlow library =1.14"
47824,Pip installation issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![Screenshot 2021-03-16 003933](https://user-images.githubusercontent.com/65313257/111207853-22f41980-85f0-11eb-8800-e72e8382452e.png)
"
47820,"OOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu Jetson Nano
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:2.4.0+nv21.2
- Python version:3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
OOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]
![Screenshot from 2021-03-15 18-49-12](https://user-images.githubusercontent.com/67184718/111161346-12787a80-85c1-11eb-9bb0-f7ebe515ac23.png)
![Screenshot from 2021-03-15 19-00-43](https://user-images.githubusercontent.com/67184718/111161388-1dcba600-85c1-11eb-960b-1b71dd26af21.png)

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
47819,UnboundLocalError: local variable 'kwargs' referenced before assignment,"- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.1
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX 3070

When I try load model from json with ``` model_from_json()``` from model with loss added by ```add_loss()``` and ```tf.add_n()```  method I get:

```
UnboundLocalError                         Traceback (most recent call last)

<ipython-input-1-ebe792783517> in <module>()
     36 json_file.close()
     37 
---> 38 loaded_model = model_from_json(loaded_model_json)  # UnboundLocalError: local variable 'kwargs' referenced before assignment
     39 print(""model_from_json ok"")

5 frames

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_json(json_string, custom_objects)
    129   config = json.loads(json_string)
    130   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
--> 131   return deserialize(config, custom_objects=custom_objects)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
    175       module_objects=LOCAL.ALL_OBJECTS,
    176       custom_objects=custom_objects,
--> 177       printable_module_name='layer')

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    356             custom_objects=dict(
    357                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 358                 list(custom_objects.items())))
    359       with CustomObjectScope(custom_objects):
    360         return cls.from_config(cls_config)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in from_config(cls, config, custom_objects)
    667     """"""
    668     input_tensors, output_tensors, created_layers = reconstruct_from_config(
--> 669         config, custom_objects)
    670     model = cls(inputs=input_tensors, outputs=output_tensors,
    671                 name=config.get('name'))

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)
   1283       if layer in unprocessed_nodes:
   1284         for node_data in unprocessed_nodes.pop(layer):
-> 1285           process_node(layer, node_data)
   1286 
   1287   input_tensors = []

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)
   1231         input_tensors = (
   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))
-> 1233       output_tensors = layer(input_tensors, **kwargs)
   1234 
   1235       # Update node index map.

UnboundLocalError: local variable 'kwargs' referenced before assignment
```

"
47818,Add Resnet-18 and Resnet-34 in tf.keras.applications.,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Yes, but would probably need help.

**Describe the feature and the current behavior/state.**
Currently there are multiple resnet variants available (50, 101, 152 + V2 options). 

Those model's are rather big and thus limiting the research and use of computer vision solutions on edge devices.

Adding requested models would make it easier to research and prototype new solutions in Tensorflow.

**Will this change the current api? How?**
It could be possible to use requested models in the same way as the `ResNet50` is called. An example could look like the following:
```python
resnet18 = tf.keras.applications.ResNet18(
    include_top=True, weights='imagenet', input_tensor=None,
    input_shape=None, pooling=None, classes=1000, **kwargs
)

resnet34 = tf.keras.applications.ResNet34(
    include_top=True, weights='imagenet', input_tensor=None,
    input_shape=None, pooling=None, classes=1000, **kwargs
)
```

**Who will benefit with this feature?**
Any user's who are working with smaller versions of computer vision models.

**Any Other info.**
This issue expands already existing request for Resnet-34:
https://github.com/tensorflow/tensorflow/issues/44099 
"
47817,HEAD build wants bazel >= 3.7.4 and <4.0.0 which are unavailable,"**System information**
- OS Platform and Distribution: Debian 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.7
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): 8.3

**Describe the problem**

Nightly builds of Tensorflow want Bazel >= 3.7.4 so at Linaro CI we fetched 3.7.4 and built TF. Worked fine.

Upstream dropped Bazel 3.7.4 release so now 3.7.2 and 4.0.0 are available on their Github releases page. We moved to 4.0.0 and then TF complains that it is too new.

So at the moment Tensorflow nightly is not buildable on our CI.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Fetch bazel 4.0.0, build and install. Then build Tensorflow.

**Any other info / logs**

00:50:24.871     spawn ./configure
00:50:24.871     You have bazel 4.0.0- (@non-git) installed.
00:50:24.871     Please downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.

Whole log: https://ci.linaro.org/job/ldcg-python-tensorflow-nightly/nodes=docker-buster-arm64-ldcg/116/console"
47816,tf.keras Lambda layer fails to infer output shape when run_functions_eagerly is set to True,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8
- CUDA/cuDNN version: 11.1
- GPU model and memory: 2048 Ti

**Describe the current behavior**
Building a keras model in functional mode with a lambda layer is failing in eager mode.
The lambda layer is trying to infer its output spec. To do so, it runs the `tf.function` decorated lambda function, but without using autograph and ends up raising an `OperatorNotAllowedInGraphError` exception.

We can bypass the problem by temporarily switching off the eager mode during the model construction.

**Describe the expected behavior**
The layer should be able to infer the output shape in eager mode.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

tf.config.run_functions_eagerly(True)


def build_model_fail():
    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)
    output = tf.keras.layers.Lambda(lambda_fn)(input)
    return tf.keras.Model(inputs=input, outputs=output)


def build_model_success():
    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)

    # temporarily setting off the eager execution
    # allows the lambda layer to infer the output spec.
    tf.config.run_functions_eagerly(False)
    output = tf.keras.layers.Lambda(lambda_fn)(input)

    # switching back to eager for runtime debugging
    tf.config.run_functions_eagerly(True)

    return tf.keras.Model(inputs=input, outputs=output)


@tf.function
def lambda_fn(input):
    i = tf.constant(0, dtype=tf.int32)
    while i < input:
        tf.print(""loop iteration"", i)
        i = i + 1
    return input


if __name__ == ""__main__"":

    # this works
    model = build_model_success()
    model(5)

    # this doesn't work
    model = build_model_fail()
    model(5)

```

"
47815,TensorFlow 2.4.1 offline build fails,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.1
- Python version: 3.7.5
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.2.89/7
- GPU model and memory: V100 

**Describe the problem**

I try to do an offline compilation of TensorFlow. I am able to partially circumvent dependencies download problems through the `--distdir` Bazel flag (locating third parties I got in `./deps` directory):

```
bazel build --config=mkl --config=opt --distdir=deps //tensorflow/tools/pip_package:build_pip_package
```
However, it fails first on two `bazel_rules` retrievals (`io_bazel_rules_go` and `io_bazel_rules_docker`) but I corrected these issues with the following patch in `WORKSPACE`:

```
diff --git a/WORKSPACE b/WORKSPACE
index 9db1d9b80eb..4284b9834cb 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -20,6 +20,40 @@ tf_repositories()
 
 register_toolchains(""@local_config_python//:py_toolchain"")
 
+# workaround for git fetch bazel_rules_go
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
+http_archive(
+    name = ""io_bazel_rules_go"",
+    sha256 = ""7c10271940c6bce577d51a075ae77728964db285dac0a46614a7934dc34303e6"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_go/releases/download/v0.26.0/rules_go-v0.26.0.tar.gz"",
+        ""https://github.com/bazelbuild/rules_go/releases/download/v0.26.0/rules_go-v0.26.0.tar.gz"",
+    ],
+)
+
+load(""@io_bazel_rules_go//go:deps.bzl"", ""go_register_toolchains"", ""go_rules_dependencies"")
+
+go_rules_dependencies()
+
+go_register_toolchains(version = ""1.16"")
+##
+
+# workaround for git fetch bazel_rules_docker
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
+http_archive(
+    name = ""io_bazel_rules_docker"",
+    sha256 = ""1698624e878b0607052ae6131aa216d45ebb63871ec497f26c67455b34119c80"",
+    strip_prefix = ""rules_docker-0.15.0"",
+    urls = [""https://github.com/bazelbuild/rules_docker/releases/download/v0.15.0/rules_docker-v0.15.0.tar.gz""],
+)
+
+load(""@io_bazel_rules_docker//toolchains/docker:toolchain.bzl"",
+    docker_toolchain_configure=""toolchain_configure""
+)
+##
+
 load(""@io_bazel_rules_closure//closure:defs.bzl"", ""closure_repositories"")
 
 closure_repositories()
```
And now I am stuck in getting `go_sdk`:

```
INFO: Repository go_sdk instantiated at:
  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:39:23: in <toplevel>
  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr
ivate/sdk.bzl:453:28: in go_register_toolchains
  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr
ivate/sdk.bzl:129:21: in go_download_sdk
Repository rule _go_download_sdk defined at:
  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr
ivate/sdk.bzl:116:35: in <toplevel>
WARNING: Download from https://golang.org/dl/?mode=json&include=all failed: class java.io.IOException 
connect timed out
WARNING: Download from https://golang.google.cn/dl/?mode=json&include=all failed: class java.io.IOExce
ption connect timed out
ERROR: An error occurred during the fetch of repository 'go_sdk':
   Traceback (most recent call last):
        File ""/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_ru
les_go/go/private/sdk.bzl"", line 70, column 21, in _go_download_sdk_impl
                ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=al
l, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3
d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out
INFO: Repository nasm instantiated at:
  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:19:16: in <toplevel>
  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:105:27: in tf_repositories
  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:58:9: in initialize_third_party
  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/nasm/workspace.bzl:6:29: in repo
Repository rule third_party_http_archive defined at:
  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/repo.bzl:216:43: in <toplevel>
INFO: Repository icu instantiated at:
  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:19:16: in <toplevel>
  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:105:27: in tf_repositories
  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:55:8: in initialize_third_party
  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/icu/workspace.bzl:11:29: in repo
Repository rule third_party_http_archive defined at:
  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/repo.bzl:216:43: in <toplevel>
Internal error thrown during build. Printing stack trace: java.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):
        File ""/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl"", line 70, column 21, in _go_download_sdk_impl
                ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])
[...]
FAILED: Build did NOT complete successfully (229 packages loaded, 3879 targets configured)
Internal error thrown during build. Printing stack trace: java.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):
        File ""/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl"", line 70, column 21, in _go_download_sdk_impl
                ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])
[...]
java.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):
        File ""/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl"", line 70, column 21, in _go_download_sdk_impl
                ctx.download(
Error in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])
[...]
FAILED: Build did NOT complete successfully (229 packages loaded, 3879 targets configured)
```
From what I get, it comes from Bazel internals and TensorFlow dependencies to `io_bazel_rules_go` (that is a bit hard to find since it is not directly in TensorFlow sources, or so it seems).

Is there anyway to circumvent this issue and to build TensorFlow offline?"
47814,micro: port op L2_POOL_2D from lite,"@tensorflow/micro

This issue tracks my work porting operator L2_POOL_2D from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1 (step 1): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2 (step 2): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences

The next 3 steps are combined into a single PR3 with separate commits:

(step 3): Copy operator from lite to micro making minimal changes and not including in the build
(step 4): Delete extra code from the micro copy of the operator
(step 5): Port micro copy of operator as necessary and add a corresponding test"
47813,Tensorflow lite lstm/bilstm reset variables why needed ? Can we avoid reset variables. ,"Getting different outputs for multiple runs in tensorflow lite invoke when reset is not called. Why different output ? Is this step causes performance issue in multiple runs because of reset function extra step during every run. why interpreter.reset_all_variables() is needed.

`# -*- coding: utf-8 -*-
""""""BiLSTMinput_Var_LastMode_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xTKC1yQ4S016p_yFFFm91TCJlHj7nnDf
""""""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()

# Add a LSTM layer with 128 internal units.
model.add(layers.Bidirectional(layers.LSTM(input_shape=(None,20),units= 128, return_sequences = False)))

# Step 2: Train & Evaluate the model.

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']);
x_test = np.zeros([3000,4,20], dtype=np.float32);
x_train = np.zeros([3000,4,20], dtype=np.float32);
y_test =  np.zeros([3000, 1,], dtype=np.float32);
y_train = np.zeros([3000, 1,], dtype=np.float32);

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 2
INPUT_SIZE = 20

concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))


# model directory.
MODEL_DIR = ""keras_bi_lstm_new""
model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

with open('bilstmmodel_1_2_20.tflite', 'wb') as f:
  f.write(tflite_model)

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
#interpreter = tf.lite.Interpreter(model_path=""bilstmmodel_last_1_2_20.tflite"")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

output_shape = output_details[0]['shape']
print(input_shape) 
print(output_shape)

myinp = np.loadtxt('img.txt')

myinp = np.resize(myinp,input_shape);
myinp = (np.float32(myinp) - 0) / 1
print(myinp)

interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0][""index""], myinp)
interpreter.invoke()
result = interpreter.get_tensor(output_details[0][""index""])
print(result)
interpreter.invoke()
result = interpreter.get_tensor(output_details[0][""index""])
print(result)

interpreter.reset_all_variables()
interpreter.invoke()
result = interpreter.get_tensor(output_details[0][""index""])
print(result)
print(result.shape)`"
