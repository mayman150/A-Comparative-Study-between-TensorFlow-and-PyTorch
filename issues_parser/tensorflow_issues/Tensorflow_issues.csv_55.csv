Issue Number,Issue Title,Issue Body
17609,The code in google_auth_provider.cc looks for application default credentials in an obsolete location.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 4.9.65
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.6.0-rc1-933-gf7acdf2ed5', '1.7.0-dev20180305')
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
gcloud auth activate-service-account --file ${CREDENTIALS_FILE}
ls ~/.config/gcloud/application_default_credentials.json
```

### Describe the problem
This is a bug where the code for reading Google credentials written by the `gcloud` tool no longer matches the behavior of the `gcloud` tool (the tool changed its behavior since this code was written).

Specifically, the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/google_auth_provider.cc#L97) tries to read credentials written by `gcloud` to a well-known location of ""~/.config/gcloud/application_default_credentials.json"".

However, `gcloud` does not write credentials to that location anymore.

Moreover, the Cloud SDK (of which `gcloud` is one component) provides no guarantees about the stability of where/how it stores credentials.

Instead, tools using these credentials are expected to use the `gcloud` tool to produce them by running:

```sh
gcloud config config-helper --format ""value(credential)""
```"
17608,Feature request: add easy way to install tensorflow as one package in win," adding easy way to install tensorflow as one package in windows .
no one should stay 4 hours to do that!"
17600,TensorBoard tutorial links to the wrong MNIST tutorial and example code,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.6
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

Sorry if this is better suited as an issue for TensorBoard, but since it's about documentation on tensorflow.org I thought it should go here.

The TensorBoard guide at https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard contains example code that it says ""is a modification of the [simple MNIST tutorial](https://www.tensorflow.org/tutorials/layers), in which we have added some summary ops."" But the tutorial it links to, https://www.tensorflow.org/tutorials/layers, is very different from the example code it gives -- that tutorial uses the tf.layers module to create a CNN, while the example code manually defines a one-hidden-layer fully-connected network. I'm not even sure what the example code is based on, since it doesn't seem that similar to https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/examples/tutorials/mnist/mnist.py. 

At the very least the link should be fixed to point to the correct original example code, but even better would be if the tutorial could be updated to show how to use TensorBoard with a network created with tf.layers."
17595,"Feature Request: quantize_weights does a signed_quantization [-127,+127] as well","- **OS Platform and Distribution: Linux Ubuntu 14.04/16.04
- **TensorFlow installed from (source or binary)**: Source v1.3
- **Bazel version (if compiling from source)**: bazel release 0.5.4
- **CUDA/cuDNN version**:  v8
- **GPU model and memory**: NVIDIA gtx1060
- **Have I written custom code**: No
- **Exact command to reproduce**:

> tensorflow/tools/graph_transforms:transform_graph

Only does an unsigned quantization of the weights. I think it is useful for many that we have a signed quantization so that a normal distribution having a median of zero (`0.0f`) converts to a `TF-8 0`. This is useful in general. Thanks. 
"
17593,tf.TensorShape equality comparison can return True for non-fully defined shapes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but trivial one-liner code.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: platform independent problem
- **TensorFlow installed from (source or binary)**: either
- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039 1.6.0
- **Python version**:  tested in 3.5 and 3.6
- **Bazel version (if compiling from source)**: not relevant
- **GCC/Compiler version (if compiling from source)**: not relevant
- **CUDA/cuDNN version**: not relevant
- **GPU model and memory**: not relevant
- **Exact command to reproduce**: see source one-liner below

### Describe the problem
tf.TensorShape equality comparison is designed to return None if any of its dimensions has a value of None. However, there is one case when this behavior fails and returns True when it shouldn't.

### Source code / logs
```python
dim = tf.Dimension(None)
tf.TensorShape(dim) == tf.TensorShape(dim) # Returns True instead of None.
tf.TensorShape([None]) == tf.TensorShape([None]) # Correctly returns None.
```

This can, of course, inadvertently appear in more complex shape manipulation codes and lead to unexpected results.

A bit of debugging suggests that if the dimension objects inside the TensorShape have the same id, they are skipped when invoking `tf.Dimension.__eq__`. This might be because dimension comparison is triggered through the list containing them rather than individually iterating them."
17590,Leak when creating many tf.layers.Conv2D with tf.eager,"
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3 LTS
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**: Python 3.5.2
- **CUDA/cuDNN version**:  cuda-9.0 
- **GPU model and memory**: TITAN X (Pascal)

- **Exact command to reproduce**:

see gist https://gist.github.com/matpalm/93680534c5c53d6e7836d3aea1a3bbb3
for minimal reproduction. run this script and watch loop times increase.

### Describe the problem

each creation of a tf.layers.Conv2D is slower than the one before. have a project based on evolutionary algorithm that creates many instances of tf.layers.Conv2D for fitness evaluation and over time entire process slows down. it's not expected this loop should slow down.

### Source code / logs
see gist above"
17589,Device mapping: no known devices. 2018-03-09 15:46:04.512611: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:,"Hi folks,

I am sure this question has been asked many many times everywhere but I couldn't get my problem resolved!
here is the error message after I execute:
`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`
 

the error:
```
2018-03-09 15:45:58.393237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-09 15:45:58.929878: E tensorflow/core/common_runtime/direct_session.cc:170] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11719016448
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ist/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1482, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/ist/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 622, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/home/ist/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
```

and when I try again I receive this message:
```
Device mapping: no known devices.
2018-03-09 15:46:04.512611: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:

```
I am working on a Ubuntu remote server which I access  from a Windows 10 Remote Desktop.
The info of Ubuntu as follows:

 lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 16.04.3 LTS
Release:        16.04
Codename:       xenial


when I execute nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.25     Driver Version: 390.25                                           |
|-------------------------------+----------------------+----------------------+
| GPU  Name: Persistence-M| Bus-Id   Disp.A | Volatile Uncorr. ECC           |
| Fan  Temp  Perf  Pwr:Usage/Cap | Memory-Usage | GPU-Util  Compute M. |
|==============================================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |  N/A                      |
| 47%   66C    P2   244W / 250W |  11146MiB / 11176MiB |  83% Default  |
+-------------------------------+----------------------+----------------------+

+------------------------------------------------------------------------------------+
| Processes:                                                                                     GPU Memory |
|  GPU       PID   Type   Process name                                                    Usage      |
|==================================================|
|    0      1168      G   /usr/lib/xorg/Xorg                                                     89MiB |
|    0      1957      G   compiz                                                                       90MiB |  
|    0      2847      C   python                                                                  10937MiB |
+-----------------------------------------------------------------------------+


and  nvcc --version:

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61

Python 3.6.4
Tensorflow-gpu 1.4.1

Have I written custom code? 
I just run a small piece of code to make sure if the GPU is utilized and the is mentioned above!
OS Platform and Distribution
also mentioned this above
TensorFlow installed from sudo pip3 install tensorflow-gpu==1.4.1
TensorFlow version 1.4.1
Bazel version didn't install it!
CUDA/cuDNN version 8.0.x / 6
GPU model and memory 

` sudo lshw -C ""display""
  *-display
       description: VGA compatible controller
       product: NVIDIA Corporation
       vendor: NVIDIA Corporation
       physical id: 0
       bus info: pci@0000:01:00.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom
       configuration: driver=nvidia latency=0
       resources: irq:148 memory:de000000-deffffff memory:c0000000-cfffffff memory:d0000000-d1ffffff ioport:e000(size=128) memory:c0000-dffff
  *-display UNCLAIMED
       description: Display controller
       product: Intel Corporation
       vendor: Intel Corporation
       physical id: 2
       bus info: pci@0000:00:02.0
       version: 00
       width: 64 bits
       clock: 33MHz
       capabilities: pciexpress msi pm cap_list
       configuration: latency=0
       resources: memory:dd000000-ddffffff memory:b0000000-bfffffff ioport:f000(size=64)`

Exact command to reproduce
```
import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

```
what should I do!
"
17588,AttentionWrapper bug with shape inference,"This is my [source code](https://github.com/soloice/tf-tutorial/blob/master/src/att_seq2seq_minimal_error.py), which receives an integer sequence, delete odd numbers, and copy the remaining even number sequence twice. For example, input: [1, 2, 3, 4, 5, 6], output: [2, 4, 6, 2, 4, 6]. Of course, padding is used during training.

If I comment [L124](https://github.com/soloice/tf-tutorial/blob/6675b3b4d3b0b1dbcc3d2541a0637cabbdd27681/src/att_seq2seq_minimal_error.py#L124) out, the code runs very well. However, if this line is enabled, it will raise the following error:

> ValueError: The shape for decoder/decoder/while/Merge_7:0 is not an invariant for the loop. It enters the loop with shape (64, 50), but has shape (?, 50) after one iteration. Provide shape invariants using either the `shape_invariants` argument of tf.while_loop or set_shape() on the loop variables.

This line is intended to disable input feeding scheme in paper ""Effective Approaches to Attention-based Neural Machine Translation"", which is the default behavior of AttentionWrapper.

Currently both my batch_size and num_steps are `None`. If I fix the batch size, i.e.: change placeholders to
```
encoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='encoder_inputs')
decoder_targets = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_targets')
decoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_inputs')
encoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='encoder_length')
decoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='decoder_length')
```
, everything works fine again.

The code is so simple, so probably it is a bug with shape inference.

"
17587,s390x build fails with boringssl error,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master
- **Bazel version (if compiling from source)**: 0.11.1
- **CUDA/cuDNN version**: Not used
- **Exact command to reproduce**: bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

TensorFlow master build is failing with below error: 
```
ERROR: /data/TF_tmp/_bazel_root/be3f47674f2731fd84874c35a2feb28b/external/kafka/BUILD:8:1: C++ compilation of rule '@kafka//:kafka' failed (Exit 1)
In file included from external/boringssl/src/include/openssl/ssl.h:145:0,
                 from external/kafka/src/rdkafka_int.h:53,
                 from external/kafka/src/rdkafka_conf.c:29:
external/boringssl/src/include/openssl/base.h:114:2: error: #error ""Unknown target CPU""
 #error ""Unknown target CPU""
```
We have disabled support for Apache Kafka Platform support through `./configure`. 


"
17586,Not found: FeedInputs: unable to find feed output ,"@GeorgeBohw
hello,i also meet this problem.could you tell me how do you solve this problem?
@tensorflowbutler 
i have use the the summarize_graph tool to guesses about likely input and output nodes,this is the command:bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/home/biao/test/occ_detect.pb
here is the message:

No inputs spotted.
No variables spotted.
Found 1 possible outputs: (name=softmax_linear/softmax_linear_1, op=Add) 
Found 850090 (850.09k) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 18 Const, 13 Identity, 5 Add, 5 Relu, 3 BiasAdd, 3 MaxPool, 3 MatMul, 3 LRN, 3 Conv2D, 2 Mul, 1 Floor, 1 QueueDequeueManyV2, 1 RandomShuffleQueueV2, 1 RandomUniform, 1 RealDiv, 1 Reshape, 1 Sub
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/biao/test/occ_detect.pb --show_flops --input_layer= --input_layer_type= --input_layer_shape= --output_layer=softmax_linear/softmax_linear_1

i guess the problem is my .pb file is fault?

"
17585,Tensorflow finding only 9gb of free memory but 11gb total for gtx 1080 ti GPU,We found on both Windows 10 and Linux machines that Tensorflow will only find 9gb of the 11gb available for GTX 1080 ti GPUs from both EVGA and MSI.  Having trouble finding a fix - does anyone know why it is doing this and how to fix it so it can get access to all the memory?
17582,sparse variable request or any alternate?,"In tensor flow (TF) , to compute gradients we have to pass some variable. Sparse tensors cannot be used as variables. Can you please tell me is there any solution for sparse matrix gradient ?
N/A
Tensorflow version 1.4"
17579,streaming curve points bug,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**: titan x 12g
- **Exact command to reproduce**: eval_train_classifier.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In the metrics_ops, there is streaming_curve_points. I added it to evaluation below code.
Everything else works flawless, but the ROC curve doesn't.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Source code:
`   # Define the metrics:
    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
        'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),
        'FPs': slim.metrics.streaming_false_positives(predictions, labels),
        'FNs': slim.metrics.streaming_false_negatives(predictions, labels),
        'TPs': slim.metrics.streaming_true_positives(predictions, labels),
        'TNs': slim.metrics.streaming_true_negatives(predictions, labels),
        'AUC': slim.metrics.streaming_auc(predictions, labels),
        'ROC curve': slim.metrics.streaming_curve_points(labels=labels,
                                                         predictions=tf.cast(predictions, tf.float32)),
    })
`

Logs:

> 2018-03-09 01:26:50.313306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties:                                                                                                                 [157/1802]
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:03:00.0
totalMemory: 11.92GiB freeMemory: 11.80GiB
2018-03-09 01:26:50.313398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-03-09 01:26:50.674278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11431 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci b
us id: 0000:03:00.0, compute capability: 5.2)
INFO:tensorflow:Restoring parameters from /tmp/glaucoma-models/resnet_v2_152_new/model.ckpt-3013
INFO:tensorflow:Evaluation [1/2]
INFO:tensorflow:Evaluation [2/2]
eval/FNs[21]
eval/FPs[11]eval/TNs[44]
eval/TPs[34]

eval/Accuracy[0.709090889]
Traceback (most recent call last):
  File ""eval_image_classifier.py"", line 196, in <module>
    tf.app.run()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""eval_image_classifier.py"", line 192, in main
    variables_to_restore=variables_to_restore)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py"", line 212, in evaluate_once
    config=session_config)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 651, in __exit__
    self._close_internal(exception_type)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 683, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 311, in end
    summary_str = session.run(self._summary_op, self._feed_dict)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tags and values not the same shape: [] != [200,2] (tag 'eval/ROC_curve')
         [[Node: eval/ROC_curve = ScalarSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](eval/ROC_curve/tags, curve_points/stack_1/_5417)]]

Caused by op u'eval/ROC_curve', defined at:
  File ""eval_image_classifier.py"", line 196, in <module>
    tf.app.run()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""eval_image_classifier.py"", line 168, in main
    op = tf.summary.scalar(summary_name, value, collections=[])
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/summary.py"", line 100, in scalar
    val = _gen_logging_ops._scalar_summary(tags=tag, values=tensor, name=scope)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 402, in _scalar_summary
    ""ScalarSummary"", tags=tags, values=values, name=name)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [200,2] (tag 'eval/ROC_curve')
         [[Node: eval/ROC_curve = ScalarSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](eval/ROC_curve/tags, curve_points/stack_1/_5417)]]
"
17571,keras.model_to_estimator feature request,"Really does not depend on system information, just a completely new feature. 

Using the:
```
tf.keras.estimator.model_to_estimator(keras_model=model,
                                                    model_dir=model_dir)
```

does not allow us to log images in tensor board while training the model. I believe this is because the keras model is serialized and then rebuilt in this function and it does not capture the summaries that you put in the original graph. 

I am able to get around this by using a Lambda layer that looks like this:

```
def viz_layer(x):
    from tensorflow.summary import histogram
    histogram('hist', x)
    return x

model.add(layers.Lambda(viz_layer))
```

So two problems with this:

1. It's pretty hacky to have to add identity layers in to the network to visualize
2. I can't get the lambda layer to work without importing tensor flow inside the layer which is pretty bad

It would be great if the original summaries were captured or if there were a more streamlined way to add summaries. "
17569,Problem w/ leaky_relu,"### Describe the problem
Having problems using `tf.nn.leaky_relu`.  Attempted to deal with the problem by trying out `tf.nn.elu` and `tf.nn.relu`, but both functions resulted in a fairly significant drop in accuracy.  I was originally building the model in Keras, but switched to TensorFlow after having a problem with the corresponding leaky_relu function in Keras.  The TensorFlow and Keras source code and error logs are below.  Thank you in advance!

### Source code / logs
TensorFlow Source Code
```
def conv2d_w_leakyrelu(x, W, b, s = 1, name = ""conlayer_i""): 
    x = tf.nn.conv2d(input = x, filter = W, strides = [1, s, s, 1], data_format = ""NHWC"", padding = ""SAME"", name = name)
    x = tf.nn.bias_add(value = x, bias = b)
    # return tf.nn.elu(features = x) # TENSORFLOW PROBLEMS WITH LEAKY_RELU, SO USING ELU ACTIVATION FOR NOW
    return tf.nn.leaky_relu(features = x, alpha = lr_alpha)
```

Keras Source Code
```
model.add(Conv2D(32, kernel_size = (im_h, im_w), strides = (s, s), padding = ""same"", kernel_initializer = KI, input_shape = input_shape))
    model.add(LeakyReLU(alpha = leakiness))
```

Below is the TensorFlow error message
<img width=""962"" alt=""screen shot 2018-03-08 at 3 13 12 pm"" src=""https://user-images.githubusercontent.com/10000384/37174004-489b7656-22e3-11e8-9118-060eba773965.png"">

Below is part of the Keras error message
<img width=""850"" alt=""screen shot 2018-03-08 at 3 23 54 pm"" src=""https://user-images.githubusercontent.com/10000384/37174559-df014b2e-22e4-11e8-961b-3be3f247cd6a.png"">
"
17568,tf.control_dependencies does not respect fed nodes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

**TLDR**: I have a node `X` which has a control dependency on `Y`. I do `session.run(fetches=[X], feed_dict={Y: value})`. TensorFlow still tries to run `Y` despite it already being fed.

**More details**:

Consider the following code:

    import tensorflow as tf
    
    p = tf.placeholder(tf.float32, name='p')
    q = tf.add(p, 2.0)
    with tf.control_dependencies([q]):
      r = tf.constant(1.0)
    
    sess = tf.Session()
    sess.run(r, feed_dict={q: 5})
    
    # InvalidArgumentError: You must feed a value for placeholder tensor 'p' with dtype float

I would have expected the `sess.run` call to succeed because I'm already feeding `q`. "
17566,tensorflow-gpu pip package is not compatible with cuda9 docker image,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary (`pip install tensorflow-gpu`)
- **TensorFlow version (use command below)**:
1.6.0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
CUDA 9, cuDNN 7
- **GPU model and memory**:
- **Exact command to reproduce**:
I was trying to build a horovod image, but this would affect anyone using the `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` base image:

```shell
docker build -t horovod https://raw.githubusercontent.com/uber/horovod/master/Dockerfile
docker run -it --rm horovod python tensorflow_mnist.py
```
### Describe the problem
When building a docker image based on `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` and doing a `pip install tensorflow-gpu==1.6.0`, the resulting image causes a crash because the base image contains cuDNN 7.1, while the tensorflow-gpu pip package was built against cuDNN 7.0.

### Source code / logs
Error messages:
```shell
2018-03-08 17:46:50.845206: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7004 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2018-03-08 17:46:50.845868: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) 
```

@flx42 "
17564,Provide tensor/outer product method 'tf.outer',"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.4.1 gpu
Python version: 3.5.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: Cuda 8.0/Cudnn 6.0
GPU model and memory: Titan xp
Exact command to reproduce: no method for outer product

Describe the problem

Tensor (outer product) is the fundamental operation on tensors, but there appears to be no method `tf.outer` in `tensorflow` analogous to `np.outer` in `numpy` to compute the outer product of arbitrary tensors. A google search pulls up these implementation suggestions on stackoverflow: https://stackoverflow.com/questions/33858021/outer-product-in-tensorflow, but these require the dimensions of the tensors to be accessed/known beforehand.


I have the following hack. It would be nice to express this in terms of a pairwise (given associativity) operation `tf.outer`.

The tensor product for an arbitrary collection of tensors can be computed:
```
def tensor_product(*e):
    """""" Tensor product of elements """"""
    if len(e) == 1:
        return e
    elif len(e) == 2:
        a, b = e
        r_a = len(a.get_shape().as_list())
        r_b = len(b.get_shape().as_list())
        s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)
        s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)
        a_reshaped = tf.reshape(a, s_a)
        b_reshaped = tf.reshape(b, s_b)
        return a_reshaped * b_reshaped
    prod = e[0]
    for tensor in e[1:]:
        prod = tensor_product(prod, tensor)
    return prod


```

The tensor product allows more elegant expression of rank-2 and greater tensors in a loss function. 
For example here is a diagonal and elliptical (weighted terms) quadratic term:

```
def diagonal_M(batch_size, d):
    """""" M_abij = delta_ab delta_ij """"""
    return tensor_product(tf.diag([1.] * batch_size), tf.diag([1.] * d))

```

```
def biased_diagonal_M(batch_size, d):
    """""" M_abij = lambda_i delta_ab delta_ij """"""
    return tensor_product(tf.diag([1.] * batch_size), tf.diag([5.] + [1.] * (d-1)))

```
ref: https://github.com/4d55397500/quadratic-forms-tensorflow"
17560,sess.run() returns invisible string for a tf.summary.merge_all() op. ,"HI, 

I have a problem to read the outpur of sess.run:
here is the code:
`        tf.summary.scalar('Loss', loss)
        tf.summary.scalar('Accuracy', accuracy)
        write_op = tf.summary.merge_all()
       summary_train = sess.run(write_op, feed_dict=feed_dict_train)`
where, write_op is a op from tf.summary.merge_all().

The output **summary_train** is something like: **\n\nLoss??\n\nAccuracy?????** 

Do you have any idea how I can get the numbers for the loss and accuracy from **summary_train**

Look forward to your response

Hao"
17559,CPU Stoped working suddendly.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17558,Timeline showing replicated processes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: Tesla k80 (11441MiB)
- **Exact command to reproduce**:python cifar10_train.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am running the standard CIPHAR example available in tensor-flow repository. I just added a few lines as suggested in this (https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659) by @prb12. If I look at the timeline created for this:
<img width=""1440"" alt=""screen shot 2018-03-08 at 10 58 30 am"" src=""https://user-images.githubusercontent.com/10864603/37161719-78b01972-22c1-11e8-98a1-fc5213711316.png"">
I see two processes running exactly at the same time. Is this a bug in the timeline or am I missing something here ? The other explanation to me looks like these two processes are running simultaneously and sharing the GPU which according to my knowledge is non-trivial. If, these two processes are actually sharing the GPU is there some code or documentation to understand how its being done by tensor-flow ? Thanks a lot for any help 

### Source code / logs
cifar10 example  : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10
my modified cifar10_train.py : https://gist.github.com/xilenteyex/bd8802d4baaf17acb8786ef0a9b60b7c "
17557,Feature request: initializing an Estimator without training,"Would it be possible to implement a method in Estimator to initialize the variables in the model? For example, it would then be possible to use `predict` before using `train` what is not possible actually. In some cases, the lack of this method could be very inconvenient."
17555,Nested while_loop does not work for automatic gradient derivation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes  I have
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6
- **TensorFlow installed from (source or binary)**: conda-forge
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```python
cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
def fn(inp):
    (outputs_fw, outputs_bw), _ = \
        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)
        return tf.concat([outputs_fw, outputs_bw], axis=2)
outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)
```

### Describe the problem
This problem seems having to do with the nested `while_loop`. I have a 4-D tensor that I want to run through a bidirectional GRU with, so I used `tf.map_fn` to map the sub tensors (which are 3-D tensor now) to `bidirectional_rnn` with the same `GRU` cells. This was going fine until I asked `TensorFlow` to generate the automatic gradient descent, which threw error suggesting `while_loop` cannot be inside another `while_loop`. 

**Reproducible code is here**:
```python
num_words = 1000
embedding_dim = 100
hidden_dim = 100
num_class = 20

words = tf.placeholder(tf.int32, [None, None, None], name='words')
words_length = tf.placeholder(tf.int32, [None, None], name='words_length')
sentences_length = tf.placeholder(tf.int32, [None], name='sentences_length')
labels = tf.placeholder(tf.int32, [None], name='labels')

with tf.variable_scope('embeddings'):
    embedding = \
        tf.get_variable('parameter', 
                        shape=(num_words, embedding_dim), 
                        dtype=tf.float32, trainable=True)
    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')
with tf.variable_scope('words_lstm'):
    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
    def fn(inp):
        (outputs_fw, outputs_bw), _ = \
            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)
        return tf.concat([outputs_fw, outputs_bw], axis=2)
    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)
with tf.variable_scope('words_attention'):
    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)
    attention = tf.layers.dense(outputs, units=1, activation=None)
    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 1, 3, 2])), perm=[0, 1, 3, 2])
outputs = tf.reduce_sum(outputs * attention, axis=2)
with tf.variable_scope('sentence_lstm'):
    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)
    (outputs_fw, outputs_bw), _ = \
        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, outputs, sequence_length=sentences_length, dtype=tf.float32)
outputs = tf.concat([outputs_fw, outputs_bw], axis=2)
with tf.variable_scope('sentence_attention'):
    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)
    attention = tf.layers.dense(hidden, units=1, activation=None)
    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])
outputs = tf.reduce_sum(outputs * attention, axis=1)
logits = tf.layers.dense(outputs, units=num_class, activation=None)
loss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')
training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)
```

**Error**
```python
INFO:tensorflow:Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop.

gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc while context: None
words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1 while context: words_lstm/map/while/while_context

Traceback for gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc:
  File ""/Users/shengc/anaconda/envs/py36/bin/ipython"", line 6, in <module>
    sys.exit(IPython.start_ipython())
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py"", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py"", line 356, in start
    self.shell.mainloop()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 480, in mainloop
    self.interact()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 471, in interact
    self.run_cell(code, store_history=True)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2856, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-cda6375ef3e5>"", line 1, in <module>
    get_ipython().run_line_magic('paste', '')
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2095, in run_line_magic
    result = fn(*args,**kwargs)
  File ""<decorator-gen-27>"", line 2, in paste
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py"", line 187, in <lambda>
    call = lambda f, *a, **k: f(*a, **k)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py"", line 199, in paste
    self.store_or_execute(block, name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py"", line 57, in store_or_execute
    self.shell.run_cell(b)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-0176473fb528>"", line 40, in <module>
    training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 131, in _TensorArrayWriteGrad
    grad = g.read(index)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 859, in read
    return self._implementation.read(index, name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 259, in read
    name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 4498, in _tensor_array_read_v3
    dtype=dtype, name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1674, in __init__
    self._control_flow_context.AddOp(self)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2251, in AddOp
    self._AddOpInternal(op)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2274, in _AddOpInternal
    real_x = self.AddValue(x)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2207, in AddValue
    real_val = grad_ctxt.grad_state.GetRealValue(val)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1050, in GetRealValue
    history_value = cur_grad_state.AddForwardAccumulator(cur_value)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 908, in AddForwardAccumulator
    name=""f_acc"")
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 3578, in _stack_v2
    stack_name=stack_name, name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

Traceback for words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1:
  File ""/Users/shengc/anaconda/envs/py36/bin/ipython"", line 6, in <module>
    sys.exit(IPython.start_ipython())
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py"", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py"", line 356, in start
    self.shell.mainloop()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 480, in mainloop
    self.interact()
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py"", line 471, in interact
    self.run_cell(code, store_history=True)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2856, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-cda6375ef3e5>"", line 1, in <module>
    get_ipython().run_line_magic('paste', '')
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2095, in run_line_magic
    result = fn(*args,**kwargs)
  File ""<decorator-gen-27>"", line 2, in paste
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py"", line 187, in <lambda>
    call = lambda f, *a, **k: f(*a, **k)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py"", line 199, in paste
    self.store_or_execute(block, name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py"", line 57, in store_or_execute
    self.shell.run_cell(b)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-0176473fb528>"", line 22, in <module>
    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 409, in map_fn
    swap_memory=swap_memory)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2934, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2720, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2662, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 399, in compute
    packed_fn_values = fn(packed_values)
  File ""<ipython-input-3-0176473fb528>"", line 20, in fn
    (outputs_fw, outputs_bw), _ =             tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 414, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 629, in dynamic_rnn
    dtype=dtype)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 688, in _dynamic_rnn_loop
    time_steps = input_shape[0]
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 573, in _slice_helper
    name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 737, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5501, in strided_slice
    name=name)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access


---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    369     try:
--> 370       xla_compile = op.get_attr(""_XlaCompile"")
    371       xla_separate_compiled_gradients = op.get_attr(

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2172         raise ValueError(
-> 2173             ""No attr named '"" + name + ""' in "" + str(self._node_def))
   2174       x = self._node_def.attr[name]

ValueError: No attr named '_XlaCompile' in name: ""words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3""
op: ""TensorArrayWriteV3""
input: ""words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter""
input: ""words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_1""
input: ""words_lstm/map/while/bidirectional_rnn/fw/fw/while/Select""
input: ""words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_2""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@words_lstm/map/while/bidirectional_rnn/fw/fw/while/gru_cell/add""
    }
  }
}


During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-3-0176473fb528> in <module>()
     38 logits = tf.layers.dense(outputs, units=num_class, activation=None)
     39 loss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')
---> 40 training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    353         aggregation_method=aggregation_method,
    354         colocate_gradients_with_ops=colocate_gradients_with_ops,
--> 355         grad_loss=grad_loss)
    356
    357     vars_with_grad = [v for g, v in grads_and_vars if g is not None]

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)
    454         gate_gradients=(gate_gradients == Optimizer.GATE_OP),
    455         aggregation_method=aggregation_method,
--> 456         colocate_gradients_with_ops=colocate_gradients_with_ops)
    457     if gate_gradients == Optimizer.GATE_GRAPH:
    458       grads = control_flow_ops.tuple(grads)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    373       xla_scope = op.get_attr(""_XlaScope"").decode()
    374     except ValueError:
--> 375       return grad_fn()  # Exit early
    376
    377   if not xla_compile:

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py in _TensorArrayWriteGrad(op, flow)
    129                                     colocate_with_first_write_call=False)
    130        .grad(source=grad_source, flow=flow))
--> 131   grad = g.read(index)
    132   return [None, None, grad, flow]
    133

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)
    857       The tensor at index `index`.
    858     """"""
--> 859     return self._implementation.read(index, name=name)
    860
    861   @tf_should_use.should_use_result

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)
    257         flow_in=self._flow,
    258         dtype=self._dtype,
--> 259         name=name)
    260     if self._element_shape:
    261       value.set_shape(self._element_shape[0].dims)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _tensor_array_read_v3(handle, index, flow_in, dtype, name)
   4496     _, _, _op = _op_def_lib._apply_op_helper(
   4497         ""TensorArrayReadV3"", handle=handle, index=index, flow_in=flow_in,
-> 4498         dtype=dtype, name=name)
   4499     _result = _op.outputs[:]
   4500     _inputs_flat = _op.inputs

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   3158         input_types=input_types,
   3159         original_op=self._default_original_op,
-> 3160         op_def=op_def)
   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,
   3162                            compute_device=compute_device)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
   1673     if self._control_flow_context is not None:
-> 1674       self._control_flow_context.AddOp(self)
   1675     self._recompute_node_def()
   1676

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddOp(self, op)
   2249             op_input_ctxt._AddOpInternal(op)
   2250             return
-> 2251     self._AddOpInternal(op)
   2252
   2253   def _AddOpInternal(self, op):

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _AddOpInternal(self, op)
   2272       for index in range(len(op.inputs)):
   2273         x = op.inputs[index]
-> 2274         real_x = self.AddValue(x)
   2275         if real_x != x:
   2276           op._update_input(index, real_x)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddValue(self, val)
   2205               forward_ctxt = forward_ctxt.GetWhileContext()
   2206           if forward_ctxt == grad_ctxt.grad_state.forward_context:
-> 2207             real_val = grad_ctxt.grad_state.GetRealValue(val)
   2208             self._external_values[val.name] = real_val
   2209             return real_val

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in GetRealValue(self, value)
   1048           # Record the history of this value in forward_ctxt.
   1049           self._grad_context.Exit()
-> 1050           history_value = cur_grad_state.AddForwardAccumulator(cur_value)
   1051           self._grad_context.Enter()
   1052           break

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddForwardAccumulator(self, value, dead_branch)
    906             max_size=maximum_iterations,
    907             elem_type=value.dtype.base_dtype,
--> 908             name=""f_acc"")
    909         # pylint: enable=protected-access
    910       if curr_ctxt: curr_ctxt.Exit()

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _stack_v2(max_size, elem_type, stack_name, name)
   3576     _, _, _op = _op_def_lib._apply_op_helper(
   3577         ""StackV2"", max_size=max_size, elem_type=elem_type,
-> 3578         stack_name=stack_name, name=name)
   3579     _result = _op.outputs[:]
   3580     _inputs_flat = _op.inputs

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   3158         input_types=input_types,
   3159         original_op=self._default_original_op,
-> 3160         op_def=op_def)
   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,
   3162                            compute_device=compute_device)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1670     self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access
   1671     for input_tensor in self.inputs:
-> 1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
   1673     if self._control_flow_context is not None:
   1674       self._control_flow_context.AddOp(self)

~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)
    198         input_op.name, """".join(traceback.format_list(input_op.traceback)))
    199     logging.info(log_msg)
--> 200     raise ValueError(error_msg + "" See info log for more details."")

ValueError: Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop. See info log for more details.
```

### Source code / logs
source code is here,
https://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb
calling `HierarchicalAttentionNetwork()._make_graph_batch(graph)` will produce the above error"
17554,tf.train.Saver() protocol buffer saving issue,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1 gpu
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda 8.0/Cudnn 6.0
- **GPU model and memory**: Titan xp
- **Exact command to reproduce**: tf.train.Saver()

### Describe the problem
When using tf.train.Saver() if you set max_to_keep=None to keep all checkpoint files created then the protocol buffer saved at 'checkpoint' will only save the name of the latest checkpoint created and not all created checkpoints. 

When you set max_to_keep=N to be any other value then it will save the latest N checkpoint names as intended. 


"
17553,"tf.python_io.TFRecordWriter() results in ""UnknownError: Failed to create a NewWriteableFile""","### System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 7, 64 Bit**
- TensorFlow installed from (source or binary): **TensorFlow installed from Conda**
- TensorFlow version (use command below): **1.6.0**
- Python version: **Python 3.6.3**
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: **9.1.85**
- GPU model and memory: **NVIDIA GeForce 820M**
- Exact command to reproduce: 
import tensorflow as tf
tf.python_io.TFRecordWriter('C:\\Users\\user_name\\Documents')

import tensorflow as tf
tf.python_io.TFRecordWriter('Any_path')

### Problem
tf.python_io.TFRecordWriter('path') doesn't create output file and returns following error:
_UnknownError: Failed to create a NewWriteableFile: 'path' : Access is denied.
; Input/output error._

### Troubleshooting
Tried with different folders and drives as ""path"". But error is consistent 

### Source code / logs

import tensorflow as tf
C:\Users\Sethu\Anaconda3\lib\site-packages\h5py\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

tf.python_io.TFRecordWriter('C:\\Users\\Sethu\\Documents')
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-2-1fe0cde88225> in <module>()
----> 1 tf.python_io.TFRecordWriter('C:\\Users\\Sethu\\Documents')

~\Anaconda3\lib\site-packages\tensorflow\python\lib\io\tf_record.py in __init__(self, path, options)
    109     with errors.raise_exception_on_not_ok_status() as status:
    110       self._writer = pywrap_tensorflow.PyRecordWriter_New(
--> 111           compat.as_bytes(path), compat.as_bytes(compression_type), status)
    112 
    113   def __enter__(self):

~\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    514             None, None,
    515             compat.as_text(c_api.TF_Message(self.status.status)),
--> 516             c_api.TF_GetCode(self.status.status))
    517     # Delete the underlying status object from memory otherwise it stays alive
    518     # as there is a reference to status from this from the traceback due to

UnknownError: Failed to create a NewWriteableFile: C:\Users\Sethu\Documents : Access is denied.
; Input/output error
"
17552,train on new dataset,i want to detect people wearing a hat.can i directly label those people wearing hats as a class  and train on this dataset?can cnn distinguish people wearing hats from those who don't?
17550,can we compute sparse matrix gradient in TF?,"In tensor flow (TF) , to compute gradients we have to pass some variable. Sparse tensors cannot be used as variables. Can you please tell me is there any solution for sparse matrix gradient ?
N/A"
17549,Setting up CI jobs for Windows Bazel build,"Now tensorflow/contrib is enabled in the Bazel Windows build. (https://github.com/tensorflow/tensorflow/pull/16659)

We can start working on setting up kokoro jobs for Bazel Windows build.

- [ ] Github presubmit
- [ ] Github postsubmit
- [ ] Internal presubmit
- [ ] Internal postsubmit
"
17548,Compilation flags are not always passed.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: b'v1.6.0-0-gd2e24b6039' 1.6.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.10.1- (@non-git)
- **GCC/Compiler version (if compiling from source)**: 6.4
- **CUDA/cuDNN version**: 9.1/7.0 (also with 9.0)
- **GPU model and memory**: 1070 Ti
- **Exact command to reproduce**: See building script

Building with enabled MPI against Openmpi fails, as mpicxx.h is not included. This is despite explicitly passing the flag `-DOMPI_SKIP_MPICXX`. I am also passing the `march=native` flag, and once installed, Tensorflow doesn't complain about my CPU having more capabilities, so this seems to be passed at least some of the time.

Adding `#define OMPI_SKIP_MPICXX` in `tensorflow/contrib/mpi/mpi_utils.h` circumvents the problem, and Tensorflow compiles nicely.

Full error message:
```
ERROR: /home/david/gits/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda/ \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \
    GCC_HOST_COMPILER_PATH=/home/david/.local/bin/gcc6.4 \
    LD_LIBRARY_PATH=/usr/lib64/openmpi/lib \
    PATH=/usr/lib64/openmpi/bin:/home/david/.local/bin:/home/david/.local/hmmer3.1/bin:/home/david/.virtualenvs/py36/bin:/usr/libexec/python3-sphinx:/home/david/.local/bin:/home/david/.local/hmmer3.1/bin:/home/david/.virtualenvs/py36/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/cuda/bin:/home/david/.local/bin:/home/david/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/david/.virtualenvs/py36/bin/python \
    PYTHON_LIB_PATH=/home/david/.virtualenvs/py36/lib/python3.6/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.1 \
    TF_CUDNN_VERSION=7 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++11' -MD -MF bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.o' -fPIC -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DTENSORFLOW_USE_MPI -DTENSORFLOW_USE_GDR '-DGRPC_ARES=0' -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/k8-py3-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/k8-py3-opt/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-py3-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-py3-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/k8-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_cuda -iquote external/grpc -iquote bazel-out/k8-py3-opt/genfiles/external/grpc -isystem external/protobuf_archive/src -isystem bazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-py3-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-py3-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/k8-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem external/grpc/include -isystem bazel-out/k8-py3-opt/genfiles/external/grpc/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc -o bazel-out/k8-py3-opt/bin/tensorflow/contrib/mpi/_objs/mpi_rendezvous_mgr/tensorflow/contrib/mpi/mpi_rendezvous_mgr.pic.o)
In file included from ./tensorflow/contrib/mpi/mpi_utils.h:28:0,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./third_party/mpi/mpi.h:2704:41: fatal error: openmpi/ompi/mpi/cxx/mpicxx.h: No such file or directory
 #include ""openmpi/ompi/mpi/cxx/mpicxx.h""
                                         ^
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1153.570s, Critical Path: 141.81s
```

Building script:
```
set -e

export PYTHON_BIN_PATH=`which python`
export PYTHON_LIB_PATH=/home/david/.virtualenvs/py36/lib/python3.6/site-packages

export GCC_HOST_COMPILER_PATH=$HOME/.local/bin/gcc6.4
export COMPUTECPP_TOOLKIT_PATH=$HOME/.local/bin/g++6.4

export TF_NEED_CUDA=1
export TF_CUDA_VERSION=9.1
export CUDA_TOOLKIT_PATH=/usr/local/cuda/
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=$CUDA_TOOLKIT_PATH
export TF_CUDA_COMPUTE_CAPABILITIES=6.1
export TF_CUDA_CLANG=0
export TF_NEED_OPENCL_SYCL=0

export TF_NEED_JEMALLOC=1
export TF_NEED_HDFS=0
export TF_NEED_GCP=0
export TF_NEED_S3=0
export TF_ENABLE_XLA=1
export TF_NEED_GDR=1
export TF_NEED_VERBS=0
export TF_NEED_MPI=1
export CC_OPT_FLAGS=""-DOMPI_SKIP_MPICXX -O2 -pipe -march=native""
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_KAFKA=0
export TF_NEED_TENSORRT=0


cd tensorflow
./configure &&
bazel build -c opt  --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package &&
bazel-bin/tensorflow/tools/pip_package/build_pip_package built_wheels/
```

Relevant extracto of `.tf_configure.bazelrc` showing the flags being saved

```build:opt --copt=-DOMPI_SKIP_MPICXX
build:opt --copt=-O2
build:opt --copt=-pipe
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```

"
17542,Feature map in tensorflow source code,"OS Platform and Distribution : ubuntu16.04
TensorFlow installed from : source
Bazel version : 0.9.0
CUDA/cuDNN version : 8 / 6
GPU model and memory: nvidia 1080ti
Exact command to reproduce ; n/a

I try to learn tensorflow source code. When doing forward propagation, each layer will produce a feature map. I want to know after each layer is completed.Where is the feature map will be stored ? 
How can find this part? I hope i can know it.

I find the this part (conv2d):

    OP_REQUIRES_OK(context,
                   GetWindowedOutputSize(input_rows, filter_rows, stride_rows,
                                         padding_, &out_rows, &pad_rows));
    OP_REQUIRES_OK(context,
                   GetWindowedOutputSize(input_cols, filter_cols, stride_cols,
                                         padding_, &out_cols, &pad_cols));
    TensorShape out_shape =
        ShapeFromFormat(data_format_, batch, out_rows, out_cols, out_depth);

    // Output tensor is of the following dimensions:
    // [ in_batch, out_rows, out_cols, out_depth ]
    Tensor* output = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));

Is the ""context"" saving the feature map after forward propagation ?
Thanks."
17541,Keras/TF1.6.0 does not support CuDNNLSTM/CuDNNGRU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) None **:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Linux Ubuntu 16.04**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.6.0**:
- **Python version 3.6.3 **: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version 9.0**:
- **GPU model and memory Quadro P4000 8116MiB **:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
TF1.6.0/Keras does not support CuDNNLSTM/CuDNNGRU.

Which is already supported on Keras 2.1.5.
  https://keras.io/layers/recurrent/

### Source code / logs
I replace from LSTM to CuDNNLSTM. and get following error.
  https://github.com/keras-team/keras/blob/2.1.5/examples/lstm_text_generation.py

---------------------------------------------------------------------------`
ImportError                               Traceback (most recent call last)
<ipython-input-19-54f42e5ce79a> in <module>()
     12 from tensorflow.python.keras.models import Sequential
     13 from tensorflow.python.keras.layers import Dense, Activation
---> 14 from tensorflow.python.keras.layers import LSTM, CuDNNLSTM
     15 from tensorflow.python.keras.optimizers import RMSprop
     16 from keras.utils.data_utils import get_file

ImportError: cannot import name 'CuDNNLSTM'

"
17540,Protobuf size explosion for TFLite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf 1.6.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: bazel release 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc 4.9.4
- **CUDA/cuDNN version**: /
- **GPU model and memory**: /
- **Exact command to reproduce**: 
`bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/tmp/gru_rnn.pb --input_format=TENSORFLOW_GRAPHDEF  --input_types=FLOAT --output_format=TFLITE --output_file=/tmp/gru_rnn.tflite --inference_type=FLOAT --inference_input_type=FLOAT --output_arrays=model/softmax,model/rnn_states`


### Describe the problem
I successfully built a tflite model for mobile usage on November, 2017, and the size of .tflite was almost the same as .pb (no quantized), which was bout 950k.

However, when I try to convert the same .pb to tflite model with tensorflow 1.6 master branch (HEAD commit f7acdf2ed5e0b9c50c1c5f4b80163255aa9e8073), I find that the size of the .tflite file is 17M, **which is 17x larger than before!!**

So I make a binary search of the commits in `tensorflow/contrib/lite` history, then I find out at which commit that thing changes: the commit 90ce80131a8b5213d9f3eb9649d63921db7874a4 can produce 950k .tflite model, but right after it 528d0c5e4d148655b797368fd55fe6304730fece will make the tflite model become 17M.

Here are some additional information:
  - Both of these two tflite models can be successfully run on mobile.
  - I did not check the commits (if there have) between these two, since I only trace the `/tensorflow/contrib/lite` history.
  - I guess the problem happened due to some of the operations I used in my frozen graph, in which I have 2 layer gru and some linear transforms. But I cannot upload the .pb file here..."
17538,strange error on mac,"here is the code:
```
import tensorflow as tf
a =tf.Variable(tf.float32,tf.zeros([1,1]))
```
system:
osx 10.12.6
macbook pro 2017

I installed tensorflow 1.6 using the following command:
`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl`

below is the debug info

```
/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/Users/long/dev/littleboy/tftest.py"", line 2, in <module>
    a =tf.Variable(tf.float32,tf.zeros([1,1]))
  File ""/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 233, in __init__
    constraint=constraint)
  File ""/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 309, in _init_from_args
    if trainable and ops.GraphKeys.TRAINABLE_VARIABLES not in collections:
  File ""/Users/long/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 614, in __bool__
    raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
```"
17537,"AVX512F is not compiled when running ""hello world"" test using Tensorflow 1.6 on Intel KNL","------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No custom code

- **TensorFlow installed from (source or binary)**:
binary, follow the instructions from intel,
https://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available#comment-1919528

Linux KNL101-04 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux KNL101-04 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.2)
tensorflow (1.6.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

It seems that AVX512F is not compiled into released tensorflow 1.6. (but other KNL specific AVX512 instructions are compiled!!!)

I run simple ""hello world"" test on KNL after install the tensorflow 1.6 by following the steps mentioned here!!

it give me the message as follows:

2018-03-08 13:08:26.924344: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F

BTW, i try to compile the tensorflow 1.6 from source code, and adding all of avxxxxx as -copt to bazel build cmd, after build tensorflow 1.6 package correctly, re-run above test program, it seems the ""AVX512F"" is compiled!!!!

### Source code / logs
Source code
simple program, just print ""hello tensorflow"" using tensorflow.

logs:
2018-03-08 13:08:26.924344: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F"
17534,Unable to use TF with a Maven project on Windows,"I get the following errors when I tried to use tensorflow with maven project on windows:

""""""
Error injecting constructor, java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.
""""""

And I use maven in project:

        <dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>tensorflow</artifactId>
            <version>1.5.0</version>
        </dependency>
        <dependency>

It works well on OS but failed on windows. Anyone knows how to fix this? Massive thanks!"
17533,tf1.6 error: tf.contrib.ffmpeg.decode_video,"@yongtang  I use tensorflow 1.6on ubuntu and decodevideo.

`with tf.Session() as sess:
  movie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')
  movie = tf.contrib.ffmpeg.decode_video(movie_bin)
  movie_ev = movie.eval()
  print(""****"",len(movie_ev))`

but get an error

`2018-03-08 10:48:23.000491: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:400] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [106, 0, 640, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_cPNCGW.err
已放弃 (核心已转储)
`
"
17530,Feature request:  Increase kMaxEagerParentSize or make it python version dependent,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A (problem occurs on startup)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
tip of tree
- **Python version**: 
3.6 (from source)
- **Bazel version (if compiling from source)**:
0.10.1
- **GCC/Compiler version (if compiling from source)**:
4.8.4
- **CUDA/cuDNN version**:
unused
- **GPU model and memory**:
n/a
- **Exact command to reproduce**:
import tensorflow

### Describe the problem
We wanted to evaluate our patches in Valgrind, so we made a custom build of python 3.6 from source (it's more valgrind-friendly), then compiled TensorFlow against that.

When this done, 'import tensorflow' fails on a python exception because the size of the base class of EagerTensor in Python 3.6 is greater than kMaxEagerParentSize.

Issue #16836 was opened on this a month ago, but closed by its creator.  When we encountered this problem, we found that we could simply set the value of kMaxEagerParentSize to 48 and everything would run with no obvious issues.

Is there a reason that kMaxEagerParentSize must always equal 32, or can it be increased, or possibly even adjusted depending on which version of python TensorFlow is being compiled for?


### Source code / logs
```
diff --git a/tensorflow/python/eager/pywrap_tensor.cc b/tensorflow/python/eager/pywrap_tensor.cc
index 3ec2109..cc1c0f7 100644
--- a/tensorflow/python/eager/pywrap_tensor.cc
+++ b/tensorflow/python/eager/pywrap_tensor.cc
@@ -163,7 +163,7 @@ PyObject* PyIntFromDataType(TF_DataType l) {
 
 extern ""C"" {
 
-static const int kMaxEagerTensorParentSize = 32;
+static const int kMaxEagerTensorParentSize = 48;
 
 // TODO(agarwal): store context handle in EagerTensor.
 typedef struct EagerTensor {
```"
17526,NPE hardware acceleration support in Qualcomm chips,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android OS
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.6 TFLite
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: See below
- **Exact command to reproduce**: N/A

Can anyone clarify what chips will support NPE and have hardware acceleration for on device inference with tf lite?

NPE SDK says 845, 820, 835, 625, 626, 650, 652, 653, 660, 630, 636, 450, 820Am.

I was under the impression that only the 845 had NPE?"
17525,Build errors for non-Android build with lite proto runtime,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Minor modifications to tensorflow/contrib/makefile/Makefile to allow for lite proto runtime 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo-based
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A (using Makefile)
- **GCC/Compiler version (if compiling from source)**: clang++ 6.6.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
1. Patch Makefile to add `-DTENSORFLOW_LITE_PROTOS` to `CXXFLAGS`
2. `make -f tensorflow/contrib/makefile/Makefile ...` (as in tensorflow/contrib/makefile/build_all_linux.sh)

### Describe the problem
A few files fail to compile because they rely on either:
1. Lite runtime protos only being used in Android builds (i.e. when `__ANDROID__` is true), or
2. All protos being `Message` objects (whereas they are `MessageLite `objects with the lite runtime).

### Source code / logs
The specific errors stem from the use of `proto.DebugString()` in the following files:
`core/framework/reader_base.cc`
`core/grappler/costs/op_level_cost_estimator.cc`
`core/grappler/costs/utils.cc`
`core/grappler/grappler_item_builder.cc`
`core/grappler/optimizers/constant_folding.cc`
`core/grappler/optimizers/dependency_optimizer.cc`

and the use of the `ReadTextProto` function in `core/grappler/inputs/utils.cc`.

I will attempt to fix this problem myself, and will use this bug to track my progress and ask questions."
17520,"New version 1.6 ""pip install --upgrade tensorflow "" installs tensorflow-gpu NOT tensorflow (cpu)....","1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
**BUG**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**WINDOWS 10.0**
- TensorFlow installed from (source or binary):
**pip (binary)**
- TensorFlow version (use command below):
**1.6**
- Python version: 
**tried both python 3.5.2 & 3.6.4**
- CUDA/cuDNN version: 
NO CUDA
- GPU model and memory:
**NO GPU**
- Exact command to reproduce:
**pip install --upgrade tensorflow**

### Describe the problem
import tensorflow as tf 
give the error (No module named ""pywrap_tensorflow"")
Issue 42011070 on stack 
There : it became clear that it is a cudannxx_x.dll , i.e. CUDA error.   
I have tensorflow-gpu running flawlesly on NVIDIA GPU
### Source code / logs
No module named _pywrap_tensorflow
"
17519,"Deploying Custom CNN implementation frameworks (fasterRCNN, maskRCNN etc) using TF-Serving","Running popular forks (fasterRCNN, maskRCNN), one finds they cannot be serialized as a GraphDef because of the usage of py_funcs. (Ref: Official tf documentation)
<img width=""800"" alt=""screen shot 2018-03-07 at 4 27 31 pm"" src=""https://user-images.githubusercontent.com/15898956/37120940-cfd0e972-2229-11e8-9db8-0a710ebb8f8c.png"">


The error observed is such: 
``2018-03-07 21:26:02.451945: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: saved_model version: 1} failed: Not found: Op type not registered 'PyFunc' in binary running on f39ddd6ef2c7. Make sure the Op and Kernel are registered in the binary running in this process``

Is there any official fork/implementation for these popular frameworks or is there some documentation which can be used to convert py_funcs to native tensorflow operations?

Also, kindly let me know if any of the new tf betas in the works support/will support it since I am looking to deploy mine.

Thanks

-----------------------------------------------
System Info:
  - Running in docker container (ubuntu/bash)
  - tf 1.5.x"
17518,tf.matrix_band_part uses int64 as default type for num_lower and num_upper,"tf.matrix_band_part uses int64 as default type for num_lower and num_upper.
But it is weird, because most return from tf.shape() and tf.size() is int32."
17516,Not thread safe about saved_model.loader.load_latency_microsecs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


== cat /etc/issue ===============================================
Darwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.0.0 (clang-800.0.42.1)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-serving-api (1.3.0)
tensorflow-tensorboard (0.1.8)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.3.0-rc2-20-g0787eee', '1.3.0')

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I'm having a log in tensorflow/serving
```
2018-03-07 19:11:57.388343: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: success. Took 22100393 microseconds.
```
the number is so big, so it should be wrong..

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

"
17514,ImportError: No module named 'tensorflow.contrib.ios_examples.benchmark.benchmark.xcodeproj',"I am trying to create exe file for python application built on tkinter(8.6), python(3.6), keras(0.14), tensorflow(1.5),  cx_Freeze(5.1). All the package installs  were done through pip.
Windows 7-64 bit
I'm using command:- python setup.py build
Below is my setup.py file:-

 import cx_Freeze
 import sys
 import matplotlib
 import os
 import pandas
 import tkinter.filedialog
 import keras
 import sklearn
 import numpy
 import tensorflow
 import openpyxl
 import datetime
 base = None
 
 os.environ['TCL_LIBRARY'] = r'C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\tcl\tcl8.6'
 os.environ['TK_LIBRARY'] = r'C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\tcl\tk8.6'
 
 if sys.platform == 'win32':
     base = ""Win32GUI""
 from glob import glob
 #data_files = [(""Microsoft.VC120.CRT"", glob(r'C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\redist\x64\Microsoft.VC120.CRT\*.*'))]    
 executables = [cx_Freeze.Executable(""app-3.py"",base=base)]
 
 cx_Freeze.setup(
         name = ""foreacast"",
         #options = {""build_exe"": {""packages"":[""tkinter"",""matplotlib""],""include_files"":[""numpy""]}}
         options = { ""build_exe"": {""packages"":[""cx_Freeze"",""datetime"",""openpyxl"",""tkinter"",""numpy"",""matplotlib"",""pandas"",""tkinter.filedialog"",""keras"",""sklearn"",""tensorflow""] }},
         version = ""0.1"",
         description = ""test"",
         #data_files=data_files,
         executables = executables)
Below is input file- app-3.py
#from tkinter import Tk, Label, Button, Frame
import tkinter as tk
from tkinter.filedialog import askopenfilename
import os
import tkinter.messagebox
import numpy as np
import xlrd
import datetime
import matplotlib.pyplot as plt
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Dropout
from keras import optimizers
from sklearn.preprocessing import MinMaxScaler
from numpy.random import seed
import tensorflow as tf
from tensorflow import set_random_seed 
from openpyxl import load_workbook
# Here, we are creating our class, Window, and inheriting from the Frame
# class. Frame is a class from the tkinter module. (see Lib/tkinter/__init__)
class Window(tk.Frame):

    # Define settings upon initialization. Here you can specify
    def __init__(self, master=None):
        
        # parameters that you want to send through the Frame class. 
        tk.Frame.__init__(self, master,background=""#66b3ff"")   

        #reference to the master widget, which is the tk window                 
        self.master = master

        #with that, we want to then run init_window, which doesn't yet exist
        self.init_window()
    
        self.master.minsize(width=1400, height=900)
        self.master.maxsize(width=1400, height=900)
        self.master.configure(background=""#8cb3d9"")
    #Creation of init_window
    def close_window():
        root.destroy()
        
    def init_window(self):

        # changing the title of our master widget      
        self.master.title(""Call Volume Forecasting"")

        tk.Label(root, text=""Call Volume Forecasting"",font = ""Helvetica 28 bold italic"",bg = ""#3333ff"",fg = ""white"",width=20).grid(row=0,column=0,columnspan=4,padx=400, pady=20,sticky=tk.W)
        
        self.CheckAdvisor=tk.IntVar()
        self.CheckShareholder=tk.IntVar()
        self.CheckRetirement=tk.IntVar()
        self.CheckFast=tk.IntVar()
        self.CheckVIP=tk.IntVar()
        self.CheckCGF=tk.IntVar()
        self.CheckWT=tk.IntVar()
        self.CheckIOBRP=tk.IntVar()
        self.CheckFT=tk.IntVar()
        self.CheckNJBEST=tk.IntVar()
        self.CheckSelectAll=tk.IntVar()
        self.CheckType = tk.IntVar()
        self.input_file = """"
        
        tk.Button(root,text=""Choose Input File"",command=self.choose_input_file,font = ""Helvetica 18 bold italic"",bg = ""#3333ff"",fg = ""white"",width=15,height=1).grid(row=1, column = 0,columnspan=2, padx=200,pady=30,sticky=tk.W)
                  
        self.input_file = tk.Text(root,width=80,height=2)
        self.input_file.grid(row=1, column=1,columnspan=3,padx=50,pady=20, sticky=tk.W)
        
        self.advisorstate = tk.Checkbutton(root, text=""Advisor"",  variable= self.CheckAdvisor,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.advisorstate.grid(row=2, column = 0, padx=100, pady=30,sticky=tk.W)
        
        self.shareholderstate = tk.Checkbutton(root, text=""Shareholder"",variable= self.CheckShareholder, font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.shareholderstate.grid(row=3, column = 0, padx=100, pady=30,sticky=tk.W)
        
        self.retirementstate= tk.Checkbutton(root, text=""Retirement"",variable= self.CheckRetirement,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.retirementstate.grid(row=4, column = 0, padx=100, pady=30,sticky=tk.W)
        
        self.faststate = tk.Checkbutton(root, text=""Fast"",variable=self.CheckFast,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.faststate.grid(row=5, column = 0, padx=100, pady=30,sticky=tk.W)
        
        self.vipstate = tk.Checkbutton(root, text=""VIP"",variable=self.CheckVIP,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.vipstate.grid(row=6, column = 0, padx=100, pady=30,sticky=tk.W)
        
        self.cgf = tk.Checkbutton(root, text=""CGF"",variable=self.CheckCGF,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.cgf.grid(row=2, column = 1, padx=10, pady=30,sticky=tk.W)
        
        self.wt = tk.Checkbutton(root, text=""WT"",variable=self.CheckWT,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.wt.grid(row=3, column = 1, padx=10, pady=30,sticky=tk.W)
        
        self.iobrp = tk.Checkbutton(root, text=""IOBRP"",variable=self.CheckIOBRP,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.iobrp.grid(row=4, column = 1, padx=10, pady=30,sticky=tk.W)
        
        self.ft = tk.Checkbutton(root, text=""FT 529"",variable=self.CheckFT,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.ft.grid(row=5, column = 1, padx=10, pady=30,sticky=tk.W)
        
        self.njbest = tk.Checkbutton(root, text=""NJBEST"",variable=self.CheckNJBEST,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"",width=10,anchor=tk.W)
        self.njbest.grid(row=6, column = 1, padx=10, pady=30,sticky=tk.W)
        
        self.selectall = tk.Checkbutton(root, text=""Select all skills"",variable=self.CheckSelectAll,command=self.cb_check,font = ""Helvetica 18 bold italic"",bg = ""light grey"",fg = ""black"")
        self.selectall.grid(row=4, column = 2,padx=10,pady=30,sticky=tk.W)
        
        self.daily = tk.Radiobutton(root, text=""Daily"",variable=self.CheckType,value=1,font = ""Helvetica 18 bold italic"",width=10,bg=""light grey"",anchor=tk.W)
        self.daily.grid(row=3, column = 3, pady=30,sticky=tk.W)
        
        self.monthly = tk.Radiobutton(root, text=""Monthly"",variable=self.CheckType,value=2,font = ""Helvetica 18 bold italic"",width=10,bg=""light grey"",anchor=tk.W)
        self.monthly.grid(row=4, column = 3, pady=30,sticky=tk.W)
        
        self.both = tk.Radiobutton(root, text=""Both"",variable=self.CheckType,value=3,font = ""Helvetica 18 bold italic"",width=10,bg=""light grey"",anchor=tk.W)
        self.both.grid(row=5, column = 3, pady=30,sticky=tk.W)
        
        tk.Button(root, text=""Build"",font = ""Helvetica 18 bold italic"",bg = ""#3333ff"",fg = ""white"",width=10,height=1,command=self.build_models).grid(row=7, column = 0, padx=100,pady=30,sticky=tk.W)
        tk.Button(root, text=""Forecast"",font = ""Helvetica 18 bold italic"",bg = ""#3333ff"",fg = ""white"",width=10,height=1,command=self.forecast_models).grid(row=7, column = 1, padx=50,pady=30,sticky=tk.W)
        tk.Button(root, text=""Clear"",font = ""Helvetica 18 bold italic"",bg = ""#3333ff"",fg = ""white"",width=10,height=1,command = self.clear).grid(row=7, column = 2, padx=50,pady=30,sticky=tk.W)
        tk.Button(root, text=""Exit"",font = ""Helvetica 18 bold italic"",bg = ""#3333ff"",fg = ""white"",width=10,height=1,command = self.close).grid(row=7, column = 3, padx=50,pady=30,sticky=tk.W)         

    def cb_check(self):
        if self.CheckSelectAll.get():
            self.advisorstate.config(state=tk.DISABLED)
            self.CheckAdvisor.set(0)
            self.shareholderstate.config(state=tk.DISABLED)
            self.CheckShareholder.set(0)
            self.retirementstate.config(state=tk.DISABLED)
            self.CheckRetirement.set(0)
            self.faststate.config(state=tk.DISABLED)
            self.CheckFast.set(0)
            self.vipstate.config(state=tk.DISABLED)
            self.CheckVIP.set(0)
            self.cgf.config(state=tk.DISABLED)
            self.CheckCGF.set(0)
            self.wt.config(state=tk.DISABLED)
            self.CheckWT.set(0)
            self.iobrp.config(state=tk.DISABLED)
            self.CheckIOBRP.set(0)
            self.ft.config(state=tk.DISABLED)
            self.CheckFT.set(0)
            self.njbest.config(state=tk.DISABLED)
            self.CheckNJBEST.set(0)
        else:
            self.advisorstate.config(state=tk.NORMAL)
            self.shareholderstate.config(state=tk.NORMAL)
            self.retirementstate.config(state=tk.NORMAL)
            self.faststate.config(state=tk.NORMAL)
            self.vipstate.config(state=tk.NORMAL)
            self.cgf.config(state=tk.NORMAL)
            self.wt.config(state=tk.NORMAL)
            self.iobrp.config(state=tk.NORMAL)
            self.ft.config(state=tk.NORMAL)
            self.njbest.config(state=tk.NORMAL)
    
    def close(self):
        root.destroy()
        
    def clear(self):
        #print(""in claer"")
        self.CheckAdvisor.set(0)
        self.CheckShareholder.set(0)
        self.CheckRetirement.set(0)
        self.CheckFast.set(0)
        self.CheckVIP.set(0)
        self.CheckCGF.set(0)
        self.CheckWT.set(0)
        self.CheckIOBRP.set(0)
        self.CheckFT.set(0)
        self.CheckNJBEST.set(0)
        self.CheckSelectAll.set(0)
        self.CheckType.set(0)
        self.input_file.delete('1.0', tk.END)
        self.advisorstate.config(state=tk.NORMAL)
        self.shareholderstate.config(state=tk.NORMAL)
        self.retirementstate.config(state=tk.NORMAL)
        self.faststate.config(state=tk.NORMAL)
        self.vipstate.config(state=tk.NORMAL)
        self.cgf.config(state=tk.NORMAL)
        self.wt.config(state=tk.NORMAL)
        self.iobrp.config(state=tk.NORMAL)
        self.ft.config(state=tk.NORMAL)
        self.njbest.config(state=tk.NORMAL)
        self.input_file.config(state=tk.NORMAL)

    def choose_input_file(self):
        self.filename = askopenfilename()
        if os.path.isfile(self.filename):
            self.input_file.configure(state=tk.NORMAL)
            self.input_file.insert(tk.INSERT,self.filename)
            self.input_file.configure(state=tk.DISABLED)
            fname = self.input_file.get(""1.0"", ""end-1c"")
            workbook = xlrd.open_workbook(fname,""w"")
            
            #print(self.input_file)
        else: 
            print(""No file chosen"")
            tkinter.messagebox.showinfo(title="""", message=""Please select input file"")
     
    def validation(self):
        if len(self.input_file.get(""1.0"", ""end-1c"")) == 0:
            tkinter.messagebox.showinfo(title="""", message=""Please select input file"")
        else:
            print(""file selected"")
            self.validate_chbox()
            
            
    def validate_chbox(self):
        if self.CheckAdvisor.get() == 0 and self.CheckShareholder.get() == 0 and self.CheckRetirement.get() == 0 and self.CheckFast.get() == 0 and self.CheckVIP.get() == 0 and self.CheckCGF.get() == 0 and self.CheckWT.get() == 0 and self.CheckIOBRP.get() == 0 and self.CheckFT.get() == 0 and self.CheckNJBEST.get() == 0 and self.CheckSelectAll.get() == 0:
            tkinter.messagebox.showinfo(title="""", message=""Please select the skill"")
        else:
            self.validate_radiobtn()
            
    def validate_radiobtn(self):
        if self.CheckType.get() == 0:
            tkinter.messagebox.showinfo(title="""", message=""Please select the forecast type"")
    
    def build_models(self):
        self.validation()
        self.build()
    
    def build(self):
        if self.CheckSelectAll.get() == 1:
            self.buildAllModels()
        else:
            self.build_advisor_selected()
    
    def buildAllModels(self):
        print(""in buildAllModels"")
    
    def forecast_models(self):
        self.validation()
        self.forecast()
        
    def forecast(self):
        if self.CheckSelectAll.get() == 1:
            self.forecastAllModels()
        else:
            self.forecast_advisor_selected() 
            
    def build_advisor_selected(self):
        if self.CheckAdvisor.get() == 1:
            if self.CheckType.get() == 1:
                print(""daily"")
                self.build_advisor_daily()
            elif self.CheckType.get() == 2:
                print(""monthly"")
                self.advisor_monthly()
            else:
                print(""both"")
                self.advisor_both()
    
    def forecast_advisor_selected(self):
        if self.CheckAdvisor.get() == 1:
            if self.CheckType.get() == 1:
                print(""daily"")
                self.forecast_advisor_daily()
            elif self.CheckType.get() == 2:
                print(""monthly"")
                self.forecast_advisor_monthly()
            else:
                print(""both"")
                self.forecast_advisor_both()
   
    def build_advisor_daily(self):
        required_data = self.read_advisor_daily()
        print(required_data)
        #global advisor_build 
        #if global advisor_build ==0:
        self.build_advisor_daily_model(required_data)
        #else:
            #print(""advisor model already built.Please forecast"")
            
    def read_advisor_daily(self):
        fname = self.input_file.get(""1.0"", ""end-1c"")
        workbook = xlrd.open_workbook(fname,""w"")
        #sheets = workbook.sheet_names()
        date_col = []
        required_data = []
        sh = workbook.sheet_by_name(""Daily_Data"")
        for rownum in range(1,sh.nrows):
            row_valaues = sh.row_values(rownum)
            date_col.append(datetime.datetime(*xlrd.xldate_as_tuple(row_valaues[0],workbook.datemode)))
            required_data.append(row_valaues[1])
    
        required_data = pd.DataFrame(required_data)
        required_data.replace('', np.nan, inplace=True)
        required_data.dropna(inplace=True)
        return required_data
    
    
    def build_advisor_daily_model(self,required_data):
        print(""in build_advisor_daily_model"")
        self.destroy()
        print(required_data)
        np.random.seed(12345) 
        tf.set_random_seed(12345)
        tkinter.messagebox.showinfo(title="""", message=""Advisor model building started"")
        print(type(required_data))
        data_set = required_data
        tkinter.messagebox.showinfo(title="""", message=""data set read"")
        model_data_set = data_set.iloc[:,:]
        model_data_set = model_data_set[model_data_set[0]>0]
        print(len(model_data_set))
        a1=len(model_data_set)
        print(type(model_data_set))
        tkinter.messagebox.showinfo(title="""", message=a1)
        modified_data = model_data_set
        modified_data = modified_data.values.reshape(-1,1)
        print(modified_data.shape)
        print(modified_data)
         
        global sc 
        sc = MinMaxScaler(feature_range=(0,1))
        train = sc.fit_transform(modified_data)
        print(train[0:5,:])
        print(train.shape)
        print(train.shape[0])
        print(train.shape[1])
        
        global y_train
        X_train = []
        y_train = []
        l = len(modified_data)
        print(l)
        for i in range(60,l):
            X_train.append(train[i-60:i,:])
            y_train.append(train[i,:])
        print(len(X_train))
        print(len(y_train))
        X_train = np.array(X_train)
        y_train = np.array(y_train)
        print(X_train.shape)
        #print(X_train)
        print(y_train.shape)
        #X_train = X_train.reshape(X_train.shape[1],1)
        print(X_train.shape[0])
        tkinter.messagebox.showinfo(title="""", message=""train data built"")
        #total 60 columns, lag=20
        global regressor
        regressor = Sequential()
        regressor.add(LSTM(units=60,return_sequences=True,input_shape=(X_train.shape[1],1)))
        regressor.add(Dropout(0.2))
        regressor.add(LSTM(units=60,return_sequences=True))
        regressor.add(Dropout(0.2))
        regressor.add(LSTM(units=60,return_sequences=True))
        regressor.add(Dropout(0.2))
        #regressor.add(LSTM(units=60,return_sequences=True))
        #regressor.add(Dropout(0.2))
        #regressor.add(LSTM(units=50,return_sequences=True))
        #regressor.add(Dropout(0.2))
        regressor.add(LSTM(units=60))
        regressor.add(Dropout(0.2))
        regressor.add(Dense(units=1))
        #rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.001)
        tkinter.messagebox.showinfo(title="""", message=""compiling model"")
        regressor.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])
        tkinter.messagebox.showinfo(title="""", message=""model compilation complete"")
        regressor.fit(X_train,y_train,epochs=300,batch_size=32,shuffle=False)
        global advisor_build
        advisor_build = 1
        print(y_train)
        tkinter.messagebox.showinfo(title="""", message=""Advisor model building completed"")
        #return (self.regressor,advisor_build,y_train)
    
    def forecast_advisor_daily(self):
        print(""in forecast"")
        global advisor_build
        print(advisor_build)
        global y_train
        global sc
        global regressor
        #advisor_build = 1
        if advisor_build == 1:
            y_test = y_train
            print(y_test)
            print(len(y_test))
            X_test_1 = []
            prediciton_list = []
            y_test = y_test.reshape(-1,1)
            print(y_test.shape)

            y_test = sc.transform(y_test)

            X_test_1.append(y_test[-60:])
            print(X_test_1)
            X_test_1 = np.array(X_test_1)
            print(X_test_1[0:5])
            print(X_test_1.shape)
            for x in range(20):

                y_pred_1 = regressor.predict(X_test_1)

                a = sc.inverse_transform(y_pred_1)
                b = a[0][0]
                X_test_1 = np.delete(X_test_1,0,axis=1)

                a = y_pred_1[0][0]

                X_test_1 = np.concatenate((X_test_1,np.zeros((1,1,1))),axis=1)
                X_test_1 = np.insert(X_test_1,59,a,axis=1)
                X_test_1 = np.delete(X_test_1,60,axis=1) 

                prediciton_list.append(b)

            print(prediciton_list)
            fname = self.input_file.get(""1.0"", ""end-1c"")
            workbook = xlrd.open_workbook(fname,""w"")
            date_col = []
            required_data = []
            sh = workbook.sheet_by_name(""Daily_Data"")
            for rownum in range(1,sh.nrows):
                row_valaues = sh.row_values(rownum)
                date_col.append(datetime.datetime(*xlrd.xldate_as_tuple(row_valaues[0],workbook.datemode)))
                required_data.append(row_valaues[1])
            required_data = pd.DataFrame(required_data)
            print(required_data)
            len1 = len(required_data)
            required_data.replace('', np.nan, inplace=True)
            z = required_data.last_valid_index()
            print(z)
            print(type(z))
            ind = z
            ind = ind + 1
            y = required_data.loc[required_data.last_valid_index()]
            print(y)
            required_data.dropna(inplace=True)
            print(required_data)
            date_col = pd.DataFrame(date_col)
            print(date_col)
            end_date = date_col.iloc[z][0]
            
            z =  z + 2
            y = z
            print(z)
            start_date = date_col.iloc[z][0]
 
            print(start_date.date())
 
 
            df2 = pd.DataFrame(prediciton_list)
            print(df2)
            df3 = []
            for i in range(ind,ind+20):
                df3.append(date_col.iloc[i][0])

            df3 = pd.DataFrame(df3,columns = [""Date""])
            print(df3)
      
 
            k = df3.loc[df3['Date'].isin(['2018-01-01','2018-01-15','2018-02-19','2018-03-30','2018-05-28','2018-07-04','2018-09-03','2018-11-23','2018-12-25'])]
            print(k)
            print(type(k))
            print(len(k))
            index_list = []
         
            if len(k) >= 1:
                for z in range(len(k)):
                    index_list.append(k.index[z])
                    print(index_list)
         
                for l in range(len(index_list)):
                    m = index_list[l]
                    df2.xs(m)[0] = 0
            else:
                print(""no holidays"")
            print(df2)
            book = load_workbook(fname)
            writer = pd.ExcelWriter(fname, engine='openpyxl') 
            writer.book = book
            writer.sheets = dict((ws.title, ws) for ws in book.worksheets)
            print(y)
            df2.to_excel(writer, ""Daily_Output"", startcol=1,startrow=y,header=None,index=False)
            
            writer.save()
            tkinter.messagebox.showinfo(title="""", message=""Advisor forecasting completed"")
            
        else:
            tkinter.messagebox.showinfo(title="""", message=""please build  first and then forecast"")
    def destroy(self):
        global advisor_build
        global regressor
        regressor = None
        global y_train
        y_train = None
        global sc
        sc = None
        advisor_build = 0

    
       
# =============================================================================
#     def mean_absolute_percentage_error(y_true, y_pred): 
#         perc = [0 if x ==0 else np.abs((x-y)/x) for x,y in zip(y_true,y_pred)]
#         return np.mean(perc[1]) * 100
# =============================================================================

root = tk.Tk()
#creation of an instance
#holiday_list = ['2018-01-01','2018-01-15','2018-02-19','2018-03-30','2018-05-28','2018-07-04','2018-09-03','2018-11-23','2018-12-25']
global advisor_build
global regressor
global y_train
global sc
app = Window(root)
#mainloop 
root.mainloop()        

Below is the error -
C:\Users\bpachkor\AppData\Local\Programs\Python\Python36>python setup.py build
Using TensorFlow backend.
running build
running build_exe
Traceback (most recent call last):
  File ""setup.py"", line 38, in <module>
    executables = executables)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\dist.py"", line 349, in setup
    distutils.core.setup(**attrs)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\c
ore.py"", line 148, in setup
    dist.run_commands()
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\d
ist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\d
ist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\c
ommand\build.py"", line 135, in run
    self.run_command(cmd_name)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\c
md.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\distutils\d
ist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\dist.py"", line 219, in run
    freezer.Freeze()
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\freezer.py"", line 616, in Freeze
    self.finder = self._GetModuleFinder()
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\freezer.py"", line 342, in _GetModuleFinder
    finder.IncludePackage(name)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\finder.py"", line 661, in IncludePackage
    self._ImportAllSubModules(module, deferredImports)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\finder.py"", line 288, in _ImportAllSubModules
    recursive)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\finder.py"", line 288, in _ImportAllSubModules
    recursive)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\finder.py"", line 288, in _ImportAllSubModules
    recursive)
  File ""C:\Users\bpachkor\AppData\Local\Programs\Python\Python36\lib\site-packag
es\cx_Freeze\finder.py"", line 283, in _ImportAllSubModules
    raise ImportError(""No module named %r"" % subModuleName)
ImportError: No module named 'tensorflow.contrib.ios_examples.benchmark.benchmar
k.xcodeproj'

I have tried doing same steps for python 3.5, but with no luck.
"
17511,android Semantic segmentation  ,"Hello,

can tensorflow for android be used for semantic segmentation?
Thank you

Lafi"
17510,Feature request: size of memory vectors in LSTM,"## Motivation

There are two different memory vectors used in LSTM cell: internal state vector (_**c**_ symbol in tensorflow convention) and output vector (_**h**_ in tensorflow convention). Let’s consider the possible sizes of both vectors:

- _**c**_ (internal state vector) – has to be the same size for both input and output of a single cell (equation 5 below)

- _**h**_ (output vector) – doesn’t have to be the same (1-4), to be more specific: input size could be arbitrary, but the output size has to be the same as _**c**_ (6)

![lstm_eq](https://user-images.githubusercontent.com/9368849/37099738-671646fa-2221-11e8-9de6-30c46bc36988.png)

The equations are copied from [a great paper](https://arxiv.org/pdf/1506.00019.pdf) of Lipton, Berkowitz and Elkan and again the notation is different: _**s**_=_**c**_).

Why setting different input and output size of _**h**_ vector is it useful? If ‘because we can’ doesn’t satisfy you I can cite [OrderMatters paper](https://arxiv.org/pdf/1511.06391.pdf) (their notation is a bit different _**h**_=_**q**_).

## Current implementation

Currently, in different LSTM cells constructors, there is only one parameter responsible for setting the size of both _**c**_ and _**h**_ vectors and it’s called _**num_units**_. IMO it’s not the best name for this param, but that’s not my point here – there is no way to set different input and output size of _**h**_ vector.

## Proposal

We could leave the _**num_units**_ param for backward compatibility with an integer value and the current behavior. On the other hand, if a user pass a tuple of length 2, we could interpret the param as : (input_size of h, output_size of h). Those two numbers are sufficient for describing both _**c**_ and _**h**_ sizes.

## Implementation

I have already implemented proposed modification for LSTMCell class and it works fine. I could implement the proposed feature for all LSTM-like cells and create PR, but first wanted to ask the community and tensorflowers if it useful form your perspective?

## System information
- **Have I written custom code**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
"
17509,Estimators Importance sampling,"Seems that [Keras/Tensorflow](http://idiap.ch/~katharas/importance-sampling/) has some form of external importance sampling support.
Do you have a plan to add [a solution](https://arxiv.org/abs/1803.00942v1) for Estimators?"
17507,Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on,"The error occurs when I have trained on one system and used `tf.Saver` to save a model and then I copied the saved model on  a second system and used the same code to restore the model. However, the absolute paths of the directories in which the code is executed are different. Hence I got this error:
```
2018-03-07 12:08:17.897739: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-07 12:08:41.813649: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813663: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813680: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813700: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813724: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813769: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813784: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813802: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
2018-03-07 12:08:41.813819: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
Traceback (most recent call last):
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1668, in <module>
    main()
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1662, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1072, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/opt/pycharm-2017.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/alex/work/python/bayes-test/bin/train.py"", line 165, in <module>
    main(**vars(parser.parse_args()))
  File ""/home/alex/work/python/bayes-test/bin/train.py"", line 97, in main
    tu.load_parameters(saver, sess, data_folder)
  File ""/home/alex/work/python/tensorflow_utils/tensorflow_utils/printing.py"", line 13, in load_parameters
    saver.restore(session, ckpt.model_checkpoint_path)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

Caused by op 'save/RestoreV2_1', defined at:
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1668, in <module>
    main()
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1662, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/opt/pycharm-2017.3.3/helpers/pydev/pydevd.py"", line 1072, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/opt/pycharm-2017.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/alex/work/python/bayes-test/bin/train.py"", line 165, in <module>
    main(**vars(parser.parse_args()))
  File ""/home/alex/work/python/bayes-test/bin/train.py"", line 87, in main
    saver = tf.train.Saver(params)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1218, in __init__
    self.build()
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1227, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1263, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 751, in _build_internal
    restore_sequentially, reshape)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 427, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 267, in restore_op
    [spec.tensor.dtype])[0])
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1021, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/opt/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen/model.ckpt: Not found: /mnt/workspace/work/python/bayes-test/bin//results/not_centered/gen; No such file or directory
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
```
Isn't the saver suppose to save a somewhat system agnostic version of the tensors? The fact that there is hardcoded absolute paths make me very suspicous?"
17506,TfLiteCameraDemo crashed on Android 8.1 with call setUseNNAPI(true),"Hi , 
I follow steps in : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite to build tensoflowlite from sources.

trying run output demo apk on Google Pixel 2   create crash :

```
03-07 06:09:44.739 22823-22837/? I/Adreno: ESXAPILOG: API logging initialized: Name = com.example.android.tflitecamerademo, PID = 22823
03-07 06:09:44.744 22823-22823/? W/linker: /data/app/com.example.android.tflitecamerademo-qSwQ-3Ztm2MtPAw8Lu3x4A==/lib/arm64/libtensorflowlite_jni.so: is missing DT_SONAME will use basename as a replacement: ""libtensorflowlite_jni.so""
03-07 06:09:44.744 22823-22823/? W/System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol ""_ZN6tflite14getCurrentTimeEv"" referenced by ""/data/app/com.example.android.tflitecamerademo-qSwQ-3Ztm2MtPAw8Lu3x4A==/lib/arm64/libtensorflowlite_jni.so""...
03-07 06:09:44.745 22823-22823/? W/linker: /data/app/com.example.android.tflitecamerademo-qSwQ-3Ztm2MtPAw8Lu3x4A==/lib/arm64/libtensorflowlite_jni.so: is missing DT_SONAME will use basename as a replacement: ""libtensorflowlite_jni.so""
03-07 06:09:44.745 22823-22823/? W/System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol ""_ZN6tflite14getCurrentTimeEv"" referenced by ""/data/app/com.example.android.tflitecamerademo-qSwQ-3Ztm2MtPAw8Lu3x4A==/lib/arm64/libtensorflowlite_jni.so""...
03-07 06:09:44.745 22823-22823/? E/zygote64: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
03-07 06:09:44.745 22823-22823/? D/AndroidRuntime: Shutting down VM
03-07 06:09:44.746 22823-22823/? E/AndroidRuntime: FATAL EXCEPTION: main
                                                   Process: com.example.android.tflitecamerademo, PID: 22823
                                                   java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
                                                       at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)
                                                       at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:48)
                                                       at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:77)
                                                       at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:87)
                                                       at com.example.android.tflitecamerademo.ImageClassifierFloatIntel.<init>(ImageClassifierFloatIntel.java:50)
                                                       at com.example.android.tflitecamerademo.CameraActivity.testImage(CameraActivity.java:71)
                                                       at com.example.android.tflitecamerademo.CameraActivity.onCreate(CameraActivity.java:36)
                                                       at android.app.Activity.performCreate(Activity.java:6999)
                                                       at android.app.Activity.performCreate(Activity.java:6990)
                                                       at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)
                                                       at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2731)
                                                       at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2856)
                                                       at android.app.ActivityThread.-wrap11(Unknown Source:0)
                                                       at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1589)
                                                       at android.os.Handler.dispatchMessage(Handler.java:106)
                                                       at android.os.Looper.loop(Looper.java:164)
                                                       at android.app.ActivityThread.main(ActivityThread.java:6494)
                                                       at java.lang.reflect.Method.invoke(Native Method)
                                                       at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:438)
                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:807)
```

Build command :  bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a

Tensorflow WORKSPACE file changes:

```
# Uncomment and update the paths in these entries to build the Android demo.
android_sdk_repository(
    name = ""androidsdk"",
   api_level = 27,
   # Ensure that you have the build_tools_version below installed in the
    # SDK manager as it updates periodically.
    build_tools_version = ""27.0.3"",
    # Replace with path to Android SDK on your system
    path = ""/home/XXX/Android/Sdk/"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/dnozik/Code/Tensorflow/android-ndk-r14b/"",
    # This needs to be 14 or higher to compile TensorFlow.
#    # Please specify API level to >= 21 to build for 64-bit
#    # archtectures or the Android NDK will automatically select biggest
#    # API level that it supports without notice.
#    # Note that the NDK version is not the API level.
    api_level=27)
```


Symbols:
XXX@XXX-VirtualBox:~/Code$ nm -S libtensorflowlite_jni.so 
nm: libtensorflowlite_jni.so: no symbols

Thanks for help.

"
17504,Unable to build tensorflow 1.5 with mpi on AWS ec2 machine,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos (4.9.77-31.58.amzn1.x86_64)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.5.5
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GRID K520, 4GB (aws g2 instance)
- **Exact command to reproduce**:

### Describe the problem

I have installed mpi using the following steps -
```
wget https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.0.0.tar.gz

tar -xvf openmpi-3.0.0.tar.gz

cd openmpi-3.0.0

./configure --disable-mpi-fortran --with-cuda=/usr/local/cuda-9.0/ --prefix /usr/local/

make

make install
```

After this, I am trying to build tensorflow from source - 

```
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/

export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_NEED_S3=1
export TF_ENABLE_XLA=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export TF_NEED_CUDA=1
export TF_CUDNN_VERSION=7.0
export CUDA_TOOLKIT_PATH=/usr/local/cuda
export CUDNN_INSTALL_PATH=/usr/local/cuda
export TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2
export TF_CUDA_CLANG=0
export TF_NEED_MPI=0
export MPI_HOME=/usr/local/mpi
export PATH=$PATH:/usr/local/mpi/include

```
I have included mpi home directory in PATH and LD_LIBRARY_PATH

While running

 ```bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package ```

I get the following error  -

```
ERROR: /home/ec2-user/tensorflow/tensorflow/contrib/mpi/BUILD:48:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_utils' failed (Exit 1)
In file included from ./tensorflow/contrib/mpi/mpi_utils.h:27:0,
                 from tensorflow/contrib/mpi/mpi_utils.cc:18:
./third_party/mpi/mpi.h:2673:41: fatal error: openmpi/ompi/mpi/cxx/mpicxx.h: No such file or directory
 #include ""openmpi/ompi/mpi/cxx/mpicxx.h""
                                         ^
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build

```

What am I missing here? Any help will be appreciated."
17502,Tensorflow failed when build with MSVC + /permissive-,"System information
•	Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
N/A 
•	OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows server 2016
•	TensorFlow installed from (source or binary):
Source
•	TensorFlow version (use command below):
Master branch latest revison
•	Python version:
Anaconda 4.1.1 (Python 3.5 64-bit) 
•	Bazel version (if compiling from source):
N/A
•	GCC/Compiler version (if compiling from source):
VS2017 15.5.7
•	CUDA/cuDNN version:
NVidia CUDA Toolkit 8.0
NVidia CUDNN 5.1
•	GPU model and memory:
N/A
•	Exact command to reproduce:
N/A

Describe the problem
Tensorflow failed when build with /permissive- by using msvc on windows. This should be tensorflow source issue, could you help fix it?

The failures like:
D:\Tensorflow\src\tensorflow/core/util/memmapped_file_system.h(61): error C2440: 'initializing': cannot convert from 'const char [21]' to 'char *const '
D:\Tensorflow\src\tensorflow/core/util/memmapped_file_system.h(61): note: Conversion from string literal loses const qualifier (see /Zc:strictStrings)
D:\Tensorflow\src\tensorflow/core/util/memmapped_file_system.h(69): error C2440: 'initializing': cannot convert from 'const char [22]' to 'char *const '
D:\Tensorflow\src\tensorflow/core/util/memmapped_file_system.h(69): note: Conversion from string literal loses const qualifier (see /Zc:strictStrings)
D:\Tensorflow\src\tensorflow\core\util\memmapped_file_system.cc(182): error C2737: 'tensorflow::MemmappedFileSystem::kMemmappedPackagePrefix': 'constexpr' object must be initialized
D:\Tensorflow\src\tensorflow\core\util\memmapped_file_system.cc(182): error C2734: 'tensorflow::MemmappedFileSystem::kMemmappedPackagePrefix': 'const' object must be initialized if not 'extern'
D:\Tensorflow\src\tensorflow\core\util\memmapped_file_system.cc(183): error C2737: 'tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef': 'constexpr' object must be initialized
D:\Tensorflow\src\tensorflow\core\util\memmapped_file_system.cc(183): error C2734: 'tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef': 'const' object must be initialized if not 'extern'

Repro steps:
1.	git clone  https://github.com/tensorflow/tensorflow D:\Tensorflow\src
2.	pushd D:\Tensorflow
3.	set PreferredToolArchitecture=x64
4.	set rel=Release
5.	set CUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\cuda""
6.	set PY=C:\ProgramData\Anaconda3
7.	set _CL_=/FS /permissive-
8.	cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON
9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.16299.0 tensorflow.sln /t:Rebuild
"
17501,convert result mismatch between command-line  and python_api,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary: by ""pip3 install --upgrade tensorflow-gpu""
- **TensorFlow version (use command below)**:
tensorflow-gpu (1.6.0)
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7
- **GPU model and memory**: Null
- **Exact command to reproduce**:

### Describe the problem
 I save my pre-trained model into .pb file according to [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) and [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py)
Once I covert to .tflite by command-line with following cmd: 
> ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=RD_Net_TransConv_fuse.pb \
--output_file=./RD_Net_TransConv_fuse.tflite \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=FLOAT \
--input_data_types =FLOAT \
--input_arrays=Placeholder \
--output_arrays=seq_4/Conv/BiasAdd \
--input_shapes=1,240,320,3 \
--logtostderr

the error show that ""TransposeConv"" is not supported : 
> 2018-03-07 15:07:49.317477: F tensorflow/contrib/lite/toco/tflite/export.cc:304] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: TransposeConv.

If I use [python_api](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md) to convert my tensor into .tflite, it work fine without any error msg.
But the .tflite can not be used in android....  it will crash when  I want to getOutputName or run

error msg in android:
> Can't get Output Names, model error:Invalid handle to Interpreter.

does tensorflow lite support ""TransposeConv""? I don't see any information in [Compatibility Guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md)

Any suggestion? Thanks~~~




"
17500,'colocate_gradients_with_ops' colocate with unused ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:v1.6.0-0-gd2e24b6039 1.6.0 
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: P100
- **Exact command to reproduce**: here

```python
import tensorflow as tf
import time

G = tf.get_default_graph()

@tf.RegisterGradient(""SummaryGrad"")
def summarize_grad_fn(op, x):
    my_name_is_not_x = tf.Print(x, [x])
    return x

def layer(x):
    l = tf.layers.conv2d(x, 64, 3, data_format='channels_first')
    l = tf.nn.relu(l)
    with G.gradient_override_map({""Identity"": ""SummaryGrad""}):
        return tf.identity(l)

img = tf.random_normal((64,3, 224,224))
l = layer(img)
l = layer(l)
l = layer(l)

loss = tf.reduce_mean(l)

opt = tf.train.GradientDescentOptimizer(0.1)
grads = opt.compute_gradients(loss, colocate_gradients_with_ops=True)
op = opt.apply_gradients(grads)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for k in range(20):
        start = time.time()
        sess.run(op)
        if k > 2:
            print(time.time() - start)
```

If I use `colocate_gradients_with_ops=False`, or remove the `tf.Print` line, the above code runs with expected speed.
Otherwise, it places the gradients on CPUs, resulting in 2x slow down, and a lot of H2D/D2H copy shown in profiling.
Perhaps this colocate option can be made smarter.

In this case, Print op depends on some gradients, but no other gradients depend on the Print op. i.e., the Print op __does not appear on the subgraph which `grads` depends on__. Therefore the colocation seems totally unnecessary here."
17499,Problem with the tensorflow installation,"I'm having problems installing the cpu-only version of tensorflow, for this I use pip3, my OS is w10, I tried it with python 3.6.4 and 3.5.2 and in no case did it work, here is the error that generates me, besides the response to the script whose answer I do not understand, tells me that I do not have the dll's and all those things, but I installed was the version of only CPU, are these files required? ah and yes, I installed it with administrator permissions both the python and the tensorflow. . .

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#5>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


ERROR: Failed to import the TensorFlow module.

WARNING! This script is no longer maintained! 
=============================================
Since TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,
and any missing DLLs will be reported when you execute the `import tensorflow`
statement. The error messages printed below refer to TensorFlow 1.3 and earlier,
and are inaccurate for later versions of TensorFlow.

- Python version is 3.6.

- TensorFlow is installed at: C:\Program Files\Python36\lib\site-packages\tensorflow

- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Download and install CUDA 8.0 from
  this URL: https://developer.nvidia.com/cuda-toolkit

- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that
  this DLL be installed in a directory that is named in your %PATH%
  environment variable. Typically it is installed in 'C:\Windows\System32'.
  If it is not present, ensure that you have a CUDA-capable GPU with the
  correct driver installed.

- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Note that installing cuDNN is a
  separate step from installing CUDA, and it is often found in a
  different directory from the CUDA DLLs. You may install the
  necessary DLL by downloading cuDNN 5.1 from this URL:
  https://developer.nvidia.com/cudnn

- Could not find cuDNN.
"
17498,the new parameter in `print_tensors_in_checkpoint_file` breaks old code,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039 1.6.0
- **Python version**: 3.6

### Describe the problem

The function `print_tensors_in_checkpoint_file` in `tensorflow/python/tools/inspect_checkpoint.py` is changed in [this commit](https://github.com/tensorflow/tensorflow/commit/2ba34173fad0d5b7d986baeb8171bdc6afdcd7bb#diff-fb7984719b22f01a7748ef847e9eb731), a new parameter `all_tensor_names` is added.

This change breaks the old code using this function, including the examples in the [Programmer's Guide](https://www.tensorflow.org/versions/master/programmers_guide/saved_model#inspect_variables_in_a_checkpoint).

I believe an elegant solution is setting the default value of the new parameter `all_tensor_names` to `False`

"
17497,"use tf.concat, get error about ExpandDims, Prod, Slice not supported when use toco to convert the model to .tflite format","detail is describe in:
https://github.com/tensorflow/tensorflow/issues/17461"
17494,TF1.6 MKL bug in concatenation op,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: Binary (Intel MKL wheel) https://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available
- **TensorFlow version (use command below)**: b'unknown' 1.6.0
- **Python version**: 3.6 
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
U-Net model graph compiles but error during training from concatenation op.
The code is here: https://github.com/mas-dse-greina/unet/blob/master/train.py


You can collect some of this information using our environment capture script:
== cat /etc/issue ===============================================
Linux lancelot24 3.10.0-693.11.6.el7.x86_64 #1 SMP Thu Jan 4 01:06:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux lancelot24 3.10.0-693.11.6.el7.x86_64 #1 SMP Thu Jan 4 01:06:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.1)
protobuf (3.5.1)
tensorflow (1.6.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0
tf.GIT_VERSION = v1.6.0-0-gd2e24b6039
tf.COMPILER_VERSION = v1.6.0-0-gd2e24b6039
Sanity check: array([1], dtype=int32)
/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./p.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is a bug in the TensorFlow 1.6. I am using the Intel-supplied pip wheel for MKL optimization. I ran the same code with the TF1.4 MKL wheel and my code worked just fine. The error messages indicate that there is a problem with the concatenation operation. I am not seeing this error with the generic TensorFlow 1.6 (i.e. pip install tensorflow). I believe it is limited to the MKL op for concatenation.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.
Train on 24800 samples, validate on 6200 samples
Epoch 1/10
Traceback (most recent call last):
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:777
	 [[Node: concatenate/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transConv6/BiasAdd, conv4b/Relu, concatenate/concat/axis, DMT/_29, conv4b/Relu:1, DMT/_30)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 571, in <module>
    settings.OUT_CHANNEL_NO, settings.MODEL_FN, settings.MODE, args)
  File ""train.py"", line 510, in train_and_predict
    callbacks=[model_checkpoint, tensorboard_checkpoint])
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1793, in fit
    validation_steps=validation_steps)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1283, in _fit_loop
    outs = f(ins_batch)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 2663, in __call__
    fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:777
	 [[Node: concatenate/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transConv6/BiasAdd, conv4b/Relu, concatenate/concat/axis, DMT/_29, conv4b/Relu:1, DMT/_30)]]

Caused by op 'concatenate/concat', defined at:
  File ""train.py"", line 571, in <module>
    settings.OUT_CHANNEL_NO, settings.MODEL_FN, settings.MODE, args)
  File ""train.py"", line 443, in train_and_predict
    model = model5_MultiLayer(args, False, False, img_rows, img_cols, input_no, output_no, print_summary=True)
  File ""train.py"", line 183, in model5_MultiLayer
    kernel_size=(3, 3), strides=(2, 2), padding='same')(conv5), conv4], axis=concat_axis)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py"", line 665, in concatenate
    return Concatenate(axis=axis, **kwargs)(inputs)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 258, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py"", line 174, in call
    return self._merge_function(inputs)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/merge.py"", line 380, in _merge_function
    return K.concatenate(inputs, axis=self.axis)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 2083, in concatenate
    return array_ops.concat([to_dense(x) for x in tensors], axis)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1175, in concat
    return gen_array_ops._concat_v2(values=values, axis=axis, name=name)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 625, in _concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/bduser/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not create a concat primitive descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:777
	 [[Node: concatenate/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transConv6/BiasAdd, conv4b/Relu, concatenate/concat/axis, DMT/_29, conv4b/Relu:1, DMT/_30)]]
"
17493,tensorflow macOS build failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
N/A

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.13

- **TensorFlow installed from (source or binary)**:
build from source branch r1.6

- **TensorFlow version (use command below)**:
1.6

- **Python version**: 
3.6

- **Bazel version (if compiling from source)**:
0.11.0-homebrew

- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)

- **CUDA/cuDNN version**:
9.1/7.0.5

- **GPU model and memory**:
Titan X Pascal

- **Exact command to reproduce**:
`bazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`


### Describe the problem
Trying to build from source, but failed with the error below

### Source code / logs
```
INFO: From Compiling tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/Half.h(508): error: explicit specialization of class ""std::__1::numeric_limits<Eigen::half>"" must precede its first use (
(388): here)

1 error detected in the compilation of ""/var/folders/4z/2frllwtj73b9vg25g4m9sxsc0000gn/T//tmpxft_00002f28_00000000-6_gru_ops_gpu.cu.cpp1.ii"".
ERROR: /Users/odin/local/tensorflow/tensorflow/contrib/rnn/BUILD:240:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_gru_ops_gpu/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.pic.o' was not created
ERROR: /Users/odin/local/tensorflow/tensorflow/contrib/rnn/BUILD:240:1: not all outputs were created or valid
```
"
17488,Feature request: tf.contrib.Predictor - add an optional ConfigProto argument to the constructor,"Have I written custom code: -
OS Platform and Distribution: -
TensorFlow installed from: -
TensorFlow version: -
Bazel version: -
CUDA/cuDNN version: -
GPU model and memory: -
Exact command to reproduce: -

The `tf.contrib.Predictor` subclasses are useful to quickly run inferences, but at the moment it's impossible to configure the `tf.Session` instance used for inference, because it is created in `__init__` without the possibility to pass a `config` to it.

My suggestion is then to add the explicit argument `config` to `Predictor`'s subclasses' init functions and pass it to the Session constructor.

Note that, technically, for the `{Contrib|Core}EstimatorPredictor` classes one could already have a `session_config` defined in `estimator.config.session_config`, so perhaps this could be the fallback value when using these classes."
17487,SparseTensor,"Hi, i want to have a prediction with continuous and categorial features.
i faced a problem with the last line with SparseTensor

i used Tensorflow version : 1.5.0 installed with anaconda
os : windows 7
conda version : 4.4.10
conda-build version : 3.0.27
python version : 3.6.3.final.0
 

there is my code : 

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf

import itertools
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import warnings
from sklearn.ensemble import IsolationForest
warnings.filterwarnings('ignore')



# categorial with continuous
train = pd.read_csv('Dataset3.csv', delimiter=';')
train.drop('Matricule',axis = 1, inplace = True)
train_numerical = train.select_dtypes(exclude=['object'])
train_numerical.fillna(0,inplace = True)
train_categoric = train.select_dtypes(include=['object'])
train_categoric.fillna('NONE',inplace = True)
train = train_numerical.merge(train_categoric, left_index = True, right_index = True)


test = pd.read_csv('Dataset3_test.csv', delimiter=';')
Matricule = test.Matricule
test.drop('Matricule',axis = 1, inplace = True)
test_numerical = test.select_dtypes(exclude=['object'])
test_numerical.fillna(0,inplace = True)
test_categoric = test.select_dtypes(include=['object'])
test_categoric.fillna('NONE',inplace = True)
test = test_numerical.merge(test_categoric, left_index = True, right_index = True)


clf = IsolationForest( random_state = 42)
clf.fit(train_numerical)
y_noano = clf.predict(train_numerical)
y_noano = pd.DataFrame(y_noano, columns = ['Top'])
y_noano[y_noano['Top'] == 1].index.values

train_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]
train_numerical.reset_index(drop = True, inplace = True)

train_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]
train_categoric.reset_index(drop = True, inplace = True)

train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]
train.reset_index(drop = True, inplace = True)

col_train = list(train_numerical.columns)
col_train_bis = list(train_numerical.columns)

col_train_cat = list(train_categoric.columns)

col_train_bis.remove('DEM')

mat_train = np.matrix(train_numerical)
mat_test  = np.matrix(test_numerical)
mat_new = np.matrix(train_numerical.drop('DEM',axis = 1))
mat_y = np.array(train.DEM)

prepro_y = MinMaxScaler()
prepro_y.fit(mat_y.reshape(len(mat_y),1))

prepro = MinMaxScaler()
prepro.fit(mat_train)

prepro_test = MinMaxScaler()
prepro_test.fit(mat_new)

train_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train)
test_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)

train[col_train] = pd.DataFrame(prepro.transform(mat_train),columns = col_train)
test[col_train_bis]  = test_num_scale

# List of features
COLUMNS = col_train
FEATURES = col_train_bis
LABEL = ""SalePrice""

FEATURES_CAT = col_train_cat
print (FEATURES_CAT)

engineered_features = []


for continuous_feature in FEATURES:
    engineered_features.append(
        tf.contrib.layers.real_valued_column(continuous_feature))

for categorical_feature in FEATURES_CAT:
    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(
        categorical_feature, hash_bucket_size=1000)
    engineered_features.append(
        tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16, combiner=""sum""))
print (engineered_features)

# Training set and Prediction set with the features to predict
training_set = train[FEATURES + FEATURES_CAT]
prediction_set = train.DEM
print (training_set)

# Train and Test
x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES + FEATURES_CAT] ,
                                                    prediction_set, test_size=0.33, random_state=42)
y_train = pd.DataFrame(y_train, columns = [LABEL])
training_set = pd.DataFrame(x_train, columns = FEATURES + FEATURES_CAT).merge(y_train, left_index = True, right_index = True)

# Training for submission
training_sub = training_set[FEATURES + FEATURES_CAT]
testing_sub = test[FEATURES + FEATURES_CAT]

# Same thing but for the test set
y_test = pd.DataFrame(y_test, columns = [LABEL])
testing_set = pd.DataFrame(x_test, columns = FEATURES + FEATURES_CAT).merge(y_test, left_index = True, right_index = True)


testing_set[FEATURES_CAT] = testing_set[FEATURES_CAT].applymap(str)

print (training_set[FEATURES_CAT])


def input_fn_new(data_set, training=True):
    continuous_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}

    categorical_cols = {k: tf.SparseTensor(
        indices=[[i, 0] for i in range(data_set[k].size)], values=data_set[k].values, dense_shape=[data_set[k].size, 1])
    for k in FEATURES_CAT}

    # Merges the two dictionaries into one.
    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))
    print(feature_cols)
    if training == True:
        # Converts the label column into a constant Tensor.
        label = tf.constant(data_set[LABEL].values)

        # Returns the feature columns and the label.
        return feature_cols, label

    return feature_cols

# Model
regressor = tf.contrib.learn.DNNRegressor(feature_columns = engineered_features,
                                          activation_fn = tf.nn.relu, hidden_units=[200, 100, 50, 25, 12])


training_set[FEATURES_CAT] = training_set[FEATURES_CAT].applymap(str)
categorical_cols = {k: tf.SparseTensor(
      indices=[[i, 0] for i in range(training_set[k].size)],
      values=training_set[k].values,
    dense_shape=[training_set[k].size, 1])
                      for k in FEATURES_CAT}

# Error: 

WARNING:tensorflow:Using temporary folder as model directory: C:\Users\S9E55~1.GHO\AppData\Local\Temp\tmpl279_r04
Traceback (most recent call last):
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 576, in merge_with
    self.assert_same_rank(other)
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 621, in assert_same_rank
    other))
ValueError: Shapes (0,) and (?, ?) must have the same rank

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 651, in with_rank
    return self.merge_with(unknown_shape(ndims=rank))
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 582, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (0,) and (?, ?) are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""prediction.py"", line 156, in <module>
    for k in FEATURES_CAT}
  File ""prediction.py"", line 156, in <dictcomp>
    for k in FEATURES_CAT}
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\sparse_tensor.py"", line 131, in __init__
    indices_shape = indices.get_shape().with_rank(2)
  File ""C:\Users\s.ghorbel\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 653, in with_rank
    raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape (0,) must have rank 2



"
17485,tf.contrib.image.transform crashes under Windows when CUDA is enabled,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. A minimal example reproducing the bug is provided below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 (x64)
- **TensorFlow installed from (source or binary)**: 
Binary installed using `pip install tensorflow-gpu`
- **TensorFlow version (use command below)**: 
b'unknown' 1.6.0 (also tested on b'unknown' 1.4.0)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  
  CUDA 9.0.176 / cuDNN 7.0.5
  CUDA 8.0.61 / cuDNN 6.14.11
- **GPU model and memory**:
(device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2, memory: 3.00GiB)
(device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0, memory: 4.00GiB)
- **Exact command to reproduce**: python example.py (see below)


### Describe the problem
The function tf.contrib.image.transform crashes when CUDA is enabled under Windows. It produces the following errors: `CUDA_ERROR_ILLEGAL_INSTRUCTION` on tensorflow 1.6 or `CUDA_ERROR_LAUNCH_FAILED` on tensorflow 1.4.
However, it functions correctly when CUDA is disabled (by setting _CUDA_VISIBLE_DEVICES_ to -1). 

I tested a variation of different parameters (such as varying the batch size, image sizes, and number of channels), but the behavior stays the same. In addition I reproduced the same error on a different machine with an older tensorflow version.

### Source code / logs
- **Code**:
```
import numpy as np
import tensorflow as tf

batch_size, image_size, channels = 1, 32, 1

data = np.zeros(
    shape=(batch_size, image_size, image_size, channels), 
    dtype=np.float32)

data_node = tf.placeholder(
    shape=(batch_size, image_size, image_size, channels),
    dtype=tf.float32)

identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)
transform = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])
data_node_transformed = tf.contrib.image.transform(data_node, transform)

data_t = tf.Session().run([data_node_transformed], feed_dict={data_node: data})
```
- **Console Output**:

  Output with tensorflow 1.6 (with GeForce GTX 970M,) :

```
2018-03-06 15:42:21.578078: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1212] Found device 0 with properties:
name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
pciBusID: 0000:01:00.0
totalMemory: 3.00GiB freeMemory: 2.48GiB
2018-03-06 15:42:21.578310: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
2018-03-06 15:42:21.890290: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2192 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
2018-03-06 15:42:22.101031: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018-03-06 15:42:22.101032: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_INSTRUCTION ::
2018-03-06 15:42:22.101185: F C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc:203] Unexpected Event status: 1
```

  Output with tensorflow 1.4 (with Quadro M1200):

```
2018-03-06 15:31:17.800588: I C:\tf_jenkins\home\workspace\rel‑win\M\windows‑gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties: 
name: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2018-03-06 15:31:17.803815: I C:\tf_jenkins\home\workspace\rel‑win\M\windows‑gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) ‑> (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)
2018-03-06 15:31:18.731252: E C:\tf_jenkins\home\workspace\rel‑win\M\windows‑gpu\PY\36\tensorflow\stream_executor\cuda\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available
2018-03-06 15:31:18.731258: E C:\tf_jenkins\home\workspace\rel‑win\M\windows‑gpu\PY\36\tensorflow\stream_executor\cuda\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
2018-03-06 15:31:18.733458: F C:\tf_jenkins\home\workspace\rel‑win\M\windows‑gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_event_mgr.cc:203] Unexpected Event status: 1
```"
17484,contrib.boosted_trees.estimator_batch modules are not available in 1.6.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS Sierra
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.6.0
- **Python version**:2.7
- **Bazel version (if compiling from source)**:0.11.0
- **CUDA/cuDNN version**:not relevant
- **GPU model and memory**:not relevant
- **Exact command to reproduce**:

trying to run the example in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/examples/mnist.py

but get an error:
ModuleNotFoundError: No module named 'tensorflow.contrib.boosted_trees.estimator_batch.estimator'

tried to install the latest version with pip and from source on python2.7 and python3.6 but still end up with the same error.

this is the content of the boosted_trees/estimator_batch folder in my installation:

custom_export_strategy.pyc
dnn_tree_combined_estimator.py
__init__.py
dnn_tree_combined_estimator.pyc
__init__.pyc
trainer_hooks.py
custom_export_strategy.py
trainer_hooks.pyc

"
17483,Less information for debugging sess.run() when fitting an image into a tensor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**: anaconda 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61
- **GPU model and memory**:  Quadro K4000 
- **Exact command to reproduce**:  act = sess.run(op_tensor, feed_dict = feed_dict)

### Describe the problem
So I am visualizing the activation map by fitting an example image into each convnet layer using **sess.run(op_tensor, feed_dict = feed_dic)**, 
where:
 **op_tensor: Tensor(""conv2_1/Relu:0"", shape=(?, 112, 112, 64), dtype=float32)**;
**feed_dic: {<tf.Tensor 'Placeholder:0' shape=(?, 224, 224, 3) dtype=float32>: array([[[[ 77.,  84.,  76.],
         .,  74.],
         ...,
         [138., 122., 113.],
         [136., 122., 115.],
         [128., 118., 112.]]]], dtype=float32)}**

However, I got the error here: 

`InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_2' with dtype float`, I did not where **Placeholder:2** comes from. So I debug into this function, I fund the error is from here: 
**return fn(*args)** from **tensorflow/python/client/session.py**, 

Here are the parameters for args:

** type 'tuple': (<Swig Object of type 'TF_DeprecatedSession *' at 0x7f7dd98a9270>, {'Placeholder:0': array([[[[ 77.,  84.,  76.],
         [ 77.,  84.,  76.],
         [ 75.,  84.,  76.],
         .......
         [128., 118., 112.]]]], dtype=float32)}, ['conv2_1/Relu:0'], [], None, None** 

More precisely, the error is caused by this:
**if c_api.TF_GetCode(self.status.status) != 0:**, where **c_api.TF_GetCode(self.status.status)** return 3.

I did whatever I can, but I can not understand your source code with too much documentation. 

For the input of **sess.run()**, I did not see any inappropriate settting, Can you help me clarify this issue? I was stuck here for several days...

BTW, I saved my CNN (**Conv1_1+relu+conv1_2+relu+pool1+conv2_1+relu+conv2_2+relu+pool2+fc1**) model with **tf.train.Saver()** , here is the files:

```
--checkpoint
--tmp-model.data-00000-of-00001
--tmp-model.index
--tmp-model.meta
```

Thanks in advance

Hao


"
17482,"The ""multiply"" operator of type Dimension has a bug.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS**: Win10 64bit 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1080Ti
- **Exact command to reproduce**: See codes below

### Describe the problem
Suppose `A` is a tensor, `L = A.shape[0]`. I found that `L*3` works while `3*L` lead to a **TypeError**. Can someone fix this?

### Source code / logs
```
import tensorflow as tf
A = tf.placeholder( tf.float32, shape = [3, 4, 5] )
L = A.shape[0]
print( type(L) )
print( L*3 )
print( 3*L )
```
Output is:
![image](https://user-images.githubusercontent.com/8580553/37040371-a1b8e12e-2194-11e8-8d23-b317458b2382.png)
"
17479,compile tensorflow1.6.0 source failed,"Error Occured:
bazel-out/host/bin/external/swig/swig -c++ -python -module tensorflow_wrap_toco -o bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.cc -outdir bazel-out/k8-py3-opt/bin/tensorflow/contrib/lite/toco/python -Iexternal/eigen_archive -Ibazel-out/k8-py3-opt/genfiles -Iexternal/protobuf_archive -Iexternal/swig -Ibazel-out/k8-py3-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/com_google_absl -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/zlib_archive -Iexternal/flatbuffers -Iexternal/highwayhash -Iexternal/gif_archive -Ibazel-out/k8-py3-opt/genfiles/external/jpeg -Iexternal/png_archive -Iexternal/arm_neon_2_x86_sse -Iexternal/farmhash_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/contrib/lite/toco/python/toco.i
:1: Error: Unable to find 'swig.swg'
:3: Error: Unable to find 'python.swg'
tensorflow/contrib/lite/toco/python/toco.i:16: Error: Unable to find ‘std_string.i'

use bazel compile tensorflow

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17476,SVD gradient is unstable for non-unique singular values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.5.0-10-g5b10b34', '1.5.0')
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: GeForce GTX 970 4GB
- **Exact command to reproduce**:

### Describe the problem
The gradient of SVD becomes `nan` when singular values are not unique.

From the code
https://github.com/tensorflow/tensorflow/blob/c6a12c77a50778e28de3590f4618bc2b62f3ecab/tensorflow/python/ops/linalg_grad.py#L332-L342
it is clearly visible that `f` becomes `inf` for equal singular values.

I briefly skimmed through the paper https://arxiv.org/abs/1509.07838 but couldn't find an explanation for this behaviour. Can someone give an intuitive example why the gradient for similar singular values should not be defined?

Similar singular values commonly appear for estimation of rotation matrices (all singular values become 1). At the moment it is impossible to use SVD for such a case.

Alternative implementations of the SVD gradients based on the same paper are:
- https://github.com/InhaDeeplearningGroup/Academic_research/blob/master/LSH/tensorflow_slim/svdGradients.py
- https://gist.github.com/psycharo/60f58d5435281bdea8b9d4ee4f6e895b

which circumvent this problem by simply replacing the `nan`s or effectively setting them to 0.

Could this be used alternatively to the current implementation that just divides by 0? Or alternatively just add a small epsilon to the difference of singular values.

Btw, I think that no gradient computation in TensorFlow should ever return faulty gradients but raise an error message to simplify debugging. I don't see the point of using `nan` or `inf` gradients."
17475,CNN with complex valued filters,"I was trying to implement a CNN with complex valued filters for an image processing task. Here is the [gist](https://gist.github.com/gautamsreekumar/81bf1ad1037ccfc5dfa8da1be28f2216l).
I tried feeding in image with `float64` dtype and I got the error `Value passed to parameter 'input' has DataType complex64 not in list of allowed values: float16, float32`. So, I changed the first layer to real-valued and cast that output of this layer to complex type. Then, I got the same error for the next layer, but only this time, `float64` is not in the list of allowed values which has only complex types. Then I added line 31 in the gist. Now, I am getting this error saying `complex128` not in the allowed values list, which now shows float types. Anyone knows how to fix this?"
17472,Tensorflow lite: crash when use the op tf.gather() ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
 No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.2
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I build the tensorflow lite demo in Android Studio using TensorFlow Lite AAR from JCenter. It works perfect on ARM v7a devices. Then I put my model to the tflitecamerademo projects, but I encounter a crash issue. The crash stack is: 
`
E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
                  Process: android.example.com.tflitecamerademo, PID: 23731
                  java.lang.IllegalArgumentException: Invalid handle to Interpreter.
                      at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)
                      at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:85)
                      at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:118)
                      at org.tensorflow.lite.Interpreter.run(Interpreter.java:96)
                      at com.example.android.tflitecamerademo.ImageClassifierFloatInception.runInference(ImageClassifierFloatInception.java:104)
                      at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:110)
                      at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:664)
                      at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
                      at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:560)
                      at android.os.Handler.handleCallback(Handler.java:743)
                      at android.os.Handler.dispatchMessage(Handler.java:95)
                      at android.os.Looper.loop(Looper.java:150)
                      at android.os.HandlerThread.run(HandlerThread.java:61)
`

The crash occurred only at the time I used tf.gather() or tf.nn.embedding_lookup(). **The tflite model conversion seems to be completed without any errors.** I visualize the model.tflite by running: 
` bazel run tensorflow/contrib/lite/tools:visualize -- model.tflite model_viz.html  `
the result show that tf.gather() use builtin code. 
![1520329163312](https://user-images.githubusercontent.com/12081085/37024964-5e1eda10-2165-11e8-9b6e-d83f345ab8d2.jpg)
I see the similar issue in https://github.com/tensorflow/tensorflow/issues/16308, but no solution.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The tensorflow code:

import tensorflow as tf
import pdb
import tempfile
import subprocess
import numpy as np
tf.contrib.lite.tempfile = tempfile
tf.contrib.lite.subprocess = subprocess

img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 299, 299, 3))
with tf.name_scope(""MobileNet""):
  
  temp = tf.constant( 1, shape=[299*3], dtype=tf.int32 )
  embs = tf.get_variable( 'embs', shape=(10, 299) )
  emb = tf.nn.embedding_lookup(embs, temp)   ### this line leads to crash ###
  img = img * 0  + tf.reshape(emb, [1, 299, 299, 3])

  out = tf.nn.avg_pool(img, [1, 1, 1, 1], [1, 299, 299, 1], 'VALID')
  
  out = tf.layers.conv2d(
        inputs=out,
        filters=1001,
        kernel_size=[5, 5],
        padding=""same"",
        activation=tf.nn.relu)

  out = tf.squeeze(out, [0, 1])
  out = tf.contrib.layers.softmax(out, scope='Predictions')

saver = tf.train.Saver(tf.all_variables())
with tf.Session() as sess:
   tf.global_variables_initializer().run()
   saver.save(sess, ""model/model.ckpt"")
   tf.train.write_graph(sess.graph_def, 'model', 'train.pb', as_text=False)"
17471,Feature request: tf.data.Dataset.unordered_merge(),"We're dealing with sequence models and we are bucketing training data by using
`tf.contrib.data.group_by_window.`
However, it's sequential by nature and too slow. 
So we want to call it in parallel giving up completeness of shuffle. The `tf.data.Dataset` API has
`shard` operation, however, it doesn't have `unordered_merge.` The `zip` operation waits all coming datasets and so isn't useful for this purpose. unordered_merge takes data from any comming dataset ready. It can be used to parallelize data flows.


Following is only for satisfying the format; This request is independent of the System Information.
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source, HEAD of master 20180226
- **TensorFlow version (use command below)**:source, HEAD of master 20180226
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.10.0; installed by bazel-0.10.0-installer-linux-x86_64.sh
- **GCC/Compiler version (if compiling from source)**:4.8.5
- **CUDA/cuDNN version**:V9.1.85
- **GPU model and memory**:Tesla V100/16GB
- **Exact command to reproduce**:N/A

"
17470,how to get tuple for tensorflow in c++,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:rc1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/7.1
- **GPU model and memory**:Nvidia GT
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Rewrite tracking code from python to c++.
I don't know how to get tuple in c++.
The python and c++ code as following:


### Source code / logs
python code:
lstm2_outputs, state2 = tf.nn.dynamic_rnn(lstm2, lstm2_inputs, initial_state=state2, swap_memory=swap_memory)

state2 is a tuple,state2[0] has shape(1,1024),state2[1] has shape(1,1024).

c++ code:
Status run_status = session->Run({ { ""Placeholder"",place_ } ,{ ""Placeholder_1"",place1_ } ,{ ""Placeholder_2"",place2_ } ,{ ""Placeholder_3"",place3_ }  ,{ ""Placeholder_4"",place4_ }, { ""Placeholder_5"",place5_ } },output_tensor_names, {}, { &outputs });

output_tensor_names:re3/lstm1/rnn/transpose_1,re3/lstm2/rnn/transpose_1,
why outputs[0]'s shape is(1,1,1024),outputs[1]'s shape is(1,1,1024),I think they should be (2,1,1024) according the output of python.
The python is the original code which is absolutely,i think my c++ code goes wrong!

in python code:
        feed_dict = {
                self.imagePlaceholder : [croppedInput0, croppedInput1],
                self.prevLstmState : lstmState,
                self.batch_size : 1,
                }
in c++ code ,i write it like this:
Status run_status = session->Run({ { ""Placeholder"",place_ } ,{ ""Placeholder_1"",place1_ } ,{ ""Placeholder_2"",place2_ } ,{ ""Placeholder_3"",place3_ }  ,{ ""Placeholder_4"",place4_ }, { ""Placeholder_5"",place5_ } },output_tensor_names, {}, { &outputs });
the shape is: place_ (2,227,227,3),place1_~place_4(1,1024),does the problem from here?

my model node is like ：
node {
  name: ""Placeholder""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_UINT8
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 227
        }
        dim {
          size: 227
        }
        dim {
          size: 3
        }
      }
    }
  }
}
node {
  name: ""Placeholder_1""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 1024
        }
      }
    }
  }
}
node {
  name: ""Placeholder_2""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 1024
        }
      }
    }
  }
}

some body know why?Give my great appreciation to you! 


"
17469,"ValueError: No attr named '_XlaCompile' in name: ""Tile_1""，please help me","/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 348, in _MaybeCompile
    xla_compile = op.get_attr(""_XlaCompile"")
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2003, in get_attr
    raise ValueError(""No attr named '"" + name + ""' in "" + str(self._node_def))
ValueError: No attr named '_XlaCompile' in name: ""Tile_1""
op: ""Tile""
input: ""Const_1""
input: ""Tile_1/multiples""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""Tmultiples""
  value {
    type: DT_INT32
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/lx/PycharmProjects/chinese/img/model/tfDect_tiny.py"", line 342, in <module>
    tfd = TFDect(logdir=""."")
  File ""/home/lx/PycharmProjects/chinese/img/model/tfDect_tiny.py"", line 293, in __init__
    global_step=self.global_step)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 343, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py"", line 523, in _TileGrad
    assert isinstance(grad, ops.Tensor)
AssertionError"
17468,DeepLab v3+ Not Supported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary: by ""pip3 install --upgrade tensorflow-gpu""
- **TensorFlow version (use command below)**:
tensorflow-gpu (1.6.0)
- **Python version**: 
Python 3.5.2 
- **Bazel version (if compiling from source)**:  0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.0 cuDNN 7
- **GPU model and memory**: Null
- **Exact command to reproduce**:
> ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=./RD_Net/RD_Net_opt.pb \
--output_file=./RD_Net.tflite \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=FLOAT \
--input_data_types=FLOAT \
--input_arrays=Placeholder \
--output_arrays=seq_4/Conv/BiasAdd \
--input_shapes=1,240,320,3

### Describe the problem
I save my pre-trained model into .pb file according to [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) and [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py)

my input/out node in .pbtxt

> node {
  name: ""Placeholder""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 1
        }
        dim {
          size: 240
        }
        dim {
          size: 320
        }
        dim {
          size: 3
        }
      }
    }
  }
}

> node {
  name: ""seq_4/Conv/BiasAdd""
  op: ""BiasAdd""
  input: ""seq_4/Conv/Conv2D""
  input: ""seq_4/Conv/biases""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}

I use the .pb to test prediction and it work fine. (the in/out node name and size are checked)
but fail in .tflite converting flow...  any suggestion?

My step:
1. clone last tensorflow (commit 72239e9b0432a26feadc2412abf89a8fb92828f0)

2. build the toco:
> bazel build tensorflow/contrib/lite/toco:toco

3. and convert my .pb into .tflite:
> ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=./RD_Net/RD_Net_opt.pb \
--output_file=./RD_Net.tflite \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=FLOAT \
--input_data_types=FLOAT \
--input_arrays=Placeholder \
--output_arrays=seq_4/Conv/BiasAdd \
--input_shapes=1,240,320,3

the error msg:

> 2018-03-06 15:39:30.859538: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1365 operators, 2579 arrays (0 quantized)
2018-03-06 15:39:30.910650: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1365 operators, 2579 arrays (0 quantized)
2018-03-06 15:39:30.967113: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:764] Check failed: output_size_shape.dimensions_count() == 1 (2 vs. 1)
Aborted (core dumped)


"
17467,Feature request: implement Rayleigh random sampling op,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 1080
- **Exact command to reproduce**: -

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

TensorFlow does not currently offer `tf.random_rayleigh`, similar to `np.random.rayleigh`. What would be the simplest way to add this op?

The distribution is:

![1c179e259da5763ce4e5dd125cd5f80e1361eb2c](https://user-images.githubusercontent.com/3104289/37019516-ca059586-2118-11e8-81ec-472bb0a7d6fa.png)
"
17464,train priblem,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Distributor ID: 
CentOS
Description:    CentOS Linux release 7.2.1511 (Core) 
Release:        7.2.1511
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: Python 2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:no
- **GPU model and memory**:no
- **Exact command to reproduce**:

ps
```
2018-03-06 11:13:09.816655: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-03-06 11:14:56.902071: W tensorflow/core/framework/op_kernel.cc:1198] Not found: checkpoint/model.ckpt-1_temp_9e8396c2f6d04e91ad3c3fbc194a9f70; No such file or directory
2018-03-06 11:14:56.902071: W tensorflow/core/framework/op_kernel.cc:1198] Not found: checkpoint/model.ckpt-1_temp_9e8396c2f6d04e91ad3c3fbc194a9f70; No such file or directory
```

work:
```
2018-03-06 11:10:24.342583: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 592fd1b6a4409f07 with config: gpu_options { allow_growth: true } allow_soft_placement: true
Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 812, in __bootstrap_inner
    self.run()
  File ""/usr/lib64/python2.7/threading.py"", line 765, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 266, in _run
    coord.request_stop(e)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 211, in request_stop
    six.reraise(*sys.exc_info())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 250, in _run
    enqueue_callable()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1251, in _single_operation_run
    self._session, None, {}, [], target_list, status, None)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
CancelledError: Step was cancelled by an explicit call to `Session::Close()`.

Traceback (most recent call last):
  File ""leave.py"", line 204, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""leave.py"", line 195, in main
    rnn_model.global_step], feed_dict={keep_prob: FLAGS.dropout_keep_prob})
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 539, in run
    run_metadata=run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1013, in run
    run_metadata=run_metadata)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1104, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1089, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    run_metadata=run_metadata))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 452, in after_run
    self._save(run_context.session, global_step)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 468, in _save
    self._get_saver().save(session, self._save_path, global_step=step)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1614, in save
    raise exc
tensorflow.python.framework.errors_impl.NotFoundError: checkpoint/model.ckpt-1_temp_5d8d532077d241a38f2d79ac2ffeb9a7; No such file or directory
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_INT64], _device=""/job:ps/replica:0/task:0/device:CPU:0""](save/ShardedFilename, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, global_step)]]
         [[Node: save/Identity_S445 = _HostRecv[client_terminated=false, recv_device=""/job:worker/replica:0/task:0/device:CPU:0"", send_device=""/job:ps/replica:0/task:0/device:CPU:0"", send_device_incarnation=-8763115780137487660, tensor_name=""edge_111_save/Identity"", tensor_type=DT_STRING, _device=""/job:worker/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'save/SaveV2', defined at:
  File ""leave.py"", line 204, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""leave.py"", line 184, in main
    config=set_config_proto()) as mon_sess:
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 380, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 787, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 511, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 972, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 977, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 668, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 431, in create_session
    self._scaffold.finalize()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 210, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 821, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1239, in __init__
    self.build()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1248, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1284, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 756, in _build_internal
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 382, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 356, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 297, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 240, in save_op
    tensors)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1174, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): checkpoint/model.ckpt-1_temp_5d8d532077d241a38f2d79ac2ffeb9a7; No such file or directory
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_INT64], _device=""/job:ps/replica:0/task:0/device:CPU:0""](save/ShardedFilename, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, global_step)]]
         [[Node: save/Identity_S445 = _HostRecv[client_terminated=false, recv_device=""/job:worker/replica:0/task:0/device:CPU:0"", send_device=""/job:ps/replica:0/task:0/device:CPU:0"", send_device_incarnation=-8763115780137487660, tensor_name=""edge_111_save/Identity"", tensor_type=DT_STRING, _device=""/job:worker/replica:0/task:0/device:CPU:0""]()]]
```

"
17463,Could the periodic resample operation be accelerated?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: NVIDIA GeForce GTX 1080, 8 GB
- **Exact command to reproduce**: python periodic_resample.py

### Describe the problem
The [periodic resample](https://www.tensorflow.org/api_docs/python/tf/contrib/periodic_resample/periodic_resample) operation costs about 700ms when tansforming a tensor from 1x360x640x27 to 1x1080x1920x3. Could it be accelerated by GPU in the future or did I miss something?

### Source code / logs
```python
import tensorflow as tf
import time

a = tf.Variable(tf.random_normal([1, 360, 640, 27], stddev=0.3))
b = tf.contrib.periodic_resample.periodic_resample(a, [1, 1080, 1920, None])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    start = time.time()
    _ = sess.run(b)
    print 'time: %f'%(time.time() - start)
```
```
2018-03-06 10:13:31.817568: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-03-06 10:13:31.949888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-06 10:13:31.950167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.873
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 6.50GiB
2018-03-06 10:13:31.950194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
time: 0.798653
```
"
17461,"Operators not supported: ExpandDims, Prod, Slice.","**System information**

OS Platform and Distribution (Linux Ubuntu 14.04):
TensorFlow installed from (build from source):
TensorFlow version (1.5):
Bazel version(0.8.1)
CPU mode

**Describe the problem**
i use an concat operator when i am training my own model, but when i convert it to .tflite by using toco tools, i got this error:
F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Prod, Slice.
Aborted (core dumped)
i do not use the operators it lists.
[models.zip](https://github.com/tensorflow/tensorflow/files/1783288/models.zip)

there are three models definition in the zip file, the model2 adn model3 works, but model1 does not. The only difference between them is model1 use: 
```
feature = tf.concat([flat1,flat2], 1)
```

to make it more clear, i quote the code block:
```
with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):

    images_input = tf.placeholder_with_default(images, shape=[None, 35, 35, 1], name='InputPlaceholder')

    net = slim.conv2d(images_input, 12, [4, 4], padding='VALID', scope='conv1')
    end_points['conv1'] = net

    net = slim.max_pool2d(net, [2, 2], 2, scope='pool4')
    end_points['pool4'] = net
    
    flat2 = slim.flatten(net)

    net = slim.conv2d(net, 64, [2, 2], padding='VALID', scope='conv5')
    end_points['conv5'] = net
    print(""conv5"")
    print(net)
    flat1 = slim.flatten(net)

    feature = flat2
    # feature = tf.concat([flat1,flat2], 1)

    with tf.variable_scope('Logits'):

      net = slim.fully_connected(feature, 100, scope='fc3')
      
      logits = slim.fully_connected(net, num_classes,
                                  biases_initializer=tf.zeros_initializer(),
                                  weights_regularizer=None,
                                  activation_fn=None,
                                  scope='logits')
      end_points['Logits'] = logits
    output = tf.multiply(logits, 1, name=""Output"")
    end_points['Output'] = output

  return logits, end_points
lite_v1.default_image_size = 35
```



"
17460,Precision in tf.gradients changed in 1.6 vs. <1.6,"### System information
Try the following code using different values for the infinitesimal 1e-15: 

```python
def TFDistance(A):
	""""""
	Compute a distance matrix of A, a coordinate matrix
	Using the factorization:
	Dij = <i|i> - 2<i|j> + <j,j>
	Args:
		A: a Nx3 matrix
	Returns:
		D: a NxN matrix
	""""""
	r = tf.reduce_sum(A*A, 1)
	r = tf.reshape(r, [-1, 1]) 
	# Tensorflow can only reverse mode grad the sqrt if all these elements
	# are nonzero so add a small infinitesimal 
	D = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r) + 1e-15
	return tf.sqrt(D)
xyzs = tf.random_uniform([2,3],dtype=tf.float64)*10.0
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
init = tf.global_variables_initializer()
sess.run(init)
sess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],feed_dict={x:0.2})
```
The issue is that in tf < 1.6 a small infinitesimal  (< 1e-15) was enough to avoid nan in tf.gradients due to 1/sqrt(0.0). For some reason in tf 1.6 this is returning a nan for anything less than 1e-13, which produces an unacceptable error in the result. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSX, pip and pip3 tf 1.6
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
'v1.6.0-0-gd2e24b6039', '1.6.0')
- **Python version**: 
2.7 and 3.3
- **CUDA/cuDNN version**:
CPU 
- **Exact command to reproduce**:
(See script above) 
### Describe the problem
In versions of tensorflow previous to 1.6 this code would not issue a nan gradient for reasonable infinitesimals (<1e-26). Suddenly the way numerical precision is handled in the gradient computation has obviously changed, and changed in a way which is clipping things near zero and doing so differently depending on if colocation of gradients is requested....   

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17458,Segmentation fault or no convergence when using XLA_CPU with distributed TensorFlow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have modified tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py to use MonitoredTrainingSession and PS/worker hosts.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version**: 1.6.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: run with a local PS and a local worker

### Describe the problem
I'm seeing segmentation fault or no convergence when using mnist_softmax_xla.py in a distributed environment (one PS and one worker, both local) and `train_step` placed on XLA_CPU.

```
with tf.device('/device:XLA_CPU:0'):
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
```

Is this the expected behavior? What is the recommended way of using XLA_CPU in distributed training? 
"
17457,realtime bounding box ,"i use tensorflow project for android

I trained my custom datsets by yolo and then i made pb file.

This detected bounding box where i used picture not realtime

But when i put yolo file  in the android project, this cannot detect.

So i screenshot the scene of android project and i trained this screenshot in pc by darkflow.

It detected well. I don't know why. I think option value is correct. plz help me T.T

"
17456,"TensorForestEstimator with TensorFlow v1.4 ""No attribute model_fn""","Setting up a TensorForestEstimator on CloudML running TensorFlow version 1.4. 
The setup is identical to a working DNNClassifier estimator (of course, swapping the parameters within the Random Forest), however, an error occurs when the TensorForestEstimator has its model_fn called.
This seems to be a bug, since the model_fn attribute is defined during initialization [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/tensor_forest/client/random_forest.py](url)

Here's the relevant parts of the code:
`  # Build an estimator.
    
graph_builder_class = tensor_forest.RandomForestGraphs
    
params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(num_classes=10, num_features=100, num_trees=200, max_nodes=1000)
    rfClf=random_forest.TensorForestEstimator(params=params,feature_columns=TF_INPUT_COLUMNS,config=config,graph_builder_class = graph_builder_class,keys_column='key2')
    
rfClf2 = tf.contrib.estimator.forward_features(rfClf, ['key1',key2', 'key3'])`


Below is the error traceback I get from the CloudML runner.

> Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py"", line 201, in <module> run_experiment(hparams) File ""/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py"", line 88, in run_experiment eval_spec) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 464, in train_and_evaluate getattr(executor, task_to_run)() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 562, in run_master self._start_distributed_training(saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 687, in _start_distributed_training saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train loss = self._train_model(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/root/.local/lib/python2.7/site-packages/RF_Model/model1.py"", line 286, in _model_fn features=features, labels=labels, mode=mode, config=estimator.config) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/extenders.py"", line 222, in new_model_fn spec = estimator.model_fn(features, labels, mode, config) AttributeError: 'TensorForestEstimator' object has no attribute 'model_fn'

I'm attempting to use tf.contrib.estimator.forward_features to past a list of keys through to the prediction output (this is reflected in the traceback when new_model_fn tries to call the TensorForestEstimator's ""model_fn attribute"") [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py](url)

It's also probably worth mentioning that the model_fn call within contrib.estimator.forward_features passes 4 parameters, whereas the model function defined within TensorForestEstimator only accepts 3 (see line 392 in the first link). 

-OS Platform: (N/A - CloudML (likely Debian workers?))
-Tensorflow running v1.4
-Relevant code mentioned above.

"
17451,Page crash when up load a well fromat tsv(files contain 300K line of vector(300 dim)),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17447,"Tensorflow Usage with the C++ API, not working","I am trying to go through the tutorial that provided on the TensorFlow website [(LINK).](https://www.tensorflow.org/tutorials/image_recognition)

Im in ```Usage with the C++ API``` part. It says: 
You can download the archive containing the GraphDef that defines the model like this (running from the root directory of the TensorFlow repository):

and then it provide the following command:
```
curl -L ""https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz"" |
  tar -C tensorflow/examples/label_image/data -xz
```

I tried to run this command in my linux terminal but it is givving me error.
Can you tell me how and where I should run this command?

Thanks


Updates:

Have I written custom code: NO I used the code provided on the tensorflow website
OS Platform and Distribution: Linux, ubuntu 16.04
TensorFlow installed from: pip
TensorFlow version: 1.6
Bazel version: was not able to installed 
CUDA/cuDNN version: cuda 9.0 cudnn7.0
GPU model and memory:  Quadro M2200 
Exact command to reproduce"
17446,"tf.image.resize_bilinear outputs weird when scale from [256, 256] to [96, 96]","I have a three band .tif image of size [256, 256, 3], I tried to scale to [128, 128], the output look fine. However, when I tried to scale to [96, 96] the output is only rgb noises.
Anyone has the same experience?
image = tf.image.resize_images(image, [96, 96])
![image](https://user-images.githubusercontent.com/22228307/36994810-d6ba3cd8-2077-11e8-8e07-e2cc7595df79.png)

![image](https://user-images.githubusercontent.com/22228307/36994793-c4bd5a10-2077-11e8-9b36-bfb7510270fb.png)
"
17445,"Minimum Cuda capability is 3.5? But, 3.0 stated on site","I'm able to run the hello world examples, but the following warning (or error) is printed. So, I can't make use of my gpu? While this maybe a simple correction on the web page, is there anyway I can get a version that allows me to run with a Cuda 3.0 card?

OS: Ubuntu 16.04
GPU: K2000M

On the linux installation page, the minimum capability is written as 3.0. But, when I try to run hello world on a cuda 3.0 card, the following is printed:

name: Quadro K2000M major: 3 minor: 0 memoryClockRate(GHz): 0.745
pciBusID: 0000:01:00.0
totalMemory: 1.95GiB freeMemory: 977.81MiB
2018-03-05 13:43:54.533246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Ignoring visible gpu device (device: 0, name: Quadro K2000M, pci bus id: 0000:01:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5."
17444,"getting error while running my model: -    ""tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17442,gradient back propagation through image transforms,"As we know from the description of tf.contrib.image.transform(images, transforms): ""Note that gradients are not backpropagated into transformation parameters"".

Here, I would like to know whether gradients can be correctly backpropagated into images?

If yes, whether it is automatic?

Do I need to explicitly define the gradient using gradient_map_override?

Any suggestions would be helpful. Thanks

Have I written custom code: N/A
OS Platform and Distribution: Ubuntu 15.04
TensorFlow installed from: pip 
TensorFlow version: 1.6.0
Bazel version: N/A
CUDA/cuDNN version: 9.0
GPU model and memory: 24G Quadro P6000
Exact command to reproduce: N/A
"
17441,Illegal instruction (core dumped),"karthick@karthick-Aspire-4739Z:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
**Illegal instruction (core dumped)**
karthick@karthick-Aspire-4739Z:~$ python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  **File ""<stdin>"", line 1, in <module>
ImportError: No module named tensorflow**
>>> exit()


how can i solve this problem but i install tensorflow using native pip on both pip3 and pip on ubuntu 16.04"
17440,"Can't ""adb install"" TfLiteCameraDemo.","I built it using bazel and got a 5.1MB TfLiteCameraDemo.apk, but it can't be installed. The console just got stuck as executing the ""adb install ...""
Your pre-build one is 23.1MB and can be installed without any problem.

Error messages:

Terminal -
adb: failed to install bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk: Failure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]

Logcat -
**03-05 14:23:22.186 1643-1684/system_process E/PackageInstaller: Commit of session 1384727845 failed: Failed to extract native libraries, res=-113**

My environment

Android Studio 3.2 Canary 4(android virtual device : Pixel 2 XL API 27)
bazel 0.11.0
tensorflow master(no custom modification)
stand-alone ndk-r14b, sdk within android-studio 3..2
MacOS High Sierra 10.13.3

My step

bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a

adb install -r bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk"
17439,tf.variable initial value hangs in a cycle for ever,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17437,Bazel build missing dependencies error with MPI,"OS: SLES12
Python version: 3.6
Bazel version: Build label: 0.11.0- (@non-git)
gcc version 7.2.0 (GCC)
No GPU
No CUDA

With MPI enabled in configure everything else disabled.

When I run the command:  bazel build --config=mkl --copt=""-DEIGEN_USE_VML"" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 -s -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures

I get the error below, I guess I could modify the build file to include the additional dependencies:

ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/statusor.h'
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/platform/port.h'
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/error.h'
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/status.h'
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'
  '/home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/stream_executor/platform/logging.h'
tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]
 bool IsGPUDevice<GPUDevice>() {
      ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build"
17436,TF 1.6 build from source fails on ppc64le,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: CUDA 9.0 / CuDNN 7.0
- **GPU model and memory**: Tesla P100-SXM2-16GB
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
When building TF 1.6 from source on ppc64 we see the following error (bug):

```gcc: error: unrecognized command line option '-march=native'```

TensorFlow 1.6 build on ppc64le does not work, because on ppc64le we would need ```-mcpu=native```.

### Source code / logs

Seems like there is some handling for ppc64le in configure.py , however this commit https://github.com/tensorflow/tensorflow/commit/c9885ea7a73ade2d3f8e4712c3a14d9da72462b8 seems to set ```-march=native``` instead of ```-mcpu=native``` for ppc64le ...


Would be great if someone could have a look at it - Thank you very much!"
17435," input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1]) NameError: name 'features' is not defined","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17433,Running model failed:Not found:FeedInputs:unable to find feed output ,"My platform is :win10 with GPU,visual studio 2015.
Application:tracking object
My code as following:
	auto place1 = Placeholder(root.WithOpName(""Placeholder_1""), tensorflow::DataType::DT_UINT8);
	auto place2 = Placeholder(root.WithOpName(""Placeholder_2""), tensorflow::DataType::DT_FLOAT);
	auto place3 = Placeholder(root.WithOpName(""Placeholder_3""), tensorflow::DataType::DT_INT32);

        unsigned unsigned char data1[6];
	auto mapped_place1_ = Eigen::TensorMap<Eigen::Tensor<unsigned char, 4, Eigen::RowMajor>>
		(MergeMatFloat.data, 2 , outputSize, outputSize, 3);
	auto eigen_place1_ = Eigen::Tensor<unsigned char, 4, Eigen::RowMajor>(mapped_place1_);
	Tensor place1_(tensorflow::DT_UINT8, tensorflow::TensorShape({ 2,outputSize,outputSize,3 }));
	place1_.tensor<unsigned char, 4  > () = eigen_place1_;

	float data2[2048] = {0};
	auto mapped_place2_ = Eigen::TensorMap<Eigen::Tensor<float, 2, Eigen::RowMajor>>
		(&data2[0], 4,2);
	auto eigen_place2_ = Eigen::Tensor<float, 2, Eigen::RowMajor>(mapped_place2_);
	Tensor place2_(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 4, 2 }));
	place2_.tensor<float, 2>() = eigen_place2_;

	std::vector<int> data3 = {1};
	auto mapped_place3_ = Eigen::TensorMap<Eigen::Tensor<int, 1, Eigen::RowMajor>>
		(&data3[0], 1);
	auto eigen_place3_ = Eigen::Tensor<int, 1, Eigen::RowMajor>(mapped_place3_);
	Tensor place3_(tensorflow::DT_INT32, tensorflow::TensorShape({ 1}));
	place3_.tensor<int, 1>() = eigen_place3_;

        .....
        std::vector<Tensor> outputs;
	Status run_status = session->Run({ { ""Placeholder_1"",place1_ } },
	{ output_layer }, {}, &outputs);

which give message:
Running model failed:Not found:FeedInputs:unable to find feed output Placeholder_1
what should i do?

my frozen.pb is like this（looks strange,i don't know what node shold i select as the input）:
  Placeholder
Placeholder_1
Placeholder_2
Placeholder_3
fifo_queue
fifo_queue_EnqueueMany
fifo_queue_DequeueMany/n
fifo_queue_DequeueMany
Reshape/shape
Reshape
Reshape_1/shape
Reshape_1
sub/y
sub
re3/conv1/W_conv/Initializer/random_uniform/shape
。。。
gradients/re3
。。。
Adam/beta1
。。。
losses/total_loss/tags
。。。
re3_1/conv1/summaries/W_conv/Rank
。。。
save/Assign_47
。。。
test/robustness/tags
。。。

who know this，appreciation！

"
17432,"""tensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'    actual_port = ntohs(addr.sin_port);"" Error While Cross Compiling for Raspberry Pi 3 with the given script ""build_raspberry_pi.sh""","### System information
- ****Have I written custom code (as opposed to using a stock example script provided in TensorFlow)****: No, Using the provided Script(build_raspberry_pi.sh)

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04LTS

- **TensorFlow installed from (source or binary)**: Source

- **TensorFlow version (use command below)**: 1.6.0

- **Python version**:  2.7

- **Bazel version (if compiling from source)**: 0.8.0

- **GCC/Compiler version (if compiling from source)**: arm-linux-gnueabihf-gcc 4.9

- **CUDA/cuDNN version**:No

- **GPU model and memory**: NO

- **Exact command to reproduce**: ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh

### Describe the problem
I'm trying to cross compile tensorflow for Raspberry Pi 3 and I'm following the below steps,

**1:- Cloning Tensorflow 1.6.0 

2:- Installing Bazel 0.8.0

3:- Running the script ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh**

**Getting Error:-** 
""tensorflow/core/platform/posix/net.cc:60:19: error: invalid 'asm': invalid operand for code 'w'
   addr.sin_port = htons(static_cast<uint16_t>(*port));
                   ^
tensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'
   actual_port = ntohs(addr.sin_port);""
**Note:-**
**""error: invalid 'asm': invalid operand for code 'w'"", Chances are, if you got this error then you were cross-compiling, probably from an x86 or x86_64 host to an ARM target. The error most likely occurred when you used, directly or indirectly, a network byte order translation function from the C library like htons(), or ntohl().**

### Source code / logs
GNU C++ (crosstool-NG crosstool-ng-1.22.0-88-g8460611) version 4.9.3 (arm-linux-gnueabihf)
	compiled by GNU C version 4.8.4, GMP version 6.0.0, MPFR version 3.1.3, MPC version 1.0.3
GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 5490cb547d5e63dcf8255bc3ba4b9a59
In file included from /usr/include/bits/byteswap.h:35:0,
                 from /usr/include/endian.h:60,
                 from /usr/include/bits/waitstatus.h:64,
                 from /usr/include/stdlib.h:42,
                 from /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/cstdlib:72,
                 from # tensorflow/core/platform/posix/net.cc:19:
tensorflow/core/platform/posix/net.cc: In function 'bool tensorflow::internal::{anonymous}::IsPortAvailable(int*, bool)':
tensorflow/core/platform/posix/net.cc:60:19: error: invalid 'asm': invalid operand for code 'w'
   addr.sin_port = htons(static_cast<uint16_t>(*port));
                   ^
tensorflow/core/platform/posix/net.cc:75:17: error: invalid 'asm': invalid operand for code 'w'
   actual_port = ntohs(addr.sin_port);
                 ^

"
17431,docker image for 1.6.0 is missing CUPTI libraries,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, 4.4.0-104-generic
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: v1.6.0-0-gd2e24b6 1.6.0
- **Bazel version**: N/A
- **CUDA/cuDNN version**: 9.0.176 (from docker image)
- **GPU model and memory**: GeForce GTX TITAN X
- **Exact command to reproduce**: (see below)

### Describe the Error

CUPTI library is missing in docker image `tensorflow/tensorflow:1.6.0-gpu-py3`. This is required when using `trace_level=tf.RunOptions.FULL_TRACE` and `run_metadata` in session.run.

### Source code / logs
To reproduce:
<!-- language: lang-py -->
    # run.py
    import tensorflow as tf

    with tf.Session().as_default() as sess:
        test_op = tf.no_op()

        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        sess.run(test_op, options=run_options, run_metadata=run_metadata)

And run:
`> nvidia-docker run --volume /path/to/run.py:/run.py --rm -it tensorflow/tensorflow:1.6.0-gpu-py3 python /run.py`

The error message is:
<!-- language: lang-none -->
    2018-03-05 00:00:00.000000: I tensorflow/stream_executor/dso_loader.cc:141] Couldn't open CUDA library libcupti.so.9.0. LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
    2018-03-05 00:00:00.000000: F ./tensorflow/stream_executor/lib/statusor.h:212] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.9.0; dlerror: libcupti.so.9.0: cannot open shared object file: No such file or directory

Additional information from within the docker image:

<!-- language: lang-bash -->
    > echo $LD_LIBRARY_PATH
    /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
    > ll /usr/local/cuda/extras/CUPTI/lib64
    ls: cannot access '/usr/local/cuda/extras/CUPTI/lib64': No such file or directory
    > find / -name libcupti*
    (not found)

The library seems to be missing in this build. Worked with tensorflow/tensorflow:1.4.0-gpu-py3."
17429,How to get the predicted probabilities for image classification using tensor flow CNN,"I am studying the tensor flow for CNN image classification following the official document(https://www.tensorflow.org/tutorials/layers) . The code in above link for image classification with tensorflow CNN, I want to get all the probabilities of each image labels in test data. I want to ask.

How to get the probabilities of predicting result for each image.
How to evaluate the result of predicted probabilities, in this example using accuracy for label classification. Thanks!


```
`import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)


def cnn_model_fn(features, labels, mode):
  """"""Model function for CNN.""""""
  # Input Layer
  # Reshape X to 4-D tensor: [batch_size, width, height, channels]
  # MNIST images are 28x28 pixels, and have one color channel
  input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])

  # Convolutional Layer #1
  # Computes 32 features using a 5x5 filter with ReLU activation.
  # Padding is added to preserve width and height.
  # Input Tensor Shape: [batch_size, 28, 28, 1]
  # Output Tensor Shape: [batch_size, 28, 28, 32]
  conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=32,
      kernel_size=[5, 5],
      padding=""same"",
      activation=tf.nn.relu)

  # Pooling Layer #1
  # First max pooling layer with a 2x2 filter and stride of 2
  # Input Tensor Shape: [batch_size, 28, 28, 32]
  # Output Tensor Shape: [batch_size, 14, 14, 32]
  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

  # Convolutional Layer #2
  # Computes 64 features using a 5x5 filter.
  # Padding is added to preserve width and height.
  # Input Tensor Shape: [batch_size, 14, 14, 32]
  # Output Tensor Shape: [batch_size, 14, 14, 64]
  conv2 = tf.layers.conv2d(
      inputs=pool1,
      filters=64,
      kernel_size=[5, 5],
      padding=""same"",
      activation=tf.nn.relu)

  # Pooling Layer #2
  # Second max pooling layer with a 2x2 filter and stride of 2
  # Input Tensor Shape: [batch_size, 14, 14, 64]
  # Output Tensor Shape: [batch_size, 7, 7, 64]
  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

  # Flatten tensor into a batch of vectors
  # Input Tensor Shape: [batch_size, 7, 7, 64]
  # Output Tensor Shape: [batch_size, 7 * 7 * 64]
  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])

  # Dense Layer
  # Densely connected layer with 1024 neurons
  # Input Tensor Shape: [batch_size, 7 * 7 * 64]
  # Output Tensor Shape: [batch_size, 1024]
  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)

  # Add dropout operation; 0.6 probability that element will be kept
  dropout = tf.layers.dropout(
      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

  # Logits layer
  # Input Tensor Shape: [batch_size, 1024]
  # Output Tensor Shape: [batch_size, 10]
  logits = tf.layers.dense(inputs=dropout, units=10)

  predictions = {
      # Generate predictions (for PREDICT and EVAL mode)
      ""classes"": tf.argmax(input=logits, axis=1),
      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
      # `logging_hook`.
      ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
  }
  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

  # Calculate Loss (for both TRAIN and EVAL modes)
  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

  # Configure the Training Op (for TRAIN mode)
  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

  # Add evaluation metrics (for EVAL mode)
  eval_metric_ops = {
      ""accuracy"": tf.metrics.accuracy(
          labels=labels, predictions=predictions[""classes""])}
  return tf.estimator.EstimatorSpec(
      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)


def main(unused_argv):
  # Load training and eval data
  mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
  train_data = mnist.train.images  # Returns np.array
  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
  eval_data = mnist.test.images  # Returns np.array
  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

  # Create the Estimator
  mnist_classifier = tf.estimator.Estimator(
      model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model"")

  # Set up logging for predictions
  # Log the values in the ""Softmax"" tensor with label ""probabilities""
  tensors_to_log = {""probabilities"": ""softmax_tensor""}
  logging_hook = tf.train.LoggingTensorHook(
      tensors=tensors_to_log, every_n_iter=50)

  # Train the model
  train_input_fn = tf.estimator.inputs.numpy_input_fn(
      x={""x"": train_data},
      y=train_labels,
      batch_size=100,
      num_epochs=None,
      shuffle=True)
  mnist_classifier.train(
      input_fn=train_input_fn,
      steps=20000,
      hooks=[logging_hook])

  # Evaluate the model and print results
  eval_input_fn = tf.estimator.inputs.numpy_input_fn(
      x={""x"": eval_data},
      y=eval_labels,
      num_epochs=1,
      shuffle=False)
  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)
  print(eval_results)


if __name__ == ""__main__"":
  tf.app.run()`
```"
17427,Feature Request: Train Using Multiple GPUs with Tensorflow in a Tower-like Fashion for tensorflow 1.4,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
2 NVIDIA GeForce GTX 1070 (8 GB each)
-**Exact command to reproduce**:
N/A

### Describe the problem
I tried training my model with a large batch size that is supposed to be enough just for two GPUs but I encounter out of memory errors. I'm using the Estimator API and I was wondering if it handles multi-gpu training like in this [example](https://www.tensorflow.org/tutorials/deep_cnn#training_a_model_using_multiple_gpu_cards) using the cifar_10 dataset. If it doesn't, it would be nice to have the Estimator API handle multi-gpu training such that we can use larger batch sizes.

### Source code / logs
Here's the architecture of the model I trained.

```
{
  ""network"":[
    {""layer_type"": ""input_layer"", ""name"": ""inputs"", ""shape"": [-1, 168, 168, 1]},
    {""layer_type"": ""l2_normalize"", ""axis"": [1, 2]},
    {""layer_type"": ""conv2d"", ""num_filters"": 16, ""kernel_size"": [3, 3]},
    {""layer_type"": ""max_pool2d"", ""pool_size"": [2, 2]},
    {""layer_type"": ""l2_normalize"", ""axis"": [1, 2]},
    {""layer_type"": ""conv2d"", ""num_filters"": 32, ""kernel_size"": [3, 3]},
    {""layer_type"": ""max_pool2d"", ""pool_size"": [2, 2]},
    {""layer_type"": ""l2_normalize"", ""axis"": [1, 2]},
    {""layer_type"": ""dropout"", ""keep_prob"": 0.5},
    {""layer_type"": ""conv2d"", ""num_filters"": 64, ""kernel_size"": [3, 3]},
    {""layer_type"": ""max_pool2d"", ""pool_size"": [2, 2]},
    {""layer_type"": ""l2_normalize"", ""axis"": [1, 2]},
    {""layer_type"": ""dropout"", ""keep_prob"": 0.5},
    {""layer_type"": ""collapse_to_rnn_dims""},
    {""layer_type"": ""birnn"", ""num_hidden"": 128, ""cell_type"": ""LSTM""},
    {""layer_type"": ""dropout"", ""keep_prob"": 0.5}
  ],
  ""output_layer"": ""ctc_decoder"",
  ""loss"": ""ctc"",
  ""metrics"": [""label_error_rate""],
  ""learning_rate"": 0.001,
  ""optimizer"": ""adam""
}
```

The image dimensions are 168x168 px and the working batch size is 240 which I would like to double.
"
17426,tf-slim resnet-50 pretrained model get wrong results when inference,"I have asked this question on stackoverflow, but no one answer. Does any can help me.
The question link to stackoverflow is [here](https://stackoverflow.com/questions/49094123/tf-slim-resnet-pretrained-model-cant-get-correct-results).
The following is the code I used to do inference.The image preprocess method following this issue [ResNet pre-processing: VGG or Inception?](https://github.com/tensorflow/models/issues/2217)
```Python
import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import imagenet
import urllib.request
from preprocessing import inception_preprocessing
import matplotlib.pyplot as plt
import numpy as np
slim = tf.contrib.slim
resnet = nets.resnet_v1

if __name__ == '__main__':
    ckpt_file_path = '../model_weights/resnet_v1_50.ckpt'
    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'
    image_string = urllib.request.urlopen(url).read()
    image = tf.image.decode_jpeg(image_string, channels=3)
    processed_image = inception_preprocessing.preprocess_image(image, 224, 224, is_training=False)
    processed_images = tf.expand_dims(processed_image, 0)
    with slim.arg_scope(nets.resnet_utils.resnet_arg_scope()):
        resnet_50, end_points = resnet.resnet_v1_50(inputs=processed_images, num_classes=1000, scope='resnet_v1_50')
        prob = tf.squeeze(resnet_50, axis=[1, 2])
    probabilities = tf.nn.softmax(prob, dim=-1)
    sess = tf.Session()
    saver = tf.train.Saver()
    saver.restore(sess, ckpt_file_path)
    np_image, results = sess.run([image, probabilities])
    results = results[0, 0:]

    plt.figure()
    plt.imshow(np_image.astype(np.uint8))
    plt.axis('off')
    plt.show()

    sorted_inds = [i[0] for i in sorted(enumerate(-results), key=lambda x: x[1])]
    names = imagenet.create_readable_names_for_imagenet_labels()
    for i in range(5):
        index = sorted_inds[i]
        print('Probability %0.2f%% => [%s]' % (results[index] * 100, names[index]))
```
The following is result:
```
Probability 1.00% => [moving van]
Probability 0.69% => [television, television system]
Probability 0.63% => [English foxhound]
Probability 0.63% => [beagle]
Probability 0.61% => [German short-haired pointer]
```
"
17424,documentation on how to use a Tensorflow model trained using the Estimator & Dataset APIs on an Android app.,"I am having a hard time finding documentation on how to use a Tensorflow model trained using the Estimator & Dataset APIs on an Android app.

Having spent hours searching for it and turned out nothing, and maybe there's a many others having the same issue, and the answer could be just a two-minutes thing for people who knows. I am raising it as an issue here. 

Can someone point me to a good reference and/or tutorial? I looked at the TensorflowInferenceInterface, but my understanding is it need you to specify which operator you want to feed the input to, but the Estimator/Dataset abstraction is at another level. So I am somewhat lost here.

Thanks.
 -Jerry

The version of Tensorflow I am using is v1.6.0-0-gd2e24b6039 1.6.0
"
17423,image_retraining example does not work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6.4
- **Exact command to reproduce**:
      python3 tensorflow/examples/image_retraining/retrain.py \
      --image_dir ~/Resources/tf-retrain-images/ \
      --learning_rate=0.0001 \
      --testing_percentage=10 \
      --validation_percentage=10 \
      --train_batch_size=32 \
      --validation_batch_size=-1 \
      --flip_left_right True \
      --random_scale=30 \
      --random_brightness=30 \
      --eval_step_interval=100 \
      --how_many_training_steps=500 \
      --architecture mobilenet_0.25_224

### Describe the problem
`retrain.py` fails with the error below when it starts to create bottleneck files for testing datasets after training is done. Looks like something wrong with making bottleneck files for test images.
FYI, it works when I checkout `retrain.py` back to commit dce9a49c.

### Source code / logs
...
INFO:tensorflow:2018-03-05 13:31:17.056049: Step 90: Validation accuracy = 89.0% (N=73)
INFO:tensorflow:2018-03-05 13:31:25.350794: Step 99: Train accuracy = 96.9%
INFO:tensorflow:2018-03-05 13:31:25.350940: Step 99: Cross entropy = 0.198750
INFO:tensorflow:2018-03-05 13:31:25.398267: Step 99: Validation accuracy = 89.0% (N=73)
Model path:  /tmp/imagenet/mobilenet_v1_0.25_224_frozen.pb
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:Creating bottleneck at /tmp/bottleneck/cat/cat.1.jpg_mobilenet_0.25_224.txt
Traceback (most recent call last):
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1070, in _run
    allow_operation=False)
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3323, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3402, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/image_retraining/retrain.py"", line 394, in create_bottleneck_file
    resized_input_tensor, bottleneck_tensor)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 326, in run_bottleneck_on_image
    {image_data_tensor: image_data})
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1073, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1486, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/tushuhei/py3env/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 1286, in main
    bottleneck_tensor)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 881, in run_final_eval
    bottleneck_tensor, FLAGS.architecture))
  File ""tensorflow/examples/image_retraining/retrain.py"", line 567, in get_random_cached_bottlenecks
    resized_input_tensor, bottleneck_tensor, architecture)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 442, in get_or_create_bottleneck
    bottleneck_tensor)
  File ""tensorflow/examples/image_retraining/retrain.py"", line 397, in create_bottleneck_file
    str(e)))
RuntimeError: Error during processing file /Users/tushuhei/Resources/tf-retrain-images/cat/cat.1.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.)
"
17422,Get cudnn version 7005 while 7101 installed only,"
ubuntu 16.04
GTX 1080ti
cuda 9.1
cudnn 7.1.1 ( libcudnn7_7.1.1.5-1+cuda9.1_amd64.deb & libcudnn7-dev_7.1.1.5-1+cuda9.1_amd64.deb )
tensorflow 1.4.1 compiled from source with cuda 9.1 and cudnn 7.1.1

I have never downloaded or installed cudnn 7.0.5. ""locate libcudnn.so.7.0"" show nothing.

2018-03-05 11:06:53.859047: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7005 (compatibility version 7000) but source was compiled with 7101 (compatibility version 7100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2018-03-05 11:06:53.859249: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) 
Aborted (core dumped)
"
17421,Unreasonable prediction on test set with resnet_v2 for specific batch size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: Binary from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.1+
- **Bazel version (if compiling from source)**: /
- **GCC/Compiler version (if compiling from source)**: / 
- **CUDA/cuDNN version**: V7.5.17
- **GPU model and memory**: Tesla M40 12G
- **Exact command to reproduce**: /

### Describe the problem
I am using resnet_v2 provided in slim to train a classifier. However, there is an unexpected behaviour when I tried to get the predictions on test set and I am sure that I turned off the training mode by pass is_training as False. 

My model returned unreasonable prediction scores on specific testing mini batch size of 650 and 649. 
I randomly checked several testing batch sizes (from 50 - 850, which my GPU can afford ), their predictions on same input data were all consistent with each other, except for size 650 and 649.  Actually even two identical patches in the mini batch had different scores when I am using batch size 650 and 649.  To verify the issue, I also tried to train the model with different input data and different training mini batch size. However, the problem is still there with the same weird mini batch size 650 and 649 :(. 

"
17419,possible issue of tensorflow.keras not handling shape correctly,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 1709 x64
- **TensorFlow installed from (source or binary)**: binary from PyPI
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: /
- **GCC/Compiler version (if compiling from source)**: /
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX1060 6GB
- **Exact command to reproduce**: /

### Describe the problem
i believe this is a bug that ``tensorflow.keras`` not handling the shape correctly, ``self.input_shape`` is provided by me which is [7514, 1] in this case. My code runs fine with ``keras`` but get this error with ``tensorflow.keras``

### Source code / logs
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-21e293afadc1> in <module>()
     22 # x_err[x_err<0] = 0
     23 from astroNN.models import load_folder
---> 24 bcnn = load_folder('astroNN_0224_run002')
     25 
     26 # x = np.random.rand(1000,7514)

d:\university\ast425\astronn\astroNN\models\__init__.py in load_folder(folder)
    144         pass
    145 
--> 146     astronn_model_obj.compile()
    147     astronn_model_obj.keras_model.load_weights(os.path.join(astronn_model_obj.fullfilepath, 'model_weights.h5'))
    148 

d:\university\ast425\astronn\astroNN\models\BayesianCNNBase.py in compile(self)
    238             self._last_layer_activation = 'softmax'
    239 
--> 240         self.keras_model, self.keras_model_predict, output_loss, variance_loss = self.model()
    241 
    242         if self.optimizer is None or self.optimizer == 'adam':

d:\university\ast425\astronn\astroNN\models\Apogee_BCNN.py in model(self)
     64 
     65     def model(self):
---> 66         input_tensor = Input(shape=self.input_shape, name='input')
     67         labels_err_tensor = Input(shape=(self.labels_shape,), name='labels_err')
     68 

~\Anaconda3\lib\site-packages\tensorflow\python\keras\_impl\keras\engine\topology.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)
    621   if dtype is None:
    622     dtype = K.floatx()
--> 623   if not shape and tensor is None:
    624     raise ValueError('Please provide to Input either a `shape`'
    625                      ' or a `tensor` argument. Note that '

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```"
17417,embed_sequence and embedding_lookup behave differently on CPU vs. GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
When using `tf.nn.embedding_lookup` or `tf.contrib.layers.embed_sequence` with -1 in the indexes the corresponding embeddings should be 0 vectors, in the same way that `tf.feature_column.categorical_column_with_identity` ignores negative values. This works correctly when trying on Colaboratory but fails on my Windows installation using the same Tensorflow version v1.6.

### Source code / logs
Both ways of initializing the embeddings produce the same error, only on Windows, may it be related to the version of the `gather` function being used from `python\ops\array_ops.py`?
```python
tf.reset_default_graph()
a = tf.constant(np.array([[0, 1, 2, -1, 4]]))
# b = tf.contrib.layers.embed_sequence(a, 5, 3, initializer=tf.random_uniform_initializer(-1.0, 1.0))
b = tf.nn.embedding_lookup(tf.get_variable('embeddings', [5, 3], initializer=tf.random_uniform_initializer(-1.0, 1.0)), a)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(b))
```

The error goes like this
> InvalidArgumentError: indices[..] = -1 is not in [0, ..)"
17415,Dataset API 'flat_map' method producing error for same code which works with 'map' method,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
 
- **TensorFlow installed from (source or binary)**: Binary
   
- **TensorFlow version (use command below)**:  1.6.0

- **Python version**: 3.5

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GeForce GTX 860M
- **Exact command to reproduce**: dataset = dataset.flat_map(lambda file_name: tf.py_func(_get_data_for_dataset, [file_name], tf.float64))

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am trying to create a create a pipeline to read mulitple CSV files using TensorFlow Dataset API and Pandas. However using the 'flat_map' method is producing errors. However if I am using 'map' method I am able to build the code and run it in session. This is the code I am using.

### Source code / logs

```
folder_name = './data/power_data/'
file_names = os.listdir(folder_name)
def _get_data_for_dataset(file_name,rows=100):#
    print(file_name.decode())
    
    df_input=pd.read_csv(os.path.join(folder_name, file_name.decode()),
                         usecols =['Wind_MWh','Actual_Load_MWh'],nrows = rows)
    X_data = df_input.as_matrix()
    X_data.astype('float32', copy=False)
    
    return X_data
dataset = tf.data.Dataset.from_tensor_slices(file_names)
dataset = dataset.flat_map(lambda file_name: tf.py_func(_get_data_for_dataset, [file_name], tf.float64))
dataset= dataset.batch(2)
iter = dataset.make_one_shot_iterator()
get_batch = iter.get_next()`
```
 I get the following error: `map_func` must return a `Dataset` object. It would also great if you could provide documentation on using Dataset API with Pandas module.
"
17413,TOCO API fail with the basic Python example,"### System information
- **OS Platform and Distribution (Linux Ubuntu 17.10)**:
- **TensorFlow installed from (pip3)**:
- **TensorFlow version (1.6)**:
- **Python 3.6.3**: 

### Describe the problem
TOCO API fail with the basic Python example:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#example-2-export-with-variables

### Error
```
Traceback (most recent call last):
  File ""fb.py"", line 15, in <module>
    sess, sess.graph_def, map(canonical_name, out_tensors))
  File ""/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 232, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File ""/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 176, in extract_sub_graph
    nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)
  File ""/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py"", line 140, in _bfs_for_reachable_nodes
    next_to_visit = target_nodes[:]
TypeError: 'map' object is not subscriptable
```

"
17411,Illegal instruction (core dumped) after running import tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 
1.6.0-cp27-cp27mu-manylinux1_x86_64 (can only guess since `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` gives me an error already)
- **Python version**: Python 2.7.12
- **Exact command to reproduce**: `import tensorflow`

I created a fresh virtual environment: `virtualenv -p python2 test_venv/`
And installed tensorflow: `pip install --upgrade --no-cache-dir tensorflow`
`import tensorflow` gives me `Illegal instruction (core dumped)`

Please help me understand what's going on and how I can fix it. Thank you.

CPU information:
```
-cpu
          description: CPU
          product: Intel(R) Core(TM) i3 CPU       M 330  @ 2.13GHz
          bus info: cpu@0
          version: CPU Version
          capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat cpufreq
```
*EDIT*
Stacktrace obtained with gdb:

```
#0  0x00007fffe5793880 in std::pair<std::__detail::_Node_iterator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, false, true>, bool> std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> >, std::allocator<std::pair<tensorflow::StringPiece const, std::function<bool (tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> > >(std::integral_constant<bool, true>, std::pair<tensorflow::StringPiece, std::function<bool (tensorflow::Variant*)> >&&) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#1  0x00007fffe5795735 in tensorflow::UnaryVariantOpRegistry::RegisterDecodeFn(std::string const&, std::function<bool (tensorflow::Variant*)> const&) () from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fffe5770a7c in tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<tensorflow::Tensor>::UnaryVariantDecodeRegistration(std::string const&) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fffe56ea165 in _GLOBAL__sub_I_tensor.cc ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007ffff7de76ba in call_init (l=<optimized out>, argc=argc@entry=2, argv=argv@entry=0x7fffffffd5c8, env=env@entry=0xa7b4d0)
    at dl-init.c:72
#5  0x00007ffff7de77cb in call_init (env=0xa7b4d0, argv=0x7fffffffd5c8, argc=2, l=<optimized out>) at dl-init.c:30
#6  _dl_init (main_map=main_map@entry=0xa11920, argc=2, argv=0x7fffffffd5c8, env=0xa7b4d0) at dl-init.c:120
#7  0x00007ffff7dec8e2 in dl_open_worker (a=a@entry=0x7fffffffb5c0) at dl-open.c:575
#8  0x00007ffff7de7564 in _dl_catch_error (objname=objname@entry=0x7fffffffb5b0, errstring=errstring@entry=0x7fffffffb5b8, 
    mallocedp=mallocedp@entry=0x7fffffffb5af, operate=operate@entry=0x7ffff7dec4d0 <dl_open_worker>, args=args@entry=0x7fffffffb5c0)
    at dl-error.c:187
#9  0x00007ffff7debda9 in _dl_open (
    file=0x7fffea7cbc34 ""/media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"", mode=-2147483646, caller_dlopen=0x51ad19 <_PyImport_GetDynLoadFunc+233>, nsid=-2, argc=<optimized out>, argv=<optimized out>, env=0xa7b4d0)
    at dl-open.c:660
#10 0x00007ffff75ecf09 in dlopen_doit (a=a@entry=0x7fffffffb7f0) at dlopen.c:66
#11 0x00007ffff7de7564 in _dl_catch_error (objname=0x9b1870, errstring=0x9b1878, mallocedp=0x9b1868, operate=0x7ffff75eceb0 <dlopen_doit>, 
    args=0x7fffffffb7f0) at dl-error.c:187
#12 0x00007ffff75ed571 in _dlerror_run (operate=operate@entry=0x7ffff75eceb0 <dlopen_doit>, args=args@entry=0x7fffffffb7f0) at dlerror.c:163
#13 0x00007ffff75ecfa1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87
#14 0x000000000051ad19 in _PyImport_GetDynLoadFunc ()
#15 0x000000000051a8e4 in _PyImport_LoadDynamicModule ()
#16 0x00000000005b7b1b in ?? ()
#17 0x00000000004bc3fa in PyEval_EvalFrameEx ()
#18 0x00000000004c136f in PyEval_EvalFrameEx ()
#19 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#20 0x00000000004b97a6 in PyEval_EvalCode ()
#21 0x00000000004b96df in PyImport_ExecCodeModuleEx ()
#22 0x00000000004b2b06 in ?? ()
#23 0x00000000004a4ae1 in ?? ()
```

*EDIT 2*
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

After downgrading to an older version of tensorflow the error goes away. I've been advised that my CPU (see information above) might not work with some improvements in the new API. If this is the case, I suppose there's no solution for my problem. Therefore, I will close this thread. Feel free to correct me though. Thank you for your support"
17410,CUDNN rnn error -Failed to call ThenRnnForward,"OS Platform and Distribution - Ubuntu 16.04
TensorFlow installed from TensorFlow version -1.4 from 
Bazel Version 0.6.1
CUDA Version 9.0.176
Machine Type -n1-standard-32 (32 vCPUs, 120 GB memory)
GPU - 4 x NVIDIA Tesla P100
I am using a cudnnrnnrelu like this ::
with tf.variable_scope('cudnn_rnn_stack', reuse = reuse) as scope:
            rnn = tf.contrib.cudnn_rnn.CudnnRNNRelu(5,n_hidden,""linear_input"", ""bidirectional"")
            output, _ = rnn(tf.transpose(layer_1,[1,0,2]), training=True)
            output_rnn_stack = tf.concat(output, 2)
Initially the epochs were running fine. But I encountered this error after 2-3 epochs:
InternalError (see above for traceback): Failed to call ThenRnnForward
         [[Node: tower_0/cudnn_rnn_stack/cudnn_rnn_relu/CudnnRNN = CudnnRNN[T=DT_FLOAT, direc
tion=""bidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""rnn_r
elu"", seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](tower_0/cudnn_
rnn_stack/transpose, tower_0/cudnn_rnn_stack/cudnn_rnn_relu/zeros, tower_0/cudnn_rnn_stack/cu
dnn_rnn_relu/Const, cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/read)]]
         [[Node: Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam/_870 = _R
ecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send
_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_nam
e=""edge_2200_Adam/update_cudnn_rnn_stack/cudnn_rnn_relu/opaque_kernel/ApplyAdam"", tensor_type
=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


Running it on a distributed gpu setting in gcp instances"
17407,"ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""embedding_1/random_uniform:0"", shape=(20000, 200), dtype=float32)'","I am not sure how to fix this error:
When I run https://github.com/laviavigdor/twitter-sentiment-analysis/issues/5 I get this error:

```
[jalal@goku twitter-sentiment-analysis]$  echo ""This is a sample tweet to predict on"" | python predict.py
Using TensorFlow backend.
2018-03-03 22:15:52.298930: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-03 22:15:52.298964: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-03 22:15:52.298974: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-03 22:15:52.298981: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-03 22:15:52.298987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-03 22:15:52.521523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.6705
pciBusID 0000:05:00.0
Total memory: 10.92GiB
Free memory: 9.92GiB
2018-03-03 22:15:52.742797: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5626add8a580 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-03-03 22:15:52.743474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.6705
pciBusID 0000:06:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-03-03 22:15:52.744161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2018-03-03 22:15:52.744178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2018-03-03 22:15:52.744185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2018-03-03 22:15:52.744197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)
2018-03-03 22:15:52.744206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)
/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.
  warnings.warn('The `nb_words` argument in `Tokenizer` '
/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py:1253: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.
  return cls(**config)
/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py:1253: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(trainable=False, name=""embedding_1"", activity_regularizer=None, input_dtype=""int32"", mask_zero=False, input_dim=20000, batch_input_shape=[None, 100..., output_dim=200, input_length=1000, embeddings_initializer=""uniform"", embeddings_regularizer=None, embeddings_constraint=None)`
  return cls(**config)
Traceback (most recent call last):
  File ""predict.py"", line 41, in <module>
    main()
  File ""predict.py"", line 20, in main
    model = load_model('model.h5')
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py"", line 239, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py"", line 313, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 139, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py"", line 1249, in from_config
    model.add(layer)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/models.py"", line 442, in add
    layer(x)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py"", line 576, in __call__
    self.build(input_shapes[0])
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/layers/embeddings.py"", line 101, in build
    dtype=self.dtype)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/engine/topology.py"", line 400, in add_weight
    constraint=constraint)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 376, in variable
    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 199, in __init__
    expected_shape=expected_shape)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 289, in _init_from_args
    initial_value, name=""initial_value"", dtype=dtype)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 549, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""embedding_1/random_uniform:0"", shape=(20000, 200), dtype=float32)'
[jalal@goku twitter-sentiment-analysis]$ 
```

How should I fix it?

```
[jalal@goku twitter-sentiment-analysis]$ conda list | grep -i keras
keras                     2.0.9                    py36_0    conda-forge
[jalal@goku twitter-sentiment-analysis]$ conda list | grep -i tensorflow
tensorflow-gpu            1.3.0                         0  
tensorflow-gpu-base       1.3.0           py36cuda8.0cudnn6.0_1  
tensorflow-tensorboard    0.1.5                    py36_0  
```
"
17405,No attributed named 'constant',"I used the following lines of code (from tensorflow.org website)

conda create -n tensorflow pip python=3.6 
activate tensorflow
pip install --ignore-installed --upgrade tensorflow

The installation seems to be successful. However, I keep get an error saying TensorFlow has no attribute named 'constant' or 'InteractiveSessions' when I use tf.constant() or tf.InteractiveSessions(). I use a 64-bit Python version 3.6.3 and the OS is Windows 7 Enterprise.  
"
17403,ValueError: features should be a dictionary of `Tensor`s. Given type: <class 'NoneType'>,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacBook Pro, MacOs High Sierra 10.13.3
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**:n.a.
- **GCC/Compiler version (if compiling from source)**: n.a.
- **CUDA/cuDNN version**: n.a.
- **GPU model and memory**:n.a.
- **Exact command to reproduce**: 
git clone https://github.com/tensorflow/models
cd models/samples/core/get_started/
python premade_estimator.py

Darwin ZZZM30774783A 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.3
== are we in docker =============================================
No
== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin17.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
== uname -a =====================================================
Darwin ZZZM30774783A 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64
== check pips ===================================================
numpy (1.12.1)
protobuf (3.4.0)
tensorflow (1.4.0)
tensorflow-tensorboard (0.4.0rc2)
== check for virtualenv =========================================
True
== tensorflow import ============================================
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)
/Users/xxx/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
== cuda libs  ===================================================


### Describe the problem
I am following the tutorial https://www.tensorflow.org/get_started/premade_estimators and I get the following error

INFO:tensorflow:Loss for final step: 4.43589.
Traceback (most recent call last):
  File ""premade_estimator.py"", line 88, in <module>
    tf.app.run(main)
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""premade_estimator.py"", line 57, in main
    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in evaluate
    name=name)
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 810, in _evaluate_model
    features, labels, model_fn_lib.ModeKeys.EVAL, self.config)
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/Users/d069082/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 167, in _dnn_model_fn
    'Given type: {}'.format(type(features)))
ValueError: features should be a dictionary of `Tensor`s. Given type: <class 'NoneType'>


"
17401,tf.constant and tf.ones behave differently with unknown shape dimension,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX 1050 Ti
- **Exact command to reproduce**: see below

### The problem
The following code works 
```
import tensorflow as tf
batch_size = tf.placeholder(tf.int32, shape=[])
ones = tf.ones((batch_size,))
```
while the following does not work
```
import tensorflow as tf
batch_size = tf.placeholder(tf.int32, shape=[])
ones = tf.constant(1, shape=(batch_size,))

>>> ValueError: setting an array element with a sequence.
```

To my mind, these should be equivalent ways of creating a tensor of ones with dynamic size. This is particularly annoying if one wants to create a constant tensor with a value different from 0 or 1.
"
17400,Eager: Allowing GPU memory growth,"Since we don't have session in eager mode, how can we allocate only as much GPU memory as needed in our program?

Or can we set the fraction of the overall amount of memory that each visible GPU should be allocated?

When will this feature be supported?"
17399,Eager: Using tfe.gradients_function with the params keyword returns a function which cannot be called with lists of values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian Stretch
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6.2
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Bazel version (if compiling from source)**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

def f(a, b):
    return a**2 + b**3

partial_grad = tfe.gradients_function(f, params=[0])
partial_grad([1., 2., 3.], [1., 2., 3.])
```

When I try to use ```tfe.gradients_function``` with the keyword params, it is not possible to call the resulting function ```partial_grad```with a list of values.

## Workarounds:

Using np.array([1., 2., 3.]) as input for partial_grad works.

## Traceback:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-cb367c5f1046> in <module>()
      1 partial_grad = tfe.gradients_function(f, params=[0])
----> 2 partial_grad([1., 2., 3.], [1., 2., 3.])

~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    512     """"""Computes the gradient of the decorated function.""""""
    513 
--> 514     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)
    515     return grad
    516 

~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    611       raise ValueError(""Functions to be differentiated cannot ""
    612                        ""receive keyword arguments."")
--> 613     val, vjp = make_vjp(f, params)(*args, **kwds)
    614     return val, vjp(dy=dy)
    615 

~/programs/miniconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    665         sources.append(args[i])
    666         tape.watch(args[i])
--> 667       result = f(*args)
    668       if result is None:
    669         raise ValueError(""Cannot differentiate a function that returns None; ""

<ipython-input-2-159855d98f00> in f(a, b)
      1 def f(a, b):
----> 2     return a**2 + b**3

TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'
```
"
17398, Failed to load the native TensorFlow runtime.,"Hello,

trying to run :
import tensorflow as tf
and I obtain the following error:
Traceback (most recent call last):
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\hp\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\hp\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\hp\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\hp\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
17397,Error triggers when gradients calculated inside the tf.map_fn function,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0-rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.2.1
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:

### Describe the problem
When I try to calculate jacobian using `map_fn` I get then following error: ``Could not write to TensorArray index 1 because it has already been read.``

I've checked one of the similar issues, but suggested solutions didn't help to solve my problem: https://github.com/tensorflow/tensorflow/issues/13983

Based on the error logs it looks to me that problem happens because I try to calculate gradient multiple times with the same set of parameters. I tried to run my script with `n_samples=1` (see code below) and I get the expected result without error. In addition, error happens with index 1, which again point that probably first iteration was finished properly and error appears when function reaches second  sample.

### Source code / logs

```python
import numpy as np
import tensorflow as tf


def jacobians(errors, parameters):
    return tf.map_fn(
        fn=lambda x, params=parameters: tf.gradients(x, params),
        elems=errors,
        dtype=[x.dtype for x in parameters],
        back_prop=False,
        name='jacobian',
        parallel_iterations=1)


x = tf.placeholder(tf.float32, (None, 2))
y = tf.placeholder(tf.float32, (None, 1))
weight = tf.Variable(np.random.random((2, 1)), dtype=tf.float32)
bias = tf.Variable(np.random.random((1, 1)), dtype=tf.float32)

prediction = tf.matmul(x, weight) + bias
errors = tf.square(y - prediction)

n_samples = 5
Js = jacobians(errors, [weight, bias])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(Js, {
        x: np.random.random((n_samples, 2)),
        y: np.random.random((n_samples, 1)),
    })
```

When I run this code I get the following exception

```
/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-03-03 11:15:16.808741: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_array_ops.cc:415 : Invalid argument: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.
Traceback (most recent call last):
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.
	 [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/itdxer/Downloads/jacobian_fail_example.py"", line 29, in <module>
    y: np.random.random((n_samples, 1))
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.
	 [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]

Caused by op 'jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3', defined at:
  File ""/Users/itdxer/Downloads/jacobian_fail_example.py"", line 23, in <module>
    Js = jacobians(errors, [weight, bias])
  File ""/Users/itdxer/Downloads/jacobian_fail_example.py"", line 11, in jacobians
    parallel_iterations=1)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 413, in map_fn
    swap_memory=swap_memory)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3278, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3013, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2953, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 403, in compute
    packed_fn_values = fn(packed_values)
  File ""/Users/itdxer/Downloads/jacobian_fail_example.py"", line 7, in <lambda>
    fn=lambda x, params=parameters: tf.gradients(x, params),
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 611, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 377, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 611, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 105, in _TensorArrayReadGrad
    w_g = g.write(index, grad)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 879, in write
    return self._implementation.write(index, value, name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 278, in write
    name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 7385, in _tensor_array_write_v3
    flow_in=flow_in, name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3270, in create_op
    op_def=op_def)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'jacobian/while/TensorArrayReadV3', defined at:
  File ""/Users/itdxer/Downloads/jacobian_fail_example.py"", line 23, in <module>
    Js = jacobians(errors, [weight, bias])
[elided 4 identical lines from previous traceback]
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2953, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 402, in compute
    packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py"", line 402, in <listcomp>
    packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 58, in fn
    return method(self, *args, **kwargs)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 58, in fn
    return method(self, *args, **kwargs)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 58, in fn
    return method(self, *args, **kwargs)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 861, in read
    return self._implementation.read(index, name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 260, in read
    name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 6468, in _tensor_array_read_v3
    dtype=dtype, name=name)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3270, in create_op
    op_def=op_def)
  File ""/Users/itdxer/.pyenv/versions/neupy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): TensorArray jacobian/TensorArray_1@jacobian/while/gradients: Could not write to TensorArray index 1 because it has already been read.
	 [[Node: jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](jacobian/while/gradients/jacobian/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3, jacobian/while/Identity, jacobian/while/gradients/Fill, jacobian/while/TensorArrayReadV3/Enter_1)]]
```"
17393,"import tensorflow failed, ""ImportError: DLL load failed"". Even after install visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3.","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10 x64
- **TensorFlow installed from (source or binary)**:  pip install --upgrade tensorflow
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
I was trying to install tensorflow **cpu version** on windows10, but always got error when import tensorflow.

I read the [common_installation_problems](https://www.tensorflow.org/install/install_windows#common_installation_problems) , tried many solution I found on github, stackoverflow, etc. I install visual studio 2015, visual studio 2017, [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587)(both 32 and 64),  msvcp140.dll can find in both System32 and SysWow64 folder. But still can't import tensorflow.

Is there something I missed out?


### Source code / logs
>import tensorflow error info:

```
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```


>run tensorflow_self_check.py result:

```
ERROR: Failed to import the TensorFlow module.

- Python version is 3.6.

- TensorFlow is installed at: C:\Users\sss\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow

- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Download and install CUDA 8.0 from
  this URL: https://developer.nvidia.com/cuda-toolkit

- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that
  this DLL be installed in a directory that is named in your %PATH%
  environment variable. Typically it is installed in 'C:\Windows\System32'.
  If it is not present, ensure that you have a CUDA-capable GPU with the
  correct driver installed.

- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Note that installing cuDNN is a
  separate step from installing CUDA, and it is often found in a
  different directory from the CUDA DLLs. You may install the
  necessary DLL by downloading cuDNN 5.1 from this URL:
  https://developer.nvidia.com/cudnn

- Could not find cuDNN.
```
"
17390,Full-fledged Java api roadmap?,"Are there any estimated plans for Java api development in terms of functionality ? The only absolute choice for Java language nowadays is Deeplearning4j, any plans to occupy a niche for Java language? 
By the term ""full-fledged"" I mean: constructing a graph from scratch (including most common math function/operations, respectively), train CNN, test CNN, recognize the result, all of this using Java language only.
[#14094](https://github.com/tensorflow/tensorflow/pull/14094) and [#16120](https://github.com/tensorflow/tensorflow/pull/16120) is still in work and latter frozen for a few days. Just want to know your vision of future. 

P.s. need to choose right framework for starting a project at the end of June."
17389,Documentation links for version 1.5 silently redirect to 1.6,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA Titan X
- **Exact command to reproduce**: Go to https://www.tensorflow.org/versions/r1.5/api_docs/python

### Describe the problem
The Python and C++ API Documentation links at https://www.tensorflow.org/versions/r1.5/api_docs/ silently redirect you to documentation for version 1.6. The links are (I assume) correct, but the URLs https://www.tensorflow.org/versions/r1.5/api_docs/python and https://www.tensorflow.org/versions/r1.5/api_docs/cc redirect to https://www.tensorflow.org/api_docs/python/ and https://www.tensorflow.org/api_docs/cc/ respectively.

I spent a while trying to diagnose a problem based on the documentation for 1.6, without realizing I was looking at 1.6 rather than 1.5. Turns out the problem was caused by an API change between 1.5 and 1.6. 

### Source code / logs
N/A"
17388,Inconsistent OMPI_SKIP_MPICXX define,"Looks like we `#define OMPI_SKIP_MPICXX` before `#include third_party/mpi/mpi.h` in two places:

https://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi_collectives/kernels/ring.h#L32-L34
https://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc#L36-L37

But it is not defined here:
https://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi/mpi_utils.h#L27

Because the first two are defined without an `#ifndef`, I can't define it in a compiler flag. We should probably add it to the last one, or at least make the first two no-ops if it's already set."
17387,[feature request] add higher order function `scanr`,"Have I written custom code: yes
OS Platform and Distribution: OSX 10.11.6
TensorFlow installed from: conda-forge
TensorFlow version: 1.5
Bazel version: N.A
CUDA/cuDNN version: N/A (CPU only)
GPU model and memory (CPU only)
Exact command to reproduce: N/A

Description:
TensorFlow has `foldl` which folds the array from first to last, and `foldr` which folds the array from last to first. It also has `scan` which returns the trace of `foldl`, but there is no `scanr` that returns the trace of `foldr`. 

`scanr` can be proven useful sometimes. Look at the following snippet that is sampled from my implementation of a [crf lstm sequence tagger](https://github.com/shengc/tf-lstm-crf-tagger/blob/master/lstm-crf-tagger.ipynb).

```python
reverse_path = tf.scan(step_path, tf.reverse(backpointers, axis=[0]), initializer=best_tag_id, back_prop=False)
best_path = tf.concat([tf.reverse(reverse_path, axis=[0])[1:], [best_tag_id]], axis=0)
```

I have to call `tf.reverse` twice, just because TensorFlow only provides `scan` from first to last.
"
17386,Tensorflow 1.6.0 cpu fails on import on Windows 10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, problem appears on import (import tensorflow as tf).
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary (pip --no-cache-dir) install --upgrade tensorflow
- **TensorFlow version (use command below)**: 1.6.0 (install says tensorflow-1.6.0-cp36-cp36m-win_amd64.whl)
- **Python version**: 3.6.4 x64
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When installing tensorflow 1.6.0 the import reports problems. See log below. Tried uninstalled r1.6 and reinstalled 1.5.0 to see if something else might be the problem. 1.5.0 works like a charm. Tried a clean install of 1.6.0 without any luck.

Ran the 'tensorflow_self_check.py' script. See log below.

### Source code / logs

```
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



------------------------

When running 'tensorflow_self_check.py' this is what is reported. Supposed to have installed the CPU version so cuda is not installed.

ERROR: Failed to import the TensorFlow module.

- Python version is 3.6.

- TensorFlow is installed at: C:\Users\Jonas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow

- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Download and install CUDA 8.0 from
  this URL: https://developer.nvidia.com/cuda-toolkit

- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Note that installing cuDNN is a
  separate step from installing CUDA, and it is often found in a
  different directory from the CUDA DLLs. You may install the
  necessary DLL by downloading cuDNN 5.1 from this URL:
  https://developer.nvidia.com/cudnn

- Could not find cuDNN.
```"
17384,broken Qualcomm link on tensorflow.org,"On tensorflow.org there is a logo with a broken link to https://qualcomm.com/ (ERR_TIMED_OUT)
Looks like it should be https://www.qualcomm.com/

Sorry, if creating an issue was wrong, but I did not find any contact possibilities.
"
17383,Performance dropped in cifar10 in TF 1.6.0 compiled with MKL,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7/3.6
- **Bazel version (if compiling from source)**: Build label: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
```
I enabled Jemalloc, XLA JIT, Amazon S3 support during configuration.
```
bazel build --config=opt --config=mkl --config=monolithic --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
sudo pip install /tmp/tensorflow_pkg/tensorflow-1.6.0-cp27-cp27mu-linux_x86_64.whl
```
```
export OMP_NUM_THREADS=72
export KMP_AFFINITY=granularity=fine,verbose,compact,1,0
export KMP_BLOCKTIME=1
export KMP_SETTINGS=1
export OMP_PROC_BIND=true

git clone https://github.com/tensorflow/models.git
cd models/tutorials/image/cifar10
python cifar10_train.py
```
### Hardware information
-**Model Name**: Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
-**CPU(s)**: 72
-**Architecture**:x86_64

### Describe the problem
I work with an AWS C5.18xlarge instance. 
With TensorFlow 1.5.0 compiled with MKL and setting MKL parameters shown above, I can get a ~2000 examples/sec speed when running cifar10_train.py. But for TensorFlow 1.6.0 with exactly same building steps, I can only get speed of ~300 examples/sec.


"
17382,https://www.tensorflow.org/versions/r1.6/ Dead link on versions page,"On https://www.tensorflow.org/versions, there's a link to 
https://www.tensorflow.org/versions/r1.6/
This link is 404"
17381,"Cannot feed value of shape (64, 25, 9) for Tensor 'Placeholder:0', which has shape '(?, 25, 25)'","i am trying to train my RNN-LSTM model in python 3.5, this is my code and my dataset is a 3D accelerometer dataset

```
X = tf.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])
Y = tf.placeholder(tf.float32, [None, config.n_classes])

with tf.Session() as sess:
    tf.global_variables_initializer().run()

    for epoch in range(training_epochs):
        cost_history = np.empty(shape=[0],dtype=float)
        for b in range(total_batches):
            offset = (b * config.batch_size) % (train_y.shape[0] - config.batch_size)
            batch_x = train_x[offset:(offset + config.batch_size), :, :]
            batch_y = train_y[offset:(offset + config.batch_size), :]

            print (""batch_x shape ="",batch_x.shape)
            print (""batch_y shape ="",batch_y.shape)

            _, c = sess.run([optimizer, cost],feed_dict={X: batch_x, Y : batch_y})
            cost_history = np.append(cost_history,c)
        loss_over_time[epoch] = np.mean(cost_history)
```
but it gives me the following error

```
Traceback (most recent call last):
  File ""C:\Users\hp\Downloads\Deep-Learning-for-Human-Activity-Recognition-master\ModelCreation\RNN\FFLSTM\fflstm.py"", line 250, in <module>
    _, c = sess.run([optimizer, cost],feed_dict={X: batch_x, Y : batch_y})
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1100, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (64, 25, 9) for Tensor 'Placeholder:0', which has shape '(?, 25, 25)'

```
and this is the shapes of my dataset
```
n_inputs len(X_train[0][0]) 25
batch_x shape = (64, 25, 9)
batch_y shape = (64, 2)
X <tf.Tensor 'Placeholder:0' shape=(?, 25, 25) dtype=float32>
Y <tf.Tensor 'Placeholder_1:0' shape=(?, 2) dtype=float32>
```"
17379,`tf.case` is not allowing the computation of the `default` tensor when it falls in the default case,"```
== cat /etc/issue ===============================================
Linux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.6.0rc1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0-rc1
tf.GIT_VERSION = v1.6.0-rc0-19-gecec1d8
tf.COMPILER_VERSION = v1.6.0-rc0-19-gecec1d8
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

### Describe the problem

`tf.case` does not seem to be computing the graph tensor related with the `default` parameter when it should fall in it. It seems to be computing all the case tensors correctly in all the other cases


### Source code / logs

The script optimizes a piecewise rectilinear function defined with `tf.case` to match the square of the argument as closely as possible in the interval (-10, 10). If the argument is greater or equal than 10, the case should be handled by the `default` argument tensor, which in the example below is `O1`, which is also the case tensor when the argument is between [-5, -1). After the function has been optimized, the script outputs the value of the piecewise function in the range (-100, 100), and we see that in the interval [10, 100), O1 seems to have the constant value -1.0 that does not depend on the input, while in the range [-5, -1) it correctly depends on it


```
import sys
import tensorflow as tf
from tensorflow.python import debug as tf_debug
import numpy as np

initia = tf.random_normal_initializer(5e-3, 1e-4)
bias_init = tf.random_normal_initializer(-1e-3, 5*1e-4)
cdtype = tf.float32
DEPTH_1 = 1
OUT_DEPTH = 1

def act(inp, **kwargs):
  return tf.nn.elu(inp, name = kwargs['name'])

I = tf.placeholder(cdtype, shape=[None,1], name='I') # input

W = tf.get_variable('W', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b = tf.get_variable('b', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O = act(tf.matmul(I, W) + b, name='O') # activation / output

W1 = tf.get_variable('W1', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b1 = tf.get_variable('b1', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O1 = act(tf.matmul(I, W1) + b1, name='O1')

W2 = tf.get_variable('W2', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b2 = tf.get_variable('b2', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O2 = act(tf.matmul(I, W2) + b2, name='O2')

W3 = tf.get_variable('W3', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b3 = tf.get_variable('b3', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O3 = act(tf.matmul(I, W3) + b3, name='O3')

W4 = tf.get_variable('W4', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b4 = tf.get_variable('b4', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O4 = act(tf.matmul(I, W4) + b4, name='O4')

W5 = tf.get_variable('W5', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights
b5 = tf.get_variable('b5', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases
O5 = act(tf.matmul(I, W5) + b5, name='O5')

eval_inp = tf.gather_nd(I,[[0,0]])

c1 = tf.gather_nd(tf.less(I , [[-5.0]]), [[0,0]])[0]
c2 = tf.gather_nd(tf.less(I , [[-1.0]]), [[0,0]])[0]
c3 = tf.gather_nd(tf.less(I , [[0.0]]), [[0,0]])[0]
c4 = tf.gather_nd(tf.less(I , [[1.0]]), [[0,0]])[0]
c5 = tf.gather_nd(tf.less(I , [[5.0]]), [[0,0]])[0]
c6 = tf.gather_nd(tf.less(I , [[10.0]]), [[0,0]])[0]

f1 = lambda: O
f2 = lambda: O1
f3 = lambda: O2
f4 = lambda: O3
f5 = lambda: O4
f6 = lambda: O5
fdef = lambda: O1

caseFun = tf.case([(c1 , f1), (c2 , f2), (c3 , f3), (c4, f4), (c5, f5), (c6,f6)], default=fdef)

distance = tf.reduce_mean( tf.square( caseFun - tf.square(eval_inp) ) )

train_op = tf.train.AdamOptimizer(1e-3).minimize(distance)
init_op = tf.global_variables_initializer()


with tf.Session() as sess:
  sess.run(init_op)

  for i in range(10000):
    s = sess.run([
      train_op,
      I, caseFun, distance
      ], feed_dict={ I: 20.0*np.random.rand(1,1) - 10.0})
    if i % 1000 == 0:
      print s

  print ""W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5""
  print sess.run([W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5], feed_dict={})
  for i in np.arange(-100.0, 100.0):
    print sess.run([I, caseFun, distance , O1
      ], feed_dict={ I: [[i]]})
```

The output looks like this:

```
....
[array([[-10.]], dtype=float32), array([[43.378143]], dtype=float32), 3206.0347, array([[27.115139]], dtype=float32)]
[array([[-9.]], dtype=float32), array([[39.456512]], dtype=float32), 1725.8613, array([[24.640066]], dtype=float32)]
[array([[-8.]], dtype=float32), array([[35.53488]], dtype=float32), 810.26294, array([[22.164993]], dtype=float32)]
[array([[-7.]], dtype=float32), array([[31.613249]], dtype=float32), 302.2991, array([[19.68992]], dtype=float32)]
[array([[-6.]], dtype=float32), array([[27.691616]], dtype=float32), 69.02924, array([[17.214848]], dtype=float32)]
[array([[-5.]], dtype=float32), array([[14.739774]], dtype=float32), 105.27224, array([[14.739774]], dtype=float32)]
[array([[-4.]], dtype=float32), array([[12.264701]], dtype=float32), 13.952459, array([[12.264701]], dtype=float32)]
[array([[-3.]], dtype=float32), array([[9.789628]], dtype=float32), 0.62351245, array([[9.789628]], dtype=float32)]
[array([[-2.]], dtype=float32), array([[7.314555]], dtype=float32), 10.986276, array([[7.314555]], dtype=float32)]
[array([[-1.]], dtype=float32), array([[0.6768117]], dtype=float32), 0.10445068, array([[4.8394823]], dtype=float32)]
[array([[0.]], dtype=float32), array([[0.02995399]], dtype=float32), 0.00089724176, array([[2.3644097]], dtype=float32)]
[array([[1.]], dtype=float32), array([[4.860016]], dtype=float32), 14.899722, array([[-0.10475975]], dtype=float32)]
[array([[2.]], dtype=float32), array([[7.3534083]], dtype=float32), 11.245347, array([[-0.9246594]], dtype=float32)]
[array([[3.]], dtype=float32), array([[9.846801]], dtype=float32), 0.7170716, array([[-0.99365956]], dtype=float32)]
[array([[4.]], dtype=float32), array([[12.340193]], dtype=float32), 13.394189, array([[-0.9994664]], dtype=float32)]
[array([[5.]], dtype=float32), array([[24.035427]], dtype=float32), 0.9304009, array([[-0.9999551]], dtype=float32)]
[array([[6.]], dtype=float32), array([[28.001446]], dtype=float32), 63.97687, array([[-0.99999624]], dtype=float32)]
[array([[7.]], dtype=float32), array([[31.96746]], dtype=float32), 290.1074, array([[-0.9999997]], dtype=float32)]
[array([[8.]], dtype=float32), array([[35.93348]], dtype=float32), 787.72955, array([[-1.]], dtype=float32)]
[array([[9.]], dtype=float32), array([[39.899498]], dtype=float32), 1689.2512, array([[-1.]], dtype=float32)]
[array([[10.]], dtype=float32), array([[-1.]], dtype=float32), 10201.0, array([[-1.]], dtype=float32)]
[array([[11.]], dtype=float32), array([[-1.]], dtype=float32), 14884.0, array([[-1.]], dtype=float32)]
.....
```"
17378,Please explain what is going on in tf.nn.raw_rnn function?,"I am trying to implement custom hidden state computation with a help of `tf.nn.raw_rnn` function. However, using the API example provided I am receiving a strange message:

```
<tensorflow.python.util.tf_should_use._add_should_use_warning.<locals>.TFShouldUseWarningWrapper at 0x7f80ac138dd8>
```


![2018-03-02-143635_1049x358_scrot](https://user-images.githubusercontent.com/7676160/36901389-45c88efc-1e27-11e8-93b1-f42c3018057e.png)

What I want to do is the following:

```
                    |--   hidden recurrent layer   --|
[10 x 32 x 100] --> LSTM cell [200] + Linear [200 x 1] --> [10 x 32 x 1]
```

Here is my implementation, well basically it is the example from official docs:

```python
   outputs, state = self._get_raw_rnn_graph(inputs, config, is_training)

   def _get_raw_rnn_graph(self, inputs, config, is_training):
        time = tf.constant(0, dtype=tf.int32)
        # define placeholders
        #_inputs = tf.placeholder(shape=(config.num_steps, config.batch_size, config.input_size),
        #                         dtype=tf.float32)
        #_batch_len = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32)
        _inputs_ta = tf.TensorArray(dtype=tf.float32, size=config.num_steps, name=""TA"")
        _inputs_ta = _inputs_ta.unstack(inputs)  # <-- throws a warning 

        # create simple LSTM cell
        cell = tf.contrib.rnn.LSTMCell(config.hidden_size)

        # create loop_fn for raw_rnn
        def loop_fn(time, cell_output, cell_state, loop_state):
            emit_output = cell_output  # == None if time = 0

            if cell_output is None:  # time = 0
                next_cell_state = cell.zero_state(config.batch_size, tf.float32)
                self._initial_state = next_cell_state
            else:
                next_cell_state = cell_state

            elements_finished = (time >= config.num_steps)
            finished = tf.reduce_all(elements_finished)
            next_input = tf.cond(finished,
                                 lambda: tf.zeros([config.batch_size, config.input_size],
                                                   dtype=tf.float32),
                                 lambda: _inputs_ta.read(time))

            # apply linear + sig transform here
            print(""before lin+sig"", next_input)
            next_input = self._linear_transform(next_input)  # [32, 200] --> [32, 1]
            print(""after lin+sig"", next_input)

            next_loop_state = None
            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)

        outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)
        outputs = outputs_ta.stack()
        return outputs, final_state
```

The line `_inputs_ta = _inputs_ta.unstack(inputs)` causes the message above and I wonder if this is actually a bug or am I doing something wrong completely. The end result is that I am not getting the correct output shape because the `next_input` comes as `[32, 100]` into my linear transform function as if LSTM cell was never applied to it. Please clarify whether `raw_rnn` is usable under Tensorflow 1.5.0. 
"
17377,Attempt failed to use tf.ConditionalAccumulator.take_grad() with GradientDescentOptimizer.apply_gradients,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Home 64-bit
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
8.0/6.0
- **GPU model and memory**:
NVIDIA 1050Ti 4Gb
- **Exact command to reproduce**:
N/A

### Describe the problem
I have implemented 3 convolutional neural network and I have used one of the `tf.ConditionalAccumulator` for each network to aggregate the partial gradients during the training for every network. More precisely, I have used `tf.GradientDescentOptimizer.compute_gradients(<a_network_loss>,<trainable_var_of_a_network>)` to take these partial gradients **feeding** the necessary placeholders to calculate the loss with partial values.

After accumulating all the necessary gradients for all networks, I have used `tf.ConditionalAccumulator.take_grad()` to take the whole gradients for each `tf.ConditionalAccumulator`, then I have zipped all the gradients with variables name and finally I have used `tf.GradientDescentOptimizer.apply_gradients(<zip>)`.
Tensorflow returns error when get called the apply_gradients because it wants  that palceholders will be **feeded** again

### Source code 
This code is just an example to show the working of my code for one tf.ConditionalAccumulator:

```python
optimizer=tf.GradientDescentOptimizer(lr)
accumulator=tf.ConditionalAccumulator(tf.float32,shape=[3,3,3,1,64])
grad_ph=tf.placeholder(tf.float32)        
accumulator_add_gradient_op=accumulator.apply_grad(grad)
grads_vars_=self.optimizer.compute_gradients(first_net.loss, var_list=first_net.vars)
grads_list=[grad_var[0] for grad_var in grads_vars]
vars_list=[grad_var[1] for grad_var in grads_vars]

count_ph=tf.placeholder(tf.int32,[])
grads_us=tf.unstack(accumulator.take_grad(count_ph))

n=0
while(...):
    ...
    grads=sess.run(grads_list,{<feeding_placeholders_with_partial_values>})
    sess.run(accumulator_add_gradient_op,{grad_ph:grads}
    n+=1
grads_and_vars=list(zip(sess.run(grads_us,{count_ph:n}),vars_list))
sess.run(optimizer.apply_gradients(grads_and_vars))
```

I have used `list` in `grads_and_vars=list(zip(sess.run(grads_us,{count_ph:n}),vars_list))` because i need to use `grads_and_vars.extend(<other_grads_and_vars>)` for other tf.ConditionalAccumulator. I tried not used sess.run() in zip(), but have the same problem.

### logs
```
2018-03-02 11:52:16.233461: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.233946: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.234279: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.234580: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.235302: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.235649: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.235928: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.236210: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.236497: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.236802: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.237108: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.237419: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.237746: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.238071: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.238404: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.238748: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.240460: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.240933: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.241344: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.241868: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.242252: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.242531: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.242811: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.243095: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.243858: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.244213: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.244493: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.244773: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.245057: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.245343: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.246162: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.246591: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.246872: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.247152: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.247492: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.248138: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.248582: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.249021: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.249406: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.249970: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.250396: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.250990: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.251452: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.252026: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.252518: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.253006: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.253455: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.253760: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.254429: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.254957: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.255390: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.255867: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
2018-03-02 11:52:16.256557: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
Traceback (most recent call last):
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_958_first_CCNN/conv4/weights/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\main.py"", line 44, in <module>
    s.train(args)
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\temp.py"", line 46, in train
    self.train_all(args)    
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\temp.py"", line 416, in train_all
    _,total_loss,summary=self.sess.run([self.optimizer.apply_gradients(grads_and_vars),self.loss,self.summary_op],{self.learning_rate:lr})
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_958_first_CCNN/conv4/weights/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'gt1', defined at:
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\main.py"", line 42, in <module>
    load_first_model=args.load_first_model)
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\temp.py"", line 31, in __init__
    self.first_CCNN=CCNN(sess,load_model=self.load_first_model,patch_size=self.patch_size,batch_size=self.batch_size,isTraining=self.train_first_model,version=self.first_CCNN_version,model_name=first_CCNN_model_path)
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\model.py"", line 36, in __init__
    self.build()
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\model.py"", line 46, in build
    self.CCNN_build()
  File ""D:\User\Desktop\LiClipse Workspace\CCNN+\src\model.py"", line 127, in CCNN_build
    self.gt = tf.placeholder(tf.float32, [None,1,1, 1], name='gt1')
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3090, in _placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""D:\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]
	 [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_958_first_CCNN/conv4/weights/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```"
17375,"tensorflow lite cannot use my own model, crashed without error log.","**Version Info:** 
tensorflow r1.5 
ubuntu 14.04 
armv7 platform 
android 6.0.1 and pc
ndk version: r14 
android studio 2.3.1 

**Describe the problem**
i write the code in c++, it can get results when using the mobilenet model from the tutorails(link: https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)
but when i use my own model converted by toco, it crashed without any information. set input and load model seemed correct, but invoke failed.

**code**

**network define**
network is defined by tf.slim framework:
```
 def lite_v1(images, num_classes=10, is_training=False, dropout_keep_prob=0.5, prediction_fn=slim.softmax, scope='lite_v1'):
    end_points = {}
      with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):
        images_input = tf.placeholder_with_default(images, shape=[None, 39, 39, 1], name='InputPlaceholder')
        net = slim.conv2d(images_input, 16, [4, 4], padding='VALID', scope='conv1')
        end_points['conv1'] = net
        feature = slim.flatten(net)
    with tf.variable_scope('Logits'):
      net = slim.fully_connected(feature, 100, scope='fc3')
      logits = slim.fully_connected(net, num_classes,
                              biases_initializer=tf.zeros_initializer(),
                              weights_regularizer=None,
                              activation_fn=None,
                              scope='logits')
      end_points['Logits'] = logits
    output = tf.multiply(logits, 1, name=""Output"")
    end_points['Output'] = output
  return logits, end_points
```
**model convert**
i trained it and use the freeze_graph.py and optimize_for_inference.py scripe in tensorflow/python/tools/ and get a frozen .pb file. then i convert it to a .tflite file by following the tutorials in github: https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite just like:
```
bazel build tensorflow/contrib/lite/toco:toco
bazel-bin/tensorflow/contrib/lite/toco/toco -- \
  --input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3
```

**c++ test code**
after get a .tflite file. and then i wrote the c++ code to read the model and image from sdcard:
```
void LoadImageFromFile(std::string file_name, std::vector<uint8_t>& output, int& out_width, int& out_height, int& out_channels)
{
cv::Mat image = cv::imread(file_name);
cv::Mat gray;
cv::cvtColor(image, gray, cv::COLOR_BGR2GRAY);

out_width = gray.cols;
out_height = gray.rows;
out_channels = gray.channels();
for(int nrow =0; nrow < out_height; nrow++)
    for(int ncol = 0; ncol < out_width; ncol++)
        output.push_back(gray.at<unsigned char>(nrow,ncol));
}
size_t writeByteBuffer(uint8_t* in, char** dst, int dst_size) {
  char* buf = (char*)in;
  if (!buf) {
    return 0;
  }
  *dst = buf;
  return dst_size;
}

int RunInferenceOnImage()
{
const int num_threads = 1;
std::string input_layer_type = ""float"";
std::vector<int> sizes = {1, 39, 39, 1};
//    std::vector<int> sizes = {1, 224, 224, 3};
std::string graph_path = ""/storage/emulated/0/DCIM/fastnetv2.tflite""; //mobilenet_quant_v1_224.tflite
std::unique_ptr<tflite::FlatBufferModel> model(tflite::FlatBufferModel::BuildFromFile(graph_path.c_str()));
if (!model)
{
    LOGD(""bkzero jni: , Failed to mmap model %s"", graph_path.c_str());
}
model->error_reporter();

#ifdef TFLITE_CUSTOM_OPS_HEADER
  tflite::MutableOpResolver resolver;
  RegisterSelectedOps(&resolver);
#else
  tflite::ops::builtin::BuiltinOpResolver resolver;
#endif

LOGD(""bkzero jni: , %s"", ""resolved reporter end"");

std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);
if (!interpreter)
{
    LOGD(""bkzero jni: , %s"", ""Failed to construct interpreter"");
}
if (num_threads != -1) {
  interpreter->SetNumThreads(num_threads);
}
int input = interpreter->inputs()[0];
if (input_layer_type != ""string"") {
  interpreter->ResizeInputTensor(input, sizes);
}

if (interpreter->AllocateTensors() != kTfLiteOk)
{
    LOGD(""bkzero jni: , %s"", ""Failed to allocate tensors!"");
}
std::string image_path = ""/storage/emulated/0/DCIM/test2.jpg"";
int image_width;
int image_height;
int image_channels;
std::vector<uint8_t> image_data;
LoadImageFromFile(image_path, image_data, image_width, image_height, image_channels);

//    const int wanted_width = 224;
//    const int wanted_height = 224;
//    const int wanted_channels = 3;
//    const float input_mean = 127.5f;
//    const float input_std = 127.5f;

const int wanted_width = 39;
const int wanted_height = 39;
const int wanted_channels = 1;
const float input_mean = 128.0f;
const float input_std = 64.0f;
assert(image_channels >= wanted_channels);
uint8_t* in = image_data.data();
float* out = interpreter->typed_tensor<float>(input);
int input_idx = interpreter->inputs()[0];
TfLiteTensor* target = interpreter->tensor(input_idx);
int num_bytes = sizes[0]*sizes[1]*sizes[2]*sizes[3];
writeByteBuffer(in, &(target->data.raw), static_cast<int>(num_bytes));
    if (interpreter->Invoke() != kTfLiteOk)
    {
        LOGD(""bkzero jni: , %s"", ""Failed to invoke!"");
    }
    const std::vector<int>& results = interpreter->outputs();
    if (results.empty()) {
      LOGD(""bkzero jni: , %s"", ""results.empty()"");
    }
    long outputs[results.size()];
    size_t size = results.size();
    for (int i = 0; i < size; ++i) {
      TfLiteTensor* source = interpreter->tensor(results[i]);
      outputs[i] = reinterpret_cast<long>(source);
    }
    uint8_t* final = (uint8_t*)outputs[0];
    for(int i = 0; i < 10; i++)
        LOGD(""bkzero jni: results, %f"", (final[i]/255.0));
    LOGD(""bkzero jni: , %s"", ""interpreter outputs read finished"");
}
```

this code can read and ouput result correct when run the example mobilenet file, mobilenet_quant_v1_224.tflite. but crash when i run my own model and give the error: Fatal signal 7 (SIGBUS), code 1, fault addr 0xdd3dd008 in tid 10528

and on PC, it also crashes, without more information."
17374,Potential resource leaks caused by unclear Java examples,"### System information

Java examples at https://www.tensorflow.org/ for tensorflow 1.6.0.

### Describe the problem

`org.tensorflow.Session.Runner.run()` returns list of closables, Javadoc clearly states that the caller is responsible to free all of them. None of the Java examples I found at https://www.tensorflow.org/ highlights that, I realized it by happy accident during in-depth reading implementation in Session.java quite long time after I wrote my code that uses TensorFlow.

```
    /**
     * Execute the graph fragments necessary to compute all requested fetches.
     *
     * <p><b>WARNING:</b> The caller assumes ownership of all returned {@link Tensor}s, i.e., the
     * caller must call {@link Tensor#close()} on all elements of the returned list to free up
     * resources.
     *
     * ...
     */
    public List<Tensor<?>> run() {
      return runHelper(false).outputs;
    }
```

I'm not sure if the examples them-self contain any resource leak or not, they free only the first element of the list, but there may be more of them (in general). I would expect an explicit loop properly freeing all the returned resources there.

Such examples for beginners should be as explicit as possible, 100% clear and understandable for anyone. A lot of people (like me) base core of their code on them which may easily introduce significant resource leak bugs to their applications.

- https://www.tensorflow.org/install/install_java, HelloTF example
- https://www.tensorflow.org/install/install_java, referenced advanced example LabelImage
- https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java

### Source code / logs

None.
"
17373,Illegal instruction on import tensorflow,"I had installed a non-optimized version of tensorflow (1.6.0) on debian (Debian GNU/Linux 8) for use within python 3.6.4.
I was told that SSE4.1 and SSE4.2 instructions were supported by my CPU but that the tensorflow I was using was not compiled to support it, so I decided to use one of the pre-compiled whl's.
This resulted in a ""illegal instruction"" error which exits python immediately.
I tried uninstalling and reinstalling the original version but the error message keeps coming back...
I cleared the pip cache as well, yet still nothing seems to help.

Any idea on how to resolve this? 
It almost appears like uninstalling doesn't remove everything?
(I know some may suggest that I work with docker, yet that still leaves me with a broken environment...)

OS Platform and Distribution: Debian GNU/Linux 8
Tensorflow installed from pip: pip install --no-cache-dir tensorflow (after uninstalling everything TF related)
TF version: 1.6.0
Bazel: not installed
CUDA/cuDNN: not installed
GPU model: not available
Exact command to reproduce: import tensorflow as tf"
17372,tf.svd and matlab svd return different results with the same input. Why?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0
- **GPU model and memory**: Nvidia GTX 750 Ti, 2G

### Describe the problem
tf.svd and matlab svd return different results with the same input. Why?

### Source code / outputs
**TensorFlow tf.svd**
TensorFlow demo code:
```
import tensorflow as tf

tf.set_random_seed(1)
''' 
original matrix 
X =
 [-0.67086124  0.22357143  0.79727304
   0.09617059  0.72314787  0.33812162
  -0.73006123  0.91153419 -0.50938189
   0.97486407 -2.13402033  0.60229129
  -0.31214711 -0.85811871  1.78664649]
''' 
X = tf.random_normal([5, 3], mean=0.0, stddev=1.0, dtype=tf.float32)

s, u, v = tf.svd(X)


with tf.Session() as sess:
    print('X=\n', sess.run(X))
    print('s=\n', sess.run(s))
    print('u=\n', sess.run(u))
    print('v=\n', sess.run(v))

```
Matlab output:
```
s=
 [ 3.6135273   2.42769122  0.81916636]
u=
 [[ 0.06457954  0.5192402  -0.50217324]
 [-0.52760589 -0.70443964 -0.21893539]
 [ 0.7611897  -0.46179301 -0.45036247]
 [ 0.32708997 -0.12951432  0.65888679]
 [-0.17625111  0.06424804 -0.25086099]]
v=
 [[ 0.18638727  0.92181212 -0.33988595]
 [ 0.95351493 -0.2531015  -0.16355196]
 [ 0.23678979  0.29360226  0.92613626]]
```
**Matlab svd**
Matlab demo code:
```
X = ....
 [-0.67086124  0.22357143  0.79727304
   0.09617059  0.72314787  0.33812162
  -0.73006123  0.91153419 -0.50938189
   0.97486407 -2.13402033  0.60229129
  -0.31214711 -0.85811871  1.78664649]
[u, s, v] = svd(X)
```
Matlab output:
```
u =
    0.0060    0.5579    0.2153    0.4231   -0.6807
   -0.1317    0.2258   -0.8417    0.4338    0.1873
   -0.3964    0.1323    0.4871    0.5077    0.5747
    0.7643   -0.3502    0.0765    0.5309    0.0739
    0.4913    0.7054    0.0457   -0.3051    0.4072
s =
    3.0475         0         0
         0    1.8932         0
         0         0    0.6537
         0         0         0
         0         0         0
v =
    0.2837   -0.5339   -0.7965
   -0.8229    0.2909   -0.4880
    0.4923    0.7939   -0.3568
```"
17370,Image retraining script memory problem and issue,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
      Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6-gpu/nightly-gpu
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.04
- **GPU model and memory**: Tesla K80 / 11441MiB
- **Exact command to reproduce**:
python retrain_quantize.py w/ certain parameters.


### Describe the problem
I was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)
1. Different memory usage in different version.
I opened allow_growth parameter in order to trace memory usage during training.
In tf-gpu-1.6.0 :

+-----------------------------------------------------------------------------+
| Processes:                                                                                      GPU Memory |
|  GPU       PID   Type   Process name                                                    Usage      |
|======================================================== |
|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |
|    0     15440      C   python                                                                     302MiB |
+-----------------------------------------------------------------------------+

But in tf-nightly-gpu:

+-----------------------------------------------------------------------------+
| Processes:                                                                                      GPU Memory |
|  GPU       PID   Type   Process name                                                    Usage      |
|======================================================== |
|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |
|    0     15747      C   python                                                                    4152MiB |
+-----------------------------------------------------------------------------+


I was wondering what causes the tremendous difference in these two versions?

2. Per the traceback below, my retraining process could not be done in the last step. 
    Due to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart 
    cause the loss of DecodeJPGInput tensor?

```
After last-1 steps, system shows:
2018-03-02 04:49:23.569386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-03-02 04:49:23.569450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-03-02 04:49:23.569460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-03-02 04:49:23.569464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-03-02 04:49:23.569688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10750 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)

```
Thanks in advance!

### Source code / logs
```
Traceback (most recent call last):
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1080, in _run
    subfeed, allow_tensor=True, allow_operation=False)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3458, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3537, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""retrain_quantize.py"", line 355, in create_bottleneck_file
    resized_input_tensor, bottleneck_tensor)
  File ""retrain_quantize.py"", line 290, in run_bottleneck_on_image
    {image_data_tensor: image_data})
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1083, in _run
    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""retrain_quantize.py"", line 1411, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""retrain_quantize.py"", line 1219, in main
    export_model(model_info, class_count, FLAGS.saved_model_dir)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1558, in __exit__
    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5059, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 4869, in get_controller
    yield default
  File ""/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5059, in get_controller
    yield g
  File ""retrain_quantize.py"", line 1211, in main
    bottleneck_tensor)
  File ""retrain_quantize.py"", line 813, in run_final_eval
    bottleneck_tensor, FLAGS.architecture))
  File ""retrain_quantize.py"", line 505, in get_random_cached_bottlenecks
    resized_input_tensor, bottleneck_tensor, architecture)
  File ""retrain_quantize.py"", line 400, in get_or_create_bottleneck
    bottleneck_tensor)
  File ""retrain_quantize.py"", line 358, in create_bottleneck_file
    str(e)))
RuntimeError: Error during processing file /home/cheyu.lin/master_dataset/dog/source_dog_wash_011682.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(""DecodeJPGInput:0"", dtype=string) is not an element of this graph.)
```"
17369,TF 1.6 cannot be installed on MacOS because of failure in installing grpcio,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.3
- **TensorFlow installed from (source or binary)**: both
- **TensorFlow version (use command below)**: 1.6
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.11
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: - 
- **Exact command to reproduce**:

On MacOS, type:
`sudo pip3 install tensorflow`
pip3 will start installing grpcio, but an error during the installation of grpcio (see below) will prevent the complete installation of tensorflow.

(This does not happen on TF 1.5, which doesn't seem to use grpcio)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
`MacBook-Pro:grpc feranick$ sudo pip3 install --upgrade tensorflow
The directory '/Users/feranick/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/feranick/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Collecting tensorflow
  Downloading tensorflow-1.6.0-cp36-cp36m-macosx_10_11_x86_64.whl (43.2MB)
    100% |████████████████████████████████| 43.2MB 37kB/s 
Requirement already up-to-date: absl-py>=0.1.6 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: wheel>=0.26 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: termcolor>=1.1.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: astor>=0.6.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: protobuf>=3.4.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: tensorboard<1.7.0,>=1.6.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: gast>=0.2.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.13.3 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)
Collecting grpcio>=1.8.6 (from tensorflow)
  Downloading grpcio-1.10.0.tar.gz (14.0MB)
    100% |████████████████████████████████| 14.0MB 110kB/s 
Requirement already up-to-date: setuptools in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)
Installing collected packages: grpcio, tensorflow
  Running setup.py install for grpcio ... error
    Complete output from command /opt/local/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/tmp/pip-build-da0d116h/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-m90opqn_-record/install-record.txt --single-version-externally-managed --compile:
    Found cython-generated files...
    running install
    running build
    running build_py
    running build_project_metadata
    creating python_build
    creating python_build/lib.macosx-10.13-x86_64-3.6
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_channel.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_common.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_plugin_wrapping.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_interceptor.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_grpcio_metadata.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_server.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    copying src/python/grpcio/grpc/_auth.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/_server_adaptations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/interfaces.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/_metadata.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/implementations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    copying src/python/grpcio/grpc/beta/_client_adaptations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework
    copying src/python/grpcio/grpc/framework/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython
    copying src/python/grpcio/grpc/_cython/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/callable_util.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/abandonment.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/stream.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/stream_util.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/future.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    copying src/python/grpcio/grpc/framework/foundation/logging_pool.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common
    copying src/python/grpcio/grpc/framework/common/style.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common
    copying src/python/grpcio/grpc/framework/common/cardinality.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common
    copying src/python/grpcio/grpc/framework/common/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces
    copying src/python/grpcio/grpc/framework/interfaces/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face
    copying src/python/grpcio/grpc/framework/interfaces/face/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face
    copying src/python/grpcio/grpc/framework/interfaces/face/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face
    copying src/python/grpcio/grpc/framework/interfaces/face/face.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base
    copying src/python/grpcio/grpc/framework/interfaces/base/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base
    copying src/python/grpcio/grpc/framework/interfaces/base/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base
    copying src/python/grpcio/grpc/framework/interfaces/base/base.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_cygrpc
    copying src/python/grpcio/grpc/_cython/_cygrpc/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_cygrpc
    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_credentials
    copying src/python/grpcio/grpc/_cython/_credentials/roots.pem -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_credentials
    running build_ext
    b'[C]       Compiling third_party/cares/cares/ares__close_sockets.c\n[C]       Compiling third_party/cares/cares/ares__get_hostent.c\n[C]       Compiling third_party/cares/cares/ares__read_line.c\n[C]       Compiling third_party/cares/cares/ares__timeval.c\n[C]       Compiling third_party/cares/cares/ares_cancel.c\n[C]       Compiling third_party/cares/cares/ares_create_query.c\n[C]       Compiling third_party/cares/cares/ares_data.c\n[C]       Compiling third_party/cares/cares/ares_destroy.c\n[C]       Compiling third_party/cares/cares/ares_expand_name.c\n[C]       Compiling third_party/cares/cares/ares_expand_string.c\n[C]       Compiling third_party/cares/cares/ares_fds.c\n[C]       Compiling third_party/cares/cares/ares_free_hostent.c\n[C]       Compiling third_party/cares/cares/ares_free_string.c\n[C]       Compiling third_party/cares/cares/ares_getenv.c\n[C]       Compiling third_party/cares/cares/ares_gethostbyaddr.c\n[C]       Compiling third_party/cares/cares/ares_gethostbyname.c\n[C]       Compiling third_party/cares/cares/ares_getnameinfo.c\n[C]       Compiling third_party/cares/cares/ares_getopt.c\n[C]       Compiling third_party/cares/cares/ares_getsock.c\n[C]       Compiling third_party/cares/cares/ares_init.c\n[C]       Compiling third_party/cares/cares/ares_library_init.c\n[C]       Compiling third_party/cares/cares/ares_llist.c\n[C]       Compiling third_party/cares/cares/ares_mkquery.c\n[C]       Compiling third_party/cares/cares/ares_nowarn.c\n[C]       Compiling third_party/cares/cares/ares_options.c\n[C]       Compiling third_party/cares/cares/ares_parse_a_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_aaaa_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_mx_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_naptr_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_ns_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_ptr_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_soa_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_srv_reply.c\n[C]       Compiling third_party/cares/cares/ares_parse_txt_reply.c\n[C]       Compiling third_party/cares/cares/ares_platform.c\n[C]       Compiling third_party/cares/cares/ares_process.c\n[C]       Compiling third_party/cares/cares/ares_query.c\n[C]       Compiling third_party/cares/cares/ares_search.c\n[C]       Compiling third_party/cares/cares/ares_send.c\n[C]       Compiling third_party/cares/cares/ares_strcasecmp.c\n[C]       Compiling third_party/cares/cares/ares_strdup.c\n[C]       Compiling third_party/cares/cares/ares_strerror.c\n[C]       Compiling third_party/cares/cares/ares_timeout.c\n[C]       Compiling third_party/cares/cares/ares_version.c\n[C]       Compiling third_party/cares/cares/ares_writev.c\n[C]       Compiling third_party/cares/cares/bitncmp.c\n[C]       Compiling third_party/cares/cares/inet_net_pton.c\n[C]       Compiling third_party/cares/cares/inet_ntop.c\n[C]       Compiling third_party/cares/cares/windows_port.c\n[AR]      Creating /private/tmp/pip-build-da0d116h/grpcio/libs/opt/libares.a\n[C]       Compiling src/boringssl/err_data.c\n'
    b""In file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:115:\n/opt/local/include/openssl/e_os2.h:276:11: error: 'OPENSSL_EXPORT' macro redefined [-Werror,-Wmacro-redefined]\n#  define OPENSSL_EXPORT extern\n          ^\nthird_party/boringssl/include/openssl/base.h:178:9: note: previous definition is here\n#define OPENSSL_EXPORT\n        ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:83:31: error: redefinition of typedef 'ASN1_INTEGER' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_INTEGER;\n                              ^\nthird_party/boringssl/include/openssl/base.h:253:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_INTEGER;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:84:31: error: redefinition of typedef 'ASN1_ENUMERATED' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_ENUMERATED;\n                              ^\nthird_party/boringssl/include/openssl/base.h:249:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_ENUMERATED;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:85:31: error: redefinition of typedef 'ASN1_BIT_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_BIT_STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:247:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_BIT_STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:86:31: error: redefinition of typedef 'ASN1_OCTET_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_OCTET_STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:254:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_OCTET_STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:87:31: error: redefinition of typedef 'ASN1_PRINTABLESTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_PRINTABLESTRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:255:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_PRINTABLESTRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:88:31: error: redefinition of typedef 'ASN1_T61STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_T61STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:257:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_T61STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:89:31: error: redefinition of typedef 'ASN1_IA5STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_IA5STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:252:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_IA5STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:90:31: error: redefinition of typedef 'ASN1_GENERALSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_GENERALSTRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:251:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_GENERALSTRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:91:31: error: redefinition of typedef 'ASN1_UNIVERSALSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_UNIVERSALSTRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:259:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_UNIVERSALSTRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:92:31: error: redefinition of typedef 'ASN1_BMPSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_BMPSTRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:248:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_BMPSTRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:93:31: error: redefinition of typedef 'ASN1_UTCTIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_UTCTIME;\n                              ^\nthird_party/boringssl/include/openssl/base.h:260:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_UTCTIME;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:94:31: error: redefinition of typedef 'ASN1_TIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_TIME;\n                              ^\nthird_party/boringssl/include/openssl/base.h:258:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_TIME;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:95:31: error: redefinition of typedef 'ASN1_GENERALIZEDTIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_GENERALIZEDTIME;\n                              ^\nthird_party/boringssl/include/openssl/base.h:250:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_GENERALIZEDTIME;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:96:31: error: redefinition of typedef 'ASN1_VISIBLESTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_VISIBLESTRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:262:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_VISIBLESTRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:97:31: error: redefinition of typedef 'ASN1_UTF8STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_UTF8STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:261:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_UTF8STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:98:31: error: redefinition of typedef 'ASN1_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef struct asn1_string_st ASN1_STRING;\n                              ^\nthird_party/boringssl/include/openssl/base.h:256:31: note: previous definition is here\ntypedef struct asn1_string_st ASN1_STRING;\n                              ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:99:13: error: redefinition of typedef 'ASN1_BOOLEAN' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef int ASN1_BOOLEAN;\n            ^\nthird_party/boringssl/include/openssl/base.h:242:13: note: previous definition is here\ntypedef int ASN1_BOOLEAN;\n            ^\nIn file included from src/boringssl/err_data.c:18:\nIn file included from /opt/local/include/openssl/err.h:122:\n/opt/local/include/openssl/ossl_typ.h:100:13: error: redefinition of typedef 'ASN1_NULL' is a C11 feature [-Werror,-Wtypedef-redefinition]\ntypedef int ASN1_NULL;\n            ^\nthird_party/boringssl/include/openssl/base.h:243:13: note: previous definition is here\ntypedef int ASN1_NULL;\n            ^\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\n20 errors generated.\nmake: *** [/private/tmp/pip-build-da0d116h/grpcio/objs/opt/src/boringssl/err_data.o] Error 1\n""
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/private/tmp/pip-build-da0d116h/grpcio/setup.py"", line 311, in <module>
        cmdclass=COMMAND_CLASS,
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/__init__.py"", line 129, in setup
        return distutils.core.setup(**attrs)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/install.py"", line 545, in run
        self.run_command('build')
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/command/build_ext.py"", line 78, in run
        _build_ext.run(self)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 186, in run
        _build_ext.build_ext.run(self)
      File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/build_ext.py"", line 339, in run
        self.build_extensions()
      File ""/private/tmp/pip-build-da0d116h/grpcio/src/python/grpcio/commands.py"", line 278, in build_extensions
        raise Exception(""make command failed!"")
    Exception: make command failed!
    
    ----------------------------------------
Command ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/tmp/pip-build-da0d116h/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-m90opqn_-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/tmp/pip-build-da0d116h/grpcio/`"
17368,The best way to pass the LSTM state between batches,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:
### System information
The problem is independent of the system information.

I am using Tensorflow 1.5

### Describe the problem
I think I will be insane. I am trying to find the best way to pass the LSTM state between batches. I have searched everything but I could not find a solution for the current implementation. Imagine I have something like:

```
cells = [rnn.LSTMCell(size) for size in [256,256]
cells = rnn.MultiRNNCell(cells, state_is_tuple=True)
init_state = cells.zero_state(tf.shape(x_hot)[0], dtype=tf.float32)
net, new_state = tf.nn.dynamic_rnn(cells, x_hot, initial_state=init_state ,dtype=tf.float32)
```

Now I would like to pass the `new_state` in each batch efficiently, so without storing it back to memory and then re-feed to tf using `feed_dict`. To be more precise, all the solutions I found use `sess.run` to evaluate `new_state` and  `feed-dict` to pass it into `init_state`. Is there any way to do so without having the bottleneck of using `feed-dict`?

I think I should use `tf.assign` in some way but the doc is incomplete and I could not find any workaround.

I am writing this problem here since It is unsolved everywhere else and I think this should be in the doc since it is a common case scenario. 

I want to thank everybody that will ask in advance.

Cheers,

Francesco Saverio
"
17366,Link error on compile_android_protobuf.sh,"TensorFlow version (use command below): r1.6 

Hi, 
I'm trying to use Makefile to compile android library(following this guideline - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) , but having an issue with compile_android_protobuf.sh. 

I tried with both NDKr16b and NDKr15c, but getting this error. 

>> ~/DL/tensorflow/tensorflow$ tensorflow/contrib/makefile/compile_android_protobuf.sh -c -a x86_64
...
...
/DL/tensorflow/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src/.libs/libprotobuf.a(common.o):common.cc:function google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&): error: undefined reference to 'stderr'
/DL/tensorflow/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src/.libs/libprotobuf.a(common.o):common.cc:function google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&): error: undefined reference to 'stderr'
collect2: error: ld returned 1 exit status

I also modified sh to add ${SYSROOT}/usr/include as include path, but still getting same error. 
"
17364,Interleaving multiple datasets together,"@mrry The current interleave functionality is basically a interleaved flat-map taking as input a single dataset. Given the current API, what's the best way to interleave multiple datasets together? Say they have already been constructed and I have a list of them. I want to produce elements from them alternatively and I want to support lists with more than 2 datasets (i.e., stacked zips and interleaves would be pretty ugly).

Thanks! :)"
17363,zeros() not tracking shape based on input tensors,"```
>>> shape = [constant([2])[0], 3]
>>> reshape([1,2,3,4,5,6], shape)
<tf.Tensor 'Reshape_13:0' shape=(2, 3) dtype=int32>
>>> zeros(shape)
<tf.Tensor 'zeros_2:0' shape=(?, 3) dtype=float32>
```
If reshape can understand the input values well enough to set a static shape for its output, shouldn't zeros() be able to?

This is using tensorflow 1.5"
17362,Missing documentation for tf.train.Saver ,"The documentation for tf.train.Saver (https://www.tensorflow.org/api_docs/python/tf/train/Saver) is not explaining how to actually use the saver. The crucial missing part:
* `saver = tf.train.Saver()` needs to be executed right before finalizing the graph"
17361,Feature request: Let Dataset.padded_batch() return a sequence_length tensor as a feature,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: MacOS High Sierra v10.13.3
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.4 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: Intel Iris Pro 1536 MB
- **Exact command to reproduce**: n/a

### Describe the problem
I am building a `tf.data.Dataset` that should return batches for model built around `tf.nn.dynamic_rnn`. The data consists of variable length time series, and i use `Dataset.padded_batch` to build batch tensors from these time series. When operating on variable length data, we want to pass a `sequence_length` tensor to `tf.nn.dynamic_rnn`. This tensor is also useful to extract the final valid outputs of the rnn for each series in the batch. It is possible to generate `sequence_length` in the model itself, knowing the padding symbol used by `Dataset.padded_batch`. This is unsatisfactory, however, as the padding symbol can be in the domain of the time series function, and thus, it is hard to know if you are looking at padding or real data.

### Feature request
I suggest modifying `Dataset.padded_batch` in such a way that it can optionally return a `sequence_length` tensor as one of it's features, storing the _true_ padding information. I think this would be very convenient for work with time series.
"
17360,C++ api: use of op::Attrs methods in gradients,"The generated op::Attrs struct returns new instances on its chainable methods, and doesn't change the original object.

https://github.com/tensorflow/tensorflow/blob/d7d7f4eea5f3a2e63e12c803d02f56726c0e0513/tensorflow/cc/framework/cc_op_gen.cc#L701

There are a few related issues e.g. https://github.com/tensorflow/tensorflow/blob/6fdb9ad1baf7686a75f9e660178f7ac595e7fc2e/tensorflow/cc/gradients/nn_grad.cc#L164 where the code assumes the underlying object is being mutated and the parameters don't actually pass through.

I guess there might be a couple of ways forward, depending on how Tensorflow prefers the C++ API:

1. Decide the Attrs chaining methods mutate the underlying object and fix the code generation.
2. Decide the Attrs chaining methods return new instances, and fix the uses.

Suggestions?

Fwiw if option 2, it might be nice to add TF_MUST_USE_RESULT to the generated API. (Unfortunately a [long-standing bug in gcc](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=38172) means this may be unreliable as an actual error across versions of gcc that contributors may use.)

/cc @suharshs @keveman "
17359,Lack of documentation about: saving-restoring graphs & other languages API about this,"I think that one of the main features of TensorFlow is its portability. I want to exploit it in a huge code written in C, where a small but computational heavy part can be computed using GPU through TF. So I started to study the problem about saving a graph, loading it to another code (possibly in another programming language) and passing it data to perform some computations.

I'm finding very difficult to understand the documentation. I try to point the main problems:
1- there is no C API documentation despite it is the only ""other language"" which satisfies the stability promises, there is only a link to the `c_api.h` file in the repo that I find very difficult to understand;
2- it is very unclear how one can save a graph which can be restored and loaded in another code written in C (or maybe C++), and I don't find where is explained how one can run the graph in the other code passing data to it. Indeed, people had to write many tutorials, or ask many questions on StackOverflow which are unfortunately very outdated (e.g. [here](https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f) and [here](https://stackoverflow.com/questions/38947658/tensorflow-saving-into-loading-a-graph-from-a-file));
3- as far as I can see there are many ways to do save/restoring but it is unclear when you should use one of these way or another. E.g. `saver.save` -> `saver.restore`; `tf.train.write_graph` -> `tf.import_graph_def` or C analogue `TF_GraphImportGraphDef`; `tf.saved_model.builder.SavedModelBuilder` -> `saved_model.loader.load` or C analogue `LoadSavedModel`.

The more I explore the documentation about this topic, the more questions arises, but I hope to have explained enough the issues I'm facing studying it. Shortly, I could say simply that we need a schematic and coherent page which can summarize the procedures about saving, loading, running graph at least from-to any languages which satisfy the stability promises, maybe with links to the specific standard methods and explanation about which of these ones could be used in a certain situation and why.
"
17358,Distributed training: Evaluation and inference best practices,"I understand tensorflow distributed training and I implemented my own script.

What I want to do now is to integrate the possibility of assigning some workers the task of asynchronously evaluate the model.

Let's say we have 6 workers, what I want to do is to use 4 of them to do asynchronous training, one to periodically evaluate the model and another one to periodically make inference on it.

My intuition to achieve this goal is to do the following:

```
...
elif FLAGS.job_name == ""worker"":

    if FLAGS.task_index <= (len(cluster_dict[""worker""][:-2]) - 1):
         logging.info(""Training worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                train_model = Model(
                    mode=tf.contrib.learn.ModeKeys.TRAIN
                )
               with tf.train.MonitoredTrainingSession(
                    is_chief=(FLAGS.task_index == 0),
                    master=server.target,
                    checkpoint_dir=ckpt_dir,
                    config=config_proto,
                    hooks=hooks
                ) as mon_sess:
                    while not mon_sess.should_stop():
                        res = train_model.train(...)
                        ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-2]) - 1):
         logging.info(""Evaluation worker started"")
         ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                eval_model = Model(
                    mode=tf.contrib.learn.ModeKeys.EVAL
                )
                ...

   elif FLAGS.task_index == (len(cluster_dict[""worker""][-1]) - 1):
        logging.info(""Inference worker started"")
        ...
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster,
                ps_tasks=len(cluster_dict[""ps""])
            )):
                infer_model = Model(
                    mode=tf.contrib.learn.ModeKeys.INFER
                )
                ...
```

Now, what about the evaluation and inference sessions? 
For training, I can use ```tf.train.MonitoredTrainingSession```, but for evaluation and inference I don't see such a cozy solution and the only possibility that I see is to use ```tf.Session```.

Regarding the actual evaluation and inference loop, I thought to use a while loop inside which the worker periodically calls  ```eval_model.eval(...)``` or  ```infer_model.infer(...)```, but this means that the evaluation is performed considering the time and not considering the global_step and the only meaning that I can give to ""periodically"" is to sleep the thread.

What do you think about this solution? Is it the correct way to asynchronously perform training, evaluation, and inference?

Alberto
"
17356,[suggestion][tf.estimator] model with multiple labels,"How about let `labels` provided to `def model_fn(features, labels, mode, params):` be `dict` but not a tensor.
I'm now modeling a model with multiple outputs to be optimized, however, I can only use `features` to substitute `labels` because it only allow a tensor. I'm using version 1.4"
17354,Throw Exception instead of Crash when out of BFC memory allocation,"For huge application model, the memory is easy to overflow and the framework will quit after plenty of Out-of-memory logs. Is it possible to just quickly throw an Exception in python runtime instead of killing the whole program?"
17353,"Error indices[0] = 0 is not in [0, 0) while training an object-detection model with tensorflow","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**:   Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8755 , 11GB
- **Exact command to reproduce**: 
floyd run --gpu --env tensorflow-1.5 --data valiok98/datasets/raspberrypi2/3:/data --data valiok98/datasets/raspberrypi2/8:/mobilenet --data valiok98/datasets/raspberrypi2/9:/images --data valiok98/datasets/raspberrypi2/10:test_ckpt 'python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config'

For the record I tried with previous TF versions and it did not make  a diffrence.
So I am currently attempting to train a custom object-detection model on tensorflow to recognize images of a raspberrypi2. Everything is already set up and running on my hardware,but due to limitations of my gpu I settled for the cloud. I have uploaded my data(train & test records ans csv-files) and my checkpoint model. That is what I get from the logs:

tensorflow:Restoring parameters from /mobilenet/model.ckpt

tensorflow:Starting Session.

tensorflow:Saving checkpoint to path training/model.ckpt

tensorflow:Starting Queues.

tensorflow:Error reported to Coordinator: <class tensorflow.python.framework.errors_impl.InvalidArgumentError'>,
indices[0] = 0 is not in [0, 0)

I also have a folder called images with the actual .jpg files and it is also on the cloud, but for some reason I must specify every directory with a preceeding forward slash / and that might be a problem, as I currently do not know whether some of the files are trying to import these images ,but could not find the path because of the missing /. If any of you happens to share a solution I would be really thankful.

And so here are my full logs:


PSTINFO:tensorflow:Restoring parameters from /mobilenet/model.ckpt
PSTINFO:tensorflow:Starting Session.
PSTINFO:tensorflow:Saving checkpoint to path training/model.ckpt
PSTINFO:tensorflow:Starting Queues.
PSTINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, indices[0] = 0 is not in [0, 0)
PST[[Node: cond_4/RandomCropImage/Gather = Gather[Tindices=DT_INT64, Tparams=DT_BOOL, validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](cond_4/Switch_4:1, cond_4/RandomCropImage/PruneCompleteleyOutsideWindow/Reshape)]]
PSTINFO:tensorflow:global_step/sec: 0
PSTINFO:tensorflow:Caught OutOfRangeError. Stopping Training.
PSTINFO:tensorflow:Finished training! Saving model to disk.
PSTTraceback (most recent call last):
PSTFile ""train.py"", line 167, in <module>
PSTtf.app.run()
PSTFile ""/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 124, in run
PST_sys.exit(main(argv))
PSTFile ""train.py"", line 163, in main
PSTworker_job_name, is_chief, FLAGS.train_dir)
PSTFile ""/code/trainer.py"", line 359, in train
PSTsaver=saver)
PSTFile ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 782, in train
PSTignore_live_threads=ignore_live_threads)
PSTFile ""/usr/local/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py"", line 826, in stop
PSTignore_live_threads=ignore_live_threads)
PSTFile ""/usr/local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 387, in join
PSTsix.reraise(*self._exc_info_to_raise)
"
17352,[Suggestion]SVD GPU Op is not efficient than CPU.,"when I used TF 1.2 version, there is no problem with using SVD

but few days ago i updated my tensorflow version to 1.5 and my code get extremely slow.

so I tested all possibility and  checked issues

finally i found that someone commited SVD GPU op made by cudasolver. in june, 2017

he said that it contain memcopy and it cause bottleneck. it is true and it make svd GPU op extremely slow than former version.

so please remove that op. and if possible, please make proper SVD GPU op  and gradients, and solve problem that tf.svd return nan value.

i uploaded my naive solution to avoid nan value problem and code for compute svd's gradients.

https://github.com/InhaDeeplearningGroup/Academic_research/blob/master/LSH/tensorflow_slim/svdGradients.py

### System information
- Linux Ubuntu 16.04
- install by binary
- TF version : i tested in 1.2, 1.3, 1.4,1.5 
- python3.5
- CUDA : i tested in 7.5, 8.0, 9.0
- GPU : gtx1070"
17351,Examples and tutorial missing from pip tensorflow package,"After searching for and eventually posting a question on StackOverflow (https://stackoverflow.com/questions/49007742/tensorflow-examples-and-tutorials-missing-from-the-pip-package) I was recommended to report this as a bug:

When installing Tensorflow (e.g. tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl from https://pypi.python.org/pypi/tensorflow/1.5.0) from pip, there is only the tutorials/mnist example in tensorflow/examples. It would be nice to have the examples available without having to get them separately from GitHub. Is there a reason why the rest of the examples are not included? Can they be included in future releases?"
17350,Feature request: Make div operator that sets 0/0=0 instead of 0/0=NaN,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Yes, code provided below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Pro, Version 1709
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
&mdash;
- **GCC/Compiler version (if compiling from source)**:
&mdash;
- **CUDA/cuDNN version**:
Using processor, not GPU
- **GPU model and memory**:
Using processor, not GPU
- **Exact command to reproduce**:
Run code provided below

### Describe the problem
When I divide 0/0 elements in TensorFlow, I get 0/0=NaN values. However, I want a division operator that returns 0/0=0 for these elements. Currently, I am using a workaround for this with a where operator, as can be seen in the d variable below. However, this decreases readability, is tedious when writing code with many div operators, and is probably less efficient than coding this functionality into source.

### Source code / logs
```
import tensorflow as tf

a = tf.constant([0], dtype=tf.float32)
b = tf.constant([0], dtype=tf.float32)
c = tf.divide(a, b)
d = tf.where(tf.less(a, 1e-7), a, a/b)

with tf.Session() as sess:
    print(sess.run(c))
    print(sess.run(e))
```"
17349,RuntimeError: TOCO failed see console for info.,"I tried your way but its giving error. . . Kindly help. . . tf.version = 1.6.0rc1 and Ubuntu

**_#ERROR is as follows:_**

Converted 2 variables to const ops.
Traceback (most recent call last):

File ""tf.py"", line 94, in 
tflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])

File ""/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 212, in toco_convert
input_data.SerializeToString())

File ""/home/siteurl/anaconda3/envs/osrco/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py"", line 134, in toco_convert_protos
(stdout, stderr))

### RuntimeError: TOCO failed see console for info.
b'2018-03-01 14:40:13.614956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: VariableV2\n2018-03-01 14:40:13.615016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: Assign\n2018-03-01 14:40:13.615059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: VariableV2\n2018-03-01 14:40:13.615083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: Assign\n2018-03-01 14:40:13.615118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: Log\n2018-03-01 14:40:13.615212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: DynamicStitch\n2018-03-01 14:40:13.615283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: Reciprocal\n2018-03-01 14:40:13.615341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171]

Converting unsupported operation: BroadcastGradientArgs\n2018-03-01 14:40:13.615384: F tensorflow/contrib/lite/toco/import_tensorflow.cc:973]

Check failed: GetBoolAttr(node, ""transpose_b"") == false (1 vs. 0)\nAborted (core dumped)\n'
None

**_#CODE is as follows:_**

import tensorflow as tf
import pandas as pd
import numpy as np
import tempfile
import subprocess
tf.contrib.lite.tempfile = tempfile
tf.contrib.lite.subprocess = subprocess

from tensorflow.python.tools import freeze_graph
from tensorflow.python.tools import optimize_for_inference_lib

print(tf.version)
data=pd.read_csv('iris.data',names=['f1','f2','f3','f4','f5'])

s=np.asarray([1,0,0])
ve=np.asarray([0,1,0])
vi=np.asarray([0,0,1])

data['f5']=data['f5'].map({'Iris-setosa':s,'Iris-versicolor':ve,'Iris-virginica':vi})

#print(data)

data=data.iloc[np.random.permutation(len(data))]

print(data)

data=data.reset_index(drop=True)

#training data
trainFeats=data.ix[0:105,['f1','f2','f3','f4']]
temp=data['f5']
trainlabels=temp[0:106]

y=tf.placeholder(tf.float32,shape=[106, 3])
#weight and bias
m=tf.Variable(tf.zeros([4,3]))
x=tf.placeholder(tf.float32,shape=[106,4],name=""Input"")
c=tf.Variable(tf.zeros([3]))
mxc = tf.nn.softmax((tf.matmul(x, m) + c) ,name=""output"")

loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(mxc), reduction_indices=[1]))

train_step = tf.train.AdamOptimizer(0.01).minimize(loss)

sess = tf.InteractiveSession()
init = tf.initialize_all_variables()
sess.run(init)

#_=tf.contrib.tflite.Convert(sess.graph_def,[x],[mxc])

#number of interations
epoch=2000
for step in range(epoch):
print(sess.run([train_step,loss], feed_dict={x: trainFeats, y:[t for t in trainlabels.as_matrix()]}))

#testData=data.ix[130,['f1','f2','f3','f4']]
#testDataInFrormat=testData.reshape(1,4)
#print(sess.run(tf.argmax(mxc),feed_dict={x:testDataInFrormat}))

tf.train.write_graph(sess.graph_def,'pbtxtFiles/','savegraph.pbtxt',as_text=True)

tf.train.Saver().save(sess,'pbtxtFiles/model.ckpt')

MODEL_NAME = 'iris'
input_graph_path = 'pbtxtFiles/savegraph.pbtxt'
checkpoint_path = 'pbtxtFiles/model.ckpt'
input_saver_def_path = """"
input_binary = False
output_node_names = ""output""
restore_op_name = ""save/restore_all""
filename_tensor_name = ""save/Const:0""
output_frozen_graph_name = 'pbtxtFiles/frozen_model_'+MODEL_NAME+'.pb'
output_optimized_graph_name = 'pbtxtFiles/optimized_inference_model_'+MODEL_NAME+'.pb'
clear_devices = True

freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,
input_binary, checkpoint_path, output_node_names,
restore_op_name, filename_tensor_name,
output_frozen_graph_name, clear_devices, """")

output_graph_def = optimize_for_inference_lib.optimize_for_inference(
sess.graph_def,
[""Input""], # an array of the input node(s)
[""output""], # an array of output nodes
tf.float32.as_datatype_enum)

tflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])
open(""wow.tflite"",""w"").write(tflite_model)

**_#IRIS.DATA is this:_**

5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
5.4,3.7,1.5,0.2,Iris-setosa
4.8,3.4,1.6,0.2,Iris-setosa
4.8,3.0,1.4,0.1,Iris-setosa
4.3,3.0,1.1,0.1,Iris-setosa
5.8,4.0,1.2,0.2,Iris-setosa
5.7,4.4,1.5,0.4,Iris-setosa
5.4,3.9,1.3,0.4,Iris-setosa
5.1,3.5,1.4,0.3,Iris-setosa
5.7,3.8,1.7,0.3,Iris-setosa
5.1,3.8,1.5,0.3,Iris-setosa
5.4,3.4,1.7,0.2,Iris-setosa
5.1,3.7,1.5,0.4,Iris-setosa
4.6,3.6,1.0,0.2,Iris-setosa
5.1,3.3,1.7,0.5,Iris-setosa
4.8,3.4,1.9,0.2,Iris-setosa
5.0,3.0,1.6,0.2,Iris-setosa
5.0,3.4,1.6,0.4,Iris-setosa
5.2,3.5,1.5,0.2,Iris-setosa
5.2,3.4,1.4,0.2,Iris-setosa
4.7,3.2,1.6,0.2,Iris-setosa
4.8,3.1,1.6,0.2,Iris-setosa
5.4,3.4,1.5,0.4,Iris-setosa
5.2,4.1,1.5,0.1,Iris-setosa
5.5,4.2,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
5.0,3.2,1.2,0.2,Iris-setosa
5.5,3.5,1.3,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
4.4,3.0,1.3,0.2,Iris-setosa
5.1,3.4,1.5,0.2,Iris-setosa
5.0,3.5,1.3,0.3,Iris-setosa
4.5,2.3,1.3,0.3,Iris-setosa
4.4,3.2,1.3,0.2,Iris-setosa
5.0,3.5,1.6,0.6,Iris-setosa
5.1,3.8,1.9,0.4,Iris-setosa
4.8,3.0,1.4,0.3,Iris-setosa
5.1,3.8,1.6,0.2,Iris-setosa
4.6,3.2,1.4,0.2,Iris-setosa
5.3,3.7,1.5,0.2,Iris-setosa
5.0,3.3,1.4,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor
6.4,3.2,4.5,1.5,Iris-versicolor
6.9,3.1,4.9,1.5,Iris-versicolor
5.5,2.3,4.0,1.3,Iris-versicolor
6.5,2.8,4.6,1.5,Iris-versicolor
5.7,2.8,4.5,1.3,Iris-versicolor
6.3,3.3,4.7,1.6,Iris-versicolor
4.9,2.4,3.3,1.0,Iris-versicolor
6.6,2.9,4.6,1.3,Iris-versicolor
5.2,2.7,3.9,1.4,Iris-versicolor
5.0,2.0,3.5,1.0,Iris-versicolor
5.9,3.0,4.2,1.5,Iris-versicolor
6.0,2.2,4.0,1.0,Iris-versicolor
6.1,2.9,4.7,1.4,Iris-versicolor
5.6,2.9,3.6,1.3,Iris-versicolor
6.7,3.1,4.4,1.4,Iris-versicolor
5.6,3.0,4.5,1.5,Iris-versicolor
5.8,2.7,4.1,1.0,Iris-versicolor
6.2,2.2,4.5,1.5,Iris-versicolor
5.6,2.5,3.9,1.1,Iris-versicolor
5.9,3.2,4.8,1.8,Iris-versicolor
6.1,2.8,4.0,1.3,Iris-versicolor
6.3,2.5,4.9,1.5,Iris-versicolor
6.1,2.8,4.7,1.2,Iris-versicolor
6.4,2.9,4.3,1.3,Iris-versicolor
6.6,3.0,4.4,1.4,Iris-versicolor
6.8,2.8,4.8,1.4,Iris-versicolor
6.7,3.0,5.0,1.7,Iris-versicolor
6.0,2.9,4.5,1.5,Iris-versicolor
5.7,2.6,3.5,1.0,Iris-versicolor
5.5,2.4,3.8,1.1,Iris-versicolor
5.5,2.4,3.7,1.0,Iris-versicolor
5.8,2.7,3.9,1.2,Iris-versicolor
6.0,2.7,5.1,1.6,Iris-versicolor
5.4,3.0,4.5,1.5,Iris-versicolor
6.0,3.4,4.5,1.6,Iris-versicolor
6.7,3.1,4.7,1.5,Iris-versicolor
6.3,2.3,4.4,1.3,Iris-versicolor
5.6,3.0,4.1,1.3,Iris-versicolor
5.5,2.5,4.0,1.3,Iris-versicolor
5.5,2.6,4.4,1.2,Iris-versicolor
6.1,3.0,4.6,1.4,Iris-versicolor
5.8,2.6,4.0,1.2,Iris-versicolor
5.0,2.3,3.3,1.0,Iris-versicolor
5.6,2.7,4.2,1.3,Iris-versicolor
5.7,3.0,4.2,1.2,Iris-versicolor
5.7,2.9,4.2,1.3,Iris-versicolor
6.2,2.9,4.3,1.3,Iris-versicolor
5.1,2.5,3.0,1.1,Iris-versicolor
5.7,2.8,4.1,1.3,Iris-versicolor
6.3,3.3,6.0,2.5,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
7.1,3.0,5.9,2.1,Iris-virginica
6.3,2.9,5.6,1.8,Iris-virginica
6.5,3.0,5.8,2.2,Iris-virginica
7.6,3.0,6.6,2.1,Iris-virginica
4.9,2.5,4.5,1.7,Iris-virginica
7.3,2.9,6.3,1.8,Iris-virginica
6.7,2.5,5.8,1.8,Iris-virginica
7.2,3.6,6.1,2.5,Iris-virginica
6.5,3.2,5.1,2.0,Iris-virginica
6.4,2.7,5.3,1.9,Iris-virginica
6.8,3.0,5.5,2.1,Iris-virginica
5.7,2.5,5.0,2.0,Iris-virginica
5.8,2.8,5.1,2.4,Iris-virginica
6.4,3.2,5.3,2.3,Iris-virginica
6.5,3.0,5.5,1.8,Iris-virginica
7.7,3.8,6.7,2.2,Iris-virginica
7.7,2.6,6.9,2.3,Iris-virginica
6.0,2.2,5.0,1.5,Iris-virginica
6.9,3.2,5.7,2.3,Iris-virginica
5.6,2.8,4.9,2.0,Iris-virginica
7.7,2.8,6.7,2.0,Iris-virginica
6.3,2.7,4.9,1.8,Iris-virginica
6.7,3.3,5.7,2.1,Iris-virginica
7.2,3.2,6.0,1.8,Iris-virginica
6.2,2.8,4.8,1.8,Iris-virginica
6.1,3.0,4.9,1.8,Iris-virginica
6.4,2.8,5.6,2.1,Iris-virginica
7.2,3.0,5.8,1.6,Iris-virginica
7.4,2.8,6.1,1.9,Iris-virginica
7.9,3.8,6.4,2.0,Iris-virginica
6.4,2.8,5.6,2.2,Iris-virginica
6.3,2.8,5.1,1.5,Iris-virginica
6.1,2.6,5.6,1.4,Iris-virginica
7.7,3.0,6.1,2.3,Iris-virginica
6.3,3.4,5.6,2.4,Iris-virginica
6.4,3.1,5.5,1.8,Iris-virginica
6.0,3.0,4.8,1.8,Iris-virginica
6.9,3.1,5.4,2.1,Iris-virginica
6.7,3.1,5.6,2.4,Iris-virginica
6.9,3.1,5.1,2.3,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
6.8,3.2,5.9,2.3,Iris-virginica
6.7,3.3,5.7,2.5,Iris-virginica
6.7,3.0,5.2,2.3,Iris-virginica
6.3,2.5,5.0,1.9,Iris-virginica
6.5,3.0,5.2,2.0,Iris-virginica
6.2,3.4,5.4,2.3,Iris-virginica
5.9,3.0,5.1,1.8,Iris-virginica

Kindly help MR. @aselle
Kindly help MR. @facaiy
Kindly help MR. @leandroBorgesFerreira
Kindly help MR. @javierluraschi
Kindly help MR. @tensorflowbutler"
17348,when run ./bazel-bin/tensorflow/python/tools/freeze_graph AttributeError: 'int' object attribute '__doc__' is read-only,"tensorflow r1.5
python2.7
followed by the tensorflow lite tutorial:
https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite
my command line:
```shell
./bazel-bin/tensorflow/python/tools/freeze_graph    --input_graph=~/Code/mobilenet_v1_1.0_224_frozen.pb     --input_checkpoint=~/Code/mobilenet_v1_1.0_224.ckpt     --input_binary=true --output_graph=~/code/frozen.pb     --output_node_names=MobileNet/Predictions/Reshape_1
and get:
Traceback (most recent call last):
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 45, in <module>
    from tensorflow.core.framework import graph_pb2
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 102, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 33, in <module>
    from tensorflow.python.framework import function
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 38, in <module>
    from tensorflow.python.ops import variable_scope as vs
  File ""/home/zero/Code/tmp/tensorflow-r1.5/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py"", line 192, in <module>
    """"""
AttributeError: 'int' object attribute '__doc__' is read-only
```"
17347,[suggestion] New option for dynamic_decode function,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1080ti, 11GB
- **Exact command to reproduce**:


### Describe the problem

I found that while using `dynamic_decode` with option `maximum_iterations`, The first priority of output length is the maximum length of given decoder input when **maximum length of decoder input < maximum_iteration**. Can you provide a new option that I can just fix the size of decoder output size, not considering the maximum length of given decoder input.

### example code

```
import tensorflow as tf


batch_size = 4
hidden_size = 5

# [batch, sequence_length, depth]

memory = tf.get_variable('memory', [4, 10, 5], dtype = tf.float32)
len_enc = tf.Variable([5,6,7,8])

# [batch, sequence_length, depth]
embd_dec_input = tf.get_variable('dec_input', [4, 8, 5], dtype = tf.float32)
len_dec = tf.Variable([3,4,5,6])


# Create an attention mechanism
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
        hidden_size, memory,
        memory_sequence_length=len_enc)

# Build decoder cell
decoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)

decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
        decoder_cell, attention_mechanism,
        attention_layer_size=hidden_size)

# Helper for decoder cell
helper = tf.contrib.seq2seq.TrainingHelper(
    embd_dec_input, len_dec
)

# Decoder initial state
initial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, initial_state,
    output_layer=None)

# Simply fix the decoder output length as the length of decoder input
max_iter = 8

outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_iter)
```

**Even my maximum length of decoder input length is 6, I want the decoder to output with maximum length 8**"
17344,Does the size of embedding have a huge influence on the training speed?,"In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:

`tf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)`

And the sentences are made of word ids and vectors will be looked up from the embedding tensor above.

`inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)`

So what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?"
17343,Very general question about .tflite,"Is the tensorflow lite always have quantized calculation and output?
Or it depends on the model's input type?
"
17342,can't use all the CPU cores,"I'm running the [ptb](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) example on my 48-core Intel Skylake CPUs (two sockets).

However, I found only 10 of the 48 cores are being used (I used the 'top' command). I want to use all the cores.

I tried the following setting, but it did not work

config_proto = tf.ConfigProto(allow_soft_placement=soft_placement, inter_op_parallelism_threads=4, intra_op_parallelism_threads=12)

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 3.10.0-693.17.1.el7.x86_64 #1 SMP
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
Did not use GPU
- **GPU model and memory**:
Did not use GPU
- **Exact command to reproduce**:


"
17341,"Cannot import Tensorflow on a system that has anaconda installed, though tensorflow installed successfully","I installed tensorflow  on ubntu 16.04 by using 
``` 
pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0-cp27-none-linux_x86_64.whl
```
it sounds like that it is installed correct, after installation it shows that:
```
Collecting setuptools (from protobuf>=3.4.0->tensorflow-gpu==1.5.0)
  Using cached setuptools-38.5.1-py2.py3-none-any.whl
Requirement already up-to-date: funcsigs>=1; python_version < ""3.3"" in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.5.0)
Requirement already up-to-date: pbr>=0.11 in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.5.0)
Installing collected packages: numpy, tensorflow-gpu, setuptools
  Found existing installation: numpy 1.14.0
    Uninstalling numpy-1.14.0:
      Successfully uninstalled numpy-1.14.0
  Found existing installation: tensorflow-gpu 1.5.0
    Uninstalling tensorflow-gpu-1.5.0:
      Successfully uninstalled tensorflow-gpu-1.5.0
  Found existing installation: setuptools 38.4.0
    Uninstalling setuptools-38.4.0:
      Successfully uninstalled setuptools-38.4.0
Successfully installed numpy-1.14.1 setuptools-38.5.1 tensorflow-gpu-1.5.0
```
Now Im tryin to import tensorflow and i receive the following error:

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/alireza/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Any help or suggestion on what should i do??"
17340,"Could some ops(assert,switch...) be removed in inference model ?","I want to convert faster-rcnn model to tensorRT, but these ops can't be converted,because invalid dtype or non data value, so I want to know how to solve the question."
17339,tf 1.6rc1 feed_dict is slow on multi-socket machines,"It seems as if 1.6rc1 introduces a copy in `feed_dict` on machines with >1 physical socket. Similar to https://github.com/tensorflow/tensorflow/issues/17233, but this also happens if numpy array is aligned. Fetching array from tensorflow, then feeding it back in happens at single-core memcpy speed. Downgrading to 1.5 restores fast behavior (4x faster)

https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py
```
# 1.6 rc1 on p3.16xlarge (2 Xeon V4 sockets) = slow
__git_version__: v1.6.0-rc1-607-g0bde713c06
https://github.com/tensorflow/tensorflow/commit/0bde713c06
python tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7
feed_cpu_tensor               :   2.5 GB/sec, min: 39.89, median: 40.18, mean: 40.26

# switch to tf 1.5, things are fast
pip install tensorflow
python tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7
feed_cpu_tensor               :  13.0 GB/sec, min:  7.67, median:  8.88, mean:  9.26

# switch to p3.8xlarge machine (1 socket), and latest TF, things are also fast
# tensorflow.__git_version__ = v1.6.0-rc1-562-g26ae3287a1
python tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51 # 10.7
feed_cpu_tensor               :  10.5 GB/sec, min:  9.49, median: 10.83, mean: 10.86
```"
17336,Feature Request: Axis support for scatter update,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: N/A (applicable to both Python and Go bindings)
- **Bazel version (if compiling from source)**: 0.10.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: N/A (CPU only)
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
GatherV2 allows one to specify axis. ResourceScatterUpdate however does not.

Use case: Consider a variable of shape `[2, 3, 5]`. It may be useful to update the first dimension with data of shape `[2]` at indices specified in the last two dimensions. AFAICT, it is not possible to do this with the current ResourceScatterNdUpdate or ResourceScatterUpdate operations. Using a variable with transposed dimensions such as `[3, 5, 2]` is a potential workaround, but is inelegant and would require that it be wrapped in transpose operations.

Can this be achieved using existing operations, and if not, is it worth create a new operation or extending an existing operation to supports it?
"
17334,Still Seeing AVX Warnings on TF 1.6 Docker Image,"### System information
```
== cat /etc/issue ===============================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux node014-jupyter-20170708-132328 4.4.0-1022-aws #31-Ubuntu SMP Tue Jun 27 11:27:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.0)
protobuf (3.2.0)
tensorflow-gpu (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Aug  1 17:13:34 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |
| N/A   59C    P0    67W / 149W |      0MiB / 11439MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61

== cat /etc/issue ===============================================
Linux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.0.post1)
tensorflow (1.6.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0
tf.GIT_VERSION = v1.6.0-0-gd2e24b6
tf.COMPILER_VERSION = v1.6.0-0-gd2e24b6
Sanity check: array([1], dtype=int32)
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Feb 28 17:32:05 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           On   | 00000000:00:1E.0 Off |                    0 |
| N/A   43C    P0    45W / 150W |     11MiB /  7618MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a

== cat /etc/issue ===============================================
Linux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux node008-jupyter-20180228-164013 4.4.0-1050-aws #59-Ubuntu SMP Tue Jan 30 19:57:10 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.0.post1)
tensorflow (1.6.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0
tf.GIT_VERSION = v1.6.0-0-gd2e24b6
tf.COMPILER_VERSION = v1.6.0-0-gd2e24b6
Sanity check: array([1], dtype=int32)
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Feb 28 17:35:44 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           On   | 00000000:00:1E.0 Off |                    0 |
| N/A   49C    P0    46W / 150W |     11MiB /  7618MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
```

```
v1.6.0-0-gd2e24b6 1.6.0
```
Image: `tensorflow/tensorflow:1.6.0-devel-gpu-py3`

### Describe the problem
I am seeing this warning message even though I thought that AVX instructions should be used in 1.6:
```
2018-02-28 17:33:10.974670: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
```
This is the same warning I saw on 1.5."
17333,map_fn produces inconsistent results when using numpy vs. tf constants,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9, cudnn 7
- **GPU model and memory**: Quadro M1200 4GB
- **Exact command to reproduce**: See below

### Describe the problem
According to the map_fn docs, second example https://www.tensorflow.org/api_docs/python/tf/map_fn

```
elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))
alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)
## alternate == [-1, 2, -3]
```

I ran this

```
elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))
output = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)
sess = tf.Session()
print(sess.run(output))
```

which returns `[-1  2 -3]` as expected

But if I run

```
elems = tf.constant(((1, 2, 3), (-1, 1, -1)), dtype=tf.int64)
output = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)
sess = tf.Session()
print(sess.run(output))
```

The output is `[2 -1]`, which is not what I expected since the input is the same numbers with the same shape.

The output should be the same? Is there something special going on with numpy arrays?

Interesting read: https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work"
17332,Feature request: fft support for complex128 ,"Tensorflow fft only supports complex64 types at the moment. I have a physics application that could make great use of tensorflow's architecture, which is not machine learning related but needs greater precision.

Since [pyculib](http://pyculib.readthedocs.io/en/latest/cufft.html) supports complex128, it should be available from nvidia. 

Is there support planned in the future?"
17330,"C++ gradients: fractional{agv,max}pool, soft{plus,sign}","Adding gradients to the C++ API for these look straightforward, as there are existing kernel ops for their gradients. If nobody has already started, I can sign up for these.

/cc @bpiel @suharshs "
17328,Feature Request: Training on device,"Hi Tensorflow,

I would like to know whether Tensorflow does support on-device training i.e creation of model on Android device. Right now Tensorflow provides Python script to generate model in the Native system and only inference we can execute on Android device. Obviously i cannot execute the Python to create the model in Android, does Tensorflow provides any Example/Feature in C++ to train the model on-device (Android)  ? 

Feature Request 

1. A Standalone C++ Application which can be cross compiled for Android to train the model from dataset. 

2. Training the model might be time consuming process if done on Android, but at least some minimal support for smaller data set would be helpful.


Regards,
Senthil
"
17327,tf.data.Dataset.padded_batch() should support padding to nearest N bytes,"### System information
TF 1.5
Python 3.6.4 (Anaconda)

### Problem description
If using `tf.data.Dataset.padded_batch()` on input with variable size, it looks like there is significant overhead first time a batch with examples of size X is used on GPU. If batch with examples of size X is used second time, the execution is approx. 4x faster. (I suppose it is due to a way tensorflow handles data.)

In case the input size varies a lot, this slows the computation enormously. It could be solved by padding the batch to nearest N bytes so that there is only limited number of sizes pushed onto GPU and overhead thus becomes negligible.

### Solution
It would be great if `padded_batch` supported argument `pad_to_nearest_bytes` or/and `allowed_batch_sizes` for enumeration of possible sizes.

Proof:
```
TIME    BATCH_SIZE (2nd dimension = the data size)
0.15205 20480
0.06180 20480
0.80608 147456
0.24141 147456
0.74360 135168
0.21659 135168
0.58724 98304
0.16206 98304
0.05387 20480
0.05694 20480
0.53993 90112
0.15452 90112
0.23547 147456
0.23576 147456
```"
17325,"Resource exhausted: OOM when allocating tensor with shape, even when batch_size is 1.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I've registered a new problem for translating English to Swedish, the code is pretty similar to TranslateEndeWmtBpe32k, with Europarl data. I'm able to train with hidden_size: 128, anything over 128 and I get Resource exhausted error, even when batch size is 1. (I'm using hparams: transformer_big).

I notice that previous posts on this issue were solved by reducing the batch size. I'm posting this because reducing batch size wasn't helpful and I'm hoping if someone encountered a similar issue.


- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux deepnlp01 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
  4 VERSION=""14.04.5 LTS, Trusty Tahr""

- **TensorFlow installed from (source or binary)**:
tf.VERSION = 1.4.1 (Installed from source)

- **Python version**: 
2.7.6

- **Bazel version (if compiling from source)**:
Build label: 0.5.2

- **GCC/Compiler version (if compiling from source)**:
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4

- **CUDA/cuDNN version**:
nvcc: NVIDIA (R) Cuda compiler driver
Cuda compilation tools, release 8.0, V8.0.61

- **GPU model and memory**:
NVIDIA TITAN X (12GB)

- **Exact command to reproduce**:
USR_DIR=<path>/python2.7/site-packages/tensor2tensor/<new_module>

t2t-trainer \
  --data_dir=$DATA_DIR \
  --problems=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$TRAIN_DIR \
  --t2t_usr_dir=$USR_DIR

### Describe the problem

The data I'm using is parallel corpus English-Swedish, available on http://www.statmt.org/europarl/ . I've created a dictionary myself of about 1,400,000 (1.4 million) words. [The problem persists even if I use a smaller vocab of about 10k words]

I'm able to train with batch_size=1024 and hidden_size=128, if I increase the hidden size to 256, even with batch_size=1, it runs out of memory. I get the following error.

### Source code / logs
This is the error I'm encountering:

```
2018-02-28 05:48:52.977027: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[90993,256]

Traceback (most recent call last):
  File ""/data/tf_venv/2.7/1.4.1_gpu/bin/t2t-trainer"", line 32, in <module>
    tf.app.run()
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/data/tf_venv/2.7/1.4.1_gpu/bin/t2t-trainer"", line 28, in main
    t2t_trainer.main(argv)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 337, in main
    execute_schedule(exp)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 287, in execute_schedule
    getattr(exp, FLAGS.schedule)()
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/framework/experimental.py"", line 64, in new_func
    return func(*args, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 717, in continuous_train_and_eval
    hooks=self._train_monitors)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 807, in _call_train
    hooks=hooks)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 783, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 521, in run
    run_metadata=run_metadata)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 892, in run
    run_metadata=run_metadata)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 967, in run
    raise six.reraise(*original_exc_info)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 952, in run
    return self._sess.run(*args, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1024, in run
    run_metadata=run_metadata)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 827, in run
    return self._sess.run(*args, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[90992,256]
         [[Node: training/gradients/AddN_96 = AddN[N=3, T=DT_FLOAT, _class=[""loc:@transformer/parallel_0_5/transformer/symbol_modality_1455878_256_2/shared/concat""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256_2/shared/concat_grad/tuple/control_dependency_14, training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256_1/shared/concat_grad/tuple/control_dependency_14, training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256/shared/concat_grad/tuple/control_dependency_14)]]
         [[Node: training/global_norm/global_norm/_4061 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_24183_training/global_norm/global_norm"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'training/gradients/AddN_96', defined at:
  File ""/data/tf_venv/2.7/1.4.1_gpu/bin/t2t-trainer"", line 32, in <module>
    tf.app.run()
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/data/tf_venv/2.7/1.4.1_gpu/bin/t2t-trainer"", line 28, in main
    t2t_trainer.main(argv)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 337, in main
    execute_schedule(exp)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py"", line 287, in execute_schedule
    getattr(exp, FLAGS.schedule)()
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/framework/experimental.py"", line 64, in new_func
    return func(*args, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 717, in continuous_train_and_eval
    hooks=self._train_monitors)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 807, in _call_train
    hooks=hooks)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 829, in wrapping_model_fn
    use_tpu=use_tpu)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 923, in estimator_model_fn
    loss, num_async_replicas=num_async_replicas)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 927, in estimator_spec_train
    train_op = self.optimize(loss, num_async_replicas=num_async_replicas)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/t2t_model.py"", line 351, in optimize
    loss, lr, self.hparams, use_tpu=common_layers.is_on_tpu())
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py"", line 67, in optimize
    colocate_gradients_with_ops=True)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensor2tensor/utils/optimize.py"", line 109, in compute_gradients
    return self._opt.compute_gradients(loss, var_list, **kwargs)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 533, in gradients
    out_grads = _AggregatedGrads(grads, op, loop_state, aggregation_method)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 872, in _AggregatedGrads
    out_grads[i] = _MultiDeviceAddN(out_grad)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 767, in _MultiDeviceAddN
    summands.append(math_ops.add_n(tensors))
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 2000, in add_n
    return gen_math_ops._add_n(inputs, name=name)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 220, in _add_n
    ""AddN"", inputs=inputs, name=name)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/data/tf_venv/2.7/1.4.1_gpu/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[90992,256]
         [[Node: training/gradients/AddN_96 = AddN[N=3, T=DT_FLOAT, _class=[""loc:@transformer/parallel_0_5/transformer/symbol_modality_1455878_256_2/shared/concat""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256_2/shared/concat_grad/tuple/control_dependency_14, training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256_1/shared/concat_grad/tuple/control_dependency_14, training/gradients/transformer/parallel_0_5/transformer/symbol_modality_1455878_256/shared/concat_grad/tuple/control_dependency_14)]]
         [[Node: training/global_norm/global_norm/_4061 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_24183_training/global_norm/global_norm"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

```
This is the code that I've modified:

```
def _get_europarl_ensv_dataset(directory, filename):
    """"""Extract the EuroParl en-sv corpus `filename` to directory unless it's there.""""""

    train_path = os.path.join(directory, filename)

    return train_path


@registry.register_problem
class TranslateEnsvEuroparl(translate.TranslateProblem):
    """"""Problem spec for EuroParl En-Sv translation, BPE version.""""""

    @property
    def approx_vocab_size(self):
        return 1455877

    @property
    def vocab_filename(self):
        return ""vocab.europarl.ensv.txt""

    def get_or_create_vocab(self, data_dir, tmp_dir, force_get=False):
        vocab_filename = os.path.join(data_dir, self.vocab_filename)
        if not tf.gfile.Exists(vocab_filename) and force_get:
            raise ValueError(""Vocab %s not found"" % vocab_filename)
        return text_encoder.TokenTextEncoder(vocab_filename, replace_oov=""UNK"")

    def generate_samples(self, data_dir, tmp_dir, dataset_split):
        """"""Instance of token generator for the WMT en->de task, training set.""""""
        train = dataset_split == problem.DatasetSplit.TRAIN
        dataset_path = (""europarl-v7.train.sv-en""
                        if train else ""europarl-v7.test.sv-en"")
        train_path = _get_europarl_ensv_dataset(tmp_dir, dataset_path)

        # Vocab
        token_path = os.path.join(data_dir, self.vocab_filename)
        if not tf.gfile.Exists(token_path):
            token_tmp_path = os.path.join(tmp_dir, self.vocab_filename)
            tf.gfile.Copy(token_tmp_path, token_path)
            with tf.gfile.GFile(token_path, mode=""r"") as f:
                vocab_data = ""<pad>\n<EOS>\n"" + f.read() + ""UNK\n""
            with tf.gfile.GFile(token_path, mode=""w"") as f:
                f.write(vocab_data)

        return text_problems.text2text_txt_iterator(train_path + "".en"",
                                                    train_path + "".sv"")

```"
17323,Save activation map for a specific convnet in the process of tf.train.MonitoredTrainingSession(),"ADDING FEATURES

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**:  2.7 anaconda
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: Quadro K400
- **Exact command to reproduce**:


### Describe the problem
I followed this tutorial from the source code of TF: **model/tutorials/image/cifar10/**, in the tutorial, you used tf.train.MonitoredTrainingSession() to log the checkpoint or tensor for future usage with TensorBoard, my question is to display the activation maps of my convnet during training process. But inside the:
`while not mon_sess.should_stop():
             mon_sess.run(train_op)`

I can not add new tensor because the Graph is finalized. So I want to ask you how to add something like **tf.summary.image**: Pass one example image to a specific convnet  after relu, and save it as png for all the filters???

I think I can easily implement this in a normal **tf.Session()**, but I am still interested if we can do it in **tf.train.MonitoredTrainingSession()**.

I think that would be great for the newbie to understand what happened for our CNN.

Thanks in advance


"
17322,A bug When use kmean and tensorboard,"When I use the tensorboard in kmeans it cause an error, if i remove the merge_summary_op with `_, d, idx = sess.run([train_op, avg_distance, cluster_idx], feed_dict={X: full_data_x})`, it will be ok!!!  I don't know why the merge op will cause the error, could anyone help me with  ?

```
InvalidArgumentError: Shape [-1,784] has negative dimensions
	 [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'Placeholder_28', defined at:
  File ""/Users/burness/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/burness/anaconda/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 151, in start
    super(ZMQIOLoop, self).start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
    self._handle_recv()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
    self._run_callback(callback, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
    callback(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-830a4475ac2c>"", line 26, in <module>
    X = tf.placeholder(tf.float32, shape=[None, num_features])
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1530, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1954, in _placeholder
    name=name)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Shape [-1,784] has negative dimensions
	 [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]



​
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-77f99f84eb11> in <module>()
     54 for i in range(1, num_steps + 1):
     55     _, d, idx, summary = sess.run([train_op, avg_distance, cluster_idx, merged_summary_op],
---> 56                          feed_dict={X: full_data_x})
     57 #     summary_writer.add_summary(summary, i)
     58     if i % 10 == 0 or i == 1:

/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    787     try:
    788       result = self._run(None, fetches, feed_dict, options_ptr,
--> 789                          run_metadata_ptr)
    790       if run_metadata:
    791         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
    995     if final_fetches or final_targets:
    996       results = self._do_run(handle, final_targets, final_fetches,
--> 997                              feed_dict_string, options, run_metadata)
    998     else:
    999       results = []

/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1130     if handle is None:
   1131       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-> 1132                            target_list, options, run_metadata)
   1133     else:
   1134       return self._do_call(_prun_fn, self._session, handle, feed_dict,

/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1150         except KeyError:
   1151           pass
-> 1152       raise type(e)(node_def, op, message)
   1153 
   1154   def _extend_graph(self):

InvalidArgumentError: Shape [-1,784] has negative dimensions
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'Placeholder', defined at:
  File ""/Users/burness/anaconda/lib/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/Users/burness/anaconda/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 151, in start
    super(ZMQIOLoop, self).start()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
    self._handle_recv()
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
    self._run_callback(callback, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
    callback(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-b3a44847e28e>"", line 25, in <module>
    X = tf.placeholder(tf.float32, shape=[None, num_features])
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1530, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1954, in _placeholder
    name=name)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Shape [-1,784] has negative dimensions
	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```

```
from __future__ import print_function

import numpy as np
import tensorflow as tf
from tensorflow.contrib.factorization import KMeans

import os
os.environ[""CUDA_VISIBLE_DEVICES""] = """"
logs_path = '/tmp/tensorflow_logs/k-means'

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)
full_data_x = mnist.train.images

num_steps = 50 
batch_size = 1024
k = 25 
num_classes = 10
num_features = 784 

X = tf.placeholder(tf.float32, shape=[None, num_features])
# Labels (for assigning a label to a centroid and testing)
Y = tf.placeholder(tf.float32, shape=[None, num_classes])

# K-Means Parameters
kmeans = KMeans(inputs=X, num_clusters=k, distance_metric='cosine',
                use_mini_batch=True)

training_graph = kmeans.training_graph()

if len(training_graph) > 6: # Tensorflow 1.4+
    (all_scores, cluster_idx, scores, cluster_centers_initialized,
     cluster_centers_var, init_op, train_op) = training_graph
else:
    (all_scores, cluster_idx, scores, cluster_centers_initialized,
     init_op, train_op) = training_graph

cluster_idx = cluster_idx[0] # fix for cluster_idx being a tuple
avg_distance = tf.reduce_mean(scores)
tf.summary.scalar(""avg_distance"", avg_distance)

init_vars = tf.initialize_all_variables()
merged_summary_op = tf.summary.merge_all()

sess = tf.Session()

sess.run(init_vars)
summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())
sess.run(init_op, feed_dict={X: full_data_x})

for i in range(1, num_steps + 1):
    _, d, idx, summary = sess.run([train_op, avg_distance, cluster_idx, merged_summary_op],
                         feed_dict={X: full_data_x})
    summary_writer.add_summary(summary, i)
    if i % 10 == 0 or i == 1:
        print(""Step %i, Avg Distance: %f"" % (i, d))


counts = np.zeros(shape=(k, num_classes))
for i in range(len(idx)):
    counts[idx[i]] += mnist.train.labels[i]

labels_map = [np.argmax(c) for c in counts]
labels_map = tf.convert_to_tensor(labels_map)


cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)
correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, 1), tf.int32))
accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

test_x, test_y = mnist.test.images, mnist.test.labels
print(""Test Accuracy:"", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))

```"
17321,toco convert to lite model incorrectly with slim.batch_norm not follows after conv.,"### System information
- **OS Platform and Distribution Linux Ubuntu 14.04**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.6.rc1**:
- **Python version 3.6**: 
- **Have I written custom code N/A**:
- **Bazel version N/A**:
- **CUDA/cuDNN version N/A**:
- **GPU model and memory N/A**:
- **Exact command to reproduce N/A**:

I create simple model with slim and convert lite model using toco
```python
      input = tf.placeholder(tf.float32, (None, 120, 120, 3), 'input')
      net = input
      net = slim.conv2d(net, 12, 7, stride=2, scope='conv1')
      net = slim.batch_norm(net)
      net = slim.max_pool2d(net, 3, stride=2, padding='SAME')
      net = slim.batch_norm(net)
      ...
```
The converted lite model from it can't allocate tensor with error 
`tensorflow\contrib\lite\kernels\mul.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 1)`

but if I insert conv2d layer befor batch_norm, the result is ok.
```python
      input = tf.placeholder(tf.float32, (None, 120, 120, 3), 'input')
      net = input
      net = slim.conv2d(net, 12, 7, stride=2, scope='conv1')
      net = slim.batch_norm(net)
      net = slim.max_pool2d(net, 3, stride=2, padding='SAME')
      net = slim.conv2d(net, 12, 7, stride=2, scope='conv111')
      net = slim.batch_norm(net)
      ...
```
I had try insert relu and the other layers before batch_norm, but it can't allocate tensor when previous layer is not conv.
How can I create batch_norm after any layer?
"
17320,Require a c++ example or documents about tensorflow lite useage,"because i have my own native and c++ code, if i want to use tensorflow lite feature and put it to my android project, i have to write tensorflow lite code in c++.
but it is too hard to find out how to get the input and output.
i have look up the ios example because part of the code is written in c++, but it crashed when i have:
float* output = interpreter->typed_output_tensor<float>(0);
for(int i = 0; i < output_size; i++)
        LOGD(""bkzero jni: , result: %f"", output[i]);
i believe it is not a bug under iOS platform.
this part of code in android example is written in java, i cannot figure out how to write an implemetation of the tensorhandle like:
long[] outputsHandles =
        run(interpreterHandle, errorHandle, sizes, dataTypes, numsOfBytes, inputs);
    if (outputsHandles == null || outputsHandles.length == 0) {
      throw new IllegalStateException(""Interpreter has no outputs."");
    }
    Tensor[] outputs = new Tensor[outputsHandles.length];
    for (int i = 0; i < outputsHandles.length; ++i) {
      outputs[i] = Tensor.fromHandle(outputsHandles[i]);
      Log.i(""bkzero: "", ""java: outputs[0] "" + outputs[i]);
    }"
17319,Model trained with `tf.data.Dataset` can not be converted to `dlc` file of snpe,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
24G
- **Exact command to reproduce**:

I trained a model with `estimator` and  `tf.data.Dataset`. But I can not convert the model file to `.dlc` file of snpe.
As you all know, the following lines are included in input function:
```
iterator = dataset.make_one_shot_iterator()
imgs = iterator.get_next()
return imgs
```
Then, I converted my checkpoint file with freeze_graph.py and get a `.pb` file. And the first three node names are shown below:
```
['OneShotIterator',
 'IteratorGetNext',
 'holi/conv2d/kernel',
 ...]
```
Thus, I think the input node is `OneShotIterator`.
And I tried to convert the `.pb` file to `.dlc` file of snpe. Lines below are the command I found from the documentation of snpe:
```
snpe-tensorflow-to-dlc --graph $SNPE_ROOT/models/inception_v3/tensorflow/inception_v3.pb
                       --input_dim Mul 299,299,3 --dlc inception_v3.dlc --out_node softmax
```
The `Mul` is the input node name, `299,299,3` is the input node shape. 
My input node is `OneShotIterator`, which has no shape however.
I do not think it is a good idea to take an iterator as a model input, any idea to change that? Or any solution to convert my `.pb` file to `.del` file?
Looking forward to your reply.
"
17317,How to Split up/Unstack/Partition a dynamic 3D Tensor into subtensors?,"In this [code from google][1] , sequence loss is being calculated by passing in 3 variables : logits , weights and targets.

How logits are defined:
> logits: A Tensor of shape
>       `[batch_size, sequence_length, num_decoder_symbols]` and dtype float.
>       The logits correspond to the prediction across all classes at each
>       timestep.

My intentions were to get **3 different tensors of shapes** batch_size , sequence_length and dec_symbols  and then use them in **tf.scan** (using sequence_length as elems)

If I print the logits tensor , this is what i get : 
> shape=(?, ?, 300) 

Which means tf.unstack is out of the equation(as it has a variable shape)

So my first question is , is it even possible ?

If yes , any suggestions ?

Thanks !

PS: Maybe Google can add a swap_memory parameter to the seq2seq.sequence_loss function ,to avoid OOM errors while calculating losses [which is what we are trying to overcome using an iterator inside]

  [1]: https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/seq2seq/python/ops/loss.py"
17316,ABI for `tensorflow::core::RefCounted` is error-prone,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: (see above)

### Describe the problem

For performance, the implementation of `RefCounted` comprises a set of inline methods. Furthermore, it contains a cunning optimization that avoids updating the refcount when the caller to `Unref()` is the only owner:

https://github.com/tensorflow/tensorflow/blob/3ba1f72f8829c566372208062fcea04ab5695dc6/tensorflow/core/lib/core/refcount.h#L88-L89

It also includes some `DCHECK` macros to ensure that various invariants hold:

https://github.com/tensorflow/tensorflow/blob/3ba1f72f8829c566372208062fcea04ab5695dc6/tensorflow/core/lib/core/refcount.h#L79

https://github.com/tensorflow/tensorflow/blob/3ba1f72f8829c566372208062fcea04ab5695dc6/tensorflow/core/lib/core/refcount.h#L90-L92

Note that that second `DCHECK` performs a side-effect to make the first one succeed. 

The release build is built with `NDEBUG` defined, so the `DCHECK` code doesn't execute. However, the default flags you get when following the [Adding an New Op](https://www.tensorflow.org/versions/r1.6/extend/adding_an_op) tutorial to build an extension do not include `-DNDEBUG`. 

Recall that the methods are marked `inline`.  This means that some code in the release binary might call (an inlined version of) `Unref()` and the store to `ref_` will be elided, but some code in an extension (e.g. one that creates a custom `ResourceBase`, which inherits from `RefCounted`) might call `~RefCounted()` and the `DCHECK` will be performed, leading to a failure.

The workaround is to add `-DNDEBUG` to the compiler flags when adding a new op. Should we update the documentation, update the `tf.sysconfig.get_compile_flags()` implementation, or modify how `RefCounted` is implemented to avoid this problem altogether? 
"
17315,feed_dict 10x slower on read-only numpy arrays,"There's a significant slowdown when feeding numpy arrays which are read-only. Looking at cpu profile, it looks like time is spent in memcpy (incorrectly reported as __nss_passwd_lookup). This memory copy seems unnnecessary

IE,
```
# arr=64-byte aligned numpy array
sess.run(some_op, feed_dict={a:arr})  # 12.6 GB/sec
arr.flags['WRITEABLE']=False
sess.run(some_op, feed_dict={a:arr})  # 1.2 GB/sec
```

[tf_numpy_benchmark.py](https://github.com/diux-dev/cluster/blob/ee5c07056a9d1dadb118aaa93e721bc81a962428/yuxin_numpy/tf_numpy_benchmark.py)

```
wget -N https://raw.githubusercontent.com/diux-dev/cluster/ee5c07056a9d1dadb118aaa93e721bc81a962428/yuxin_numpy/tf_numpy_benchmark.py
python tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf --num-iters=51
feed_cpu_tensor               :  12.9 GB/sec, min:  7.75, median:  8.98, mean:  9.01

python tf_numpy_benchmark.py --benchmark=feed_cpu_tensor --allocator=tf_readonly --num-iters=51
feed_cpu_tensor               :   1.1 GB/sec, min: 88.48, median: 91.55, mean: 93.30
```
cc @alextp"
17311,Shape inference with dynamic shapes,"I was wondering whether it is possible to use a variable as a placeholder for underspecified/dynamic shapes. Let's say the `batch_size` is dynamic (e.g. as a result from using `tf.Dataset.batch` which may produce an incomplete batch at the end of the dataset), so we have an input of images of shape `[?, h, w, c]`. Couldn't the shape inference procedure create an internal variable for `?` such that it would be able to infer the other variables at least, e.g. for functions like `tf.zeros_like`. Sorry, if this is off-topic."
17307,Dataset map KeyError from external tensors,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Arch 15 Feb 2018
- **TensorFlow installed from (source or binary)**: binary - `tensorflow-gpu` via `pip`
- **TensorFlow version (use command below)**: `v1.5.0-0-g37aa430d84 1.5.0`
- **Python version**: `3.6.4`
- **CUDA/cuDNN version**: `9.0.176`, `7.0.5-2`
- **GPU model and memory**: NVIDIA GeForce GTX 860M, 4GB
- **Exact command to reproduce**: `python dataset-map.py`

### Describe the problem

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The following script raises the subsequent `KeyError`. I don't see anything obvious in the documentation that indicates that referencing tensors not defined in the callback function is invalid, though perhaps this is a common feature/limitation of library functions that call user functions to generate subgraphs, such as `Dataset#map` and `tf.cond`.

This exact issue has an obvious workaround, but it gets a little trickier when you're referencing something more complex than a simple `tf.zeros` tensor.

The type of `Dataset` doesn't matter - I'm just using `from_generator` for brevity.

### Source code / logs

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

#### Source ([permalink](https://gist.github.com/skeggse/1da719766772da217451a64fc20b1983))

```py
import tensorflow as tf

dataset = tf.data.Dataset.from_generator(lambda: None, tf.float32)

# raises no errors
dataset.map(lambda i: (i, tf.zeros(tf.float32)))

zeros = tf.zeros(tf.float32)
# raises KeyError: 'zeros:0' (see below)
dataset.map(lambda i: (i, zeros))
```

#### Traceback

```
Traceback (most recent call last):
  File ""dataset-map.py"", line 10, in <module>
    dataset.map(lambda i: (i, zeros))
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 780, in map
    return MapDataset(self, map_func)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1591, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 486, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 321, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 376, in _create_definition_if_needed_impl
    out_names=self._out_names)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/graph_to_function_def.py"", line 185, in graph_to_function_def
    func.ret[k] = input_dict[o.name]
KeyError: 'zeros:0'
```"
17306,CudnnRNN TensorCore Support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: V100
- **Exact command to reproduce**: N/A

### Describe the problem

Feature/Status Request: Enable use of NVIDIA's TensorCores in CudnnRNN, CudnnLSTM. The requirements specified by NVIDIA are currently detailed at [1].

[1] http://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#tensor_ops
"
17303,"%matplotlib inline invalid syntax for Python 2.7, Python3.5, Python3.6 and IronPython 6.2.1","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX El Capitan
- **TensorFlow installed from (source or binary)**: NO
- **TensorFlow version (use command below)**: N/A
- **Python version**: Tried 2.7, 3.5 and 3.6.4 as well as ipython 6.2.1 same issue
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: python <scriptname>

You can collect some of this information using our environment capture script:

Let's not be ridiculous.

### Describe the problem
This is an issue with this page on your site:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb

It relates to the very first script on that page the last line of which is:
%matplotlib inline

This line of code yields a syntax error on every single platform I have tested the script (copied verbatim) on.  There is obviously something missing from your documentation.

### Source code / logs
lesson1$ ipython importall.py 
  File ""/Users/****/MachineLearning/lesson1/importall.py"", line 17
    %matplotlib inline
    ^
SyntaxError: invalid syntax

lesson1$ python importall.py 
  File ""importall.py"", line 17
    %matplotlib inline
    ^
SyntaxError: invalid syntax"
17302,Failed to load the native TensorFlow runtime,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 
- **Python version**: tensorflow_gpu-1.5
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: CUDA = 8, cuDNN = v6
- **GPU model and memory**: Nvidia 920MX , 2 GB

### Describe the problem
Not able to import Tensorflow.
Is it because I installed CUDA 8 rather than 9?

### Source code / logs
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/scene-analysis/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

"
17301,Tensorflow 1.5.0 manylinux binary requires Numpy 1.14.1,"On Travis-CI using Python 3.6 Numpy 1.13.2 is installed. Trying to use the `tensorflow` manylinux binary from PyPi will result in:

```
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
```

A log from a broken build is here:

https://travis-ci.org/TwentyBN/ionn/jobs/344881411

Fixing the `.travis.yml` like so:

https://github.com/TwentyBN/ionn/commit/24ced51bb26d78aa5fed265ed9196f47da4d7d6d

Produces a green build like here:

https://travis-ci.org/TwentyBN/ionn/jobs/346796560

Reading the build log, Numpy is updated like so:

```
$ pip install -U numpy
Collecting numpy
  Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)
    100% |████████████████████████████████| 12.2MB 57kB/s 
Installing collected packages: numpy
  Found existing installation: numpy 1.13.3
    Uninstalling numpy-1.13.3:
      Successfully uninstalled numpy-1.13.3
Successfully installed numpy-1.14.1
```

Please set the minimum version for Tensorflow 1.5.0 on Python 3.6 to Numpy 1.14.1. Thanks!"
17300,"Why Run a short TensorFlow program，print b'Hello,TensorFlow' ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17297,Expectations of MonitoredTrainingSession for saving and restoring (fixes inside),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No, anything discussed here is valid with the cifar10 model script -- https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.p

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 16.04, but should be platform independent

- **TensorFlow installed from (source or binary)**:

Binary

- **TensorFlow version (use command below)**:

1.5

- **Python version**: 

3.5

- **Bazel version (if compiling from source)**:

N / A

- **GCC/Compiler version (if compiling from source)**:

N / A

- **CUDA/cuDNN version**:

7

- **GPU model and memory**:

- **Exact command to reproduce**:

### Describe the problem

`MonitoredTrainingSession` is the simplest and, to my knowledge, often recommended way to run a training session that can easily be saved and restored. I think there is some confusion here that could be cleared up by fixing two things:

1) `MonitoredTrainingSession` does not restore the global step.

If one creates a step with `tf.train.get_or_create_global_step()`, and even if this is passed into the optimizer, `MonitoredTrainingSession` will not restore the correct value in a new session. Instead, it starts it again from 0. This has been mentioned several times, here are some references:

- https://github.com/tensorflow/tensorflow/issues/6081
- https://stackoverflow.com/questions/36113090/how-to-get-the-global-step-when-restoring-checkpoints-in-tensorflow
- https://stackoverflow.com/questions/47932738/tensorflow-restoring-model-in-a-monitoredsession?rq=1

The only solution that I've found works is to extract the step from the checkpoint filename manually, and add an operation to set it on restore, via something like

`step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])`

It would be great if this could be fixed so global steps are properly restored as well.

2) Instead of `checkpoint_dir` as an argument, `MonitoredTrainingSession` should probably have a `save_checkpoint_dir` and a `restore_checkpoint_dir`.

The reason why this is important is that TensorBoard does not support appending to summary files. This means that a restored session, to my knowledge, can only be viewed in TensorBoard if a new directory is used to write the new event files instead of the directory it was restored from. If that is not done, TensorBoard simply strips the events away and does not display them. It took me time to understand this was what was happening.

Unfortunately, `MonitoredTrainingSession` does not support a different save directory at the moment, so the way to make `MonitoredTrainingSession` work with TensorBoard on restore is to create your own summary hooks. But this defeats part of what `MonitoredTrainingSession` is supposed to do for you.

I believe this would be fixed with different save and restore directory, as specified above. A comment about how a different save directory is needed for TensorBoard visualization would also be helpful in the `MonitoredTrainingSession` documentation."
17296,retraining on gpu causing high clone loss,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:  0.9
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: gtx 1050 ti
- **Exact command to reproduce**:

Tensorflow gpu produces strange clone loss(very high) while retraining with resnet 50, the loss though is not produced while retraining on a cpu. What could be the cause of this behaviour?
![resnet_50](https://user-images.githubusercontent.com/6937974/36727785-895590c4-1be4-11e8-8449-c5a8c7d28fc0.png)

"
17294,FileWriter instance can not invoke close normally,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
4.9.0-4-amd64 #1 SMP Debian 4.9.65-3+deb9u1 (2017-12-23) x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**:
via pip3 for python3.5.3 
- **TensorFlow version (use command below)**:
v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**: NO
- **GCC/Compiler version (if compiling from source)**: NO
- **CUDA/cuDNN version**: NO
- **GPU model and memory**: 256G RAM
- **Exact command to reproduce**:
python virtualenv and tensorboard

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

### Describe the problem
loggingMXNet summary data with tensorflow-tensorboard, but FileWriter close method can not execute successfully without any errors or warnings

### Source code / logs
in my case , without any extra info, but just i use minist dataset to training MXNet digits recongnition model and wrap tensorboard event writer into MXNet fit callback , by location, i found close method failed to exec.


IS there anyone faced with similar/same issue? if exists, please share your ideas.
"
17293,Bazel Build fails after updating,"After updating my local copy of TensorFlow from the github repository the Bazel build failed with the following error (see below). However, the same command successfully built version 1.5 of TensorFlow. 

OS: SLES12
Python version: 3.6
Bazel version: Build label: 0.7.0- (@non-git)
gcc version 7.2.0 (GCC)
No GPU
No CUDA

build command:
bazel build --config=mkl --copt=""-DEIGEN_USE_VML"" -s -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures

ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/BUILD:399:12: Label '//tensorflow:tools/integration_tests/gcs_smoke_test/gcs_smoke.py' crosses boundary of subpackage 'tensorflow/tools/integration_tests/gcs_smoke_test' (perhaps you meant to put the colon here: '//tensorflow/tools/integration_tests/gcs_smoke_test:gcs_smoke.py'?).
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/BUILD:399:12: Label '//tensorflow:tools/integration_tests/gcs_smoke_test/setup.sh' crosses boundary of subpackage 'tensorflow/tools/integration_tests/gcs_smoke_test' (perhaps you meant to put the colon here: '//tensorflow/tools/integration_tests/gcs_smoke_test:setup.sh'?).
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/BUILD:399:12: Label '//tensorflow:tools/integration_tests/gcs_smoke_test/BUILD.bazel' crosses boundary of subpackage 'tensorflow/tools/integration_tests/gcs_smoke_test' (perhaps you meant to put the colon here: '//tensorflow/tools/integration_tests/gcs_smoke_test:BUILD.bazel'?).
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/BUILD:399:12: Label '//tensorflow:tools/integration_tests/gcs_smoke_test/teardown.sh' crosses boundary of subpackage 'tensorflow/tools/integration_tests/gcs_smoke_test' (perhaps you meant to put the colon here: '//tensorflow/tools/integration_tests/gcs_smoke_test:teardown.sh'?).
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/BUILD:399:12: Label '//tensorflow:tools/integration_tests/gcs_smoke_test/test_wrapper.sh' crosses boundary of subpackage 'tensorflow/tools/integration_tests/gcs_smoke_test' (perhaps you meant to put the colon here: '//tensorflow/tools/integration_tests/gcs_smoke_test:test_wrapper.sh'?).
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/tools/pip_package/BUILD:134:1: Target '//tensorflow:windows' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.
ERROR: /home/hpc/pr28fa/di72giz/TENSORFLOW/tensorflow/tensorflow/tools/pip_package/BUILD:134:1: Target '//tensorflow:windows_msvc' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed.


"
17292,"Feature request: add operators support in tflite : ExpandDims, Prod, Slice","i have my own network trained, and when i want to convert it to tflite mode, the toco gives my the error:
2018-02-27 16:57:12.122124: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 103680 bytes, theoretical optimal value: 103680 bytes.
2018-02-27 16:57:12.122280: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Prod, Slice.
Aborted (core dumped)
for slice(for example, to change the tf.slice to [n:m] operator), i have tried several way but none of them work."
17291,Screen goes black - CUDA_ERROR_UNKNOWN  on TensorFlow-GPU 1.5,"**Machine specs:**
Operating System: Windows 7 64 bits
TensorFlow version: 1.5.0 
Python version: 3.6 (Anaconda)
CUDA/cuDNN version: CUDA 9.0, cudNN 7.0.5
GPU model and memory: Quadro K2000 2GB
Note: GPU is also driving the display.

While testing, my demo runs well until the screen goes black for a short instant of time and it crashes with the following error message:

```
2018-02-27 11:45:23.275524: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: AVX
2018-02-27 11:45:23.533539: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with pro
perties:
name: Quadro K2000 major: 3 minor: 0 memoryClockRate(GHz): 0.954
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.79GiB
2018-02-27 11:45:23.552540: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow dev
ice (/device:GPU:0) -> (device: 0, name: Quadro K2000, pci bus id: 0000:01:00.0,
 compute capability: 3.0)
[I 11:45:53.864 NotebookApp] Saving file at /Mask_RCNN/demo.ipynb
2018-02-27 11:46:41.003970: W C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\core\common_runtime\bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran
 out of memory trying to allocate 2.07GiB. The caller indicates that this is not
 a failure, but may mean that there could be performance gains if more memory is
 available.
2018-02-27 11:46:52.468626: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\stream_executor\cuda\cuda_driver.cc:1080] failed to synchronize the
 stop event: CUDA_ERROR_UNKNOWN
2018-02-27 11:46:52.584632: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Internal: error destroying C
UDA event in context 00000000258FE360: CUDA_ERROR_UNKNOWN
2018-02-27 11:46:52.585632: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Internal: error destroying C
UDA event in context 00000000258FE360: CUDA_ERROR_UNKNOWN
2018-02-27 11:46:52.585632: F C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3
6\tensorflow\stream_executor\cuda\cuda_dnn.cc:2289] failed to set stream for cud
nn handle: CUDNN_STATUS_MAPPING_ERROR
[I 11:47:24.287 NotebookApp] KernelRestarter: restarting kernel (1/5), keep rand
om ports
WARNING:root:kernel 134641e2-3f62-4019-9536-be72010310fa restarted
```

I tried for several days different ideas from different issues:
Checked carefully the video tutorial from youtube, nvidia installation manual, etc.
Reinstalled cuda, cudnn
Downgrade tf, cuda, cudnn

Could you suggest any method to detect what is wrong and how to solve the problem?

Post-data:
**Code works fine on tensorflow without GPU.**"
17290,Windows Compile Issue:  absl is not a class or namespace name,"Hi,
I am trying to compile tensorflow from source in windows using bazel 0.11.0 and vc14, and I am getting the following error:
`~\igwos0gz\execroot\org_tensorflow\tensorflow\core\platform\windows\port.cc(153): error C2653: 'absl': is not a class or namespace name
`
Have I written custom code? no
OS Platform and Distribution? windows 10 - x64
TensorFlow installed from? git repo
TensorFlow version? current master branch (1.7rc?)
Bazel version? 0.11.0
CUDA/cuDNN version? 9.1/7
GPU model and memory? nvidia 960m
Exact command to reproduce?
`bazel build -c opt --action_env=USE_MSVC_WRAPPER=1 //tensorflow/tools/pip_package:build_pip_package`"
17289,TFLite 'Minimum' Operator,"I have a fine-tuned MobileNet model, in frozen graph format. I ran Toco to create a TFLite model, but got the following error:

```Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: TensorFlowMinimum.```

Examining the ops in the graph, I see ones like `mobilenet_1.00_224/conv1_relu/Minimum`.

"
17288,"OutOfRangeError (see above for traceback): box_index has values outside [0, batch_size)","```
SYSTEM INFO:
OS: Ubuntu 16.04 LTS
Tensorflow version: 1.4.0
Python version: 2.7.12
```
My issue is very similar to this one here: [issue #10618](https://github.com/tensorflow/tensorflow/issues/10618)
I have been doing a multi-GPU session and I get some weird behavior from Tensorflow. 
My script only can finish its run in 1 out of 6 attempts, and in each finished execution, the result is slightly different from the single-GPU execution mode (unrepeatable error). If the execution cannot finish, most of the time it reports `OutOfRangeError (see above for traceback): box_index has values outside [0, batch_size)`
, which is not supposed to appear for tensorflow 1.4 and above, according to the issue above. 
Moreover, sometimes it would report `InternalError (see above for traceback): Blas SGEMM launch failed : m=14700, n=2048, k=1024`

Any advice would be greatly appreciated, thanks!"
17287,text manipulation in tensorflow,"Hi,
I want to make my text files in lower case. I tried following code:


```
def _parse_line(line):

       line_split = tf.string_split([line], '\t')
       raw_text = tf.py_func(lambda x: x.strip().lower(), [line_split.values[0]], tf.string)
       label = tf.string_to_number(line_split.values[1], out_type=tf.int32)
       return {""raw_text"": raw_text, ""label"": label}

sess = tf.Session()
f = './test.txt'
dataset = tf.data.TextLineDataset([f])
dataset = dataset.map(_parse_line)
i = dataset.make_one_shot_iterator()
c = i.get_next()
sess.run(c)
```

and unfortunately I got error:

slice index 1 of dimension 0 out of bounds.
	 [[Node: strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_STRING, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](StringSplit:1, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)]]
	 [[Node: IteratorGetNext_9 = IteratorGetNext[output_shapes=[[], <unknown>], output_types=[DT_INT32, DT_STRING], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator_9)]]

I'd be so grateful to help me."
17285,"tensorflow lite so cannot add opencv dependence, cause ""library not found"" error","Version info:
tensorflow r1.4
ubuntu 14.04
armv7 platform
android 5.1.1
ndk version: r14
android studio 2.3.1
when compile tensorflow lite so, if i add the opencv dependence like:
deps = [
        ""//tensorflow/contrib/lite:context"",
        ""//tensorflow/contrib/lite:framework"",
        ""//tensorflow/contrib/lite:schema_fbs_version"",
        ""//tensorflow/contrib/lite/kernels:builtin_ops"",
        ""@dlib_arm_v7//:dlib"",
        ""@seeta_arm_v7//:seeta"",
        #""@opencv_jni//:opencv_jni"",
    ],
the compiling is ok, but when i move the .so file to my android project, it will give the error cannot find the library.
if i comment the opencv line, it is ok.
it is a Compatibility problem between opencv and tensorflow lite.

"
17284,tensor-valued seeds in tf.data API can result in nondeterministic results,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.5.0-11-g4588350f20 1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See code below

### Describe the problem
The `tf.data` API allows/requires seeds to be provided that are `tf.tensor`s. This is an issue when the graph-level seed has been set to 0, and the provided op-level seed tensor takes on a value of 0. As noted in the comments in the code for `tf.get_seed`, a `(0, 0)` seed is problematic because the C++ ops assume this means nondeterminism. Of course, when a user is specifying these seeds, they're expecting deterministic behaviour. Unfortunately, `tf.get_seed` only checks for this issue for the case where the seeds are ints, not tensors. See the code below for an example.

I would have been happy to submit a PR for this, but I have no idea where the fix should be for this bug. As I'm not especially familiar with the code base, it's not apparent whether it's even possible to have the code for `tf.get_seed` to check the value of a tensor seed. If not, I'm guessing the `tf.data` API would need to provide the checks.

### Source code / logs
The following code reproduces the bug for me:
```python
import tensorflow as tf

tf.set_random_seed(0)
seed_tensor = tf.placeholder(tf.int64, shape=[], name='data_seed')
data = tf.data.Dataset.range(10).shuffle(10, seed=seed_tensor)
iterator = tf.data.Iterator.from_structure(tf.int64, tf.TensorShape([]))

init = iterator.make_initializer(data)
value = iterator.get_next()

print('First run: ', end='')
with tf.Session() as sess1:
    sess1.run(init, feed_dict={seed_tensor: 0})
    values = []
    while True:
        try:
            values.append(str(sess1.run(value)))
        except tf.errors.OutOfRangeError:
            break

    print(', '.join(values))

print('Second run: ', end='')
with tf.Session() as sess2:
    sess2.run(init, feed_dict={seed_tensor: 0})
    values = []
    while True:
        try:
            values.append(str(sess2.run(value)))
        except tf.errors.OutOfRangeError:
            break

    print(', '.join(values))
```

The result I get is:
```
First run: 8, 4, 6, 9, 1, 0, 5, 7, 3, 2
Second run: 1, 6, 3, 0, 7, 5, 2, 9, 8, 4
```

I would expect the first and second runs to produce the exact same sequence, though the particular sequence might differ from environment to environment.

If I change the `feed_dict` to provide a value of `0` for `seed_tensor` for both runs, I get the expected result:

```
First run: 5, 2, 0, 1, 7, 4, 9, 3, 8, 6
Second run: 5, 2, 0, 1, 7, 4, 9, 3, 8, 6
```

If I change the `shuffle()` call to take a value of 0 directly (i.e., `...shuffle(10, seed=0)), I get the expected result as well:

```
First run: 4, 8, 7, 1, 3, 2, 6, 9, 0, 5
Second run: 4, 8, 7, 1, 3, 2, 6, 9, 0, 5
```
For the record, I encountered this issue because I'm using an `Iterator` via `Iterator.from_structure`, and when I use that iterator on a `Dataset.shuffle()` I get the same order for each epoch. To get around this, I provided the epoch number as a seed to `Dataset.shuffle()`, with the first epoch being epoch 0. In my case I can avoid this bug by just starting the epoch count at 1, but it took me a while to track down, and there may be other cases where the API is used in a similar way that would be problematic for those expecting deterministic results."
17275,Feature request: tf.gather with multidimensional axis,"As the title says, I would like to gather multiple dimensions in the tf.gather function. An example situation would be:

a = tf.random_normal([2,3,4])
b = tf.gather(a, [[0, 0], [1, 0]], axis=[1,2])
assert b[:, 0] == a[:, 0, 0]
assert b[:, 1] == a[:, 1, 0]

When a dimension in axis is not listed, the function will gather all the entries in that dimension, as shown above. Is this possible?"
17274,Unable to get FLOPs on model with tf.nn.bidirectional_dynamic_rnn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian/Sid
- **TensorFlow installed from (source or binary)**: binary, tensorflow-gpu
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0/CuDNN v7
- **GPU model and memory**: GTX1080, 8GB

Running the model under `benchmark_model`, I'm unable to get any FLOPs value, and there are several strange pieces in the output:
```
2018-02-26 14:37:55.738267: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/fw/fw/while/Switch
2018-02-26 14:37:55.738500: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/bw/bw/while/Switch_1
2018-02-26 14:37:55.738507: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/bw/bw/while/Switch
2018-02-26 14:37:55.739153: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/fw/fw/while/Switch_3
2018-02-26 14:37:55.739165: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/fw/fw/while/Switch_2
2018-02-26 14:37:55.739171: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/fw/fw/while/Switch_4
2018-02-26 14:37:55.739300: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/bw/bw/while/Switch_3
2018-02-26 14:37:55.739307: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/bw/bw/while/Switch_4
2018-02-26 14:37:55.739311: W tensorflow/core/util/stat_summarizer.cc:78] Output tensor changed between runs for 'bidirectional_rnn/bw/bw/while/Switch_2
```

And
```
2018-02-26 14:38:33.921383: E tensorflow/tools/benchmark/benchmark_model.cc:579] FLOPs calculation failed with Internal: Retval[4] has already been set.
```"
17273,"Numerical error with tf.nn.conv2d, tf.nn.moments and tf.sqrt","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (see code below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux Enterprise Server 12.2 (x86_64)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0 (git v1.5.0-0-g37aa430d84) & also present in 1.4.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.10.1
- **GCC/Compiler version (if compiling from source)**: GCC/7.2.0
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: See script below

### Describe the problem
There is a numerical error with a very specific sequence of operations, that make tf.sqrt() return a tensor of zeros when applied on the output of tf.nn.moments (regardless of the value returned by tf.nn.moments). This error happens when the input of tf.nn.moments comes from tf.nn.conv2d, but not when removing the latter.

This error happens when running on a cluster where TensorFlow has been installed from source with ""-m64 -march=native -mtune=native"". We tried upgrading from 1.4.0 to 1.5.0, but the error still occurs. The following flags were used during compilation:

TF_NEED_JEMALLOC=""1""
TF_NEED_HDFS=""1""
TF_NEED_OPENCL=""0""
TF_NEED_CUDA=""0""
TF_ENABLE_XLA=""0""
TF_CUDA_CLANG=""0""
TF_NEED_GCP=""0""
TF_NEED_MKL=""1""
TF_NEED_VERBS=""0""
TF_NEED_MPI=""1""
TF_NEED_S3=""0""
TF_NEED_GDR=""0""
TF_NEED_OPENCL_SYCL=""0""
TF_MKL_ROOT=""[DIR]/mkl-dnn/external/mklml_lnx_2018.0.1.20171007""
export MPI_HOME=""[DIR]/2018.1.038/compilers_and_libraries_2018.1.163/linux/mpi/intel64""

Android build: No.

Compiled using Intel MPI plus complete MKL (2018.1.038) loaded in the environment.


We also tested this same code in two other machines, where it works properly. One of them is an older cluster, with TensorFlow compiled from source using the same configuration; the main difference is that the processors in this cluster do not support AVX512. The other machine is a laptop where TensorFlow was installed with pip.

### Source code / logs

The smallest code to reproduce the error is the following:

```
import tensorflow as tf
import numpy as np

# TensorFlow version
print('TF version:', tf.__version__, tf.__git_version__, '\n')

# Create placeholders
o = tf.placeholder(tf.float32, [None, 84, 84, 4])

# Create graph: conv(x, w) -> (x-mean(x))/std(x)
x = o

w = tf.get_variable(""w"", [3, 3, x.get_shape()[-1], 16])
x = tf.nn.conv2d(x, w, [1, 1, 1, 1], padding=""SAME"")  # remove this and the error disappears...

avg, variance = tf.nn.moments(x, np.arange(len(x.get_shape().as_list()) - 1), keep_dims=True)
epsilon = 1e-3  # for numerical stability in sqrt

variance = tf.Print(variance, [avg], message='mean(x): ')
variance = tf.Print(variance, [variance], message='variance(x): ')
variance = tf.Print(variance, [tf.sqrt(variance + epsilon)], message='sqrt(variance(x)): ')
variance = tf.Print(variance, [tf.sqrt(avg**2 + epsilon)], message='sqrt(mean(x)^2): ')

x = (x - avg) / (tf.sqrt(variance + epsilon))

# Run
sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())
sess.run(x, feed_dict={o: np.random.rand(32, 84, 84, 4)})
```

And the output:

mean(x): [[[[0.176351935 -0.11745432 -0.17503795]]]...]
variance(x): [[[[0.0344150327 0.0378020629 0.0353332]]]...]
sqrt(variance(x)): [[[[0 0 0]]]...]
sqrt(mean(x)^2): [[[[0 0 0]]]...]
"
17272,Using tf.estimator.Estimator with save_checkpoint_steps leads to Tensorboard warnings,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.4.0-104-generic
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**: GeForce GTX TITAN X

### Describe the problem
When using `tf.estimator.train_and_evaluate(...)` with an `tf.estimator.Estimator` configurated with `tf.contrib.learn.RunConfig(save_checkpoints_steps=10, ...)`, a `CheckpointSaverHook` will be created automatically. This `CheckpointSaverHook` will save the graph and graph_def to the summary writer every time it is triggered (see [CheckpointSaverHook.before_run](https://github.com/tensorflow/tensorflow/blob/36b83257287148ddb4aab02c641d1ac9e192a136/tensorflow/python/training/basic_session_run_hooks.py#L440)). 

### Basic code example:

<!-- language: lang-py -->

    estimator = tf.estimator.Estimator(
        model_fn, model_dir, params,
        config=tf.estimator.RunConfig(
            save_checkpoints_steps=100, 
            save_summary_steps=100
        )
    )
    train_spec = tf.estimator.TrainSpec(train_fn)
    eval_spec = tf.estimator.TrainSpec(eval_fn)
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

When starting Tensorboard on the written summary, it will output a hundreds of warnings because of multiple graph defs in the summary which I guess slows it down a lot on startup:

<!-- language: lang-none -->

    W0117 18:47:30.278879 Reloader tf_logging.py:86] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.
    W0117 18:47:30.279753 Reloader tf_logging.py:86] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.

I see there might be issues when using multiple graphs, but for a single graph this seems unpracticable.

Related stack overflow discussion: https://stackoverflow.com/questions/48316888"
17270,"same code, tensorflow/contribe/android works, but tensorflow/contribe/lite/java/src/main/native doesn't, with error: No implementation found","tensorflow r1.4
ubuntu 14.04
armv7 platform
for short, the same code witch can be used in tensorflow/cotribe/android/jni, when move to tensorflow/contribe/lite/java/src/main/native, the java code cannot find the implementation.
use the command line:
bazel build -c opt --cxxopt='-std=c++11' //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a
to build an android so file, it is ok.
and i modify the BUILD file to add my own code:
cc_library(
    name = ""native_framework_only"",
    srcs = [
        #""run_stats_jni.cc"",
        #""exception_jni.cc"",
        #""nativeinterpreterwrapper_jni.cc"",
        #""tensor_jni.cc"",
        #""tensorflow_lite_jni.cc"",
        #""run_stats_lite_jni.cc"",
    ] + select({
        # The Android toolchain makes ""jni.h"" available in the include path.
        # For non-Android toolchains, generate jni.h and jni_md.h.
        ""//tensorflow:android"": [],
        ""//conditions:default"": [
            "":jni.h"",
            "":jni_md.h"",
        ],
    }),
    hdrs = [
        ""run_stats_jni.h"",
        #""exception_jni.h"",
        #""nativeinterpreterwrapper_jni.h"",
        #""tensor_jni.h"",
        #""tensorflow_lite_jni.h"",
        #""run_stats_lite_jni.h"",
        #""testlite.h"",
    ],
    copts = tflite_copts(),
    includes = select({
        ""//tensorflow:android"": [],
        ""//conditions:default"": ["".""],
    }),
    linkopts = [
        ""-lm"",
        ""-ldl"",
        ""-llog"",
    ],
    tags = [
        ""manual"",
    ],
    deps = [
        ""//tensorflow/contrib/lite:context"",
        ""//tensorflow/contrib/lite:framework"",
        ""//tensorflow/contrib/lite:schema_fbs_version"",
        ""//tensorflow/contrib/lite/kernels:builtin_ops"",
        ""@dlib_arm_v7//:dlib"",
        ""@seeta_arm_v7//:seeta"",
        ""@opencv_jni//:opencv_jni"",
    ],
    alwayslink = 1,
)
the file run_stats_jni.h is just from tensorflow/contribe/android/jni, the function i want to call from native is:
JNIEXPORT jint RUN_STATS_METHOD(jniSetInterval)(JNIEnv* env, jclass clazz, jint interval)
{
    int val = (int)interval;
    return JNI_OK;
}
and using the command line:
bazel build -c opt --cxxopt='-std=c++11' //tensorflow/contrib/lite/java:libtensorflowlite_jni.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a
and when i using nm -D command to check the libtensorflowlite_jni.so and libtensorflow_inference.so, they all have the same name Java_org_tensorflow_jniSetInterval. but libtensorflow_inference.so works, but libtensorflowlite_jni.so does not.
the error is: 
No implementation found for int org_tensorflow.jniSetInterval(int) (tried Java_org_tensorflow_jniSetInterval and Java_org_tensorflow_jniSetInterval__I)

lack of document to tell us how to add own code to tensorflow lite, and it is wierd, may be a bug."
17269,Can't load model file,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
   env from [google colab](https://colab.research.google.com/)
- **TensorFlow installed from (source or binary)**:
  env from [google colab](https://colab.research.google.com/)
- **TensorFlow version (use command below)**:
``` 1.6.0-rc1  ```
- **Python version**: 
```python 3.6```

### Describe the problem
I have code like this:

```
slim.assign_from_checkpoint_fn(
        pretrained_model_file,
        variables_to_restore,
        ignore_missing_vars=True)
```

when  `pretrained_model_file = ""vgg_19.ckpt""`, where vgg_19.ckpt is a checkpoint file from pre-trained model, it raise Exception:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-29-b803c1b9c409> in <module>()
     10 
     11 
---> 12 demo()

<ipython-input-29-b803c1b9c409> in demo()
      7             ""1000"", ""--network"", ""/content/vgg_19.ckpt"",
      8             ""--checkpoint-iterations"", ""10"", ""--checkpoint-output"", ""/content/tmp-tv-%s.jpg""]
----> 9     neural_style_main(args)
     10 
     11 

/usr/local/lib/python3.6/dist-packages/neural_style_demo/neural_style.py in main(args)
    178         pooling=options.pooling,
    179         print_iterations=options.print_iterations,
--> 180         checkpoint_iterations=options.checkpoint_iterations
    181     ):
    182         output_file = None

/usr/local/lib/python3.6/dist-packages/neural_style_demo/stylize.py in stylize(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations, content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight, learning_rate, beta1, beta2, epsilon, pooling, print_iterations, checkpoint_iterations)
     77                                                     naming=""style-{}"".format(i),
     78                                                     pretrained_model_file=pretrained_model_file,
---> 79                                                     checkpoint_exclude_scopes=checkpoint_exclude_scopes)
     80 
     81         for index, layer in enumerate(STYLE_LAYERS):

/usr/local/lib/python3.6/dist-packages/neural_style_demo/losses.py in get_style_features(model_name, style_image, image_size, style_layers, naming, pretrained_model_file, checkpoint_exclude_scopes)
     97                 break
     98         if not excluded:
---> 99             variables_to_restore.append(var)
    100 
    101     return assign_from_checkpoint_fn(

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in callback(session)
    688     saver = tf_saver.Saver(var_list, reshape=reshape_variables)
    689     def callback(session):
--> 690       saver.restore(session, model_path)
    691     return callback
    692   else:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)
   1754       raise ValueError(""The specified path: %s is a file.""
   1755                        "" Please specify only the path prefix""
-> 1756                        "" to the checkpoint files."" % save_path)
   1757     logging.info(""Restoring parameters from %s"", save_path)
   1758     if context.in_graph_mode():

ValueError: The specified path: /content/vgg_19.ckpt is a file. Please specify only the path prefix to the checkpoint files.
```

If I change pretained_model_file to the folder contains checkpoint file, it raise ` Exception: No checkpoint found... `.

If I force to change tensorflow from 1.6 to 1.4, and  `pretrained_model_file = ""vgg_19.ckpt""`, this code can work!


"
17268,CI: TensorFlow build is failing on Windows,"Before merging #16659, we should fix the TF build on Windows first.

Current failure:
```
subprocess.CalledProcessError: Command '['bazel', '--batch', 'version']' returned non-zero exit status 36
```
Culprit is https://github.com/tensorflow/tensorflow/commit/671baf080238025da9698ea980cd9504005f727c
because `f.write('import %s\n' % _TF_BAZELRC)` writes backslash into bazelrc file without escaping.
I'll send a fix.
FYI, @gunan @martinwicke @case540 
"
17267,CMP0002 error when building TensorFlow cc unit tests,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Window 10
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:VS2015 x64
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When I tried to do TensorFlow CMake build on Window 10 like [this](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake),there is no error.

Then I tried to build cc unit tests. I added -Dtensorflow_BUILD_CC_TESTS=ON to the command

`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\swigwin-3.0.12\swigwin-3.0.12\swig.exe -DPYTHON_EXECUTABLE=D:\Local\anaconda3\python.exe -DPYTHON_LIBRARIES=D:\Local\anaconda3\libs\python36.lib -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_CC_TESTS=ON`

The cmake infomation is as following:

```
-- Building for: Visual Studio 14 2015
-- Selecting Windows SDK version 10.0.14393.0 to target Windows 10.0.16299.
-- The C compiler identification is MSVC 19.0.24215.1
-- The CXX compiler identification is MSVC 19.0.24215.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Found PythonInterp: D:/Local/anaconda3/python.exe (found version ""3.6.3"")
-- Found PythonLibs: D:/Local/anaconda3/libs/python36.lib (found version ""3.6.3"")
-- Found SWIG: C:/swigwin-3.0.12/swigwin-3.0.12/swig.exe (found version ""3.0.12"")

```
but I got several errors like the following

```
CMake Error at tf_tests.cmake:73 (add_executable):
  add_executable cannot create target
  ""tensorflow_core_profiler_internal_tfprof_show_test"" because another target
  with the same name already exists.  The existing target is an executable
  created in source directory ""D:/yinb_6/1.4/tensorflow/contrib/cmake"".  See
  documentation for policy CMP0002 for more details.
Call Stack (most recent call first):
  tf_tests.cmake:46 (AddTest)
  tf_tests.cmake:529 (AddTests)
  CMakeLists.txt:325 (include)
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17266,'num' must be either a scalar or vector in tf.unstack?,"```
batch_size = tf.placeholder([],dtype=tf.int32) 
tf_x = tf.placeholder(tf.float32, [None,timestep_size,input_size])     
  
lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)
init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)

inputs = tf.unstack(tf_x,num=batch_size,axis=0)

ValueError: 'num' must be either a scalar or vector, but saw tensor
```

When training and test, I want to use the different batch size. But in `unstack` function, the `num` parameter doesn't support a tensor as input. However, in `zero_state` function, the first parameter can support a tensor as input. I think the new version should support this practical feature. Thanks!"
17263,A good and practical feature is removed,"```
batch_size = tf.placeholder([],dtype=tf.int32) 
lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)
init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)

ValueError: prefix tensor must be either a scalar or vector, but saw tensor
```
When training and test, I want to use the different batch size. But in `zero_state` function, the first parameter doesn't support a tensor as input. To my knowledge, I don't meet the error with the version of tensorflow before 1.2. Now I update tensorflow to 1.2.1. I think that using the different batch size is a practical function. 
In stackoverflow, a same problem is post, but it is not solved.
[https://stackoverflow.com/questions/44961181/drqn-prefix-tensor-must-be-either-a-scalar-or-vector-but-saw-tensor](url)
How can I solve this problem? Thanks!
"
17260,Running on DLAMI ,"I've got this error when running this script on Amazon aws. 

I just want to know if this error is related to the memory space on the aws or within my installation of the project? If this is possible on first sight. 
So that I now where to ask the more specific questions.


ERROR OUTPUT:
```

2018-02-25 16:31:13.224347: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.56GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-25 16:31:14.783453: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 800.00MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-25 16:31:27.854629: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 128.00MiB.  Current allocation summary follows.
2018-02-25 16:31:27.854710: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): 	Total Chunks: 84, Chunks in use: 84. 21.0KiB allocated for chunks. 21.0KiB in use in bin. 6.8KiB client-requested in use in bin.
2018-02-25 16:31:27.854733: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): 	Total Chunks: 41, Chunks in use: 41. 20.5KiB allocated for chunks. 20.5KiB in use in bin. 20.5KiB client-requested in use in bin.
2018-02-25 16:31:27.854752: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): 	Total Chunks: 35, Chunks in use: 35. 35.2KiB allocated for chunks. 35.2KiB in use in bin. 35.0KiB client-requested in use in bin.
2018-02-25 16:31:27.854770: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): 	Total Chunks: 30, Chunks in use: 30. 60.0KiB allocated for chunks. 60.0KiB in use in bin. 60.0KiB client-requested in use in bin.
2018-02-25 16:31:27.854787: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854804: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): 	Total Chunks: 1, Chunks in use: 0. 8.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854821: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): 	Total Chunks: 8, Chunks in use: 8. 150.0KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.
2018-02-25 16:31:27.854832: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854843: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854869: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.854883: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): 	Total Chunks: 17, Chunks in use: 16. 11.51MiB allocated for chunks. 11.01MiB in use in bin. 10.25MiB client-requested in use in bin.
2018-02-25 16:31:27.854904: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): 	Total Chunks: 2, Chunks in use: 1. 2.70MiB allocated for chunks. 1.14MiB in use in bin. 800.0KiB client-requested in use in bin.
2018-02-25 16:31:27.854918: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): 	Total Chunks: 8, Chunks in use: 7. 25.00MiB allocated for chunks. 21.88MiB in use in bin. 21.88MiB client-requested in use in bin.
2018-02-25 16:31:27.854935: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): 	Total Chunks: 2, Chunks in use: 1. 10.25MiB allocated for chunks. 4.00MiB in use in bin. 3.12MiB client-requested in use in bin.
2018-02-25 16:31:27.854955: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): 	Total Chunks: 8, Chunks in use: 7. 103.46MiB allocated for chunks. 90.96MiB in use in bin. 87.50MiB client-requested in use in bin.
2018-02-25 16:31:27.854973: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): 	Total Chunks: 2, Chunks in use: 1. 41.00MiB allocated for chunks. 16.00MiB in use in bin. 12.50MiB client-requested in use in bin.
2018-02-25 16:31:27.854990: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): 	Total Chunks: 5, Chunks in use: 4. 228.00MiB allocated for chunks. 180.00MiB in use in bin. 180.00MiB client-requested in use in bin.
2018-02-25 16:31:27.855008: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): 	Total Chunks: 9, Chunks in use: 6. 657.16MiB allocated for chunks. 448.78MiB in use in bin. 340.00MiB client-requested in use in bin.
2018-02-25 16:31:27.855025: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): 	Total Chunks: 8, Chunks in use: 8. 1.21GiB allocated for chunks. 1.21GiB in use in bin. 1.00GiB client-requested in use in bin.
2018-02-25 16:31:27.855042: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): 	Total Chunks: 5, Chunks in use: 5. 1.30GiB allocated for chunks. 1.30GiB in use in bin. 1.25GiB client-requested in use in bin.
2018-02-25 16:31:27.855079: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 128.00MiB was 128.00MiB, Chunk State: 
2018-02-25 16:31:27.855099: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680000 of size 1280
2018-02-25 16:31:27.855114: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680500 of size 256
2018-02-25 16:31:27.855128: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680600 of size 256
2018-02-25 16:31:27.855142: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680700 of size 1046784
2018-02-25 16:31:27.855157: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702780000 of size 2048
2018-02-25 16:31:27.855168: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702780800 of size 2048
2018-02-25 16:31:27.855178: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702781000 of size 2048
2018-02-25 16:31:27.855190: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702781800 of size 2048
2018-02-25 16:31:27.855201: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782000 of size 256
2018-02-25 16:31:27.855214: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782100 of size 256
2018-02-25 16:31:27.855228: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782200 of size 1024
2018-02-25 16:31:27.855243: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782600 of size 1024
2018-02-25 16:31:27.855258: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782a00 of size 1024
2018-02-25 16:31:27.855272: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782e00 of size 1024
2018-02-25 16:31:27.855286: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783200 of size 1024
2018-02-25 16:31:27.855300: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783600 of size 256
2018-02-25 16:31:27.855313: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783700 of size 256
2018-02-25 16:31:27.855327: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783800 of size 512
2018-02-25 16:31:27.855341: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783a00 of size 512
2018-02-25 16:31:27.855354: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783c00 of size 512
2018-02-25 16:31:27.855368: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783e00 of size 512
2018-02-25 16:31:27.855382: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784000 of size 512
2018-02-25 16:31:27.855396: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784200 of size 256
2018-02-25 16:31:27.855409: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784300 of size 256
2018-02-25 16:31:27.855423: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784400 of size 256
2018-02-25 16:31:27.855436: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784500 of size 256
2018-02-25 16:31:27.855450: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784600 of size 256
2018-02-25 16:31:27.855463: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784700 of size 256
2018-02-25 16:31:27.855476: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784800 of size 256
2018-02-25 16:31:27.855489: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784900 of size 256
2018-02-25 16:31:27.855502: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784a00 of size 256
2018-02-25 16:31:27.855516: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784b00 of size 256
2018-02-25 16:31:27.855531: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784c00 of size 256
2018-02-25 16:31:27.855544: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784d00 of size 256
2018-02-25 16:31:27.855558: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784e00 of size 256
2018-02-25 16:31:27.855572: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784f00 of size 256
2018-02-25 16:31:27.855585: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785000 of size 256
2018-02-25 16:31:27.855601: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785100 of size 512
2018-02-25 16:31:27.855615: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785300 of size 512
2018-02-25 16:31:27.855628: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785500 of size 512
2018-02-25 16:31:27.855642: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785700 of size 512
2018-02-25 16:31:27.855655: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785900 of size 512
2018-02-25 16:31:27.855668: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785b00 of size 256
2018-02-25 16:31:27.855682: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785c00 of size 256
2018-02-25 16:31:27.855700: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785d00 of size 1024
2018-02-25 16:31:27.855723: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786100 of size 1024
2018-02-25 16:31:27.855749: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786500 of size 1024
2018-02-25 16:31:27.855765: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786900 of size 1024
2018-02-25 16:31:27.855778: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786d00 of size 1024
2018-02-25 16:31:27.855797: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787100 of size 256
2018-02-25 16:31:27.855820: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787200 of size 256
2018-02-25 16:31:27.855843: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787300 of size 2048
2018-02-25 16:31:27.855864: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787b00 of size 2048
2018-02-25 16:31:27.855875: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702788300 of size 2048
2018-02-25 16:31:27.855888: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702788b00 of size 2048
2018-02-25 16:31:27.855898: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789300 of size 2048
2018-02-25 16:31:27.855911: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789b00 of size 256
2018-02-25 16:31:27.855927: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789c00 of size 256
2018-02-25 16:31:27.855949: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789d00 of size 256
2018-02-25 16:31:27.855969: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789e00 of size 256
2018-02-25 16:31:27.855991: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789f00 of size 256
2018-02-25 16:31:27.856011: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70278a000 of size 19200
2018-02-25 16:31:27.856035: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70278eb00 of size 819200
2018-02-25 16:31:27.856055: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702856b00 of size 19200
2018-02-25 16:31:27.856077: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285b600 of size 512
2018-02-25 16:31:27.856096: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285b800 of size 512
2018-02-25 16:31:27.856118: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285ba00 of size 1024
2018-02-25 16:31:27.856138: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285be00 of size 1196544
2018-02-25 16:31:27.856162: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702980000 of size 4194304
2018-02-25 16:31:27.856182: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702d80000 of size 16777216
2018-02-25 16:31:27.856205: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703d80000 of size 2048
2018-02-25 16:31:27.856224: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703d80800 of size 524288
2018-02-25 16:31:27.856247: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00800 of size 256
2018-02-25 16:31:27.856267: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00900 of size 256
2018-02-25 16:31:27.856289: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00a00 of size 2048
2018-02-25 16:31:27.856308: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e01200 of size 16248320
2018-02-25 16:31:27.856332: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x704d80000 of size 67108864
2018-02-25 16:31:27.856351: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80000 of size 256
2018-02-25 16:31:27.856373: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80100 of size 1024
2018-02-25 16:31:27.856393: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80500 of size 3276800
2018-02-25 16:31:27.856416: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7090a0500 of size 819200
2018-02-25 16:31:27.856435: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709168500 of size 819200
2018-02-25 16:31:27.856457: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230500 of size 512
2018-02-25 16:31:27.856476: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230700 of size 512
2018-02-25 16:31:27.856498: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230900 of size 3276800
2018-02-25 16:31:27.856518: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709550900 of size 3276800
2018-02-25 16:31:27.856539: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870900 of size 512
2018-02-25 16:31:27.856558: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870b00 of size 512
2018-02-25 16:31:27.856580: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870d00 of size 19200
2018-02-25 16:31:27.856600: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709875800 of size 1024
2018-02-25 16:31:27.856622: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709875c00 of size 1024
2018-02-25 16:31:27.856646: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709876000 of size 13107200
2018-02-25 16:31:27.856669: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70a4f6000 of size 13107200
2018-02-25 16:31:27.856688: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b176000 of size 819200
2018-02-25 16:31:27.856711: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b23e000 of size 819200
2018-02-25 16:31:27.856730: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b306000 of size 2048
2018-02-25 16:31:27.856751: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b306800 of size 2048
2018-02-25 16:31:27.856775: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b307000 of size 524288
2018-02-25 16:31:27.856798: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b387000 of size 524288
2018-02-25 16:31:27.856813: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407000 of size 256
2018-02-25 16:31:27.856828: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407100 of size 256
2018-02-25 16:31:27.856844: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407200 of size 2048
2018-02-25 16:31:27.856866: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407a00 of size 2048
2018-02-25 16:31:27.856885: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b408200 of size 13107200
2018-02-25 16:31:27.856909: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70c088200 of size 13598208
2018-02-25 16:31:27.856929: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70cd80000 of size 52428800
2018-02-25 16:31:27.856952: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70ff80000 of size 81788928
2018-02-25 16:31:27.856972: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80000 of size 256
2018-02-25 16:31:27.856994: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80100 of size 1024
2018-02-25 16:31:27.857013: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80500 of size 1024
2018-02-25 16:31:27.857035: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80900 of size 256
2018-02-25 16:31:27.857054: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80a00 of size 3276800
2018-02-25 16:31:27.857076: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7150a0a00 of size 3276800
2018-02-25 16:31:27.857099: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7153c0a00 of size 13107200
2018-02-25 16:31:27.857119: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x716040a00 of size 13107200
2018-02-25 16:31:27.857130: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7185c0a00 of size 524288
2018-02-25 16:31:27.857143: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718640a00 of size 524288
2018-02-25 16:31:27.857153: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7186c0a00 of size 524288
2018-02-25 16:31:27.857165: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718740a00 of size 2048
2018-02-25 16:31:27.857180: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718741200 of size 2048
2018-02-25 16:31:27.857203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718741a00 of size 2048
2018-02-25 16:31:27.857222: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718742200 of size 2048
2018-02-25 16:31:27.857244: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718742a00 of size 2048
2018-02-25 16:31:27.857263: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718743200 of size 2048
2018-02-25 16:31:27.857285: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718743a00 of size 3276800
2018-02-25 16:31:27.857308: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718a63a00 of size 3276800
2018-02-25 16:31:27.857328: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c3a00 of size 1024
2018-02-25 16:31:27.857339: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c3e00 of size 1024
2018-02-25 16:31:27.857352: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4200 of size 1024
2018-02-25 16:31:27.857362: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4600 of size 1024
2018-02-25 16:31:27.857375: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4a00 of size 1024
2018-02-25 16:31:27.857390: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4e00 of size 1024
2018-02-25 16:31:27.857412: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5200 of size 1024
2018-02-25 16:31:27.857434: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5600 of size 1024
2018-02-25 16:31:27.857453: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5a00 of size 1024
2018-02-25 16:31:27.857464: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5e00 of size 819200
2018-02-25 16:31:27.857477: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71948de00 of size 819200
2018-02-25 16:31:27.857487: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e5e00 of size 512
2018-02-25 16:31:27.857500: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6000 of size 512
2018-02-25 16:31:27.857515: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6200 of size 512
2018-02-25 16:31:27.857538: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6400 of size 512
2018-02-25 16:31:27.857560: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6600 of size 512
2018-02-25 16:31:27.857579: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6800 of size 512
2018-02-25 16:31:27.857590: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6a00 of size 512
2018-02-25 16:31:27.857604: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6c00 of size 512
2018-02-25 16:31:27.857618: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6e00 of size 512
2018-02-25 16:31:27.857631: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e7000 of size 19200
2018-02-25 16:31:27.857648: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196ebb00 of size 800000
2018-02-25 16:31:27.857670: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af000 of size 256
2018-02-25 16:31:27.857698: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af100 of size 256
2018-02-25 16:31:27.857717: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af200 of size 256
2018-02-25 16:31:27.857727: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af300 of size 256
2018-02-25 16:31:27.857740: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af400 of size 256
2018-02-25 16:31:27.857750: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af500 of size 256
2018-02-25 16:31:27.857762: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af600 of size 256
2018-02-25 16:31:27.857777: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af700 of size 256
2018-02-25 16:31:27.857800: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af800 of size 256
2018-02-25 16:31:27.857822: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af900 of size 19200
2018-02-25 16:31:27.857842: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4400 of size 256
2018-02-25 16:31:27.857860: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4500 of size 256
2018-02-25 16:31:27.857874: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4600 of size 256
2018-02-25 16:31:27.857887: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4700 of size 256
2018-02-25 16:31:27.857897: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4800 of size 256
2018-02-25 16:31:27.857910: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4900 of size 256
2018-02-25 16:31:27.857924: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4a00 of size 256
2018-02-25 16:31:27.857947: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4b00 of size 256
2018-02-25 16:31:27.857970: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4c00 of size 256
2018-02-25 16:31:27.857989: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4d00 of size 256
2018-02-25 16:31:27.858000: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4e00 of size 256
2018-02-25 16:31:27.858012: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4f00 of size 256
2018-02-25 16:31:27.858022: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5000 of size 256
2018-02-25 16:31:27.858035: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5100 of size 256
2018-02-25 16:31:27.858050: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5200 of size 256
2018-02-25 16:31:27.858072: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5300 of size 256
2018-02-25 16:31:27.858091: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5400 of size 256
2018-02-25 16:31:27.858113: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5500 of size 256
2018-02-25 16:31:27.858132: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5600 of size 256
2018-02-25 16:31:27.858153: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5700 of size 256
2018-02-25 16:31:27.858176: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5800 of size 256
2018-02-25 16:31:27.858195: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5900 of size 256
2018-02-25 16:31:27.858206: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5a00 of size 256
2018-02-25 16:31:27.858219: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5b00 of size 256
2018-02-25 16:31:27.858229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5c00 of size 256
2018-02-25 16:31:27.858242: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5d00 of size 256
2018-02-25 16:31:27.858257: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5e00 of size 256
2018-02-25 16:31:27.858279: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5f00 of size 256
2018-02-25 16:31:27.858299: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6000 of size 512
2018-02-25 16:31:27.858321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6200 of size 256
2018-02-25 16:31:27.858343: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6300 of size 256
2018-02-25 16:31:27.858363: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6400 of size 512
2018-02-25 16:31:27.858373: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6600 of size 512
2018-02-25 16:31:27.858386: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6800 of size 512
2018-02-25 16:31:27.858396: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6a00 of size 512
2018-02-25 16:31:27.858409: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6c00 of size 512
2018-02-25 16:31:27.858425: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6e00 of size 512
2018-02-25 16:31:27.858447: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b9200 of size 256
2018-02-25 16:31:27.858470: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b9300 of size 819200
2018-02-25 16:31:27.858489: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881300 of size 512
2018-02-25 16:31:27.858500: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881500 of size 512
2018-02-25 16:31:27.858513: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881700 of size 512
2018-02-25 16:31:27.858523: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881900 of size 512
2018-02-25 16:31:27.858536: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881b00 of size 512
2018-02-25 16:31:27.858550: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881d00 of size 512
2018-02-25 16:31:27.858573: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881f00 of size 512
2018-02-25 16:31:27.858595: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719882100 of size 512
2018-02-25 16:31:27.858615: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719882300 of size 512
2018-02-25 16:31:27.858626: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2500 of size 1024
2018-02-25 16:31:27.858638: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2900 of size 1024
2018-02-25 16:31:27.858648: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2d00 of size 1024
2018-02-25 16:31:27.858661: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3100 of size 1024
2018-02-25 16:31:27.858676: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3500 of size 1024
2018-02-25 16:31:27.858698: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3900 of size 1024
2018-02-25 16:31:27.858718: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3d00 of size 1024
2018-02-25 16:31:27.858740: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba4100 of size 1024
2018-02-25 16:31:27.858768: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba4500 of size 1024
2018-02-25 16:31:27.858791: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a824900 of size 2048
2018-02-25 16:31:27.858810: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a825100 of size 2048
2018-02-25 16:31:27.858821: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a825900 of size 2048
2018-02-25 16:31:27.858834: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a826100 of size 2048
2018-02-25 16:31:27.858845: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a826900 of size 2048
2018-02-25 16:31:27.858858: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a827100 of size 2048
2018-02-25 16:31:27.858872: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a827900 of size 2048
2018-02-25 16:31:27.858889: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a828100 of size 2048
2018-02-25 16:31:27.858915: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a828900 of size 2048
2018-02-25 16:31:27.858935: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9100 of size 256
2018-02-25 16:31:27.858956: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9200 of size 256
2018-02-25 16:31:27.858978: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9300 of size 256
2018-02-25 16:31:27.858998: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9400 of size 256
2018-02-25 16:31:27.859014: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9500 of size 256
2018-02-25 16:31:27.859036: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9600 of size 19200
2018-02-25 16:31:27.859076: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8ae100 of size 19200
2018-02-25 16:31:27.859093: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b2c00 of size 19200
2018-02-25 16:31:27.859108: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7700 of size 256
2018-02-25 16:31:27.859123: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7800 of size 256
2018-02-25 16:31:27.859145: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7900 of size 52428800
2018-02-25 16:31:27.859169: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71dab7900 of size 120358656
2018-02-25 16:31:27.859189: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x716cc0a00 of size 26214400
2018-02-25 16:31:27.859201: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x718d83a00 of size 6553600
2018-02-25 16:31:27.859214: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719555e00 of size 1638400
2018-02-25 16:31:27.859224: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x7197b7000 of size 8704
2018-02-25 16:31:27.859237: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719882500 of size 3276800
2018-02-25 16:31:27.859253: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719ba4900 of size 13107200
2018-02-25 16:31:27.859271: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x71a829100 of size 524288
2018-02-25 16:31:27.859295: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x724d80000 of size 268435456
2018-02-25 16:31:27.859315: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x734d80000 of size 134217728
2018-02-25 16:31:27.859338: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x73cd80000 of size 33554432
2018-02-25 16:31:27.859359: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x73ed80000 of size 50331648
2018-02-25 16:31:27.859381: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x741d80000 of size 50331648
2018-02-25 16:31:27.859400: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x744f80000 of size 268435456
2018-02-25 16:31:27.859422: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x759fe0300 of size 134217728
2018-02-25 16:31:27.859441: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x765fe0300 of size 67108864
2018-02-25 16:31:27.859463: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x769fe0300 of size 67108864
2018-02-25 16:31:27.859482: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x771fe0300 of size 318373120
2018-02-25 16:31:27.859505: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x754f80000 of size 84280064
2018-02-25 16:31:27.859524: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x761fe0300 of size 67108864
2018-02-25 16:31:27.859546: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x76dfe0300 of size 67108864
2018-02-25 16:31:27.859565: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x785280000 of size 272646144
2018-02-25 16:31:27.859588: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x795684000 of size 134217728
2018-02-25 16:31:27.859621: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79d684000 of size 189252352
2018-02-25 16:31:27.859642: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7a8b00300 of size 67108864
2018-02-25 16:31:27.859661: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7acb00300 of size 268435456
2018-02-25 16:31:27.859683: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7bcb00300 of size 260734976
2018-02-25 16:31:27.859703: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7cc3a8300 of size 134217728
2018-02-25 16:31:27.859724: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7d43a8300 of size 134217728
2018-02-25 16:31:27.859744: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7dc3a8300 of size 182811904
2018-02-25 16:31:27.859767: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: 
2018-02-25 16:31:27.859792: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 84 Chunks of size 256 totalling 21.0KiB
2018-02-25 16:31:27.859821: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 41 Chunks of size 512 totalling 20.5KiB
2018-02-25 16:31:27.859847: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 34 Chunks of size 1024 totalling 34.0KiB
2018-02-25 16:31:27.859869: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB
2018-02-25 16:31:27.859893: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 30 Chunks of size 2048 totalling 60.0KiB
2018-02-25 16:31:27.859915: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 8 Chunks of size 19200 totalling 150.0KiB
2018-02-25 16:31:27.859940: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 524288 totalling 3.00MiB
2018-02-25 16:31:27.859971: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 800000 totalling 781.2KiB
2018-02-25 16:31:27.859997: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 8 Chunks of size 819200 totalling 6.25MiB
2018-02-25 16:31:27.860019: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1046784 totalling 1022.2KiB
2018-02-25 16:31:27.860043: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1196544 totalling 1.14MiB
2018-02-25 16:31:27.860065: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 3276800 totalling 21.88MiB
2018-02-25 16:31:27.860089: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 4194304 totalling 4.00MiB
2018-02-25 16:31:27.860111: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 5 Chunks of size 13107200 totalling 62.50MiB
2018-02-25 16:31:27.860135: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 13598208 totalling 12.97MiB
2018-02-25 16:31:27.860157: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 16248320 totalling 15.50MiB
2018-02-25 16:31:27.860182: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 16777216 totalling 16.00MiB
2018-02-25 16:31:27.860203: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 33554432 totalling 32.00MiB
2018-02-25 16:31:27.860229: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 50331648 totalling 48.00MiB
2018-02-25 16:31:27.860251: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 2 Chunks of size 52428800 totalling 100.00MiB
2018-02-25 16:31:27.860275: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 4 Chunks of size 67108864 totalling 256.00MiB
2018-02-25 16:31:27.860297: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 81788928 totalling 78.00MiB
2018-02-25 16:31:27.860322: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 120358656 totalling 114.78MiB
2018-02-25 16:31:27.860343: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 5 Chunks of size 134217728 totalling 640.00MiB
2018-02-25 16:31:27.860368: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 182811904 totalling 174.34MiB
2018-02-25 16:31:27.860395: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 189252352 totalling 180.48MiB
2018-02-25 16:31:27.860417: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 260734976 totalling 248.66MiB
2018-02-25 16:31:27.860442: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 3 Chunks of size 268435456 totalling 768.00MiB
2018-02-25 16:31:27.860464: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 272646144 totalling 260.02MiB
2018-02-25 16:31:27.860488: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 318373120 totalling 303.62MiB
2018-02-25 16:31:27.860509: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 3.27GiB
2018-02-25 16:31:27.860535: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: 
Limit:                  3832020992
InUse:                  3511868160
MaxInUse:               3571212544
NumAllocs:                     528
MaxAllocSize:            419430400

2018-02-25 16:31:27.860578: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **************x*************_********_****_*************x****************************xxx***********x
2018-02-25 16:31:27.860631: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[64,128,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
2018-02-25 16:31:27.860635: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 256.00MiB.  Current allocation summary follows.
2018-02-25 16:31:27.860691: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): 	Total Chunks: 84, Chunks in use: 84. 21.0KiB allocated for chunks. 21.0KiB in use in bin. 6.8KiB client-requested in use in bin.
2018-02-25 16:31:27.860714: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): 	Total Chunks: 41, Chunks in use: 41. 20.5KiB allocated for chunks. 20.5KiB in use in bin. 20.5KiB client-requested in use in bin.
2018-02-25 16:31:27.860733: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): 	Total Chunks: 35, Chunks in use: 35. 35.2KiB allocated for chunks. 35.2KiB in use in bin. 35.0KiB client-requested in use in bin.
2018-02-25 16:31:27.860753: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): 	Total Chunks: 30, Chunks in use: 30. 60.0KiB allocated for chunks. 60.0KiB in use in bin. 60.0KiB client-requested in use in bin.
2018-02-25 16:31:27.860770: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860786: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): 	Total Chunks: 1, Chunks in use: 0. 8.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860804: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): 	Total Chunks: 8, Chunks in use: 8. 150.0KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.
2018-02-25 16:31:27.860819: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860834: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860849: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860865: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-25 16:31:27.860882: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): 	Total Chunks: 17, Chunks in use: 16. 11.51MiB allocated for chunks. 11.01MiB in use in bin. 10.25MiB client-requested in use in bin.
2018-02-25 16:31:27.860899: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): 	Total Chunks: 2, Chunks in use: 1. 2.70MiB allocated for chunks. 1.14MiB in use in bin. 800.0KiB client-requested in use in bin.
2018-02-25 16:31:27.860917: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): 	Total Chunks: 8, Chunks in use: 7. 25.00MiB allocated for chunks. 21.88MiB in use in bin. 21.88MiB client-requested in use in bin.
2018-02-25 16:31:27.860934: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): 	Total Chunks: 2, Chunks in use: 1. 10.25MiB allocated for chunks. 4.00MiB in use in bin. 3.12MiB client-requested in use in bin.
2018-02-25 16:31:27.860951: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): 	Total Chunks: 8, Chunks in use: 7. 103.46MiB allocated for chunks. 90.96MiB in use in bin. 87.50MiB client-requested in use in bin.
2018-02-25 16:31:27.860968: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): 	Total Chunks: 2, Chunks in use: 1. 41.00MiB allocated for chunks. 16.00MiB in use in bin. 12.50MiB client-requested in use in bin.
2018-02-25 16:31:27.860986: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): 	Total Chunks: 5, Chunks in use: 4. 228.00MiB allocated for chunks. 180.00MiB in use in bin. 180.00MiB client-requested in use in bin.
2018-02-25 16:31:27.861003: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): 	Total Chunks: 9, Chunks in use: 6. 657.16MiB allocated for chunks. 448.78MiB in use in bin. 340.00MiB client-requested in use in bin.
2018-02-25 16:31:27.861020: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): 	Total Chunks: 8, Chunks in use: 8. 1.21GiB allocated for chunks. 1.21GiB in use in bin. 1.00GiB client-requested in use in bin.
2018-02-25 16:31:27.861036: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): 	Total Chunks: 5, Chunks in use: 5. 1.30GiB allocated for chunks. 1.30GiB in use in bin. 1.25GiB client-requested in use in bin.
2018-02-25 16:31:27.861053: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 256.00MiB was 256.00MiB, Chunk State: 
2018-02-25 16:31:27.861068: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680000 of size 1280
2018-02-25 16:31:27.861082: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680500 of size 256
2018-02-25 16:31:27.861096: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680600 of size 256
2018-02-25 16:31:27.861110: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702680700 of size 1046784
2018-02-25 16:31:27.861125: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702780000 of size 2048
2018-02-25 16:31:27.861139: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702780800 of size 2048
2018-02-25 16:31:27.861153: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702781000 of size 2048
2018-02-25 16:31:27.861167: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702781800 of size 2048
2018-02-25 16:31:27.861181: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782000 of size 256
2018-02-25 16:31:27.861194: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782100 of size 256
2018-02-25 16:31:27.861208: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782200 of size 1024
2018-02-25 16:31:27.861222: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782600 of size 1024
2018-02-25 16:31:27.861235: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782a00 of size 1024
2018-02-25 16:31:27.861248: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702782e00 of size 1024
2018-02-25 16:31:27.861261: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783200 of size 1024
2018-02-25 16:31:27.861274: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783600 of size 256
2018-02-25 16:31:27.861288: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783700 of size 256
2018-02-25 16:31:27.861302: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783800 of size 512
2018-02-25 16:31:27.861316: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783a00 of size 512
2018-02-25 16:31:27.861329: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783c00 of size 512
2018-02-25 16:31:27.861342: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702783e00 of size 512
2018-02-25 16:31:27.861356: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784000 of size 512
2018-02-25 16:31:27.861369: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784200 of size 256
2018-02-25 16:31:27.861382: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784300 of size 256
2018-02-25 16:31:27.861396: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784400 of size 256
2018-02-25 16:31:27.861410: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784500 of size 256
2018-02-25 16:31:27.861424: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784600 of size 256
2018-02-25 16:31:27.861437: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784700 of size 256
2018-02-25 16:31:27.861450: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784800 of size 256
2018-02-25 16:31:27.861463: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784900 of size 256
2018-02-25 16:31:27.861476: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784a00 of size 256
2018-02-25 16:31:27.861487: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784b00 of size 256
2018-02-25 16:31:27.861497: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784c00 of size 256
2018-02-25 16:31:27.861509: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784d00 of size 256
2018-02-25 16:31:27.861523: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784e00 of size 256
2018-02-25 16:31:27.861536: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702784f00 of size 256
2018-02-25 16:31:27.861549: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785000 of size 256
2018-02-25 16:31:27.861563: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785100 of size 512
2018-02-25 16:31:27.861576: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785300 of size 512
2018-02-25 16:31:27.861589: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785500 of size 512
2018-02-25 16:31:27.861603: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785700 of size 512
2018-02-25 16:31:27.861616: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785900 of size 512
2018-02-25 16:31:27.861630: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785b00 of size 256
2018-02-25 16:31:27.861644: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785c00 of size 256
2018-02-25 16:31:27.861657: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702785d00 of size 1024
2018-02-25 16:31:27.861674: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786100 of size 1024
2018-02-25 16:31:27.861688: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786500 of size 1024
2018-02-25 16:31:27.861702: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786900 of size 1024
2018-02-25 16:31:27.861715: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702786d00 of size 1024
2018-02-25 16:31:27.861728: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787100 of size 256
2018-02-25 16:31:27.861742: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787200 of size 256
2018-02-25 16:31:27.861756: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787300 of size 2048
2018-02-25 16:31:27.861769: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702787b00 of size 2048
2018-02-25 16:31:27.861782: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702788300 of size 2048
2018-02-25 16:31:27.861798: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702788b00 of size 2048
2018-02-25 16:31:27.861811: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789300 of size 2048
2018-02-25 16:31:27.861824: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789b00 of size 256
2018-02-25 16:31:27.861838: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789c00 of size 256
2018-02-25 16:31:27.861851: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789d00 of size 256
2018-02-25 16:31:27.861864: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789e00 of size 256
2018-02-25 16:31:27.861878: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702789f00 of size 256
2018-02-25 16:31:27.861891: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70278a000 of size 19200
2018-02-25 16:31:27.861905: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70278eb00 of size 819200
2018-02-25 16:31:27.861919: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702856b00 of size 19200
2018-02-25 16:31:27.861933: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285b600 of size 512
2018-02-25 16:31:27.861947: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285b800 of size 512
2018-02-25 16:31:27.861960: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285ba00 of size 1024
2018-02-25 16:31:27.861974: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70285be00 of size 1196544
2018-02-25 16:31:27.861989: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702980000 of size 4194304
2018-02-25 16:31:27.862003: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x702d80000 of size 16777216
2018-02-25 16:31:27.862017: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703d80000 of size 2048
2018-02-25 16:31:27.862032: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703d80800 of size 524288
2018-02-25 16:31:27.862045: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00800 of size 256
2018-02-25 16:31:27.862059: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00900 of size 256
2018-02-25 16:31:27.862073: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e00a00 of size 2048
2018-02-25 16:31:27.862087: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x703e01200 of size 16248320
2018-02-25 16:31:27.862101: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x704d80000 of size 67108864
2018-02-25 16:31:27.862115: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80000 of size 256
2018-02-25 16:31:27.862129: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80100 of size 1024
2018-02-25 16:31:27.862143: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x708d80500 of size 3276800
2018-02-25 16:31:27.862156: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7090a0500 of size 819200
2018-02-25 16:31:27.862169: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709168500 of size 819200
2018-02-25 16:31:27.862183: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230500 of size 512
2018-02-25 16:31:27.862197: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230700 of size 512
2018-02-25 16:31:27.862210: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709230900 of size 3276800
2018-02-25 16:31:27.862224: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709550900 of size 3276800
2018-02-25 16:31:27.862238: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870900 of size 512
2018-02-25 16:31:27.862251: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870b00 of size 512
2018-02-25 16:31:27.862265: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709870d00 of size 19200
2018-02-25 16:31:27.862278: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709875800 of size 1024
2018-02-25 16:31:27.862292: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709875c00 of size 1024
2018-02-25 16:31:27.862305: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x709876000 of size 13107200
2018-02-25 16:31:27.862319: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70a4f6000 of size 13107200
2018-02-25 16:31:27.862332: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b176000 of size 819200
2018-02-25 16:31:27.862345: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b23e000 of size 819200
2018-02-25 16:31:27.862358: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b306000 of size 2048
2018-02-25 16:31:27.862372: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b306800 of size 2048
2018-02-25 16:31:27.862385: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b307000 of size 524288
2018-02-25 16:31:27.862399: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b387000 of size 524288
2018-02-25 16:31:27.862407: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407000 of size 256
2018-02-25 16:31:27.862420: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407100 of size 256
2018-02-25 16:31:27.862434: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407200 of size 2048
2018-02-25 16:31:27.862444: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b407a00 of size 2048
2018-02-25 16:31:27.862457: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70b408200 of size 13107200
2018-02-25 16:31:27.862472: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70c088200 of size 13598208
2018-02-25 16:31:27.862486: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70cd80000 of size 52428800
2018-02-25 16:31:27.862497: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x70ff80000 of size 81788928
2018-02-25 16:31:27.862510: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80000 of size 256
2018-02-25 16:31:27.862524: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80100 of size 1024
2018-02-25 16:31:27.862537: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80500 of size 1024
2018-02-25 16:31:27.862548: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80900 of size 256
2018-02-25 16:31:27.862560: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x714d80a00 of size 3276800
2018-02-25 16:31:27.862574: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7150a0a00 of size 3276800
2018-02-25 16:31:27.862584: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7153c0a00 of size 13107200
2018-02-25 16:31:27.862593: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x716040a00 of size 13107200
2018-02-25 16:31:27.862603: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7185c0a00 of size 524288
2018-02-25 16:31:27.862612: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718640a00 of size 524288
2018-02-25 16:31:27.862625: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7186c0a00 of size 524288
2018-02-25 16:31:27.862638: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718740a00 of size 2048
2018-02-25 16:31:27.862649: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718741200 of size 2048
2018-02-25 16:31:27.862658: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718741a00 of size 2048
2018-02-25 16:31:27.862667: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718742200 of size 2048
2018-02-25 16:31:27.862676: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718742a00 of size 2048
2018-02-25 16:31:27.862685: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718743200 of size 2048
2018-02-25 16:31:27.862697: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718743a00 of size 3276800
2018-02-25 16:31:27.862707: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x718a63a00 of size 3276800
2018-02-25 16:31:27.862724: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c3a00 of size 1024
2018-02-25 16:31:27.862738: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c3e00 of size 1024
2018-02-25 16:31:27.862748: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4200 of size 1024
2018-02-25 16:31:27.862757: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4600 of size 1024
2018-02-25 16:31:27.862767: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4a00 of size 1024
2018-02-25 16:31:27.862776: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c4e00 of size 1024
2018-02-25 16:31:27.862786: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5200 of size 1024
2018-02-25 16:31:27.862793: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5600 of size 1024
2018-02-25 16:31:27.862806: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5a00 of size 1024
2018-02-25 16:31:27.862820: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7193c5e00 of size 819200
2018-02-25 16:31:27.862829: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71948de00 of size 819200
2018-02-25 16:31:27.862842: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e5e00 of size 512
2018-02-25 16:31:27.862858: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6000 of size 512
2018-02-25 16:31:27.862868: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6200 of size 512
2018-02-25 16:31:27.862878: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6400 of size 512
2018-02-25 16:31:27.862887: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6600 of size 512
2018-02-25 16:31:27.862897: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6800 of size 512
2018-02-25 16:31:27.862906: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6a00 of size 512
2018-02-25 16:31:27.862915: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6c00 of size 512
2018-02-25 16:31:27.862925: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e6e00 of size 512
2018-02-25 16:31:27.862934: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196e7000 of size 19200
2018-02-25 16:31:27.862944: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7196ebb00 of size 800000
2018-02-25 16:31:27.862957: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af000 of size 256
2018-02-25 16:31:27.862967: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af100 of size 256
2018-02-25 16:31:27.862981: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af200 of size 256
2018-02-25 16:31:27.862990: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af300 of size 256
2018-02-25 16:31:27.863004: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af400 of size 256
2018-02-25 16:31:27.863026: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af500 of size 256
2018-02-25 16:31:27.863051: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af600 of size 256
2018-02-25 16:31:27.863088: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af700 of size 256
2018-02-25 16:31:27.863099: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af800 of size 256
2018-02-25 16:31:27.863112: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197af900 of size 19200
2018-02-25 16:31:27.863126: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4400 of size 256
2018-02-25 16:31:27.863136: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4500 of size 256
2018-02-25 16:31:27.863146: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4600 of size 256
2018-02-25 16:31:27.863156: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4700 of size 256
2018-02-25 16:31:27.863165: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4800 of size 256
2018-02-25 16:31:27.863178: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4900 of size 256
2018-02-25 16:31:27.863196: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4a00 of size 256
2018-02-25 16:31:27.863222: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4b00 of size 256
2018-02-25 16:31:27.863240: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4c00 of size 256
2018-02-25 16:31:27.863254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4d00 of size 256
2018-02-25 16:31:27.863268: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4e00 of size 256
2018-02-25 16:31:27.863278: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b4f00 of size 256
2018-02-25 16:31:27.863287: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5000 of size 256
2018-02-25 16:31:27.863295: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5100 of size 256
2018-02-25 16:31:27.863305: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5200 of size 256
2018-02-25 16:31:27.863318: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5300 of size 256
2018-02-25 16:31:27.863333: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5400 of size 256
2018-02-25 16:31:27.863346: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5500 of size 256
2018-02-25 16:31:27.863356: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5600 of size 256
2018-02-25 16:31:27.863368: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5700 of size 256
2018-02-25 16:31:27.863381: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5800 of size 256
2018-02-25 16:31:27.863394: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5900 of size 256
2018-02-25 16:31:27.863408: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5a00 of size 256
2018-02-25 16:31:27.863421: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5b00 of size 256
2018-02-25 16:31:27.863431: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5c00 of size 256
2018-02-25 16:31:27.863441: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5d00 of size 256
2018-02-25 16:31:27.863450: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5e00 of size 256
2018-02-25 16:31:27.863459: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b5f00 of size 256
2018-02-25 16:31:27.863472: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6000 of size 512
2018-02-25 16:31:27.863485: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6200 of size 256
2018-02-25 16:31:27.863495: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6300 of size 256
2018-02-25 16:31:27.863505: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6400 of size 512
2018-02-25 16:31:27.863514: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6600 of size 512
2018-02-25 16:31:27.863523: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6800 of size 512
2018-02-25 16:31:27.863536: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6a00 of size 512
2018-02-25 16:31:27.863544: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6c00 of size 512
2018-02-25 16:31:27.863554: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b6e00 of size 512
2018-02-25 16:31:27.863564: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b9200 of size 256
2018-02-25 16:31:27.863573: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7197b9300 of size 819200
2018-02-25 16:31:27.863586: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881300 of size 512
2018-02-25 16:31:27.863599: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881500 of size 512
2018-02-25 16:31:27.863610: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881700 of size 512
2018-02-25 16:31:27.863619: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881900 of size 512
2018-02-25 16:31:27.863632: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881b00 of size 512
2018-02-25 16:31:27.863646: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881d00 of size 512
2018-02-25 16:31:27.863660: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719881f00 of size 512
2018-02-25 16:31:27.863670: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719882100 of size 512
2018-02-25 16:31:27.863682: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719882300 of size 512
2018-02-25 16:31:27.863696: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2500 of size 1024
2018-02-25 16:31:27.863706: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2900 of size 1024
2018-02-25 16:31:27.863715: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba2d00 of size 1024
2018-02-25 16:31:27.863724: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3100 of size 1024
2018-02-25 16:31:27.863734: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3500 of size 1024
2018-02-25 16:31:27.863747: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3900 of size 1024
2018-02-25 16:31:27.863761: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba3d00 of size 1024
2018-02-25 16:31:27.863778: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba4100 of size 1024
2018-02-25 16:31:27.863792: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x719ba4500 of size 1024
2018-02-25 16:31:27.863802: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a824900 of size 2048
2018-02-25 16:31:27.863811: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a825100 of size 2048
2018-02-25 16:31:27.863821: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a825900 of size 2048
2018-02-25 16:31:27.863830: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a826100 of size 2048
2018-02-25 16:31:27.863843: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a826900 of size 2048
2018-02-25 16:31:27.863856: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a827100 of size 2048
2018-02-25 16:31:27.863866: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a827900 of size 2048
2018-02-25 16:31:27.863876: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a828100 of size 2048
2018-02-25 16:31:27.863889: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a828900 of size 2048
2018-02-25 16:31:27.863904: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9100 of size 256
2018-02-25 16:31:27.863914: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9200 of size 256
2018-02-25 16:31:27.863927: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9300 of size 256
2018-02-25 16:31:27.863940: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9400 of size 256
2018-02-25 16:31:27.863949: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9500 of size 256
2018-02-25 16:31:27.863959: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8a9600 of size 19200
2018-02-25 16:31:27.863972: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8ae100 of size 19200
2018-02-25 16:31:27.863985: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b2c00 of size 19200
2018-02-25 16:31:27.863999: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7700 of size 256
2018-02-25 16:31:27.864008: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7800 of size 256
2018-02-25 16:31:27.864022: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71a8b7900 of size 52428800
2018-02-25 16:31:27.864036: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x71dab7900 of size 120358656
2018-02-25 16:31:27.864050: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x716cc0a00 of size 26214400
2018-02-25 16:31:27.864063: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x718d83a00 of size 6553600
2018-02-25 16:31:27.864077: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719555e00 of size 1638400
2018-02-25 16:31:27.864087: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x7197b7000 of size 8704
2018-02-25 16:31:27.864097: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719882500 of size 3276800
2018-02-25 16:31:27.864106: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x719ba4900 of size 13107200
2018-02-25 16:31:27.864115: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x71a829100 of size 524288
2018-02-25 16:31:27.864129: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x724d80000 of size 268435456
2018-02-25 16:31:27.864140: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x734d80000 of size 134217728
2018-02-25 16:31:27.864153: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x73cd80000 of size 33554432
2018-02-25 16:31:27.864168: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x73ed80000 of size 50331648
2018-02-25 16:31:27.864179: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x741d80000 of size 50331648
2018-02-25 16:31:27.864192: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x744f80000 of size 268435456
2018-02-25 16:31:27.864205: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x759fe0300 of size 134217728
2018-02-25 16:31:27.864219: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x765fe0300 of size 67108864
2018-02-25 16:31:27.864229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x769fe0300 of size 67108864
2018-02-25 16:31:27.864242: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x771fe0300 of size 318373120
2018-02-25 16:31:27.864256: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x754f80000 of size 84280064
2018-02-25 16:31:27.864266: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x761fe0300 of size 67108864
2018-02-25 16:31:27.864276: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x76dfe0300 of size 67108864
2018-02-25 16:31:27.864289: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x785280000 of size 272646144
2018-02-25 16:31:27.864303: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x795684000 of size 134217728
2018-02-25 16:31:27.864317: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79d684000 of size 189252352
2018-02-25 16:31:27.864327: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7a8b00300 of size 67108864
2018-02-25 16:31:27.864340: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7acb00300 of size 268435456
2018-02-25 16:31:27.864354: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7bcb00300 of size 260734976
2018-02-25 16:31:27.864364: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7cc3a8300 of size 134217728
2018-02-25 16:31:27.864374: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7d43a8300 of size 134217728
2018-02-25 16:31:27.864387: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7dc3a8300 of size 182811904
2018-02-25 16:31:27.864401: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: 
2018-02-25 16:31:27.864418: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 84 Chunks of size 256 totalling 21.0KiB
2018-02-25 16:31:27.864434: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 41 Chunks of size 512 totalling 20.5KiB
2018-02-25 16:31:27.864450: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 34 Chunks of size 1024 totalling 34.0KiB
2018-02-25 16:31:27.864465: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB
2018-02-25 16:31:27.864477: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 30 Chunks of size 2048 totalling 60.0KiB
2018-02-25 16:31:27.864492: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 8 Chunks of size 19200 totalling 150.0KiB
2018-02-25 16:31:27.864507: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 524288 totalling 3.00MiB
2018-02-25 16:31:27.864523: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 800000 totalling 781.2KiB
2018-02-25 16:31:27.864538: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 8 Chunks of size 819200 totalling 6.25MiB
2018-02-25 16:31:27.864553: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1046784 totalling 1022.2KiB
2018-02-25 16:31:27.864568: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1196544 totalling 1.14MiB
2018-02-25 16:31:27.864583: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 3276800 totalling 21.88MiB
2018-02-25 16:31:27.864598: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 4194304 totalling 4.00MiB
2018-02-25 16:31:27.864609: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 5 Chunks of size 13107200 totalling 62.50MiB
2018-02-25 16:31:27.864623: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 13598208 totalling 12.97MiB
2018-02-25 16:31:27.864639: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 16248320 totalling 15.50MiB
2018-02-25 16:31:27.864654: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 16777216 totalling 16.00MiB
2018-02-25 16:31:27.864669: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 33554432 totalling 32.00MiB
2018-02-25 16:31:27.864684: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 50331648 totalling 48.00MiB
2018-02-25 16:31:27.864695: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 2 Chunks of size 52428800 totalling 100.00MiB
2018-02-25 16:31:27.864706: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 4 Chunks of size 67108864 totalling 256.00MiB
2018-02-25 16:31:27.864721: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 81788928 totalling 78.00MiB
2018-02-25 16:31:27.864736: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 120358656 totalling 114.78MiB
2018-02-25 16:31:27.864751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 5 Chunks of size 134217728 totalling 640.00MiB
2018-02-25 16:31:27.864766: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 182811904 totalling 174.34MiB
2018-02-25 16:31:27.864778: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 189252352 totalling 180.48MiB
2018-02-25 16:31:27.864794: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 260734976 totalling 248.66MiB
2018-02-25 16:31:27.864810: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 3 Chunks of size 268435456 totalling 768.00MiB
2018-02-25 16:31:27.864825: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 272646144 totalling 260.02MiB
2018-02-25 16:31:27.864840: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 318373120 totalling 303.62MiB
2018-02-25 16:31:27.864851: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 3.27GiB
2018-02-25 16:31:27.864868: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: 
Limit:                  3832020992
InUse:                  3511868160
MaxInUse:               3571212544
NumAllocs:                     528
MaxAllocSize:            419430400

2018-02-25 16:31:27.864895: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **************x*************_********_****_*************x****************************xxx***********x
2018-02-25 16:31:27.864926: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_call
    return fn(*args)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1329, in _run_fn
    status, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: gradients/discriminator_1/d_h1_conv/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/discriminator/Maximum_grad/Shape, discriminator/d_h1_conv/w/read, gradients/discriminator_1/d_h1_conv/BiasAdd_grad/tuple/control_dependency)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 100, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""main.py"", line 83, in main
    dcgan.train(FLAGS)
  File ""/home/ubuntu/DCGAN-tensorflow/model.py"", line 258, in train
    feed_dict={ self.inputs: batch_images, self.z: batch_z })
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: gradients/discriminator_1/d_h1_conv/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/discriminator/Maximum_grad/Shape, discriminator/d_h1_conv/w/read, gradients/discriminator_1/d_h1_conv/BiasAdd_grad/tuple/control_dependency)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'gradients/discriminator_1/d_h1_conv/Conv2D_grad/Conv2DBackpropInput', defined at:
  File ""main.py"", line 100, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""main.py"", line 83, in main
    dcgan.train(FLAGS)
  File ""/home/ubuntu/DCGAN-tensorflow/model.py"", line 147, in train
    .minimize(self.d_loss, var_list=self.d_vars)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 499, in _Conv2DGrad
    data_format=data_format),
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 817, in conv2d_backprop_input
    dilations=dilations, name=name)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'discriminator_1/d_h1_conv/Conv2D', defined at:
  File ""main.py"", line 100, in <module>
    tf.app.run()
[elided 0 identical lines from previous traceback]
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""main.py"", line 78, in main
    sample_dir=FLAGS.sample_dir)
  File ""/home/ubuntu/DCGAN-tensorflow/model.py"", line 86, in __init__
    self.build_model()
  File ""/home/ubuntu/DCGAN-tensorflow/model.py"", line 111, in build_model
    self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True)
  File ""/home/ubuntu/DCGAN-tensorflow/model.py"", line 318, in discriminator
    h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))
  File ""/home/ubuntu/DCGAN-tensorflow/ops.py"", line 58, in conv2d
    conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 639, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: gradients/discriminator_1/d_h1_conv/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/discriminator/Maximum_grad/Shape, discriminator/d_h1_conv/w/read, gradients/discriminator_1/d_h1_conv/BiasAdd_grad/tuple/control_dependency)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

```
"
17259,"after restore trained model checkpoint, the prefetch capacity become 500 which is 5 during training","when I restore my trained model and do predict it out of memory problem, how can I restore the original capacity? or how can I reset the capacity after restore?
I am using python-gpu 1.5
below is the operation information:
when training:
name: ""prefetch_queue""
op: ""PaddingFIFOQueueV2""
device: ""/device:CPU:0""
attr {
  key: ""capacity""
  value {
    i: 5
  }
}

after restore:
name: ""prefetch_queue""
op: ""PaddingFIFOQueueV2""
attr {
  key: ""capacity""
  value {
    i: 500
  }
}
"
17257,Saver: Don't fail on restoring variables not present in a checkpoint.,"This is a feature request.

Let's consider a scenario where one trains multiple models and uses them in combination (like it is done in GANs).

To simplify the process of saving and restoring variables that are partly shared across the models (Pretraining Model, Training Model, Evaluation Model, Infer Model) one could instantiate the whole graph, containing all operations and variables, and save it.

Then in order to do pre-training only the subset of graph elements that is required for pretraining is used.

This results in the overhead of having to build the whole model (which might consist of multiple sub models) the first time the model is run even though only a smaller part (e.g. Pretraining Model) is required.

Another issue arises when using different optimizers in different training stages (e.g. SGD first then Adam). As Adam creates additional variables Adam has to be instantiated during the first training stage so restoring from a checkpoint does not fail when restoring with Adam instead of SGD.

This restriction of having to build everything despite parts not being required results in more complicated code. If it would be possible to silently fail when a variable is not found in a checkpoint, so it can be initialized with `tf.global_variables_initializer()` instead, would allow better structuring of code.

I have looked through all current issues regarding this problem and I have found a couple that face a similar problem and where a `QuietlyFailRestoringSaver` could solve this problem:
https://github.com/tensorflow/tensorflow/issues/12032
https://github.com/tensorflow/tensorflow/issues/16781

I might consider building this if there is enough support for it. I am open for feedback."
17256,Download the training images,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17255,tensorflow suddenly cannot detect my GPUs,"I was working with tensorflow-gpu, and it worked properly before. But just now, suddenly my Tensorflow cannot run on GPU anymore and when I query my devices with `list_local_devices()`, it shows that I don't have any GPU! But my nvidia-smi properly displayed the info of my 2 GPUs. It's very weird since I did't do anything."
17254,"DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead 1.6rc0","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- Ubuntu 16.04
- Tensorflow installed from source, latest HEAD from master (CPU)
- 1.6rc0
- Python 3.5.2
- Bazel 0.10.1
- gcc-5


### Describe the problem
When using tensorflow, I got hundred of DeprecationWarnings: This message exactly 

> /home/jerome/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)

EDIT : fixed replacing line 560 in tensor_util.py by : 

`return np.frombuffer(tensor.tensor_content, dtype=dtype).reshape(shape)`

Thanks for any kind of help :)

"
17253, Higher level C++ API feature request,"Hi,
I seems the C++ API only have frontend API to access tensorflow core, is there a plan to make higher level APIs, such as various algorithms and the estimator API, thanks.

I am sorry if this is not the right place to post this."
17252,Retrain.py script failing with nan-error at random without any code changes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Working on a notebook with MacOS High Sierra v10.13.1 in a Docker container using Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: Using CPU 
- **GPU model and memory**:
- **Exact command to reproduce**: 

```
python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --summaries_dir=tf_files/training_summaries/ \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=/ml/data/images
```

### Describe the problem

My set up is based on the tensorflow-for-poets tutorial. To test if everything is working I use the contained flower dataset. I tried to train the default inceptionV3 as well as the mobilenet architectures. However, training fails at some random point with the below listed error. This error does not seem to appear (or at least not as regular) if I train 500 or less training steps.

Based on what I read here the thrown error is indicating model instability. Since I changed neither code nor data this seems a bit strange. I think it might be a bug but you can probably help me figuring out if this is the case.

### Source code / logs

This is the error:

```
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/ml/tensorflow-for-poets/scripts/retrain.py"", line 1296, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/ml/tensorflow-for-poets/scripts/retrain.py"", line 1041, in main
    ground_truth_input: train_ground_truth})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Nan in summary histogram for: activations
	 [[Node: activations = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](activations/tag, final_result)]]

Caused by op u'activations', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/ml/tensorflow-for-poets/scripts/retrain.py"", line 1296, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/ml/tensorflow-for-poets/scripts/retrain.py"", line 1001, in main
    model_info['bottleneck_tensor_size'], model_info['quantize_layer'])
  File ""/ml/tensorflow-for-poets/scripts/retrain.py"", line 754, in add_final_training_ops
    tf.summary.histogram('activations', final_tensor)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py"", line 193, in histogram
    tag=tag, values=values, name=scope)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py"", line 189, in _histogram_summary
    ""HistogramSummary"", tag=tag, values=values, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Nan in summary histogram for: activations
	 [[Node: activations = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](activations/tag, final_result)]]
```"
17250,What defines the output tensor shape of tf.layers.conv2d_transpose?,"When using `tf.layers.conv2d_transpose` what defines the output tensor shape?
For example: if the input was 4x4x512, for the output to be 8x8x256 the filters can be given, but how are is the height and width defined? Or else is it always two times the input height and width?

Thanks. "
17249,TypeError: Failed to convert object of type <type 'list'> to Tensor.,"
I want to construct a dataset in which each element is a list of tensors with different shapes, because I need a dataset to contain my parsed image informations with different shapes. If I run the code like below:

`dataset = tf.data.Dataset.from_tensor_slices([[np.array([1]),np.array([1,2])],[np.array([2]), np.array([4, 5])]])`

It reports `TypeError: Failed to convert object of type <type 'list'> to Tensor.`. How should I resolve this? Thanks!
"
17248,The labels do not appear on android app screen,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: Tensorflow For Poets 2 
- **Python version**: 3
You can obtain the TensorFlow version with
v1.5.0-0-g37aa430d84 1.5.0

### Describe the problem

Hello dear friends,
I literally followed all instructions in Tenforflow For Poets.
and for Tensorflow For Poets Mobile.

I visited all the sites, blogs and forums, but I could not find any solution or similar of this problem.

The first android application was not opened, it was resolved in some way.
Now it opens but the labels do not appear.

I trained with Inception v3. I optimized it, stripped it, tried it on all emulators, tried it on my own phone.

I'm adding a screenshot so you can look at it.

Thank you in advance for your answers ...

![screenshot from 2018-02-25 05-58-19](https://user-images.githubusercontent.com/26048237/36637916-358ee1dc-19f7-11e8-96d4-59d5eac06c93.png)


### Source code / logs
02-25 06:23:37.531 4375-4375/? I/zygote: Not late-enabling -Xcheck:jni (already on)
02-25 06:23:37.538 4375-4375/? W/zygote: Unexpected CPU variant for X86 using defaults: x86
02-25 06:23:37.582 4375-4382/? I/zygote: Debugger is no longer active
02-25 06:23:37.810 4375-4375/? I/InstantRun: starting instant run server: is main process
02-25 06:23:37.852 4375-4375/? D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:38.025 4375-4375/? D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:38.025 4375-4375/? D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:38.029 4375-4394/? D/OpenGLRenderer: HWUI GL Pipeline
02-25 06:23:38.048 4375-4375/? D/tensorflow: CameraActivity: onPause org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:38.048 4375-4375/? D/tensorflow: CameraActivity: Requesting finish
02-25 06:23:38.119 4375-4394/? I/OpenGLRenderer: Initialized EGL, version 1.4
02-25 06:23:38.119 4375-4394/? D/OpenGLRenderer: Swap behavior 1
02-25 06:23:38.120 4375-4394/? W/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...
02-25 06:23:38.120 4375-4394/? D/OpenGLRenderer: Swap behavior 0
02-25 06:23:38.180 4375-4394/? D/EGL_emulation: eglCreateContext: 0xab032640: maj 2 min 0 rcv 2
02-25 06:23:38.686 4375-4394/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xab032640: ver 2 0 (tinfo 0xab00b1d0)
02-25 06:23:38.711 4375-4375/org.tensorflow.demo I/Choreographer: Skipped 35 frames!  The application may be doing too much work on its main thread.
02-25 06:23:38.882 4375-4394/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xab032640: ver 2 0 (tinfo 0xab00b1d0)
02-25 06:23:39.475 4375-4394/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xab032640: ver 2 0 (tinfo 0xab00b1d0)
02-25 06:23:39.677 4375-4375/org.tensorflow.demo D/tensorflow: CameraActivity: onStop org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:39.678 4375-4375/org.tensorflow.demo D/tensorflow: CameraActivity: onDestroy org.tensorflow.demo.ClassifierActivity@e5ff96d
02-25 06:23:45.425 4375-4375/org.tensorflow.demo D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@186f2a1
02-25 06:23:45.436 4375-4375/org.tensorflow.demo I/CameraManagerGlobal: Connecting to camera service
02-25 06:23:45.447 4375-4375/org.tensorflow.demo I/tensorflow: CameraActivity: Camera API lv2?: false
02-25 06:23:45.467 4375-4375/org.tensorflow.demo D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@186f2a1
02-25 06:23:45.467 4375-4375/org.tensorflow.demo D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@186f2a1
02-25 06:23:45.773 4375-4375/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480
02-25 06:23:45.774 4375-4375/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Valid preview sizes: [640x480]
02-25 06:23:45.774 4375-4375/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Rejected preview sizes: []
02-25 06:23:45.774 4375-4375/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Exact size match found.
02-25 06:23:45.844 4375-4394/org.tensorflow.demo D/EGL_emulation: eglMakeCurrent: 0xab032640: ver 2 0 (tinfo 0xab00b1d0)
02-25 06:23:46.007 4375-4375/org.tensorflow.demo I/TensorFlowImageClassifier: Reading labels from: retrained_labels.txt
02-25 06:23:46.009 4375-4375/org.tensorflow.demo I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
02-25 06:23:46.009 4375-4375/org.tensorflow.demo E/zygote: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
02-25 06:23:46.009 4375-4375/org.tensorflow.demo I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
02-25 06:23:46.022 4375-4375/org.tensorflow.demo W/native: cpu_feature_guard.cc:34 The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.
02-25 06:23:46.022 4375-4375/org.tensorflow.demo W/native: cpu_feature_guard.cc:34 The TensorFlow library was compiled to use SSE2 instructions, but these aren't available on your machine.
02-25 06:23:46.022 4375-4375/org.tensorflow.demo W/native: cpu_feature_guard.cc:34 The TensorFlow library was compiled to use SSE3 instructions, but these aren't available on your machine.
02-25 06:23:46.024 4375-4375/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
02-25 06:23:46.596 4375-4375/org.tensorflow.demo E/tensorflow: CameraActivity: Exception!
                                                               java.lang.RuntimeException: Failed to load model from 'file:///android_asset/rounded_graph_str.pb'
                                                                   at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
                                                                   at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                                                                   at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:113)
                                                                   at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)
                                                                   at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1124)
                                                                   at android.os.Handler.dispatchMessage(Handler.java:105)
                                                                   at android.os.Looper.loop(Looper.java:164)
                                                                   at android.app.ActivityThread.main(ActivityThread.java:6541)
                                                                   at java.lang.reflect.Method.invoke(Native Method)
                                                                   at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                                                                   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
                                                                Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](Mul, conv/conv2d_params). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
                                                                   at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)
                                                                   at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                                                                   at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103) 
                                                                   at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:113) 
                                                                   at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119) 
                                                                   at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1124) 
                                                                   at android.os.Handler.dispatchMessage(Handler.java:105) 
                                                                   at android.os.Looper.loop(Looper.java:164) 
                                                                   at android.app.ActivityThread.main(ActivityThread.java:6541) 
                                                                   at java.lang.reflect.Method.invoke(Native Method) 
                                                                   at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) 
                                                                   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) 
02-25 06:23:49.350 4375-4380/org.tensorflow.demo I/zygote: Do partial code cache collection, code=20KB, data=21KB
02-25 06:23:49.350 4375-4380/org.tensorflow.demo I/zygote: After code cache collection, code=20KB, data=21KB
02-25 06:23:49.350 4375-4380/org.tensorflow.demo I/zygote: Increasing code cache capacity to 128KB
02-25 06:23:55.613 4375-4380/org.tensorflow.demo I/zygote: Do partial code cache collection, code=61KB, data=49KB
02-25 06:23:55.614 4375-4380/org.tensorflow.demo I/zygote: After code cache collection, code=61KB, data=49KB
02-25 06:23:55.614 4375-4380/org.tensorflow.demo I/zygote: Increasing code cache capacity to 256KB

"
17246,Fetching value of Variable unnecessarily slow,"Doing sess.run(var) is about 5x slower than sess.run(var+1). 

python [variable_fetch_bug_report.py](https://github.com/diux-dev/cluster/blob/26f8e01bd79e49fe2d1dac342dd90493f693b85c/yuxin_numpy/variable_fetch_bug_report.py)

100MB variable
fetch_cpu_variable  : 2.5 GB/sec, min: 40.74, median: 41.33, mean: 42.08
fetch_cpu_variable_add: 12.6 GB/sec, min: 7.96, median: 8.54, mean: 8.71
fetch_cpu_variable_concat: 14.0 GB/sec, min: 7.12, median: 8.14, mean: 8.28

TensorFlow version info:
version: 1.7.0-dev20180221
__git_version__: v1.6.0-rc1-337-gd100729
https://github.com/tensorflow/tensorflow/commit/d100729
"
17243,Would a vector field layer be a welcome addition?,"Here is a [paper](https://arxiv.org/pdf/1802.08235v1.pdf) I stumbled upon yesterday. It describes an interesting, visual approach of separation of mixed set of points in high dimensional spaces into linearly separable set of points by learning a vector field that carries input points to new locations.

Coupled with Tensorboard, such a layer would provide intuition as to what the network is doing. We would ""see"" how it warps the space to provide linear separability. 

I have started coding up such a layer to run some visualizations. Note that this would be a first contribution to the code base, so excuse any dumb things as I am learning the code base. 
and I am wondering if anyone wants to collaborate on the implementation, and if upon successful implementation and tests, such a layer could be accepted into the code base.
"
17242,Getting ValueError: setting an array element with a sequence from tf.contrib.keras.preprocessing.image.ImageDatagenerator.flow,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.6.0-rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5,4.0

I am trying to do Data Augmentation in Tensorflow. I have written this code.

    import numpy as np
    import tensorflow as tf
    import tensorflow.contrib.keras as keras
    import time, random
    
    def get_image_data_generator():
    	return keras.preprocessing.image.ImageDataGenerator(
        rotation_range=get_random_rotation_angle(),\
        width_shift_range=get_random_wh_shift(),\
        height_shift_range=get_random_wh_shift(),\
        shear_range=get_random_shear(),\
        zoom_range=get_random_zoom(),\
        horizontal_flip=get_random_flip(),\
        vertical_flip=get_random_flip(),\
        preprocessing_function=get_random_function())
        
    def augment_data(image_array,label_array):
    	print image_array.shape
    	images_array = image_array.copy()
    	labels_array = label_array.copy()
    	#Create a list of various datagenerators with different arguments
    	datagenerators = []
    	ndg = 10
        #Creating 10 different generators
    	for ndata in xrange(ndg):
    		datagenerators.append(get_image_data_generator())
        #Setting batch_size to be equal to no.of images
    	bsize = image_array.shape[0]
    	print bsize
        #Obtaining the augmented data
    	for dgen in datagenerators:
    		dgen.fit(image_array)
    		(aug_img,aug_label) = dgen.flow(image_array,label_array,batch_size=bsize,shuffle=True)
    		print aug_img.shape
            #Concatenating with the original data
    		images_array = np.concatenate([images_array,aug_img],axis=0)
    		labels_array = np.concatenate([labels_array,aug_label],axis=0)
    	return (images_array,labels_array)
   
When I run the code using

`augment_data(image_array,label_array)`

I get an error which says

    Traceback (most recent call last):
      File ""cnn_model.py"", line 40, in <module>
        images_array,labels_array = augment_data(image_array,label_array)
      File ""/media/siladittya/d801fb13-809a-41b4-8f2e-d617ba103aba/ISI/code/2. known_object_detection/aug_data.py"", line 47, in augment_data
        (aug_img,aug_label) = dgen.flow(image_array,label_array,batch_size=10000,shuffle=True)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py"", line 1018, in next
        return self._get_batches_of_transformed_samples(index_array)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py"", line 991, in _get_batches_of_transformed_samples
        batch_x[i] = x
    ValueError: setting an array element with a sequence.

Edit :: I am getting this error even if I pass a single image as argument.

What am I doing wrong here? I can't understand. Please help."
17241,TFlite: tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo does not work,"The TfLiteCameraDemo shows ""Uninitialized Classifier or invalid context"" after launching.

My environment
1. android 8.1 on pixel 2
2. bazel 0.11.0
3. tensorflow commit c7e966b5e35ee7fc511c1efe84dba8d3558f2b1c
4. stand-alone ndk-r14b, sdk within android-studio 3.0.1
5. ubuntu 16.04

My step
1. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
2. adb install -r bazel-bin/tensorflow/contrib/lite/java/demo/app/src/main/TfLiteCameraDemo.apk"
17240,Tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17237,"""ld: unknown option: --icf=all"" Bazel build tensorflow-lite label_image Error","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: max os 10.12.6
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.5
- **Python version**:   2.7 anaconda
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none


### Describe the problem
```
bazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image
```

### Source code / logs
```
WARNING: Config values are not defined in any .rc file: opt.
WARNING: /private/var/tmp/_bazel_ericyue/c53d920e143a3bbcf51dc97baaf3590c/external/protobuf_archive/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_ericyue/c53d920e143a3bbcf51dc97baaf3590c/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions.
INFO: Found 1 target...
ERROR: /Users/ericyue/workspace/tensorflow/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1).
ld: unknown option: --icf=all
collect2: error: ld returned 1 exit status
Target //tensorflow/contrib/lite/examples/label_image:label_image failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.229s, Critical Path: 0.10s
```"
17233,4x slowdown in feed_dict in tf-nightly-gpu,"Upgrading to `pip install tf-nightly-gpu` from `pip install tensorflow` (1.5) slows down feeding about 4x

Feeding 100MB array used to take 15ms, and it takes 60ms after the change. I think this is due to change in alignment requirements for AVX-compiled binary. I want to start the thread to figure out how to regain the performance before these changes make it into official release

benchmark: [align_feed_bug.py](https://github.com/diux-dev/cluster/blob/0edf19d919ee02273c7beb3af2ea378496a95610/yuxin_numpy/align_feed_bug.py)

```
# version: 1.5.0
python align_feed_bug.py
feed-cpu-variable   : min: 17.17, median: 19.95, mean: 19.82

# After upgrading to tf nightly
# version: 1.7.0-dev20180221
python align_feed_bug.py
feed-cpu-variable   : min: 53.97, median: 57.15, mean: 66.60
```

I've tried using @eamartin 's recipe (https://github.com/numpy/numpy/issues/5312#issuecomment-299533915) to make sure numpy array are 128-byte aligned, but that didn't make any difference in speed
```
python align_feed_bug.py --align=1
feed-cpu-variable   : min: 49.54, median: 50.50, mean: 60.06
```

cc @martinwicke "
17232,Cmake build doesn't work on ubuntu 16.04,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Latest from master branch
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: Cuda 9.1, cuDNN 7.0.5
- **GPU model and memory**: GeForce GTX TITAN X, 12288 MB
- **Exact command to reproduce**:
git clone https://github.com/tensorflow/tensorflow
cd tensorflow/tensorflow/contrib/cmake/
mkdir build && cd build 
ccmake .. [correct cuda version and add nccl library, idk why that was mandatory though.]
make -j


### Describe the problem
I am trying to compile tensorflow with cmake on ubuntu 16.04. Some issues I have faced till now:

**1. cuda/extras/CUPTI/include/cupti.h no found (In cupti_wrapper.h)**
Temp fix: I saw ""cuda/extras/CUPTI/include"" is already included in the cmake. So, changed ""cuda/extras/CUPTI/include/cupti.h"" => ""cupti.h"" and it works.

**2. src/nccl.h not found (In nccl_manager.h and nccl_ops.cc)**
Temp fix: nccl v2 doesnt have src folder but nccl.h can be found in /usr/include/. So, changed
""src/nccl.h"" => nccl.h and it works

**3. GRPC is not linked to libpywrap_tensorflow_internal.so (Not fixed yet)**
`[100%] Running SWIG to generate Python wrappers
[100%] Building CXX object CMakeFiles/pywrap_tensorflow_internal.dir/pywrap_tensorflow_internal.cc.o
[100%] Linking CXX shared library libpywrap_tensorflow_internal.so
grpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_txt_done_cb(void*, int, int, unsigned char*, int)':
grpc_ares_wrapper.cc:(.text+0x256): undefined reference to `ares_parse_txt_reply_ext'
grpc_ares_wrapper.cc:(.text+0x267): undefined reference to `ares_strerror'
grpc_ares_wrapper.cc:(.text+0x363): undefined reference to `ares_free_data'
grpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_srv_query_done_cb(void*, int, int, unsigned char*, int)':
grpc_ares_wrapper.cc:(.text+0x65a): undefined reference to `ares_parse_srv_reply'
grpc_ares_wrapper.cc:(.text+0x677): undefined reference to `ares_free_data'
grpc_ares_wrapper.cc:(.text+0x683): undefined reference to `ares_strerror'
grpc_ares_wrapper.cc:(.text+0x7cf): undefined reference to `ares_gethostbyname'
grpc_ares_wrapper.cc:(.text+0x830): undefined reference to `ares_gethostbyname'
grpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `on_hostbyname_done_cb(void*, int, int, hostent*)':
grpc_ares_wrapper.cc:(.text+0x964): undefined reference to `ares_strerror'
grpc_ares_wrapper.cc:(.text+0xb7d): undefined reference to `ares_inet_ntop'
grpc_ares_wrapper.cc:(.text+0xc62): undefined reference to `ares_inet_ntop'
grpc/src/grpc/libgrpc_unsecure.a(grpc_ares_wrapper.cc.o): In function `grpc_dns_lookup_ares_impl(char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**)':
`
**4. Can't fix with GRPC OFF in ccmake (Not fixed yet)**

There are so many issues in CMake and I am not able to build directly on ubuntu 16.04. Can someone look into this? Thanks!"
17227,Unable to load retrained MobileNet,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux on x86_64
- **TensorFlow installed from (source or binary)**: binary/pip
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.4
- **Exact command to reproduce**:
`python3 label_image/label_image.py \
--graph=output_graph.pb --labels=output_labels.txt \
--input_layer=Mul \
--output_layer=final_result \
--input_mean=128 --input_std=128 \
--image=images.jpg`

### Describe the problem
Attempting to use a retrained MobileNet model (retrained with retrain.py) results in 

`Traceback (most recent call last):
  File ""label_image/label_image.py"", line 144, in <module>
    classify()
  File ""label_image/label_image.py"", line 116, in classify
    graph = load_graph(model_file)
  File ""label_image/label_image.py"", line 34, in load_graph
    graph_def.ParseFromString(f.read())
google.protobuf.message.DecodeError: Error parsing message`

when trying to use the label_image example, and 

`Traceback (most recent call last):
  File ""label_image/label_image.py"", line 144, in <module>
    classify()
  File ""label_image/label_image.py"", line 116, in classify
    graph = load_graph(model_file)
  File ""label_image/label_image.py"", line 34, in load_graph
    graph_def.ParseFromString(f.read())
  File ""/usr/lib/python3.6/site-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1083, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1120, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 633, in DecodeField
    if value._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1120, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 612, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1120, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 743, in DecodeMap
    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1109, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 799, in _SkipGroup
    new_pos = SkipField(buffer, pos, end, tag_bytes)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/usr/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.` 

when using `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` as mentioned in [https://github.com/tensorflow/tensorflow/issues/582](url)

I also get the same issues when trying to quantize the model.
"
17223,"How to run a trained model of TensorFlow object detection in Java application with GPU support with boxes output (x, y, w, h)?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04/Windows 10
- **TensorFlow installed from (source or binary)**: No
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA GTX 1080 8GB
- **Exact command to reproduce**:

### Describe the problem
Here I want to ask and sure that: Is this possible to run a trained model (using python to train) in a Java application with GPU support and detect the object and get the result of matches and the x, y point and the width and height? 
I am going to use python to train my own dataset by using one of the model e.g: `ssd_mobilenet_v1_coco  `OR `faster_rcnn_resnet101_coco` and then use the trained model in the Java application with GPU support with boxes output. So I want to sure that does the TensorFlow support this task or not? If yes, is there any good tutorial to show the usage? If not, why, and when it can be supported?

Thanks in advance!!
"
17221,tf.contrib.signal.stft: Incompatible gradient shape,"**Fails on TF 1.5.0 and 1.6.0rc1**

I need to compute gradients of a signal through a STFT with a smaller frame length than FFT length. Assumed this would truncate the window in the gradient computation or fail on some assertion but this fails ungracefully.

### Sample code
```python
def test_gradient_computation(frame_length, fft_length):
    graph = tf.Graph()
    with graph.as_default():
        x = tf.get_variable('input', [1, 16000], tf.float32)
        x = tf.contrib.signal.stft(
            x,
            frame_length=frame_length,
            frame_step=frame_length // 2,
            fft_length=fft_length
        )

        x = tf.abs(x)
        y = tf.ones_like(x)

        loss = tf.losses.mean_squared_error(x, y)

        optimizer = tf.train.GradientDescentOptimizer(1e-3)
        train_op = optimizer.minimize(loss)
        with tf.Session() as session:
            session.run(tf.global_variables_initializer())
            session.run(train_op)
```

### This works:
```python
n = 1024
test_gradient_computation(frame_length=n, fft_length=n)
test_gradient_computation(frame_length=n, fft_length=n*2)
```

### But this fails:
```python
n = 1024
test_gradient_computation(frame_length=n*2, fft_length=n)
```
> ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: stft/rfft.  Input index: 0. Original input shape: (1, 14, 2048).  Calculated input gradient shape: (1, 14, 1024)

"
17220,Tensorflow-produced HLO graphs cannot be rendered by graphviz,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No, I'm using standard `mnist_softmax_xla.py` with unmodified tensorflow (cloned from master)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04: 4.13.0-32-generic #35~16.04.1-Ubuntu SMP Thu Jan 25 10:13:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**:
Source, with XLA and CUDA
- **TensorFlow version (use command below)**:
tf.VERSION = 1.6.0-rc0
tf.GIT_VERSION = v1.5.0-2285-g4448430
tf.COMPILER_VERSION = v1.5.0-2285-g4448430
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
bazel release 0.10.0
- **GCC/Compiler version (if compiling from source)**:
gcc 5.4.0 20160609
- **CUDA/cuDNN version**:
== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
- **GPU model and memory**:
Nvidia Titan V Driver Version: 387.34, 12058MiB
- **Exact command to reproduce**:
    TF_XLA_FLAGS=""--xla_hlo_graph_path=/home/user/test/hlo_graphs --xla_generate_hlo_graph=.*"" python3 mnist_softmax_xla.py

### Describe the problem
I want to run XLA JIT on the `mnist_softmax_xla.py` example script, and then view the generated HLO to see what was compiled. To do this, I run:

    $> TF_XLA_FLAGS=""--xla_hlo_graph_path=/home/user/test/hlo_graph --xla_generate_hlo_graph=.*"" python3 mnist_softmax_xla.py

This produces a lot of text:

    **[omitted]**
    2018-02-23 12:42:22.413434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1016] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10979 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.0)
    2018-02-23 12:42:22.743524: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7fc63c001800 executing computations on platform CUDA. Devices:
    2018-02-23 12:42:22.743569: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): Graphics Device, Compute Capability 7.0
    **[omitted]**
    2018-02-23 12:42:22.796890: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1395] computation cluster_0__XlaCompiledKernel_true__XlaNumConstantArgs_0__XlaNumResourceArgs_0_.v83 [GPU-ir-emit-prepare: after copy-insertion, pipeline end]: /home/user/test/hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot
    2018-02-23 12:42:22.973757: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.1.108, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.
    2018-02-23 12:42:24.527632: I tensorflow/stream_executor/dso_loader.cc:147] successfully opened CUDA library libcupti.so.9.1 locally
    **[omitted]**

Apart from the warning for the ptxas version (which I believe there is no way for me to fix because I'm using the latest available version of CUDA that I can get from Nvidia), this appears to be successful and the compiled execution can be seen in the timeline.json. However, I want to view the HLO graph with `graphviz`, so using the above output, I try to load `hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot`. However, I get an error:

    $> dot -Tpng hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot -o graph.png
    Error: hlo_graph/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.dot: syntax error in line 10 near ''

Line 10 is the large stylesheet definition in the `.dot` file. It's very long and I am not familiar with the syntax, but it 'seems' like there are no missing/supernumerary quotations anywhere. Is this a known problem or have I done something incorrectly? Is there another renderer (i.e. not `graphviz`) that I can target out-of-the-box?

I've attached the aforementioned `.dot` file as a `.txt`:
[hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.txt](https://github.com/tensorflow/tensorflow/files/1751694/hlo_graph_47.mcrbox-6dffc700-28862-565e07de51010.txt)

The graphviz version I'm using is:

    dot - graphviz version 2.38.0 (20140413.2041)
"
17219,KeyError: u'SSTableReaderV2',"when i use pretained model **(MobileNet_v1_0.50_160)** , it results in error **(KeyError: u'SSTableReaderV2' )**."
17218,Retrained quantized mobilenet introduces unsupported Pow operator,"here's a code which can reproduce the problem
from https://github.com/tensorflow/tensorflow/issues/15122#issuecomment-363126160

BUT When  I run the toco command,  Fatal error happens:
```
2018-02-23 19:38:31.164513: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 66 operators, 156 arrays (1 quantized)
2018-02-23 19:38:31.165597: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 66 operators, 156 arrays (1 quantized)
2018-02-23 19:38:31.167279: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 66 operators, 156 arrays (1 quantized)
2018-02-23 19:38:31.221783: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:354] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Pow) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
```

I'm wondering how to use quantized mobilenet with tensorflow-lite in ios ?

```
ARCHITECTURE=mobilenet_1.0_224_quantized                                                           
DATA_DIR=~/tensorflow-for-poets-2/tf_files                                                         
TRAINING_DIR=/tmp/tf_files                                                                         
                                                                                                   
python tensorflow/tensorflow/examples/image_retraining/retrain.py \                                
  --bottleneck_dir=$TRAINING_DIR/bottlenecks \                                                     
  --how_many_training_steps=500 \                                                                  
  --model_dir=$TRAINING_DIR/models \                                                               
  --summaries_dir=$TRAINING_DIR/training_summaries/""${ARCHITECTURE}"" \                             
  --output_graph=$TRAINING_DIR/retrained_graph.pb \                                                
  --output_labels=$TRAINING_DIR/retrained_labels.txt \                                             
  --architecture=""${ARCHITECTURE}"" \                                                               
  --image_dir=$DATA_DIR/flower_photos  

rm -f /$TRAINING_DIR/${ARCHITECTURE}.tflite                              
                        
tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \                                           
  --input_file=$TRAINING_DIR/retrained_graph.pb \                                                  
  --input_format=TENSORFLOW_GRAPHDEF \                                                             
  --output_format=TFLITE \                                                                         
  --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \                                            
  --inference_type=QUANTIZED_UINT8 \                                                               
  --input_array=Placeholder \                                                                      
  --output_array=final_result \                                                                    
  --input_shape=1,224,224,3 \                                                                      
  --mean_value=128 \                                                                               
  --std_value=128                                                                                  
```

"
17217,Could not find a version that satisfies the requirement tensorflow-gpu (from versions: ),"Can anyone help with this? I tried quite a few times even wiped the computer and started from scratch.

**Environment**
- CPU: Intel i7
- Memory: 16GB
- OS: Windows 10 64bit
- Nvidia GTX 1060 (latest driver installed: 390.77-desktop-win10-64bit-international-whql)
- Visual Studio Community
- CUDA 8.0 for Windows 10
- cuDNN 5.1 for CUDA 8.0
- Python 3.6.4


I have a few versions available on local hard drive but all of them are giving me the same error message.
- tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
- tensorflow-1.6.0rc1-cp35-cp35m-win_amd64.whl
- tensorflow-1.6.0rc1-cp36-cp36m-win_amd64.whl


**By running `python -c ""from pip import pep425tags;print(pep425tags.supported_tags)""`**:
`[('cp36', 'cp36m', 'win32'), ('cp36', 'none', 'win32'), ('py3', 'none', 'win32'), ('cp36', 'none', 'any'), ('cp3', 'none', 'any'), ('py36', 'none', 'any'), ('py3', 'none', 'any'), ('py35', 'none', 'any'), ('py34', 'none', 'any'), ('py33', 'none', 'any'), ('py32', 'none', 'any'), ('py31', 'none', 'any'), ('py30', 'none', 'any')]`"
17216,terminate called after throwing an instance of 'std::system_error',"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.1 LTS (GNU/Linux3.13.0-105-generic x86_64)
- **TensorFlow installed from (source or binary)**: binary (tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl)
- **TensorFlow version (use command below)**: 1.7.0-dev20180222
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CPU(Intel Xeon E5-2620 v3)
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
```



### Describe the problem
After I installed tensorflow which was downloaded from the main page you provided on github

[tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl](http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.6,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp36-cp36m-linux_x86_64.whl)

I tried to  test the tensorflow and when I run `sess = tf.Session()`

I got the error:

```
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
Aborted
```

I use anaconda create a new python 3.6 enviroment for tensorflow. After I downloaded the .whl package I use 

`pip install tf_nightly-1.head-cp36-cp36m-linux_x86_64.whl`

to install the tensorflow

I've seen a similar problem and in that case his system is CentOS. So is anyone having a similar experience with this problem?
"
17215,No clear documentation about how optimizer works through tf.tile() ,"If some one uses this type of code for batch learning,
`w1=tf.get_variable(""W1"", [300,300], dtype=tf.float32, initializer=init)
W1=tf.tile(tf.expand_dims(w1,axis=0), [batch_size,1,1], name=""batch_W1"")
x1=tf.placeholder(name=""context"",dtype=tf.float32,shape=[None,None,300]) ;
y=tf.matmul(x1,W1)`
how the optimizer will combine the gradients from `W1 [batch_size,300,300]` to `w1[300,300]`?
I asked this question in stackoverflow also but none replied.
https://stackoverflow.com/questions/48933564/is-this-the-appropriate-way-to-set-up-weights-in-batch-learning-in-tensorflow
"
17214,"Unimplemented: TensorArray has size zero, but element shape [24,24,?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: python version - pip, c++ version - makefile
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem
I have tensorflow frozen graph. When I read it using python api everything is good but if I do it via c++ api it fails with the error:
```
Unimplemented: TensorArray has size zero, but element shape [24,24,?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[Node: TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[""loc:@TensorArray""], dtype=DT_INT32, element_shape=[24,24,?], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](TensorArray, TensorArrayStack/range, while/Exit_1)]]
```

### Source code / logs
I guess this is the problem related with while_loop op. This is some part of code that constructs tensorflow graph:
```
dets = convert_to_square(boxes_c)
num_box = tf.shape(dets)[0]
h = tf.shape(x)[0]
w = tf.shape(x)[1]
x0, y0, dy, dx, ex, ey, edx, edy, tmpw, tmph = pad(dets, h, w, num_box)
x_int = tf.cast(x, tf.int32)
ims_resized = tf.TensorArray(tf.int32, num_box)
i = 0
[i, ims_resized, x_int, tmph, tmpw, dy, edy, dx, edx, y0, ey, x0, ex] = tf.while_loop(cond, generate,
                            [i, ims_resized, x_int, tmph, tmpw, dy, edy, dx, edx, y0, ey, x0, ex])

ims_resized_normal_tensor0 = ims_resized.stack()
ims_resized_normal_tensor = (tf.cast(ims_resized_normal_tensor0, tf.float32) - 127.5) / 128.0
ims_resized = ims_resized.close()
```
"
17213,Level 3 warnings in core headers,"I found some level 3 warnings in core headers:

```
tensorflow/core/framework/node_def_util.h(118): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
tensorflow/core/lib/core/arena.h(46): warning C4267: 'argument': conversion from 'size_t' to 'const int', possible loss of data
tensorflow/core/graph/graph.h(475): warning C4244: 'return': conversion from 'const tensorflow::int64' to 'int', possible loss of data
tensorflow/core/graph/graph.h(480): warning C4244: 'return': conversion from 'tensorflow::int64' to 'int', possible loss of data
include\tensorflow/core/graph/graph.h(509): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
tensorflow/core/graph/graph.h(518): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
```

This issue can be fixed using static_cast<>. I'm using VS2017 15.5.6 on Windows 10."
17212,got error when run eval.py in object detection api,"Hi ,
i faced with this error when i run eval.py:

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
Traceback (most recent call last):
File ""eval.py"", line 142, in
tf.app.run()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
_sys.exit(main(argv))
File ""eval.py"", line 138, in main
FLAGS.checkpoint_dir, FLAGS.eval_dir)
File ""/home/mm/models/research/object_detection/evaluator.py"", line 240, in evaluate
save_graph_dir=(eval_dir if eval_config.save_graph else ''))
File ""/home/mm/models/research/object_detection/eval_util.py"", line 448, in repeated_checkpoint_run
return metrics
UnboundLocalError: local variable 'metrics' referenced before assignment

########################################################################
i once saw that must be initial metrics : in the beginning of repeated_checkpoint_run i i added this :
metrics={}
pr_value={}
global_step={}
i this case , that error don't face but after for a while processing don't give me any value :
mm@mm:~/models/research/object_detection$ python3 eval.py \

        --logtostderr \
        --checkpoint_dir=training_ssd_mobile/model.ckpt-200000 \
        --eval_dir=eval_log\
        --pipeline_config_path=ssd_mobilenet_v1_coco_2017_11_17/ssd_mobilenet_v1_focal_loss_coco.config

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0"
17211,"Getting Redeclaration Error of ""error: redeclaration of 'P_ALL'    P_ALL,  /* Wait for any child.  */"" while cross compiling for raspberry pi 3","# I'm trying to cross compile TensorFlow for Raspberry pi 3 and every time m getting the error like ""error: redeclaration of 'P_ALL'
   P_ALL,  /* Wait for any child.  */""

# I Followed these steps,
1.Installed Bazel
2.Cloned Tensorflow and checkout r1.5
3.Running the ""./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh""

Error:-/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/grpc/BUILD:431:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/baladev/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o' -fPIC '-DGRPC_ARES=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/grpc -iquote bazel-out/armeabi-opt/genfiles/external/grpc -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -isystem external/grpc/include -isystem bazel-out/armeabi-opt/genfiles/external/grpc/include -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/grpc/src/core/lib/support/subprocess_posix.cc -o bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o)
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:101:3: error: redeclaration of 'P_ALL'
   P_ALL,  /* Wait for any child.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:52:3: note: previous declaration 'idtype_t P_ALL'
   P_ALL,  /* Wait for any child.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:102:3: error: redeclaration of 'P_PID'
   P_PID,  /* Wait for specified process.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:53:3: note: previous declaration 'idtype_t P_PID'
   P_PID,  /* Wait for specified process.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:103:3: error: redeclaration of 'P_PGID'
   P_PGID  /* Wait for members of process group.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:54:3: note: previous declaration 'idtype_t P_PGID'
   P_PGID  /* Wait for members of process group.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:104:3: error: conflicting declaration 'typedef enum idtype_t idtype_t'
 } idtype_t;
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:55:3: note: previous declaration as 'typedef enum idtype_t idtype_t'
 } idtype_t;
   ^
INFO: Elapsed time: 984.156s, Critical Path: 40.58s
FAILED: Build did NOT complete successfully

_NOTE:-I'm Cross Compiling for Raspberry PI 3 In ubuntu 16.04LTS_

M I missing something??If yes kindly let me know.
Thanks in advance!!"
17210,NotFoundError: Op type not registered 'KafkaDataset' in binary.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: - 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('unknown', '1.6.0-rc1')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.8.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:

~~~python
from tensorflow.contrib.kafka.python.ops import kafka_dataset_ops
from tensorflow.python.data.ops import iterator_ops
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

topics = array_ops.placeholder(dtypes.string, shape=[None])
num_epochs = array_ops.placeholder(dtypes.int64, shape=[])
batch_size = array_ops.placeholder(dtypes.int64, shape=[])

repeat_dataset = kafka_dataset_ops.KafkaDataset(
    topics, group=""test"", eof=True).repeat(num_epochs)
batch_dataset = repeat_dataset.batch(batch_size)

iterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)
init_op = iterator.make_initializer(repeat_dataset)
init_batch_op = iterator.make_initializer(batch_dataset)
get_next = iterator.get_next()
~~~

ENV: https://pastebin.com/89aihba7

### Describe the problem
Getting `NotFoundError` even with `TF_NEED_KAFKA=1` and `--define with_kafka_support=true`:
~~~
NotFoundError: Op type not registered 'KafkaDataset' in binary running on 68a9f992375e. Make sure the Op and Kernel are registered in the binary running in this process.
~~~
`KafkaDataset` was merged into master last month. Is there something missing that needs to be done in order to utilize the new op? 


### Source code / logs
https://pastebin.com/5Vy6b6kf"
17209,"Expected float32, got range(0, 3) of type 'range' instead.","### System information
- **What is the top-level directory of the model you are using**:
      C:\Users\Administrator\Documents\Projects\models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Because my previous [issue,](https://github.com/tensorflow/tensorflow/issues/17208) I disable the argument.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
     Windows 10
- **TensorFlow installed from (source or binary)**:
      Binary
- **TensorFlow version (use command below)**:
      1.5
- **CUDA/cuDNN version**:
     CUDA v9.0
- **GPU model and memory**:
      NVDIA GeForce GT 730

### Describe the problem
If my description or log is not clear enough, please tell me. Thanks in advance!

### Source code / logs
  File ""C:/Users/Administrator/Documents/Projects/models/research/object_detection/my_train.py"", line 164, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py"", line 255, in train
    train_config.optimizer)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\builders\optimizer_builder.py"", line 50, in build
    learning_rate = _create_learning_rate(config.learning_rate)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\builders\optimizer_builder.py"", line 108, in _create_learning_rate
    learning_rate_sequence)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\utils\learning_schedules.py"", line 155, in manual_stepping
    tf.constant(range(num_boundaries), dtype=tf.int32),
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 214, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 433, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 344, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected float32, got range(0, 3) of type 'range' instead."
17208,__init__() got an unexpected keyword argument 'dct_method',"### System information
- **What is the top-level directory of the model you are using**:
      C:\Users\Administrator\Documents\Projects\models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Just changed the filename of 'train.py' to 'my_train.py'.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
     Windows 10
- **TensorFlow installed from (source or binary)**:
      Binary
- **TensorFlow version (use command below)**:
      1.5
- **CUDA/cuDNN version**:
     CUDA v9.0
- **GPU model and memory**:
      NVDIA GeForce GT 730

### Describe the problem
Is the new version have some bugs or my installation is incorrect? because I saw the source code of the 'Image' class of tf.example_decoder, it is no an argument named 'dec_method' in the 'init' method. If my description or log is not clear enough, please tell me. Thanks in advance!

### Source code / logs
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""C:/Users/Administrator/Documents/Projects/models/research/object_detection/my_train.py"", line 120, in get_next
    dataset_builder.build(config)).get_next()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\builders\dataset_builder.py"", line 138, in build
    label_map_proto_file=label_map_proto_file)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\data_decoders\tf_example_decoder.py"", line 110, in __init__
    dct_method=dct_method),
TypeError: __init__() got an unexpected keyword argument 'dct_method'"
17207,TF Keras inference is way slower than Numpy,"I'm working on a reinforcement learning model implemented with Keras and Tensorflow. I have to do frequent calls to model.predict() on single inputs.

While testing inference on a simple pretrained model, I noticed that using Keras' model.predict is WAY slower than just using Numpy on stored weights. Why is it that slow and how can I accelerate it? Using pure Numpy is not viable for complex models.

    import timeit
    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense
    
    w = np.array([[-1., 1., 0., 0.], [0., 0., -1., 1.]]).T
    b = np.array([ 15., -15., -21., 21.])
    
    model = Sequential()
    model.add(Dense(4, input_dim=2, activation='linear'))
    model.layers[0].set_weights([w.T, b])
    model.compile(loss='mse', optimizer='adam')
    
    state = np.array([-23.5, 17.8])
    
    def predict_very_slow():
        return model.predict(state[np.newaxis])[0]
    
    def predict_slow():
        ws = model.layers[0].get_weights()
        return np.matmul(ws[0].T, state) + ws[1]
    
    def predict_fast():
        return np.matmul(w, state) + b
    
    print(
        timeit.timeit(predict_very_slow, number=10000),
        timeit.timeit(predict_slow, number=10000),
        timeit.timeit(predict_fast, number=10000)
    )
    # 5.168972805004538 1.6963867129435828 0.021918574168087623
    # 5.461319456664639 1.5491559107269515 0.021502970783442876

I'm using Tensorflow for CPU, version 1.5.0 installed from pypi for python 3.5 on Windows 10."
17206,CPU restrictions do not reduce thread count,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux c72 4.13.0-17-generic #20~16.04.1-Ubuntu SMP Mon Nov 6 14:18:00 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b'v1.4.1-5-gabf3c6d' 1.4.1

- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: please see below

### Describe the problem

It seems that the tensorflow configuration proto lets you specify the number of CPUs TF should use, but this does not limit the default number of threads TF creates in its thread pools. This seems like unexpected behavior to me. The default number of threads created by TF when no options are specified at all is equal to the number of cores TF detects on the machine it's running on. However, even when limiting the number of CPUs the thread count goes up. On a machine with many CPUs, I expected limiting to work, but it did not, until I limited the thread pool size explicitly.

This isn't a big deal but seemed like it could be a bug, or at least makes the CPU device count option a bit unintuitive.

```
(cpuc-3.5) 18:21:27 vladf@c72:~$ python -c ""import tensorflow as tf
> tf.Session(config=tf.ConfigProto())
> import os
> pid = os.getpid()
> os.system('ps -o nlwp {}'.format(pid))""
NLWP
 144
(cpuc-3.5) 18:21:30 vladf@c72:~$ python -c ""import tensorflow as tf
> tf.Session(config=tf.ConfigProto(device_count={'CPU': 4}))
> import os
> pid = os.getpid()
> os.system('ps -o nlwp {}'.format(pid))""
NLWP
 144
(cpuc-3.5) 18:22:35 vladf@c72:~$ python -c ""import tensorflow as tf
> tf.Session(config=tf.ConfigProto(device_count={'CPU': 4},inter_op_parallelism_threads=4,intra_op_parallelism_threads=4))
> import os
> pid = os.getpid()
> os.system('ps -o nlwp {}'.format(pid))""
NLWP
  56
```
"
17204,HtoD takes 2.5x longer than D2H,"I'm noticing that HtoD copies are taking significantly longer than DtoH.

IE, doing `sess.run(gpu_var)` vs `sess.run(assign_gpu_var, feed_dict={gpu_var.initial_value: arr})`
100MB tensor takes 8ms on V100 to fetch (12.5GB/sec), but 21.67ms to feed (4.5 GB/sec)
 
Benchmark
https://github.com/diux-dev/cluster/blob/db10c890530e7ded9e4a933596803e3ae0de1db0/yuxin_numpy/square_minimize_cpu_pipeline.py

Is there a way to make it faster? (something to do with memory pinning?)

<img width=""1152"" alt=""screenshot 2018-02-22 17 47 07"" src=""https://user-images.githubusercontent.com/23068/36574018-6e853e74-17f8-11e8-80dd-7509e7c643a2.png"">

TensorFlow:
version: 1.5.0
__git_version__: v1.5.0-0-g37aa430d84
https://github.com/tensorflow/tensorflow/commit/37aa430d84
"
17203,Modify distributed TF examples to take kubeflow's TF_CONFIG as well as command line arguments.,"Currently most of the examples under `tensorflow/tools/dist_test` require command line arguments to be passed in to construct the cluster spec.

The tensorflow operator is standardizing under a representation of the cluster spec within a variable called `TF_CONFIG`, which looks like this:

```
{""cluster"":{""master"":[""myjob237-master-06pz-0:2222""],""ps"":[""myjob237-ps-06pz-0:2222"",""myjob237-ps-06pz-1:2222""],""worker"":[""myjob237-worker-06pz-0:2222""]},""task"":{""type"":""master"",""index"":0},""environment"":""cloud""}
```

It would be ideal of the examples were modified to take this variable so we can reduce the shimming required in Kubeflow related efforts, without breaking backward compatibility and keeping the command line args."
17201,Tensorflow build incorrectly complained about Bazel version,"When I built Tensorflow, it complained my bazel was 0.4.5, asked me to upgrade to bazel 0.5.4 or above.
So I upgraded to the newest bazel 0.10.1. Then when I built Tensorflow again, it still complained:

Current Bazel version is 0.10.1, expected at least 0.5.4

So Tensorflow thinks 0.1 is less than 0.5, it did not treat that as version 10 v.s. 5.
Please fix.

Thank you!
Jan

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17200,"TimeDistributed wrapper works with ""standalone"" Keras, but not with ""built-in"" Keras","The following lines worked with Keras (2.0.8) and Tensorflow (pre 1.4).

`input_tensor = Input((input_epochs_per_output_epoch, samples_per_epoch, features_per_epoch))`

`inner_intermediate = TimeDistributed(BatchNormalization(axis = -1), name=""bn_input"")(input_tensor)`

`many lines...`

`inner_out = TimeDistributed(Bidirectional(GRU(intermediate_features, return_sequences=False, recurrent_dropout=recurrent_dropout, dropout=normal_dropout, kernel_initializer=kernel_initializer, kernel_regularizer=l2(l2_lambda), implementation=lstm_implementation), merge_mode='concat'), name=""inner_out"")(inner_intermediate)`


However, with Tensorflow 1.5.0, using the Keras within Tensorflow, the final line fails with:
> ValueError: as_list() is not defined on an unknown TensorShape.

Because there are a lot of lines between 2 and 3, which have various layers wrapped by TimeDistributed, I've figured out that it is only GRU and LSTM layers that cause this error. I've tried specifying the input_shape for both the the wrappers and/or the layers themselves, to no avail."
17199,Error raised incorrectly in CheckInputFromValidContext,"Happens in TF 1.5.

**Error message and logs:**
INFO:tensorflow:Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop.

gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc while context: None
transducer_training/while/rnn/strided_slice while context: transducer_training/while/while_context

Traceback for gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc:
  File ""./loop_error.py"", line 219, in <module>
    model = Model(cons_manager=constants_manager)
  File ""./loop_error.py"", line 44, in __init__
    self.targets, self.train_op, self.loss = self.build_training_step()
  File ""./loop_error.py"", line 211, in build_training_step
    train_op = tf.train.AdamOptimizer().minimize(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py"", line 949, in _SelectGrad
    return (None, array_ops.where(c, grad, zeros),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 2540, in where
    return gen_math_ops._select(condition=condition, x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 4043, in _select
    ""Select"", condition=condition, t=x, e=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1674, in __init__
    self._control_flow_context.AddOp(self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2251, in AddOp
    self._AddOpInternal(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2274, in _AddOpInternal
    real_x = self.AddValue(x)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2207, in AddValue
    real_val = grad_ctxt.grad_state.GetRealValue(val)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1050, in GetRealValue
    history_value = cur_grad_state.AddForwardAccumulator(cur_value)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 908, in AddForwardAccumulator
    name=""f_acc"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 3578, in _stack_v2
    stack_name=stack_name, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

Traceback for transducer_training/while/rnn/strided_slice:
  File ""./loop_error.py"", line 219, in <module>
    model = Model(cons_manager=constants_manager)
  File ""./loop_error.py"", line 42, in __init__
    self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()
  File ""./loop_error.py"", line 183, in build_full_transducer
    tf.while_loop(cond, body, init_state, parallel_iterations=1)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2934, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2720, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2662, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""./loop_error.py"", line 119, in body
    dtype=tf.float32, initial_state=encoder_hidden_state_t)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 629, in dynamic_rnn
    dtype=dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py"", line 688, in _dynamic_rnn_loop
    time_steps = input_shape[0]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 573, in _slice_helper
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 737, in strided_slice
    shrink_axis_mask=shrink_axis_mask)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 5501, in strided_slice
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access


Traceback (most recent call last):
  File ""./loop_error.py"", line 219, in <module>
    model = Model(cons_manager=constants_manager)
  File ""./loop_error.py"", line 44, in __init__
    self.targets, self.train_op, self.loss = self.build_training_step()
  File ""./loop_error.py"", line 211, in build_training_step
    train_op = tf.train.AdamOptimizer().minimize(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py"", line 949, in _SelectGrad
    return (None, array_ops.where(c, grad, zeros),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 2540, in where
    return gen_math_ops._select(condition=condition, x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 4043, in _select
    ""Select"", condition=condition, t=x, e=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1674, in __init__
    self._control_flow_context.AddOp(self)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2251, in AddOp
    self._AddOpInternal(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2274, in _AddOpInternal
    real_x = self.AddValue(x)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2207, in AddValue
    real_val = grad_ctxt.grad_state.GetRealValue(val)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1050, in GetRealValue
    history_value = cur_grad_state.AddForwardAccumulator(cur_value)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 908, in AddForwardAccumulator
    name=""f_acc"")
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 3578, in _stack_v2
    stack_name=stack_name, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1672, in __init__
    control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_util.py"", line 200, in CheckInputFromValidContext
    raise ValueError(error_msg + "" See info log for more details."")
ValueError: Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop. See info log for more details.




**Code to reproduce:**


    import logging
    import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    
    logging.getLogger().setLevel(logging.DEBUG)
    # NOTE: Time major
    
    # ---------------- Constants Manager ----------------------------
    class ConstantsManager(object):
        def __init__(self, input_dimensions, input_embedding_size, inputs_embedded, encoder_hidden_units,
                     transducer_hidden_units, vocab_ids, input_block_size, beam_width):
            assert transducer_hidden_units == encoder_hidden_units, 'Encoder and transducer have to have the same amount' \
                                                                    'of hidden units'
            self.input_dimensions = input_dimensions
            self.vocab_ids = vocab_ids
            self.E_SYMBOL = len(self.vocab_ids)
            self.vocab_ids.append('E_SYMBOL')
            self.GO_SYMBOL = len(self.vocab_ids)
            self.vocab_ids.append('GO_SYMBOL')
            self.vocab_size = len(self.vocab_ids)
            self.input_embedding_size = input_embedding_size
            self.inputs_embedded = inputs_embedded
            self.encoder_hidden_units = encoder_hidden_units
            self.transducer_hidden_units = transducer_hidden_units
            self.input_block_size = input_block_size
            self.beam_width = beam_width
            self.batch_size = 1  # Cannot be increased, see paper
            self.log_prob_init_value = 0
    
    # ----------------- Model ---------------------------------------
    
    
    class Model(object):
        def __init__(self, cons_manager):
            self.var_list = []
            self.cons_manager = cons_manager
            self.max_blocks, self.inputs_full_raw, self.transducer_list_outputs, self.start_block, self.encoder_hidden_init,\
                self.trans_hidden_init, self.logits, self.encoder_hidden_state_new, \
                self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()
    
            self.targets, self.train_op, self.loss = self.build_training_step()
    
        def build_full_transducer(self):
            with tf.variable_scope('transducer_training'):
    
                embeddings = tf.Variable(tf.random_uniform([self.cons_manager.vocab_size,
                                                            self.cons_manager.input_embedding_size], -1.0, 1.0),
                                         dtype=tf.float32,
                                         name='embedding')
                # Inputs
                max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')  # total amount of blocks to go through
                if self.cons_manager.inputs_embedded is True:
                    input_type = tf.float32
                else:
                    input_type = tf.int32
                inputs_full_raw = tf.placeholder(shape=(None, self.cons_manager.batch_size,
                                                        self.cons_manager.input_dimensions), dtype=input_type,
                                                 name='inputs_full_raw')  # shape [max_time, 1, input_dims]
                transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                         name='transducer_list_outputs')  # amount to output per block
                start_block = tf.placeholder(dtype=tf.int32, name='transducer_start_block')  # where to start the input
    
                encoder_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.encoder_hidden_units), dtype=tf.float32,
                                                     name='encoder_hidden_init')
                trans_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.transducer_hidden_units), dtype=tf.float32,
                                                   name='trans_hidden_init')
    
                # Temporary constants, maybe changed during inference
                end_symbol = tf.get_variable(name='end_symbol',
                                             initializer=tf.constant_initializer(self.cons_manager.vocab_size),
                                             shape=(), dtype=tf.int32)
    
                # Turn inputs into tensor which is easily readable#
    
                inputs_full = tf.reshape(inputs_full_raw, shape=[-1, self.cons_manager.input_block_size,
                                                                 self.cons_manager.batch_size,
                                                                 self.cons_manager.input_dimensions])
    
                # Outputs
                outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
                init_state = (start_block, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
                # Initiate cells, NOTE: if there is a future error, put these back inside the body function
                encoder_cell = tf.contrib.rnn.LSTMCell(num_units=self.cons_manager.encoder_hidden_units)
                transducer_cell = tf.contrib.rnn.LSTMCell(self.cons_manager.transducer_hidden_units)
    
                def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
                    return current_block < start_block + max_blocks
    
                def body(current_block, outputs_int, encoder_hidden, trans_hidden):
    
                    # --------------------- ENCODER ----------------------------------------------------------------------
                    encoder_inputs = inputs_full[current_block]
                    encoder_inputs_length = [tf.shape(encoder_inputs)[0]]
                    encoder_hidden_state = encoder_hidden
    
                    if self.cons_manager.inputs_embedded is True:
                        encoder_inputs_embedded = encoder_inputs
                    else:
                        encoder_inputs = tf.reshape(encoder_inputs, shape=[-1, self.cons_manager.batch_size])
                        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
                    # Build model
    
                    # Build previous state
                    encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
                    encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, self.cons_manager.encoder_hidden_units])
                    encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, self.cons_manager.encoder_hidden_units])
                    encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
                    #   encoder_outputs: [max_time, batch_size, num_units]
                    encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                        encoder_cell, encoder_inputs_embedded,
                        sequence_length=encoder_inputs_length, time_major=True,
                        dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
                    # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
                    encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
                    encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new,
                                                          shape=[2, -1, self.cons_manager.encoder_hidden_units])
    
                    # --------------------- TRANSDUCER --------------------------------------------------------------------
                    encoder_raw_outputs = encoder_outputs
                    # Save/load the state as one tensor, use encoder state as init if this is the first block
                    trans_hidden_state = tf.cond(current_block > 0, lambda: trans_hidden, lambda: encoder_hidden_state_new)
                    transducer_amount_outputs = transducer_list_outputs[current_block - start_block]
    
                    # Model building
                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                        embedding=embeddings,
                        start_tokens=tf.tile([self.cons_manager.GO_SYMBOL],
                                             [self.cons_manager.batch_size]),  # TODO: check if this looks good
                        end_token=end_symbol)  # vocab size, so that it doesn't prematurely end the decoding
    
                    attention_states = tf.transpose(encoder_raw_outputs,
                                                    [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
                    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                        self.cons_manager.encoder_hidden_units, attention_states)
    
                    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                        transducer_cell,
                        attention_mechanism,
                        attention_layer_size=self.cons_manager.transducer_hidden_units)
    
                    projection_layer = layers_core.Dense(self.cons_manager.vocab_size, use_bias=False)
    
                    # Build previous state
                    trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
                    trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, self.cons_manager.transducer_hidden_units])
                    trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, self.cons_manager.transducer_hidden_units])
                    trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
                    decoder = tf.contrib.seq2seq.BasicDecoder(
                        decoder_cell, helper,
                        decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                        output_layer=projection_layer)
    
                    outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                                output_time_major=True,
                                                                                                maximum_iterations=transducer_amount_outputs)
                    logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
                    decoder_prediction = outputs.sample_id  # For debugging
    
                    # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
                    transducer_hidden_state_new = tf.concat(
                        [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                        axis=0)
                    transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                             shape=[2, -1, self.cons_manager.transducer_hidden_units])
    
    
                    # Note the outputs
                    outputs_int = outputs_int.write(current_block - start_block, logits)
    
                    return current_block + 1, outputs_int, encoder_hidden_state_new, transducer_hidden_state_new
    
                _, outputs_final, encoder_hidden_state_new, transducer_hidden_state_new = \
                    tf.while_loop(cond, body, init_state, parallel_iterations=1)
    
                # Process outputs
                outputs = outputs_final.concat()
                logits = tf.reshape(
                    outputs,
                    shape=(-1, 1, self.cons_manager.vocab_size))  # And now its [max_output_time, batch_size, vocab]
    
                # For loading the model later on
                logits = tf.identity(logits, name='logits')
                encoder_hidden_state_new = tf.identity(encoder_hidden_state_new, name='encoder_hidden_state_new')
                transducer_hidden_state_new = tf.identity(transducer_hidden_state_new, name='transducer_hidden_state_new')
    
            train_saver = tf.train.Saver()  # For now save everything
    
            return max_blocks, inputs_full_raw, transducer_list_outputs, start_block, encoder_hidden_init,\
                trans_hidden_init, logits, encoder_hidden_state_new, transducer_hidden_state_new, train_saver
    
        def build_training_step(self):
            targets = tf.placeholder(shape=(None,), dtype=tf.int32, name='targets')
            targets_one_hot = tf.one_hot(targets, depth=self.cons_manager.vocab_size, dtype=tf.float32)
    
            targets_one_hot = tf.Print(targets_one_hot, [targets], message='Targets: ', summarize=10)
            targets_one_hot = tf.Print(targets_one_hot, [tf.argmax(self.logits, axis=2)], message='Argmax: ', summarize=10)
    
            stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot,
                                                                             logits=self.logits)
            loss = tf.reduce_mean(stepwise_cross_entropy)
    
            # ERROR happens in this line
            train_op = tf.train.AdamOptimizer().minimize(loss)
            train_op = None
            return targets, train_op, loss
    
    
    constants_manager = ConstantsManager(input_dimensions=1, input_embedding_size=11, inputs_embedded=False,
                                         encoder_hidden_units=100, transducer_hidden_units=100, vocab_ids=[0, 1, 2],
                                         input_block_size=1, beam_width=5)
    model = Model(cons_manager=constants_manager)
    
    with tf.Session() as sess:
        writer = tf.summary.FileWriter(""/tmp/graph"", sess.graph_def)
        sess.run(tf.global_variables_initializer())
        writer.flush()
        writer.close()



**Picture of the graph and offending tensor (in red)**

The `rnn` box is in a while loop itself.

![graph](https://user-images.githubusercontent.com/9123400/36559487-0d147680-17c3-11e8-9ae9-d2dca5950579.jpeg)
"
17195,"C++ gradients for MaxPool3D, AvgPool, AvgPool3D","Looks like these just need to connect the gradient to an existing core op. Anyone already working on it? Otherwise, I'll sign up for these three.

/cc @bpiel @suharshs"
17192,TrainingHelper & ScheduledEmbeddingTrainingHelper  GPU Error - works on CPU,"### System information
- **Have I written custom code **: Yes
- **OS Platform and Distribution**:16.04.3 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4.1
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
- **CUDA/cuDNN version**:CUDA 8.0 / 6.0
- **GPU model and memory**:GeForce GTX 1080, 8G RAM, NVIDIA DRIVER: 390.25
- **Exact command to reproduce**:NA

###  problem
Having trouble running an RNN dynamic decoder on GPU when using the following  decoding helpers:
tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper 
tf.contrib.seq2seq.TrainingHelper 

It seems that there is a problem with the sequence length stopping condition, as the problem
is not present when using  tf.contrib.seq2seq.GreedyEmbeddingHelper as a decoder helper.

On CPU all functions are working correctly, the problem arise on GPUs only.
Tried on TF-1.4 compiled from source, TF-1.4 installed from pip, and TF-1.5 installed from source.
They all fail

### Error log:
```
NotFoundError (see above for traceback): No registered 'Switch' OpKernel for GPU devices compatible with node Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=[""loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_STRING]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_QINT32]
  device='CPU'; T in [DT_QUINT8]
  device='CPU'; T in [DT_QINT8]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
	 [[Node: Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/cond/TensorArrayReadV3/Switch = Switch[T=DT_RESOURCE, _class=[""loc:@Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Decoders/StackedDecoder_text_rnn_0/ScheduledEmbeddingSamplingWrapper/TrainingHelper/TensorArray/_281, Decoders/StackedDecoder_text_rnn_0/decoder/TrainingHelperInitialize/All)]]
```
"
17191,ValueError: Unknown activation function:relu6  while converting MobileNet under Keras to estimator using model_to_estimator,"Hi, I try to convert a mobilenet model under `tf.keras.application` to estimator using model_to_estimator.
I get an error (`ValueError: Unknown activation function:relu6`) due to relu6 is a customized activation defined in mobilenet.

Thanks for help.

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.6r
- **Python version**: 
3.5
- **Bazel version (if compiling from source)**:
0.10.1
- **GCC/Compiler version (if compiling from source)**:
GCC
- **CUDA/cuDNN version**:
9.0
- **GPU model and memory**:
GTX 1070
- **Have I written custom code**: 
No
- **Exact command to reproduce**:
```python
import tensorflow as tf
keras_mobilenet= tf.keras.applications.mobilenet.MobileNet(weights=None)
keras_mobilenet.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),
                          loss='categorical_crossentropy',
                          metric='accuracy')
mobilenet_estimator = tf.keras.estimator.model_to_estimator(keras_model=keras_mobilenet)
```





"
17190,tf.contrib.quantize: layer not quantized in absence of activation,"In absence of an activation function, e.g., `tf.layers.Conv2D(...,activation=None,...)`, the graph matcher using the `activation_pattern` at
https://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/contrib/quantize/python/quantize.py#L185-L192
won't match the layer that has no activation function, and consequently, that layer won't be quantized. 

Maybe previously, instead of no activation function, `tf.identity` was used? The package `tf.contrib.quantize` searches for the identity op.
https://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/contrib/quantize/python/quantize.py#L35

Note that if `activation = None`, then no operation is inserted:
https://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/python/layers/convolutional.py#L192-L193

Example: Box predictors in SSD from the TF Object Detection API.
https://github.com/tensorflow/models/blob/be9b80251af1bc798553c9e5135f8b0f19fa0a81/research/object_detection/core/box_predictor.py#L688-L689

@suharshs "
17188,TF1.3 to 1.5 drastic changes (apparently) due to gRPC,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.1.1503 (Core)
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7.15(with TF 1.5) and 2.7.5(with TF1.3)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I built a distributed training system with TF1.3 and it works smoothly except that it sporadically freeze. I was informed by @mrry that TF1.5 has an upgrade version of gRPC that fixes a deadlock bug that causes servers to hang.

So I tried to upgrade to 1.5 today but then a lot of unexpected problems came up and not sure if I am missing anything important for 1.5 version. 

TL;DR
Soon after I launch a job, it died right away and all of the time it is the PS showing various errors like ""unavailable: OS Error"", ""Stream removed"" and ""Transport closed"" during sess.run(tf.global_variables_initializer). Also this problem is more likely to happen if I launched more PS.

So, is there any important dependencies I have to check when upgrading from 1.3 to 1.5?"
17187,TensorFlow - cuInit: CUDA_ERROR_NO_DEVICE,"Hi, I was using TensorFlow with GPU support these past few months and it worked without any issues. I have installed cuda v8.0 and also have the cudNN library.  Recently, I started using TensorFlow for a project and noticed that it isn't computing on the GPU, and is using the GPU instead. I'm running TensorFlow 1.2.1 on Windows 10, with CUDA v8.0.

Here is the code that I ran on the Python Interpreter:
`import tensorflow as tf`
`sess=tf.Session()`

Here's the message on the command prompt:

`>>> import tensorflow as tf
>>> sess=tf.Session()
2018-02-22 13:24:51.069445: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.079084: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.085961: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.092933: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.102940: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.110151: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.116103: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.123166: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:24:51.705468: **E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE**
2018-02-22 13:24:51.717633: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: Cipher
2018-02-22 13:24:51.728408: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:165] hostname: Cipher`

I already tried a bunch of solutions such as changing `CUDA_VISIBLE_DEVICES=0`, and even adding cudnn to my `PATH` variables


I have not used TensorFlow in this example to write any custom code. I am running Windows 10, running TensorFlow 1.2 with GPU support, which I installed from the TensorFlow website. The GPU version is supported by CUDA v8.0. I have an NVIDIA 920M with a memory of 4GB."
17186,Conv2D is not called if it has only control dependency,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
2
- **Bazel version (if compiling from source)**:
0.5.4
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
Titan XP, 12GB

### Describe the problem
If conv2d operation has control dependency in the graph, the operation is not executed at all. 
I've tested with tf.tfprof and timeline. If I connect the conv2d with its value, it is finally executed.
Is it intended behavior in TensorFlow?

### Source code / logs
    ```
    opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY
    opts['min_accelerator_micros'] = 1
    run_metadata = tf.RunMetadata()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    #config.allow_soft_placement = True
    config.inter_op_parallelism_threads=0
    config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0
    batch = 32
    size = 224
    in_channels = 3
    out_channels = 1
    filter_size = 7
    inp = tf.ones(shape=[batch, size, size, in_channels], dtype=tf.float32)
    v = tf.Variable(inp)
    filters = tf.ones(shape=[filter_size, filter_size, in_channels, out_channels], dtype=tf.float32)
    conv2d = tf.nn.conv2d(v, filters, [1,1,1,1], padding='VALID')
    with tf.control_dependencies([conv2d]): 
      train = tf.no_op() 
    with tf.Session(config=config) as sess:
      sess.run(tf.global_variables_initializer())
      sess.run(train, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)
      # conv2d is not executed

      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)
      # conv2d is executed
    root_node = tf.contrib.tfprof.model_analyzer.print_model_analysis(
                  tf.get_default_graph(),
                  run_meta=run_metadata,
                  tfprof_options=opts)
```"
17185,"Tensorflow 1.6 ALWAYS looking for libcublas.so.8.0 with Cuda 9.0, Cudnn 7.0 (and libcublas.so.9.0)","### System information
- **OS Platform and Distribution** -  Linux Ubuntu 16.04
- **TensorFlow installed from** - source 
- **TensorFlow version** - tensorflow-1.6.0rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:  0.10.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: Cuda 9.0/ cuDNN 7.0
- **GPU model and memory**: Nvidia Quadro M1200 w/4GB GDDR5
- **Exact command to reproduce**: `import tensorflow as tf`
- **Have I written custom code**: No. Just trying to import tensorflow in python. The only line of code I wrote is  `import tensorflow as tf`

### Problem
Tensorflow is looking for **libcublas.so.8.0** although I linked the cuda path during configuration to /usr/local/cuda-9.0  (also tried linking /usr/local/cuda ). How do I run tensorflow with cuda 9.0 and cudnn 7.0? 

Outputs of: 
`ldconfig -v` :

> libcublas.so.9.0 -> libcublas.so.9.0.176
> libcudnn.so.7 -> libcudnn.so.7.0.5

`import tensorflow as tf`

> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory
> Failed to load the native TensorFlow runtime.

## Complete Traceback
`import tensorflow as tf`

> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/home/user/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory
> 
> Failed to load the native TensorFlow runtime.
> 
"
17184,Error Creating Predictor from Core Estimator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**:  3.5.3
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: Titan V, 12288 MB
- **Exact command to reproduce**: See below

### Describe the problem
Creating a Predictor object by calling `from_estimator` results in the following error:

```python
----> 5 est_pred= predictor.from_estimator(est, serving_input_receiver_fn)

c:\libraries\python35\lib\site-packages\tensorflow\contrib\predictor\predictor_factories.py in from_estimator(estimator, serving_input_receiver_fn, output_key, graph)
     86       `Estimator`.
     87   """"""
---> 88   if isinstance(estimator, estimator.Estimator):
     89     raise TypeError('Espected estimator to be of type '
     90                     'tf.python.estimator.Estimator, but got type '

AttributeError: 'Estimator' object has no attribute 'Estimator'
```

### Source code / logs
To reproduce this error:
```python
import tensorflow as tf
from tensorflow.contrib import predictor

def model_fn(features, labels, mode):
    # this will trigger other errors once the type check is passed
    return None
est = tf.estimator.Estimator(model_fn=model_fn)

def serving_input_receiver_fn():
    inputs = {'X': tf.placeholder(shape=[None, 1], dtype=tf.float32)}
    return tf.estimator.export.ServingInputReceiver(inputs, inputs)

est_pred = predictor.from_estimator(est , serving_input_receiver_fn)
```

It appears the type check on predictor_factories.py's line 88 has a typo, since ```estimator``` is overridden by the function argument. 
"
17183,Feature Request: Create CTC loss function that Has an Optional Sequence Length Argument,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem
Since the `sequence_length` parameter of CTC functions are each dependent on the input (specifically the number of time steps), why not let them determine the sequence lengths themselves? Or have a separate function that determines the sequence lengths.

### Source code / logs
I was thinking of adding something like this to ctc_loss (assuming inputs are batch major and of shape [None, num_time_steps, num_classes]):

```
batch_size = input.shape[0]
sequence_length = input.shape[1]
sequence_lengths = tf.fill([batch_size], sequence_length)  
```

Unfortunately, this throws an error.

`TypeError: Expected int32 passed to parameter 'dims' of op 'Fill', got [Dimension(None)] of type 'list' instead.`"
17182,"Log ""Waiting for new checkpoint at"" in tf.contrib.training.evaluate_repeatedly only after the checkpoint is found","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0

### Problem description
I use `tf.contrib.training.evaluate_repeatedly` function found here: https://github.com/tensorflow/tensorflow/blob/37aa430d84ced579342a4044c89c236664be7f68/tensorflow/contrib/training/python/training/evaluation.py#L345

The two parameters that are relevant here are:
`timeout=1, timeout_fn=my_timeout_fn`, where `my_timeout_fn` is a custom function and returns True or False. Basically I want to stop evaluation loop if some condition is met, in which case the function will return 1. I have to set timeout to some small value, in this case 1 sec, so that timeout_fn is triggered often. The issue with this is that I get the next log message very often:
```
Waiting for new checkpoint at /path/to/dir
```
I set timeout=10, but still I don't want to see that useless log message every 10 seconds.

The message comes from here:
https://github.com/tensorflow/tensorflow/blob/37aa430d84ced579342a4044c89c236664be7f68/tensorflow/contrib/training/python/training/evaluation.py#L192

One of the solutions would be to put this logging command inside the caller of `wait_for_new_checkpoint` function and execute it once before `while True` loop on  line 248, and then right after ` yield checkpoint_path` on line 264. This way we get this message only after a checkpoint is found.
"
17179,Question about freeze the graph,"I found a model which has the checkpoint file inside .pd

```
## Saving the lastest checkpoint to protobuf file
flow --model cfg/yolo-new.cfg --load -1 --savepb
```
[https://github.com/thtrieu/darkflow](url)

So do I still have to freeze the graph?"
17178,Failed to build graph_transforms,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Fedora 24
- **TensorFlow installed from (source or binary)**:
both... 
- **TensorFlow version (use command below)**:
master
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
Build label: 0.10.0- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Mar 9 06:25:03 +50057 (1517485040703)
Build timestamp: 1517485040703
Build timestamp as int: 1517485040703

- **GCC/Compiler version (if compiling from source)**:
gcc (GCC) 7.3.1 20180130 (Red Hat 7.3.1-2)

### Describe the problem
I need to use graph_transforms as explained in documentation to be able to load a pb file in OpenCV
So I launched that command from github clone project:

```
bazel build tensorflow/tools/graph_transforms:transform_graph
```

After 20 minutes of compilation:

```
ERROR: /home/pafer/Projects/Lab/tensorflow/tensorflow/cc/BUILD:510:1: Executing genrule //tensorflow/cc:remote_fused_graph_ops_genrule failed (Exit 127)
bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: symbol lookup error: bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc: undefined symbol: _ZN6google8protobuf5Arena13CreateMessageIN10tensorflow9AttrValueEEEPT_PS1_
Target //tensorflow/tools/graph_transforms:transform_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2304.405s, Critical Path: 88.76s
FAILED: Build did NOT complete successfully
```

I tried `./configure`, and remove Android build, GCE build and so on. Here the configure output:

```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/lib64/python3.6/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/bin/python3""
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""0""
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
```
Note: why ./configure says ""true"" on kafka, s3, and so on...? I said ""no"" at ./configure prompts
"
17177,logits or log probabilities?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4 and 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Tried several
- **GPU model and memory**: CPU and GPU, but GPU was nvidia p100
- **Exact command to reproduce**: See ""source code"" below

### Describe the problem
There seems to be some confusion about logits vs log probabilities.  In some cases logits seem to mean just log probabilities.  In other cases they seem to mean actual logits, `log(p/(1-p))` which is quite a different thing.  It seems possible to me that this is causing downstream bugs, potentially big ones in edward, though I haven't looked closely.  

Other people [also seem confused](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow).

### Source code / logs
```
import tensorflow as tf
from tensorflow.python.ops.distributions import util as distribution_util
import numpy as np

tf.reset_default_graph()
sess=tf.Session()

inp = tf.placeholder(dtype=tf.float32)
logits,probs=distribution_util.get_logits_and_probs(logits=inp)

categ=tf.distributions.Categorical(logits=inp)
vals=categ.sample(100000)

probsv,valsv=sess.run([probs,vals],feed_dict={inp:np.log([.2,.3,.5])})

print(probsv)
print([np.mean(valsv==x) for x in range(3)])
```

```
[0.16666667 0.23076922 0.33333334]
[0.20078, 0.29984, 0.49938]
```

So in some cases `logits` seems to mean log probabilities, but on other cases
it seem to actually mean logits.  "
17176,Feature request: Provide API to test whether a copy of TensorFlow has MKL support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.5.0-9-gc959ec7
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
I have built a copy of TensorFlow with MKL-DNN support. The MKL convolution operators work much better with the ""channels_first"" (NCHW) data format. On non-MKL, non-GPU builds, the ""channels_first"" data format does not work for me, because a CPU version of average pooling is not implemented for that data format.

I would like to have my scripts test whether the current copy of TensorFlow has either MKL or GPU support, and if so, to use the ""channels_first"" data format. I can easily test for GPU support with `tf.test.is_built_with_cuda()`. However, there does not seem to be a corresponding API to test for MKL-DNN support. Would it be possible to add one?
"
17175,tf.einsum not replicating np.einsum; struggles to read input with whitespace,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Not sure
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**:  NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
- **Exact command to reproduce**: See below.


### Problem
Unlike `np.einsum`, I have found that `tf.einsum` struggles to parse input strings with spaces.
Either it throws an error, or it produces varying results for the same syntax (e.g. `ij,jk->ik` gives different results than `ij,jk-> ik`). 

I believe the problem traces to the regular expressions match in line 164 (carried to line 178) of this file: `tensorflow/tensorflow/python/ops/special_math_ops.py`.


### Source code
```
import tensorflow as tf
import numpy as np

import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

x = tf.range(5,10,  dtype=tf.float64)
y = tf.ones(shape=(2,6), dtype=tf.float64)

# Each invocation of tf.einsum produces a different result
tf.einsum(""i,jk->ijk"", x, y)    # shape = (5,2,6)
tf.einsum(""i,jk-> ijk"", x, y)   # shape = ()
tf.einsum(""i, jk -> ijk"", x, y) # Error

x = np.arange(5,10,  dtype=np.float64)
y = np.ones(shape=(2,6), dtype=np.float64)

# In contrast, np.einsum produces consistent results
np.einsum(""i,jk->ijk"", x, y)    # Array 5x2x6
np.einsum(""i,jk-> ijk"", x, y)   # Array 5x2x6
np.einsum(""i, jk -> ijk"", x, y) # Array 5x2x6
```"
17170,Provide a way to convert feature columns to tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

### Describe the problem

The only way to convert a list of feature columns to tensors in TensorFlow 1.5 is by passing them to an [`input_layer`](https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer). This would do the per-column conversion and concatenate the resulting tensors. While useful in some cases, this approach is a bit limiting, as it does not allow to 

* concatenate on an axis different than `axis=1` (think sequence data),
* extract tensors for the individual columns. 

Both limitations can be mitigated by introducing a new public function converting from `_FeatureColumn` to `Tensor` (or even from a list of feature columns to a list of tensors). In the latter case, it could just be [`_internal_input_layer`](https://github.com/tensorflow/tensorflow/blob/17103a0b8dfdd7abd2b0dcb14f905aba879ff3a4/tensorflow/python/feature_column/feature_column.py#L164) without the concatenation in the end. 

What do you think? If this sounds good, I can submit a PR."
17169,tf.contrib.layers output_collections inconsistency,"Please correct me if I'm wrong, but there seems to be an inconsistency between the pooling and the convolution layers in `tf.contrib.layers` when it comes to adding variables to the `outputs_collection`, and using `tf.get_variable_scope().reuse_variables()`.

Here is the relevant part of the model definition.
```
def build_network(input):
    with tf.variable_scope('scope') as sc:
        ...
        net = slim.max_pool2d(net, [2, 2], scope='pool')
        ...
        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
        ...
        skip = slim.conv2d(end_points[sc.name + '/pool'], out_channels, [1, 1], scope='skip')
        ...
        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
        return net, end_points
```

And here is my test, which fails:
```
network, end_points = build_network(input)
tf.get_variable_scope().reuse_variables()
network, end_points = build_network(input)
```

The reason the test fails is that whereas my convolutions all end up with the same names in the `end_points` dictionary both times (e.g. `scope/conv1/conv1_1`), the max_pooling layers end up with different names (e.g. the first time the network is built we get `scope/pool`, and the second time we get `scope_1/pool`). This means that the `end_points['scope/pool']` fails since the dictionary contains `'scope_1/pool'` as a key.

I've done some breakpoint digging and it turns out that the behavior [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L1063) and [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L2266) differs due to convolutions using variable scopes and pooling using name scopes.

Now I can potentially fix my code by using `sc.original_name_scope` instead of `sc.name`, but then the end_points dictionary is still wrong. I could manually fix the dictionary, but this seems hacky. I feel like it could be made such that the behaviour is consistent across the different layers. What do you think?"
17168,Can you document tf metrics use in tf.keras?,"There is no official documentation on how to use tf metrics in tf.keras. 
Two stackoverflow reference but I don't know if it is a API stable solution: 
https://stackoverflow.com/questions/45947351/how-to-use-tensorflow-metrics-in-keras
https://stackoverflow.com/questions/43158719/how-can-i-use-tensorflow-metric-function-within-keras-models"
17167,error in validating tensorflow installation,"i installed tensorflow with cuda support on windows 10 and i'm trying to validate it,
i used this script https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c and the result is
![11](https://user-images.githubusercontent.com/15125749/36480211-c33bef0a-170b-11e8-89e6-8167389c5812.PNG)

then i tried import tensorflow as tf from python console and:
![22](https://user-images.githubusercontent.com/15125749/36480256-ecba34e0-170b-11e8-9860-53b33ce49d21.PNG)
"
17166,custom matrix multiplication,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17165,cifar 10 can not be downloaded,"Hi, 

I am trying to follow the tutorial from the source code of tensor flow: /model/tutorials/cifar10/

In the cifar10.py, the link 

> DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'

does not work, I guess it is the problem from Canada.

Do you have a solution for this???

Thanks
"
17164,Catch / recover from CUDNN_STATUS_BAD_PARAM,"Hi all,

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win7
* TensorFlow installed from (source or binary): source
* TensorFlow version (use command below): 1.3
* GCC/Compiler version (if compiling from source): VC14_64
* CUDA/cuDNN version: 8 / 6
* GPU model and memory: Quadro K2000M
* Bazel version (if compiling from source): N/A
* Exact command to reproduce: N/A

I am using the C++ API to load and execute trained models.
I sometimes experience a problem regarding the ensemble of CuDNN and tensorflow:
When I invoke Session::Run and the input tensor I am providing is too small for the network (which is fully convolutional and thus has no fixed input extents given), CuDNN crashes with CUDNN_STATUS_BAD_PARAM.
Is there any way to catch this as an exception or somehow recover from that failure? Or is there a way to determine whether the given tensor extent will lead to such an error before I actually throw it at the network?

Thanks!"
17159,Estimator WarmStartSettings Cause Extreme Training Slowdown.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary(official docker image)
- **TensorFlow version (use command below)**: 1.6.0-rc1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: 9.0 / 7.0.5.15-1
- **GPU model and memory**: GeForce GTX 1060 6GB
- **Exact command to reproduce**: 

```py
warm_start_from = None 
if FLAGS.warm_start:
    warm_start_from = tf.estimator.WarmStartSettings(
        ckpt_to_initialize_from=FLAGS.warm_start,
        # NOTE: attempted with and without `vars_to_warm_start` set
        # vars_to_warm_start='^(?!.*(RMSProp|global_step))'
    )

my_model = tf.estimator.Estimator(
    model_fn=my_model_fn,
    model_dir=FLAGS.model,
    warm_start_from=warm_start_from
)
```

### Describe the problem

I am attempting to fine tune a model trained with RMSProp. I only want to load the model weights from the checkpoint, not the RMSProp state. To achieve this, I am using Tensorflow 1.6-rc1's `WarmStartSettings`.

When I train the model without `WarmStartSettings`, each minibatch takes around 0.5 seconds. However, when I attempt to use that checkpoint with `WarmStartSettings`, each minibatch takes around 1.7 seconds.

Having inspected the logs to make sure the weights are not being initialised for every batch, It seems like this is a bug in Tensorflow. It should also be noted that I experienced similar problems in Tensorflow 1.5 when using `init_from_checkpoint` in my `model_fn` (as seen in https://github.com/tensorflow/tensorflow/issues/10155, which led me to discover the `WarmStartSettings` API).

### Source code / logs

#### Without WarmStartSettings (train for one epoch)

```sh
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Train data shape...
(30000, 500)
(30000, 2048)
Eval data shape...
(5000, 500)
(5000, 2048)
Test data shape...
(5000, 500)
(5000, 2048)
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6243687f90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}
Training Epoch #1...
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-02-20 20:31:08.668914: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-02-20 20:31:08.767997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-20 20:31:08.768353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 3.84GiB
2018-02-20 20:31:08.768368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:31:08.919155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3579 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.
INFO:tensorflow:loss = 0.43950942, step = 1
INFO:tensorflow:global_step/sec: 196.162
INFO:tensorflow:loss = 0.46729112, step = 101 (0.510 sec)
INFO:tensorflow:global_step/sec: 205.419
INFO:tensorflow:loss = 0.35870957, step = 201 (0.487 sec)
INFO:tensorflow:global_step/sec: 206.952
INFO:tensorflow:loss = 0.32010338, step = 301 (0.483 sec)
INFO:tensorflow:global_step/sec: 204.129
INFO:tensorflow:loss = 0.3345171, step = 401 (0.490 sec)
INFO:tensorflow:global_step/sec: 209.188
INFO:tensorflow:loss = 0.34134513, step = 501 (0.478 sec)
INFO:tensorflow:global_step/sec: 205.325
INFO:tensorflow:loss = 0.33101806, step = 601 (0.487 sec)
INFO:tensorflow:global_step/sec: 207.222
INFO:tensorflow:loss = 0.34841973, step = 701 (0.483 sec)
INFO:tensorflow:global_step/sec: 206.455
INFO:tensorflow:loss = 0.33363524, step = 801 (0.484 sec)
INFO:tensorflow:global_step/sec: 208.705
INFO:tensorflow:loss = 0.33115995, step = 901 (0.479 sec)
INFO:tensorflow:global_step/sec: 205.786
INFO:tensorflow:loss = 0.32718512, step = 1001 (0.486 sec)
INFO:tensorflow:global_step/sec: 205.812
INFO:tensorflow:loss = 0.33765212, step = 1101 (0.486 sec)
INFO:tensorflow:global_step/sec: 205.522
INFO:tensorflow:loss = 0.33326942, step = 1201 (0.487 sec)
INFO:tensorflow:global_step/sec: 208.248
INFO:tensorflow:loss = 0.34017226, step = 1301 (0.480 sec)
INFO:tensorflow:global_step/sec: 205.824
INFO:tensorflow:loss = 0.32448825, step = 1401 (0.486 sec)
INFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.
INFO:tensorflow:Loss for final step: 0.30791047.
Evaluating Epoch #5...
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-02-20-20:31:17
INFO:tensorflow:Graph was finalized.
2018-02-20 20:31:17.344022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:31:17.344148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-02-20-20:31:17
INFO:tensorflow:Saving dict for global step 1500: global_step = 1500, loss = 0.32683796
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
2018-02-20 20:31:17.575632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:31:17.575748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:From scripts/my_model.py:85: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead
2018-02-20 20:31:17.738903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:31:17.739028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
R@1: 1.2
R@5: 2.9
R@10: 4.7
```

#### With WarmStartSettings

```sh
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Train data shape...
(30000, 500)
(30000, 2048)
Eval data shape...
(5000, 500)
(5000, 2048)
Test data shape...
(5000, 500)
(5000, 2048)
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fccf90c7f90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}
Training Epoch #1...
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='output/checkpoint/', vars_to_warm_start='^(?!.*(RMSProp|global_step))', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: ('output/checkpoint/',)
INFO:tensorflow:Warm-starting variable: my_model/dense_3/kernel; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_3/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_3/kernel
INFO:tensorflow:Warm-starting variable: my_model/dense_3/bias; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_3/bias:0 from checkpoint output/checkpoint/ with my_model/dense_3/bias
INFO:tensorflow:Warm-starting variable: my_model/dense_2/bias; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_2/bias:0 from checkpoint output/checkpoint/ with my_model/dense_2/bias
INFO:tensorflow:Warm-starting variable: my_model/dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_1/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_1/kernel
INFO:tensorflow:Warm-starting variable: my_model/dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_1/bias:0 from checkpoint output/checkpoint/ with my_model/dense_1/bias
INFO:tensorflow:Warm-starting variable: my_model/dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense/kernel:0 from checkpoint output/checkpoint/ with my_model/dense/kernel
INFO:tensorflow:Warm-starting variable: my_model/dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense/bias:0 from checkpoint output/checkpoint/ with my_model/dense/bias
INFO:tensorflow:Warm-starting variable: my_model/dense_2/kernel; prev_var_name: Unchanged
INFO:tensorflow:Initialize variable my_model/dense_2/kernel:0 from checkpoint output/checkpoint/ with my_model/dense_2/kernel
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-02-20 20:31:44.926116: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-02-20 20:31:45.024835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-20 20:31:45.025212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 3.84GiB
2018-02-20 20:31:45.025242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:31:45.174051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3579 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.
INFO:tensorflow:loss = 0.32987013, step = 1
INFO:tensorflow:global_step/sec: 56.8537
INFO:tensorflow:loss = 0.3100884, step = 101 (1.759 sec)
INFO:tensorflow:global_step/sec: 57.4584
INFO:tensorflow:loss = 0.32932568, step = 201 (1.740 sec)
INFO:tensorflow:global_step/sec: 58.9717
INFO:tensorflow:loss = 0.30233204, step = 301 (1.696 sec)
INFO:tensorflow:global_step/sec: 57.5253
INFO:tensorflow:loss = 0.3428804, step = 401 (1.738 sec)
INFO:tensorflow:global_step/sec: 57.8845
INFO:tensorflow:loss = 0.331369, step = 501 (1.728 sec)
INFO:tensorflow:global_step/sec: 58.2558
INFO:tensorflow:loss = 0.3059027, step = 601 (1.717 sec)
INFO:tensorflow:global_step/sec: 58.3863
INFO:tensorflow:loss = 0.32942265, step = 701 (1.713 sec)
INFO:tensorflow:global_step/sec: 58.454
INFO:tensorflow:loss = 0.31594634, step = 801 (1.711 sec)
INFO:tensorflow:global_step/sec: 58.8543
INFO:tensorflow:loss = 0.31210855, step = 901 (1.699 sec)
INFO:tensorflow:global_step/sec: 58.7531
INFO:tensorflow:loss = 0.33128193, step = 1001 (1.702 sec)
INFO:tensorflow:global_step/sec: 57.5272
INFO:tensorflow:loss = 0.31882855, step = 1101 (1.739 sec)
INFO:tensorflow:global_step/sec: 58.2324
INFO:tensorflow:loss = 0.32009652, step = 1201 (1.717 sec)
INFO:tensorflow:global_step/sec: 58.8266
INFO:tensorflow:loss = 0.31766868, step = 1301 (1.700 sec)
INFO:tensorflow:global_step/sec: 57.1415
INFO:tensorflow:loss = 0.304159, step = 1401 (1.750 sec)
INFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.
INFO:tensorflow:Loss for final step: 0.30592433.
Evaluating Epoch #5...
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-02-20-20:32:12
INFO:tensorflow:Graph was finalized.
2018-02-20 20:32:12.146704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:32:12.146840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-02-20-20:32:12
INFO:tensorflow:Saving dict for global step 1500: global_step = 1500, loss = 0.32321918
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
2018-02-20 20:32:12.352526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:32:12.352642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from output/my_model/model.ckpt-1500
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:From scripts/my_model.py:85: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead
2018-02-20 20:32:12.522432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-20 20:32:12.522561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 45 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
R@1: 1.7
R@5: 4.3
R@10: 7.4
```
"
17158,Feature Request: Better error reporting for tflite conversion,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MacOS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Not using
- **GPU model and memory**: Radeon Pro 455
- **Exact command to reproduce**: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite \
  --inference_type=FLOAT \
  --input_arrays=input \
  --output_arrays=output \
  --input_shapes=1 \
  --output_shapes=1

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Overview
I have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite. 

Model conversation seems to work
When I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.

Use on mobile doesn't work
When I implement my tflite model into an iOS app and try to grab a node with 
````
float* out = interpreter->typed_tensor<float>();
````
I sometimes get a valid pointer back but other times I get a null pointer depending on the node. 

After retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working. 

My reqeust
Please update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile. 

Tensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!

My graph code:
````
valid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=""input"")

embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=""embeddings"")

norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name=""norm"")
normalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')
valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name=""valid_embeddings"")
similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name=""similarity"")
output = tf.reshape(similarity, [-1], name=""output"")
````

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Output from tflite conversion:
````
(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000
2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)
2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)
2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)
2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)
2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000
````"
17156,LookupError: gradient registry has no entry for: Svd," Hello All,

I am using singular values of the weights at each convolution layer as a regularizer and adding it in kernel_regularizer=self.l2_reg(). Where l2_reg is the function which return that.
```

        def l2_reg(weights):
                """""" Reshaping the matrxi in to 2D tensor for enforcing orthogonality""""""
                w = tf.identity(weights)

                """"""Calculating the Loss Obtained""""""
                s1 = tf.linalg.svd(reg, full_matrices=True,compute_uv=False)
                ortho_loss = tf.nn.l2_loss(s1)
                return ortho_loss
```
Error I am getting is:
```
raceback (most recent call last):
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 551, in gradients
    grad_fn = ops.get_gradient_function(op)
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2134, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/framework/registry.py"", line 93, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: Svd

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_dense_net.py"", line 142, in <module>
    model = DenseNet(data_provider=data_provider, **model_params)
  File ""/home/bansa01/densenet_final/tmp_spectral/models/dense_net.py"", line 85, in __init__
    self._build_graph()
  File ""/home/bansa01/densenet_final/tmp_spectral/models/dense_net.py"", line 461, in _build_graph
    cross_entropy + l2_loss_total + reg_loss_total)
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 343, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/keras-python3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 555, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'Initial_convolution/conv2d/kernel/Regularizer/Svd' (op type: Svd)
```
I Understand that this is due to not able to calculate the gradient for SVD operation, but then how do I remove this issue.
Nitin"
17155,Memory leak in tf.unique on GPU,"### System 1 information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GeForce GTX TITAN, 6Gb
- **Exact command to reproduce**: n/a

### System 2 information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0-dev20180219
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: TITAN X (Pascal), 12Gb
- **Exact command to reproduce**: n/a

### Describe the problem
The `tf.unique` op leaks memory when it is placed on GPU. 
I wrote a simple script (below) that runs a couple of `tf.unique` ops. I track the memory usage with `psutil.Process(getpid()).memory_info().rss` command. I ran the script on System 1 twice: once on CPU and GPU (specified by `tf.device`), for `num_steps=2000`. Here is the plot of the memory usage:
![mem_usage1](https://user-images.githubusercontent.com/5220571/36442332-583efeb6-166d-11e8-960f-b7bb4990a9a8.png)

So, the memory usage of `CPU unique` is pretty flat, but the `GPU unique` is inconclusive. Here is the memory usage on System 1 with `num_steps=10000`:
![mem_usage2](https://user-images.githubusercontent.com/5220571/36442382-88e30238-166d-11e8-8a39-624a80676267.png)

Relatively recent version of tensorflow (System 2) also has the problem:
![mem_usage3](https://user-images.githubusercontent.com/5220571/36442531-07075100-166e-11e8-84e4-0103c7e4de72.png)


### Source code / logs

```
import tensorflow as tf
import numpy as np
import psutil
from os import getpid

val_num = 8*256*256
val_dim = 5
max_val = 200
num_steps = 10000


def main():

    with tf.device(""/gpu:0""):
        x = tf.placeholder(tf.int32, [val_num, val_dim])
    
        def tf_unique_row_idxs(inp, max_dim=None, name=''):
            with tf.variable_scope('tf_unique_row_idxs_'+name) as scope:
                if not max_dim:
                    max_dim = inp.get_shape().as_list()[1]
                new_vals = inp[:,0]
                new_vals = tf.cast(new_vals, dtype=tf.int32)
                _, idx = tf.unique(new_vals, out_idx=tf.int32)
                for j in range(1, max_dim):
                    new_vals = inp[:,j]
                    new_vals = tf.cast(new_vals, dtype=tf.int32)
                    val_min = tf.reduce_min(new_vals)
                    val_max = tf.reduce_max(new_vals)
                    idx_shift = val_max - val_min + 1
                    vals = idx*idx_shift + new_vals - val_min
                    uvals, idx = tf.unique(vals, out_idx=tf.int32)
                max_pos = tf.shape(uvals, out_type=tf.int32)[0] + 0
                return idx, max_pos
    
        idxs, max_pos = tf_unique_row_idxs(x)
    
    
    process = psutil.Process(getpid())

    cur_config=tf.ConfigProto(allow_soft_placement=False,log_device_placement=False)
    sess = tf.Session(config=cur_config)
    sess.run(tf.global_variables_initializer())
    

    mem_usage = np.zeros([num_steps], dtype=np.float32)

    np.random.seed(0)
    for i in range(num_steps):
        cur_x = np.random.randint(0, max_val, [val_num, val_dim], dtype=np.int32)
        cur_feed_dict = {x: cur_x}
        cur_max_pos, cur_idxs = sess.run([max_pos, idxs], feed_dict=cur_feed_dict)
        mem_usage[i] = process.memory_info().rss/2**30
        if i%100==0:
            print(i/num_steps)
    

    np.savetxt('unique_memlog.txt', mem_usage)

if __name__ == ""__main__"":
    main()

```
"
17154,Bazel doesn't use the optimization flag specified in configuration,"Hi

I was compiling TensorFlow on a PowerPC machine and it kept failing with the error ""unrecognized command line option -march=native"" when compiling the file pcre_byte_order.c. This happened even though I specified the default flag -mcpu=native when running ./configure. I also used the --cxxopt=-mcpu=native flag to no avail, and the only way to solve the problem was to remove the --config=opt flag when invoking bazel."
17153,How to sync worker models of KMeansClustering in distributed tensorflow?,"### System information
-**Have I written custom code**: yes
-**OS Platform and Distribution**: Open SUSE Leap 42.3
-**TensorFlow installed from:** python pip
-**TensorFlow version**: 1.6.0
-**Python version**: 2.7
-**Bazel version**  :N/A
-**CUDA/cuDNN version** : N/A
-**GPU model and memory** : N/A
-**Exact command to reproduce** : N/A

Hi, 
I am trying to use distributed tensorflow over KMeansClustering. I have one **parameter server** and two **workers**. **Training data** in both the workers are **different**.  After training, cluster centers in the two workers are different. Is there a function in tensorflow which can be called to sync the models while training so that the cluster   centers are similar if not same.

**Source Code** 

```
def startCluster(jobName,taskId):
    parameter_servers = [""localhost:2222""]
    workers = [""localhost:2223"",
               ""localhost:2224""]

    cluster = tf.train.ClusterSpec({""ps"": parameter_servers, ""worker"": workers})
    server = tf.train.Server(
        cluster,
        job_name=jobName,
        task_index=taskId)

    return cluster,server

def trainModelInParallel(cluster,server,taskId):
    k = 4
    n = 1000
    variables = 2

    points = np.random.uniform(0, 1, [n, variables])

    # Between-graph replication
    with tf.device(tf.train.replica_device_setter(
            worker_device=""/job:worker/task:%d"" % taskId,
            cluster=cluster)):

        input_fn = lambda: tf.train.limit_epochs(tf.convert_to_tensor(
                                    points, dtype=tf.float32), num_epochs=1)
        kmeans = tf.contrib.factorization.KMeansClustering(
            num_clusters=k, use_mini_batch=False,model_dir=defaultModel)


    sv = tf.train.Supervisor(is_chief=(taskId == 0),save_model_secs=1)

    with sv.prepare_or_wait_for_session(server.target) as sess:

        for _ in xrange(10):
            kmeans.train(input_fn)
            centers = kmeans.cluster_centers()
            print centers[0],centers[1],centers[2],centers[3]


if __name__ == ""__main__"":
    if len(sys.argv) == 1:
        print""Please pass job name and task ID""
        sys.exit()

    jobName = sys.argv[1]
    taskId = (sys.argv[2])
    defaultModel += taskId
    cluster, server = startCluster(jobName, int(taskId))

    if jobName == ""ps"":
        server.join()
    else:
        trainModelInParallel(cluster,server,int(taskId))

```
**Command Line** 
To execute the above code please enter following commands in 3 different terminals:
```
python kmeansDistributed.py ps 0
python kmeansDistributed.py worker 0
python kmeansDistributed.py worker 1
```

@ccolby Your inputs will be very helpful."
17151,ERROR: Unrecognized option: --python_path,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit e5e03ef3148303b3dfed89a1492dedf92b45be25 (HEAD -> master, origin/master, origin/HEAD)
- **Python version**:  Python 2.7.13
- **Bazel version (if compiling from source)**: 0.10.1
- **GCC/Compiler version (if compiling from source)**:  4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: see log


### Describe the problem
TF failed to build from source.
When building from source, Bazel does not recognize --python_path set during ./configure
Please see the log below.

### Source code / logs
**C02PK120FVH6**:tensorflow neitan01$ ./configure
WARNING: current bazel installation is not a release version.
Make sure you are running at least bazel 0.5.4
Please specify the location of python. [Default is /usr/local/opt/python/bin/python2.7]: 


Found possible Python library paths:
  /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: y
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: y
GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: y
VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished


**C02PK120FVH6**:tensorflow neitan01$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package
Killed non-responsive server process (pid=1017)
.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=181
INFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/tools/bazel.rc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt
INFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python2.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages --force_python=py2 --host_force_python=py2 --python_path=/usr/local/opt/python/bin/python2.7 --define with_gcp_support=true --define with_xla_support=true --define with_gdr_support=true --define with_verbs_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --define grpc_no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
**ERROR: Unrecognized option: --python_path=/usr/local/opt/python/bin/python2.7**


**C02PK120FVH6**:tensorflow neitan01$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=181
INFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/tools/bazel.rc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt
INFO: Reading options for 'build' from /Users/neitan01/src/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python2.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages --force_python=py2 --host_force_python=py2 --python_path=/usr/local/opt/python/bin/python2.7 --define with_gcp_support=true --define with_xla_support=true --define with_gdr_support=true --define with_verbs_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --define grpc_no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
ERROR: Unrecognized option: --python_path=/usr/local/opt/python/bin/python2.7


**C02PK120FVH6**:tensorflow neitan01$ bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0


**C02PK120FVH6**:tensorflow neitan01$ brew info bazel
**bazel: stable 0.10.1** (bottled)
Google's own build tool
https://bazel.build/
/usr/local/Cellar/bazel/0.5.1 (10 files, 138.0MB)
  Poured from bottle on 2017-06-16 at 16:45:38
/usr/local/Cellar/bazel/0.10.1 (12 files, 93.4MB) *
  Poured from bottle on 2018-02-15 at 23:19:51
From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/bazel.rb
==> Requirements
Required: java = 1.8 ✔, macOS >= 10.10 ✔
==> Caveats
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d

zsh completions have been installed to:
  /usr/local/share/zsh/site-functions
"
17150,Problem with Keras sparse_categorical_crossentropy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip install)
- **TensorFlow version (use command below)**: 1.5.0 (Keras 2.1.2-tf)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: Can't remember.
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: See below.

### Background

This issue seems to be specifically about Keras with TensorFlow so I have posted it here.

I have a Keras model for doing Machine Translation of human languages. It has an encoder and decoder each of which use the `Embedding` and `GRU` layers from Keras. The output of the decoder is a one-hot encoded array.

My data-set is from Europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many GB of memory.

One solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time. But that's not a very elegant solution.

The correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the model's output. Keras' has a built-in loss-function for doing exactly this called `sparse_categorical_crossentropy`. However, it doesn't seem to work as intended.

### Error

The following shows the essential parts of the code.

    # (Omitted code for building neural network ...)

    # Output of the decoder-part of the neural network.
    decoder_dense = Dense(num_words,
                          activation='softmax',
                          name='decoder_output')
    decoder_output = decoder_dense(decoder_gru_output)

    model = Model(inputs=[encoder_input, decoder_input],
                  outputs=[decoder_output])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy')

    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)

Everything runs fine except `model.fit()` at the end which gives this error:

    ValueError: Error when checking target: expected decoder_output to have 3 dimensions, but got array with shape (20000, 67)

This is the shape of the model's output:

    >>> decoder_output.get_shape()
    TensorShape([Dimension(None), Dimension(None), Dimension(10000)])

This is the shape of the target-data, which is a 2-dim array of integer-values:

    y_data['decoder_output'].shape
    >>> (20000, 67)

Note that I only allow sequences of length 67 for the decoder's output.

### Working Solution

We can use TensorFlow's implementation of sparse cross-entropy, which seems to work as intended.

First we need to have a linear activation on the output of the decoder:

    decoder_dense = Dense(num_words,
                          activation='linear', # NOTE: changed from 'softmax'
                          name='decoder_output')

Then we need a wrapper-function for the loss that is compatible with Keras:

    def sparse_loss(y_true, y_pred):
        return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,
                                                              logits=y_pred)

Then we need to create a placeholder variable for the batch of target-values. Once again I only allow sequences of length 67 (this is of course a variable in my own code).

    decoder_target = tf.placeholder(dtype='int32', shape=(None, 67))

    model_train.compile(optimizer='adam,
                        loss=sparse_loss,
                        target_tensors=[decoder_target])

This works fine and we can train it by calling:

    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)

Maybe Keras should use TensorFlow's sparse-cross-entropy more directly, because it seems to handle higher-dim data better?

### Documentation

Looking at the implementation of `sparse_categorical_crossentropy` in Keras there is actually some reshaping going on there, but the doc-string doesn't make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done, so it's impossible to know whether it is a bug or a feature I am experiencing, and how to deal with it properly.

The doc-string needs to be made more clear by someone who understands the intention of this code.

Furthermore, the doc-string needs to be ""exported"" somehow to the online docs because it is not shown here: https://keras.io/losses/#sparse_categorical_crossentropy
"
17149,Cannot assign a device for operation 'dense_0/dense/Tensordot/ListDiff',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04.3 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**:
2.7.12
- **Bazel version (if compiling from source)**:
not from source
- **GCC/Compiler version (if compiling from source)**:
not from source
- **CUDA/cuDNN version**:
9.0 / 7
- **GPU model and memory**:
GTX 1070 8GB
- **Exact command to reproduce**:
  - loading model through tensorflow.contrib.predictor.from_saved_model
  - loading model through tensorflow serving (built from latest source, with GPU support)

### Describe the problem
I'm trying to load Estimator saved using export_savedmodel.
the model is a speech to text model with CNN, ctc_loss and ctc_greedy_decoder, trained on single GTX 1070. Training run successfully and the model is also saved. but when i load it with tensorflow serving or from_saved_model i got this error:
```
failed: Invalid argument: Cannot assign a device for operation 'dense_0/dense/Tensordot/ListDiff': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]
  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]
```

### Source code / logs
[error.txt](https://github.com/tensorflow/tensorflow/files/1739607/error.txt)
[source code](https://github.com/tensorflow/tensorflow/files/1739608/bug.zip) (run estimator_CNN.py)


"
17147,ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory," I installed cuda 9.1 and set the path as suggested.
Should i install cuda 8.0 for resolving this problem?
Error:

ImportError: Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/opt/conda/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/opt/conda/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory "
17146,tensorflow/contrib/lite/examples/label_image,"Build it for desktop machines (tested on Ubuntu and OS X)

bazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image

I am not able to build the this example on my Ubuntu 16.04 Intel Desktop using the command above.
Using TF 1.5 and the error is related to NEON. Can I run .tflite models on Desktop?

ERROR: /home/ashish/tensorflow_1.5/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1)
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProduct(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProduct(float const*, float const*, int, float*)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProductAccumulate(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProductAccumulate(float const*, float const*, int, float*)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::BatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonBatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::Sub1Vector(float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonSub1Vector(float const*, int, float*)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ClipVector(float const*, int, float, float*): error: undefined reference to 'tflite::tensor_utils::NeonClipVector(float const*, int, float, float*)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorShiftLeft(float*, int, float): error: undefined reference to 'tflite::tensor_utils::NeonVectorShiftLeft(float*, int, float)'
bazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int): error: undefined reference to 'tflite::tensor_utils::NeonReductionSumVector(float const*, float*, int, int)'
collect2: error: ld returned 1 exit status
Target //tensorflow/contrib/lite/examples/label_image:label_image failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 194.676s, Critical Path: 14.43s
FAILED: Build did NOT complete successfully"
17145,Feature Request: Add eval metrics to GANEstimator.,"At the moment I can't see a way to pass in evaluation metrics (from tfgan) to the GANEstimator when I am using the tf.contrib.Learn.Experiment class. I want to be able to use the tf.contrib.gan.eval functions but can't see a way to do this. Am I missing  something or has this not been implemented?
Have I written custom code - N/A
OS Platform and Distribution - N/A
TensorFlow installed from - N/A
TensorFlow version - 1.6
Bazel version - N/A
CUDA/cuDNN version - N/A
GPU model and memory - N/A
Exact command to reproduce - N/A"
17143,error tf1.5 : tf.contrib.ffmpeg.decode_video," I use tensorflow 1.5 on ubuntu and decodevideo.

`with tf.Session() as sess:
summary_writer = tf.summary.FileWriter('/home/xucl/app/tensorboard_log/keras_training')
movie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')
movie = tf.contrib.ffmpeg.decode_video(movie_bin)
movie_ev = movie.eval()`

the link of ffmpeg tensorflow
[https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/ffmpeg](url)

but get an error

`F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:401] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [0, 0, 0, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_Bi0OjG.err 已放弃 (核心已转储)`"
17138,Windows installation page lists wrong cudnn version,"The windows installation pages specifically asks to use cuDNN 6 - [link](https://www.tensorflow.org/install/install_windows). Then, when running tensorflow it looks specifically for cuDNN7.

    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))
ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn

Would be great if the page modified this to specifically ask for cuDNN 7.

"
17137,Wrong Bazel version check when building from source,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: 0.10.1
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: 8.0 / 7.0
- **GPU model and memory**: GeForce GTX 970, 4GB VRAM
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
I am trying to build TensorFLow 1.5 with Bazel 0.10.1 from the Google apt repo ([`http://storage.googleapis.com/bazel-apt`](http://storage.googleapis.com/bazel-apt)).
The build immediately fails with a misleading error message:
`Current Bazel version is 0.10.1, expected at least 0.5.4`
because the build script is comparing strings instead of integers of the version numbers.

Backtrace:
```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
ERROR: /home/christian/Development/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/christian/Development/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/christian/.cache/bazel/_bazel_christian/d6e111ab803bbbeb04cac5fc3b321976/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/christian/.cache/bazel/_bazel_christian/d6e111ab803bbbeb04cac5fc3b321976/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.1
ERROR: Error evaluating WORKSPACE file
ERROR: /home/christian/Development/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/christian/Development/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/christian/Development/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/christian/Development/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.1, expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file

```"
17136,Feature Request: Make beam_search_decoder use multiple CPU threads.,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:N.A.
- **GCC/Compiler version (if compiling from source)**:4.9.4
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX 1080Ti
- **Exact command to reproduce**: N.A.

### Describe the problem
Add multiple threads support for tf.nn.ctc_beam_search_decoder, for now the tf.nn.ctc_beam_search_decoder is using CPU and only run on 1 thread(although multiple threads has been provided), make it use several CPU threads to run would be very useful as the decoding process of the beam_search_decoder is now the bottleneck of the inference.
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
I use the nvidia-smi and htop to observe the CPU and GPU usage when running a test CTC decoding process, only 1 CPU thread is used and 0 GPU-Utilitiy has been observed during decoding."
17135,Inconsistency on the Dataset document,"### System information
N/A

### Describe the problem
There are some inconsistencies in the document about Dataset.

#### Problem 1
In https://www.tensorflow.org/get_started/premade_estimators, i.e. the link to the most recent stable release (v1.5 for now), in ""Getting the sample code"" section, there is something that reads:

> The program described in this document is premade_estimator.py. This program uses iris_data.py To fetch its training data.

The links of `premade_estimator.py` and `iris_data.py` in the above sentence are to the master branch, instead of the branch corresponding to the correct version. It is not a good idea to point to the master branch because:

If you follow the link of  `iris_data.py`, you can see https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py#L39
`return dataset` which is not the same as the `return dataset.make_one_shot_iterator().get_next()` in the document. This confuses the users.

To fix this, I would suggest to change the script that generates the web page to automatically generate links pointing to the correct github tag.

#### Problem 2
In https://www.tensorflow.org/versions/master/get_started/premade_estimators
i.e. the ""Getting Started with TensorFlow"" for the master branch,  the example code has already been changed to return a `Dataset`, which is consistent with https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py#L39
This is good here.

But in https://www.tensorflow.org/versions/master/get_started/datasets_quickstart
the sample code still returns `dataset.make_one_shot_iterator().get_next()`

I would suggest making it consistent by returning a `Dataset` in the above code.  And if needed, consider adding something like:
> Since version 1.5, `Estimator` now supports `Dataset: input_fn` to return a `Dataset` instead of `Tensors`. So `return dataset.make_one_shot_iterator().get_next()` can be simplified to `return dataset`


### Source code / logs
N/A
"
17133,Documentation for LSTMStateTuple is misleading,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **TensorFlow installed from (source or binary)**: no
- **TensorFlow version (use command below)**: 1.5

The [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple) for `LSTMStateTuple` states that

> Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state and `h` is the output.

The property naming is very confusing. It suggests that `h` is the `h`idden state, but the documentation contradicts this intuition."
17132,Flag names no longer compatible with argparse,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6.3
- **Exact command to reproduce**:

```
import tensorflow as tf
tf.app.flags.DEFINE_string(
    name='flag-name',
    default=None,
    help='Flag description.'
)
print(tf.app.flags.FLAGS.flag_name)
```

### Describe the problem
Up to version 1.4.1 you would be able to access the flag defined as ""flag-name"" via `FLAGS.flag_name` just like in [argparse](https://docs.python.org/3/library/argparse.html). Since 1.5.0 this functionality is no longer available.

### Source code / logs
Here's the traceback:
```
Traceback (most recent call last):
  File ""flagtest.py"", line 15, in <module>
    print(tf.app.flags.FLAGS.flag_name)
  File ""/code/venv3/lib/python3.6/site-packages/tensorflow/python/platform/flags.py"", line 85, in __getattr__
    return wrapped.__getattr__(name)
  File ""/code/venv3/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 470, in __getattr__
    raise AttributeError(name)
AttributeError: flag_name
```

Since up to version 1.4.1 this was the default functionality, bringing this back should not affect many projects.
"
17130,Java: SIGSEGV when `Tensors.create`-ing from an uninitialized array,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
  macOS 10.13.3
  JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
- **TensorFlow installed from (source or binary)**: maven
```xml
    <dependency>
      <groupId>org.tensorflow</groupId>
      <artifactId>tensorflow</artifactId>
      <version>1.4.0</version>
    </dependency>
```
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```java
    // JUnit test case
    public void testSigSegv() {
        byte[][] bb = new byte[3][];  // note: new byte[3][1] doesn't crash, presumably it is initialized by the compile

        bb[0] = new byte[] { 0 };
        bb[1] = new byte[] { 1 };
        // no bb[2]

        // next line sigsegv's
        Tensors.create(bb);
    }
```

### Describe the problem

Using `Tensors.create` to get a `Tensor<String>` from an uninitialized 2D `byte` array (`byte[][]`) results in a `SIGSEGV` from JNI. See full log below.

### Source code / logs

When running the above test case, I get the following output.
```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x000000010571f0b3, pid=30179, tid=0x0000000000001a03
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.dylib+0x31f0b3]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /Users/ben/AGLabs/nfl/JavaProjects/nlp/hs_err_pid30179.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

I'm not familiar enough with JNI in general to know if I'm expecting too much. I won't be surprised if you mark this `wontfix`, I was just surprised to be able to crash the process with a segfault given some bad data. Easy enough to work around (I will size/init my arrays more carefully) but thought you might want to know.

Cheers! Thanks so much for this library!
"
17129,Error when training custom object with tensorflow.,"###System Information
Have I written custom code(No)
OS Platform and Distribution(Linux 16.04 LTS)
TensorFlow installed from(python3 package)
TensorFlow version(1.5.0)
CPU(Intel Core i5-3320M)



I am trying to train my custom object by using tensorflow, however, when I run command it shows an error with protobuf:

**File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py"", line 1152, in ConsumeIdentifierOrNumber raise self.ParseError('Expected identifier or number, got %s.' % result) 
 google.protobuf.text_format.ParseError: 235:1 : Expected identifier or number, got <.**

I am using following command to run train.py:

**python3 train.py --logtostderr --train_dir=training/--pipeline_config_path=training/ssd_mobilenet_v1_pets.config**

Also I am attaching screenshot with the error.

![51eur](https://user-images.githubusercontent.com/22894915/36381522-be5f4336-157d-11e8-957b-002758acca77.jpg)

I am beginner in this topic, so I would be grateful for any help."
17128,How can I get kernel wait time for GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
2
- **Bazel version (if compiling from source)**:
0.5.4
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
Titan XP, 12GB
- **Exact command to reproduce**:
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I've tested the code to get kernel queue wait time for GPU, but GPU kernel time is constants even though the number of operations is increased. Instead, the CPU kernel time is changed like below.
Can I get the information to analyze the results? Moreover, profiling of kernel queue wait time would be useful. Do you have any plan to add that kind of features?

#ops| GPU kernel time(us)| CPU kernel time (us)
50 | 1862 | 184
100 | 1746 | 181
150 |  1750 | 167
200 | 1750 | 173
250 | 1751 | 307
300 | 1756 | 551
350 | 1757 | 721
400 | 1767 | 859
450 | 1767 | 959

 

### Source code / logs
    ```
    opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY
    opts['min_accelerator_micros'] = 1
    run_metadata = tf.RunMetadata()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    #config.allow_soft_placement = True
    config.inter_op_parallelism_threads=0
    config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0
    with tf.Session(config=config) as sess:
      sess.run(tf.global_variables_initializer())
      #sess.run(conv_ops)
      #print('first itereation ends')
      #sess.run(conv_ops)
      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)

      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)

    root_node = tf.contrib.tfprof.model_analyzer.print_model_analysis(
                  tf.get_default_graph(),
                  run_meta=run_metadata,
                  tfprof_options=opts)
```"
17127,Write to tensor array fails within while loop,"



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
 Yes I am writing a while loop that reads and also updates the tensor array that's being passed to it.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
When I attempt to write to a tensor array within a while loop, it leads me to an error 
""Could not write to TensorArray index 0 because it has already been read.""

### Source code 


```
def tf_soft_nms(boxes,scores,nms_t,dt_t):

  sigma = tf.constant(0.3)
  y1 = tf.gather(boxes,0,axis=1)
  x1 = tf.gather(boxes,1,axis=1)
  y2 = tf.gather(boxes,2,axis=1)
  x2 = tf.gather(boxes,3,axis=1)
  area = tf.multiply((y2-y1) , (x2-x1))


  picked_ixs = tf.TensorArray(tf.int32,clear_after_read=False,size=0,dynamic_size=True)
  scores_ta = tf.TensorArray(scores.dtype,clear_after_read=False,size=scores.shape[0],colocate_with_first_write_call=False)
  scores_ta = scores_ta.unstack(scores)
  loop_vars = (picked_ixs,scores_ta,nms_t,dt_t)
  score_p = tf.Print(scores,[scores],""all_Scores"")

  def cond(picked_ixs,scores,nms_t,dt_t):
    scores_tf = scores.stack()
    valid_indexes = tf.greater(scores_tf,dt_t)
    valid = tf.Print(valid_indexes,[valid_indexes],""Conditional check"")
    #scores = scores.write(0,tf.constant(0.0))
    return tf.reduce_any(valid_indexes)

  def nms_iter(picked_ixs,scores_ta,nms_t,dt_t):
    global area
    global boxes
    #sort scores pick top score
    scores_tf = scores_ip.stack()
    scores_ixs = tf.nn.top_k(scores_tf,tf.shape(scores_tf)[0]).indices
    pick = scores_ixs[0]


    #write top score to picked
    write_index = picked_ixs.size()
    picked_ixs = picked_ixs.write(write_index,pick)
    scores_ta = scores_ta.write(pick,tf.constant(0.2))
    picked_tf = picked_ixs.stack()
     return picked_ixs,scores_ta,nms_t,dt_t

```

"
17124,tf serving - dependency_optimizer - Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree,"I am trying to serve a tensorflow/nmt model with tf serving. I copied my SavedModel into the docker image and I'm serving it using:

`bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=model_test --model_base_path=/serving/SavedModel &> model_test.log &`

Here the logs: 

```
root@8de7a26d2e65:/serving# cat model_test.log
2018-02-19 10:07:01.227129: I tensorflow_serving/model_servers/main.cc:153] Building single TensorFlow model file config:  model_name: rxn_test model_base_path: /serving/SavedModel
2018-02-19 10:07:01.228912: I tensorflow_serving/model_servers/server_core.cc:444] Adding/updating models.
2018-02-19 10:07:01.228996: I tensorflow_serving/model_servers/server_core.cc:499]  (Re-)adding model: rxn_test
2018-02-19 10:07:01.330004: I tensorflow_serving/core/basic_manager.cc:716] Successfully reserved resources to load servable {name: rxn_test version: 1}
2018-02-19 10:07:01.330050: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: rxn_test version: 1}
2018-02-19 10:07:01.330072: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: rxn_test version: 1}
2018-02-19 10:07:01.330097: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /serving/SavedModel/1
2018-02-19 10:07:01.330124: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: /serving/SavedModel/1
2018-02-19 10:07:01.338657: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-19 10:07:01.371196: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:159] Restoring SavedModel bundle.
2018-02-19 10:07:01.641102: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:194] Running LegacyInitOp on SavedModel bundle.
2018-02-19 10:07:01.659991: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: success. Took 329854 microseconds.
2018-02-19 10:07:01.660289: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: rxn_test version: 1}
2018-02-19 10:07:01.664151: I tensorflow_serving/model_servers/main.cc:315] **Running ModelServer at 0.0.0.0:9000 ...** <--- it runs 
```

When I try to predict something, using predict.py:

`python predict.py --server=localhost:9000 --inputs='input string' `

 it fails with following error: 

```
Traceback (most recent call last):
  File ""/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 193, in _blocking_unary_unary
    credentials=_credentials(protocol_options))
  File ""/envs/tf35/lib/python3.5/site-packages/grpc/_channel.py"", line 487, in __call__
    return _end_unary_response_blocking(state, call, False, deadline)
  File ""/envs/tf35/lib/python3.5/site-packages/grpc/_channel.py"", line 437, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.INVALID_ARGUMENT, Incomplete graph, missing 1 inputs for dynamic_seq2seq/decoder/decoder/GatherTree)>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""predict.py"", line 50, in <module>
    tf.app.run()
  File ""/envs/tf35/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""predict.py"", line 45, in main
    result = stub.Predict(request, 60.0)  # 60 secs timeout
  File ""/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 309, in __call__
    self._request_serializer, self._response_deserializer)
  File ""/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 195, in _blocking_unary_unary
    raise _abortion_error(rpc_error_call)
grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""Incomplete graph, missing 1 inputs for dynamic_seq2seq/decoder/decoder/GatherTree"")
```

And in the model_test.log
```
2018-02-19 10:07:16.628894: E external/org_tensorflow/tensorflow/core/grappler/optimizers/dependency_optimizer.cc:584] Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree
2018-02-19 10:07:16.638890: E external/org_tensorflow/tensorflow/core/grappler/optimizers/dependency_optimizer.cc:584] Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree
```

Did anyone experience this?


"
17121,Feature Request for the back-propagated errors in intermediate layers,"After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:

     I->(W1)->C1->(W2)->C2->(W3)->O

`I` is the input, `O` is the output, `W1,W2,W3` is the weights for 3 layers. `C1` and `C2` are the outputs for the first two layers. With `O` and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to `C1` and `C2`?  

I know we could get the parameter operators as follows:

    W1_op = tf.get_default_graph().get_tensor_by_name('W1')
    W1_op = ...

My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).

I know that we could use the `tf.test.check_gradient` to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.

In the Caffe framework, it seems those `errors` were saved in `diff` memory for each layer. I want to get these back-propagated `errors` in each layer. Does anybody know how to get that?"
17120,Imagenet classification with VGG16 pretrained weights (Keras interface) doesnt seem to work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Described below in detail

### Describe the problem
I tried to use VGG16 image net classifier which is given through keras interface in tensorflow (tf.keras.applications.VGG16) by grabbing the graph given by Keras and then using it. But it doesn't seem to work.   

I thought it might be issue with how I am using it ,but after thinking over it a lot I have concluded that this might an issue with Tensorflow. I had posed about it on SO at https://stackoverflow.com/questions/48850537/issue-with-imagenet-classification-with-vgg16-pretrained-weights

### Source code / logs
```
import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.python.keras._impl.keras.applications import imagenet_utils


model = tf.keras.applications.VGG16()
VGG = model.graph

VGG.get_operations()
input = VGG.get_tensor_by_name(""input_1:0"")
output = VGG.get_tensor_by_name(""predictions/Softmax:0"")
print(input)
print(output)

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = np.expand_dims(image_array, axis=0)
image_array = image_array.astype(np.float32)
image_array = image_array/255


with tf.Session(graph=VGG) as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    pred = (sess.run(output,{input:image_array}))
    print(imagenet_utils.decode_predictions(pred))
```
And below is the result I get when run:  

**Tensor(""input_1:0"", shape=(?, 224, 224, 3), dtype=float32)
Tensor(""predictions/Softmax:0"", shape=(?, 1000), dtype=float32)**

**[[('n02281406', 'sulphur_butterfly', 0.0022673723), ('n01882714', 'koala', 0.0021256246), ('n04325704', 'stole', 0.0020583202), ('n01496331', 'electric_ray', 0.0020416214), ('n01797886', 'ruffed_grouse', 0.0020229272)]]**

Clearly the classfication results are not the one expected.  

Also If I dont run init_op for global variable initializing I get an error **Attempting to use uninitialized value block1_conv1/bias**"
17119,Apparent thread-safety issue in tensorflow/core/kernels/queue_op.h,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.6.0-rc1-277-g993006fa76', '1.6.0-rc1')
- **Python version**: Python 2.7.14
- **Bazel version (if compiling from source)**: 0.10.1-homebrew
- **GCC/Compiler version (if compiling from source)**: 4.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0.5
- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory (CUDA compute capability 3.0)
- **Exact command to reproduce**: N/A

### Describe the problem
Clang warns about a thread-safety issue in `tensorflow/core/kernels/queue_op.h` at lines 46 and 47 which warnings appear to be valid.

Here is the code around that line:

```c++
  void Compute(OpKernelContext* context) override {
    ResourceOpKernel<QueueInterface>::Compute(context);
    if (resource_ && context->track_allocations()) {                          // Line 46
      context->record_persistent_memory_allocation(resource_->MemoryUsed());  // Line 47
    }
  }
```

No lock is held on `mu_`.

If there is no thread safety issue, I think that a comment should be added to explain why, as it's not clear.

### Source code / logs
<pre>
./tensorflow/core/kernels/queue_op.h:46:9: warning: reading variable 'resource_' requires holding mutex 'mu_' [-Wthread-safety-analysis]
    if (resource_ && context->track_allocations()) {
        ^
./tensorflow/core/kernels/queue_op.h:47:52: warning: reading variable 'resource_' requires holding mutex 'mu_' [-Wthread-safety-analysis]
      context->record_persistent_memory_allocation(resource_->MemoryUsed());
                                                   ^
</pre>"
17116,Apparent thread-safety issue in tensorflow/core/common_runtime/executor.cc,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.6.0-rc1-277-g993006fa76', '1.6.0-rc1')
- **Python version**: Python 2.7.14
- **Bazel version (if compiling from source)**: 0.10.1-homebrew
- **GCC/Compiler version (if compiling from source)**: 4.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0.5
- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory (CUDA compute capability 3.0)
- **Exact command to reproduce**: N/A

### Describe the problem
Clang warns about a thread-safety issue in `tensorflow/core/common_runtime/executor.cc` at line 2338 which warning appears to be valid.

Here is the code around that line:

```c++
  if (parent_frame != nullptr) {
    mutex_lock paranet_frame_lock(parent_frame->mu);
    // Propagate all the dead exits to the parent frame.
    for (const Node* node : frame->dead_exits) {                         // Line 2338
      auto parent_iter_state = parent_frame->GetIteration(parent_iter);
```

A lock is held on `parent_frame->mu`, but not `frame->mu`.

If there is no thread safety issue, I think that a comment should be added to explain why, as it's not clear.

### Source code / logs
<pre>
tensorflow/core/common_runtime/executor.cc:2338:27: warning: reading variable 'dead_exits' requires holding mutex 'frame->mu' [-Wthread-safety-precise]
    for (const Node* node : frame->dead_exits) {
                          ^
tensorflow/core/common_runtime/executor.cc:2338:27: note: found near match 'parent_frame->mu'
tensorflow/core/common_runtime/executor.cc:2338:27: warning: reading variable 'dead_exits' requires holding mutex 'frame->mu' [-Wthread-safety-precise]
    for (const Node* node : frame->dead_exits) {
                          ^
tensorflow/core/common_runtime/executor.cc:2338:27: note: found near match 'parent_frame->mu'
</pre>"
17115,Feature Request: Let Estimator take custom every_n_iter for LoggingTensorHook,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Amazon Deep Learning AMI
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: NVIDIA K80
- **Exact command to reproduce**: N/A


### Describe the problem
Currently, `tf.estimator.Estimator` functions automatically log every 100 steps, from this line: https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/estimator/estimator.py#L760

Can there be an optional argument (something like train_loss_logging_frequency=100) passed in instead? PR here: https://github.com/tensorflow/tensorflow/pull/17117, https://github.com/tensorflow/tensorflow/pull/17157

"
17108,tensorflow build failure on windows,"```
:: Windows Server 2012R2 build of Tensorflow

:: INSTALL 
:: - MSVC Community 2015 Update 3
:: - C/C++   19.00.24215.1 for x64
:: - ANACONDA        4.4.4 (Python 3.5.5)
:: - CMake          3.10.2
:: - SWIG           3.0.12
:: - GIT            2.15.1.windows.2
:: - NVIDIA CUDA       8.0 
:: - NVIDIA CUDNN      6.0
::
:: No BAZEL
:: TensorFlow version (latest from GIT repository)
```

```
       ""G:\test\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj"" (default target) (102) ->
       (CustomBuild target) -> 
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(685): error : function call must have a constant value in a constant expression [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(718): error : function call must have a constant value in a constant expression [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(716): error : class template ""tensorflow::functor::BatchNarrowMatrixTransposeDispatcher"" has already been defined [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\ap\p\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]

```"
17107,cmake test fail on Windows,"```
:: - MSVC Community 2015 Update 3
:: - ANACONDA        4.4.4 (Python 3.5.5)
:: - CMake          3.10.2
:: - SWIG           3.0.12
:: - GIT            2.15.1.windows.2
:: - NVIDIA CUDA       8.1 
:: - NVIDIA CUDNN     6.0
```

Command
cmake .. ^
-A x64 ^
-DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=%SWIGEXE% ^
-DPYTHON_EXECUTABLE=%PYEXE% ^
-DPYTHON_LIBRARIES=%PYLIB% ^
-DPYTHON_INCLUDE_DIR=%PYINC% ^
-DNUMPY_INCLUDE_DIR=%NPYINC% ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=%CUDNNH% ^
-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^
-Dtensorflow_BUILD_CC_TESTS=OFF ^
-Dtensorflow_TF_NIGHTLY=OFF ^
-Dtensorflow_CUDA_VERSION=8.0

This code block fails an causes a CMAKE error
""Selected compiler (or version) is not supported for CUDA""

```
  # Test compatibility of compiler on CUDA
  try_compile(CUDA_TEST_COMPILE_C
     ${CMAKE_CURRENT_BINARY_DIR}/tests/cuda
    ${CMAKE_CURRENT_SOURCE_DIR}/tests/cuda/compatibility_test.c
    CMAKE_FLAGS -DINCLUDE_DIRECTORIES=${CUDA_INCLUDE_DIRS})
  try_compile(CUDA_TEST_COMPILE_CXX
    ${CMAKE_CURRENT_BINARY_DIR}/tests/cuda
    ${CMAKE_CURRENT_SOURCE_DIR}/tests/cuda/compatibility_test.cc
    CMAKE_FLAGS -DINCLUDE_DIRECTORIES=${CUDA_INCLUDE_DIRS})
  if(NOT (CUDA_TEST_COMPILE_C AND CUDA_TEST_COMPILE_CXX))
    message(FATAL_ERROR ""Selected compiler (or version) is not supported for CUDA"")
  endif()
```
"
17106,Bug: tf.dynamic_partition appears to produce bad outputs on GPU (0s appended) (TF 1.5/1.6),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16
- **TensorFlow installed from (source or binary)**: Pip installed 1.5 and 1.6rc0
- **TensorFlow version (use command below)**: 1.5 and 1.6rc0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: nvidia k80
- **Exact command to reproduce**: 
```
import tensorflow as tf
import numpy as np
x  = tf.constant(np.random.randn(3072))
inds = [0]*189 + [1]*184 + [2]*184 + [3]*191 + [4]*192 + [5]*195 + [6]*195 + [7]*195 + [8]*188 + [9]*195 + [10]*188 + [11]*202 + [12]*194 + [13]*194 + [14]*194 + [15]*192
assert(len(inds) == 3072)
partitioned = tf.dynamic_partition(x, inds, 16)
sess = tf.InteractiveSession()
res = sess.run(partitioned)
print(res[-1].shape) # This should be (192,) but is (198,)
```

### Describe the problem

There appears to be a bug with tf.dynamic_partition. When I run the above commands, I see that there are some extra 0s appended to the final partitioned value, and its shape is incorrect - I'm not sure why. 

I checked that this problem does not appear to occur with Tensorflow 1.4, but does occur with Tensorflow 1.5/1.6rc0 on their GPU versions. It does NOT appear to have any issue in the CPU version of Tensorflow 1.5/1.6rc0. I was able to reproduce the error on two separate computers with GPUs.

This _might_ be related to https://github.com/tensorflow/tensorflow/issues/16872.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17105,Typ it,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17104,Feature Request: Adamax optimizer,"It will be really nice to have a adamax implementation along with the regular adam optimization algorithm. Adamax superior than adam in certain cases, generally in models with word embedding.

"
17103,Tensorflow 1.5 issue on ipython notebook,"I recently upgrade tensorflow using the command "" py -3.6 -m pip install --upgrade tensorflow "" 

and after that why i run python on command prompt windows 10 tensorflow works like charm
but when i open ipython notebook for python3 and try to import tensorflow i get this error

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     39 # pylint: disable=undefined-variable
     40 del python
---> 41 del core
     42 # pylint: enable=undefined-variable

NameError: name 'core' is not defined"
17102,tf.data.Dataset API how to batch un-same size images,"
### System information

tensorflow1.5.0

### Describe the problem

`tf.data.Dataset` API how to batch un-same size images?

### Source code / logs

I don't want pad or crop to resize original image, how to batch all images using dataset api. if I call it directly:

```
train_data = tf.data.Dataset.from_tensor_slices((tf.constant(all_images), tf.constant(all_labels)))
    train_data = train_data.map(input_map_fn)
    train_data = train_data.batch(tf.contrib.data.batch_and_drop_remainder(2))
```
error says:
```
InvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [3543,3543,3] and element 1 had shape [1024,768,3].
```
Seems due the un same size of images."
17101,ImportError: Could not find 'cudart64_90.dll'.,"Installed tensorflow 1.5.0 on windows 10 education (version 1709) using ""C:\> pip3 install --upgrade tensorflow-gpu""

Installed CUDA 9.0 from https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exenetwork

Installed cuDNN 7.0.5 for CUDA 9.0 from https://developer.nvidia.com/rdp/cudnn-download

I have python 3.6.3 and nvidia gtx 1060 6GB

Exact command to reproduce: python neural.py

Bazel version: N/A

### Describe the problem
Tried creating a simple network but running into the error described in title. I checked the directory where the CUDA_PATH refers to and the file is there.

### Source code / logs
    from keras.models import Sequential
    from keras.layers import Dense
    import pandas as pd
    
    def load_mnist(path):
    	train = pd.read_csv(path + 'train.csv')
    	y = train.ix[:,0]
    	train = train.drop('label',1)
    	test = pd.read_csv(path + 'test.csv')
    	return [train, y, test]
    
    [train, y, test] = load_mnist('data/')
    model = Sequential()
    model.add(Dense(12, input_dim=8, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam')
    
    model.fit(train, y, epochs=150, batch_size=10)
    
    predictions = model.predict(test)

![importerror](https://user-images.githubusercontent.com/25662411/36347024-c66f8fea-144c-11e8-905e-b519759c8def.PNG)
"
17097,Feature request: complex support in initializers,"AFAIK no initializers work with tf.complex64 and tf.complex128 (is there a mathematical reason for this?).

My current workaround is to call them twice (once for the real part and once for the imaginary), as
```py
def complex_initializer(base_initializer):
    f = base_initializer()

    def initializer(*args, dtype=tf.complex64, **kwargs):
        real = f(*args, **kwargs)
        imag = f(*args, **kwargs)
        return tf.complex(real, imag)

    return initializer


tf.get_variable(
        name='my_complex_variable',
        shape=[1],
        dtype=tf.complex64,
        initializer=complex_initializer(tf.random_normal_initializer))
```
which seems to work fine so I'm wondering why this isn't built-in already?
"
17095,tensorflow_self_check.py needs updates for CUD 9.0 to look for cudnn64_7.dll,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 7
- **TensorFlow installed from (source or binary)**: pip.exe install --upgrade tensorflow-gpu
- **TensorFlow version (use command below)**:  b'unknown' '1.5.0'
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA GeForce GTX 660
- **Exact command to reproduce**:tensorflow_self_check.py

### Describe the problem
The current tensorflow_self_check.py script needs updates for CUDA 9.0 for use with tensorflow 1.5.0. 

I followed the script error messages and suggestions and removed CUDA 9.0 and installed the older CUDA 8.0 and the requisite cudnn64_6.dll. The script told me that I had all the required .dll, but tensorflow was still not working and to go file a bug report.

When I manually did ""import tensorflow"" and followed the detailed error messages, I realized that tensorflow 1.5.0 really does support CUDA 9.0 and really does want cudnn64_7.dll, see below.

ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn

With CUD 9.0 and cudnn64_7.dll installed, tensorflow 1.5.0 is working, GPU is enabled. The tensorflow_self_check.py needs an update to look for cudnn64_7.dll. Maybe also get rid of code that looks for older dll versions?

### Source code / logs
N/A
"
17093,enqueue inside while loop does not work as expected,"VERSION 1.5.0-rc0
GIT_VERSION v1.3.0-rc1-7323-g8d5741f
Compiled from source

```python
with tf.device('cpu:0'):
    x = tf.contrib.framework.local_variable(1024)
    q = tf.FIFOQueue(-1, tf.int32, shapes=[[]])

    def cond(_):
        remaining = tf.assign_sub(x, 1, use_locking=True)
        with tf.control_dependencies([q.enqueue(remaining)]):
            return remaining > 0

    def body(_):
        return _

    op = tf.while_loop(cond, body, [tf.constant([])], parallel_iterations=1, back_prop=False).op

x.initializer.run()
op.run()
q.dequeue_many(q.size()).eval()
# array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
# expected: array([1023, 1022, 1021, ..., 2, 1, 0], dtype=int32)
```"
17092,[BUG] GPU memory is not freed before execution of following operation + report_tensor_allocations_upon_oom is wrong,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: both
- **TensorFlow version (use command below)**: 1.4 and 1.6
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: ?
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9 / 6
- **GPU model and memory**: Tesla P100-PCIE-16GB
- **Exact command to reproduce**:

The following code defines an operation that performs two big multiplications and a sum reduction on the GPU:
```
def op(alpha, Xder, Xdertest, i):
      cols = size #tf.shape(X)[0]
      first = tf.matmul(Xder, alpha, transpose_a=True, name=""first_{}"".format(i))            # cols x num_des x 1  
      with tf.control_dependencies([first]):
            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=""xdt_{}"".format(i))   # cols x num_dim x num_des
            third = tf.matmul(xdt, first, name=""third_{}"".format(i))                         # cols x num_dim x 1
            total = tf.reduce_sum(third, name=""total_{}"".format(i))                          # single number
      return total 
```
This will run fine if performed once, but if you repeat it with:
```
singleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)
with tf.control_dependencies([singleExecution]):
      secodExecution = op(alpha, Xder, Xdertest[1,:,:], 1)
      doubleExecution = singleExecution + secodExecution # this should only add two doubles!
```
 it will produce an OOM-Execption. The expected behavior would be the calculation of the first result, clearing the GPU of all used memory and then calculating the second result.

Complete Code to reproduce:

```
import tensorflow as tf
from tensorflow.python.client import timeline
import numpy as np
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-size', type=int, default=100000)
parser.add_argument('-displayPlacement',action='store_true', default=False)
args = parser.parse_args()

rows = 2
size = args.size
num_des = 190
num_dim = 60

def op(alpha, Xder, Xdertest, i):
      cols = size #tf.shape(X)[0]
      first = tf.matmul(Xder, alpha, transpose_a=True, name=""first_{}"".format(i))            # cols x num_des x 1  
      with tf.control_dependencies([first]):
            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=""xdt_{}"".format(i))   # cols x num_dim x num_des
            third = tf.matmul(xdt, first, name=""third_{}"".format(i))                         # cols x num_dim x 1
            total = tf.reduce_sum(third, name=""total_{}"".format(i))                          # single number
      return total  


Xder = tf.placeholder(tf.float64, [None, num_dim, num_des], name=""XDerivation"")
Xdertest = tf.placeholder(tf.float64, [None, num_dim, num_des], name=""XDerivation2"")
alpha = tf.placeholder(tf.float64, [None, num_dim, 1], name=""alpha"")


singleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)
with tf.control_dependencies([singleExecution]):
      secodExecution = op(alpha, Xder, Xdertest[1,:,:], 1)
      doubleExecution = singleExecution + secodExecution

fdict = {
      alpha: np.random.rand(size, num_dim, 1),
      Xder: np.random.rand(size, num_dim, num_des),
      Xdertest: np.random.rand(rows, num_dim, num_des),
}
print(""Memory of input:"")
print(""alpha: {:.2f} MB"".format(size*num_dim * 8. / 1024**2))
print(""Xder: {:.2f} MB"".format(size*num_dim*num_des * 8. / 1024**2))
print(""Xdertest: {:.2f} MB"".format(rows*num_dim*num_dim * 8. / 1024**2))

print(""first operation should need Xder + alpha + result: {:.2f} MB"".format(( size*num_dim*num_des + size*num_dim  + size*num_des )* 8. / 1024**2))
print(""result of first operation alone needs: {:.2f} MB"".format(( size*num_des )* 8. / 1024**2))
print(""xdt operation should need : {:.2f} MB"".format(( size*num_dim*num_des )* 8. / 1024**2))
print(""third operation should need xdt + first + result: {:.2f} MB"".format((size*num_dim + size*num_des + size*num_dim*num_des )* 8. / 1024**2))

config = tf.ConfigProto()
config.gpu_options.allow_growth = False
config.log_device_placement = args.displayPlacement
sess = tf.Session(config=config) 
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE, report_tensor_allocations_upon_oom = True)
run_metadata = tf.RunMetadata()
print(sess.run(singleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))
print(""singleExecution finished!"")

fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)
with open('timeline_single.json', 'w') as f:
      f.write(chrome_trace)

run_metadata = tf.RunMetadata()
print(sess.run(doubleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))
print(""doubleExecution finished!"")
fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)
with open('timeline_double.json', 'w') as f:
      f.write(chrome_trace)
```
You may have to change the size of the tensor to trigger the OOM if you use a different GPU. 
The test-Code will also produce timelines, one for the single execution and a second for the second execution (only if you choose a size small enough - for example 70000 on the P100)

The print created by the report_tensor_allocations_upon_oom is not helpful, because it indicates a nearly complete free GPU. Total Log of the execution:

```
 $$  CUDA_VISIBLE_DEVICES=1 python gpuMem.py 
Memory of input:
alpha: 45.78 MB
Xder: 8697.51 MB
Xdertest: 0.05 MB
first operation should need Xder + alpha + result: 8888.24 MB
result of first operation alone needs: 144.96 MB
xdt operation should need : 8697.51 MB
third operation should need xdt + first + result: 8888.24 MB
2018-02-17 14:02:30.712016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:0b:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2018-02-17 14:02:30.712072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-02-17 14:02:31.024207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15128 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:0b:00.0, compute capability: 6.0)
2018-02-17 14:02:36.312709: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally
8563879389.460578
singleExecution finished!
2018-02-17 14:02:53.755140: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.49GiB.  Current allocation summary follows.
2018-02-17 14:02:53.755215: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755233: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755256: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2018-02-17 14:02:53.755271: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755285: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755358: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755375: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755390: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755427: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 1, Chunks in use: 1. 178.2KiB allocated for chunks. 178.2KiB in use in bin. 178.1KiB client-requested in use in bin.
2018-02-17 14:02:53.755445: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755487: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755502: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755532: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 1, Chunks in use: 1. 45.78MiB allocated for chunks. 45.78MiB in use in bin. 45.78MiB client-requested in use in bin.
2018-02-17 14:02:53.755570: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-02-17 14:02:53.755589: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 1, Chunks in use: 1. 144.96MiB allocated for chunks. 144.96MiB in use in bin. 144.96MiB client-requested in use in bin.
2018-02-17 14:02:53.755606: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 2, Chunks in use: 1. 14.59GiB allocated for chunks. 8.49GiB in use in bin. 8.49GiB client-requested in use in bin.
2018-02-17 14:02:53.755622: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 8.49GiB was 256.00MiB, Chunk State: 
2018-02-17 14:02:53.755645: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 6.09GiB | Requested Size: 781.2KiB | in_use: 0, prev:   Size: 144.96MiB | Requested Size: 144.96MiB | in_use: 1
2018-02-17 14:02:53.755662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10216400000 of size 1280
2018-02-17 14:02:53.755676: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10216400500 of size 9120000000
2018-02-17 14:02:53.755688: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10435d82d00 of size 48000000
2018-02-17 14:02:53.755699: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10438b49900 of size 182528
2018-02-17 14:02:53.755711: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10438b76200 of size 152000000
2018-02-17 14:02:53.755723: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x10441c6b800 of size 6543709184
2018-02-17 14:02:53.755731: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: 
2018-02-17 14:02:53.755746: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2KiB
2018-02-17 14:02:53.755759: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 182528 totalling 178.2KiB
2018-02-17 14:02:53.755772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 48000000 totalling 45.78MiB
2018-02-17 14:02:53.755786: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 152000000 totalling 144.96MiB
2018-02-17 14:02:53.755799: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 9120000000 totalling 8.49GiB
2018-02-17 14:02:53.755812: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 8.68GiB
2018-02-17 14:02:53.755827: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: 
Limit:                 15863893197
InUse:                  9320183808
MaxInUse:               9322583808
NumAllocs:                      22
MaxAllocSize:           9120000000

2018-02-17 14:02:53.755846: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ***********************************************************_________________________________________
2018-02-17 14:02:53.755879: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at tile_ops.cc:123 : Resource exhausted: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1361, in _do_call
    return fn(*args)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _run_fn
    target_list, status, run_metadata)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](ExpandDims, xdt_0/multiples)]]

Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  144.96MiB from first_0


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""gpuMem.py"", line 65, in <module>
    print(sess.run(doubleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](ExpandDims, xdt_0/multiples)]]

Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  144.96MiB from first_0


Caused by op 'xdt_0', defined at:
  File ""gpuMem.py"", line 30, in <module>
    singleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)
  File ""gpuMem.py"", line 19, in op
    xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=""xdt_{}"".format(i))   # cols x num_dim x num_des
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5587, in tile
    ""Tile"", input=input, multiples=multiples, name=name)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](ExpandDims, xdt_0/multiples)]]

Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  144.96MiB from first_0
 
```


"
17091,Build Android on Windows failed,"------------------------

### System information
- **Have I written custom code**: N/A
- **OS Platform**: Windows10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: 6.3
- **CUDA/cuDNN version**: CUDA v9.1
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I saw your instruction on [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](url) , but I want to replace the model in the demo to my retrained model. Is there anyone know how to do that? If my description is not clear enough, please tell me. Thanks in advance.

### Source code / logs
Error:Execution failed for task ':buildNativeMake'.
> A problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh''"
17090,XLA CHECK-fails when using tf.random_normal op,"this issue is about XLA core dump similar to #12683

### code
```
import tensorflow as tf
import sys
D = 2
A = tf.random_normal(shape=[D, D, 2], dtype=tf.float32,name=""A"")
B = tf.random_normal(shape=[D, D, 2], dtype=tf.float32, name=""B"")
if len(sys.argv)%2:
    fun = tf.random_normal
else:
    fun = tf.ones
E = fun(shape=[D], dtype=tf.float32, name=""EBA"")
H = tf.reshape(tf.constant([[0.25,0,0,0],[0,-0.25,0.5,0],[0,0.5,-0.25,0],[0,0,0,0.25]],
                           dtype=tf.float32),[2,2,2,2],name=""Hamiltonian"")
EA = tf.multiply(A,tf.reshape(E,[D,1,1]))
AB = tf.tensordot(EA,B,[[1],[0]],name=""AB"")
S, U, V = tf.svd(tf.reshape(AB,[2*D,2*D]))
UU = tf.transpose(tf.multiply(tf.reshape(U[:,:D],[D,2,D]),tf.reshape(E,[D,1,1])),[0,2,1],name=""nA"")
data = UU / tf.reduce_max(UU)
config = tf.ConfigProto()
if len(sys.argv)>2:
    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
print(sess.run(data))
```

### output
```
 ✘ hzhangxyz@zhanghao  ~/Documents/test  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py
2018-02-17 17:13:19.904413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-17 17:13:19.904879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2018-02-17 17:13:19.904896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0
2018-02-17 17:13:20.109302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-17 17:13:20.109333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-02-17 17:13:20.109342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-02-17 17:13:20.109495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
2018-02-17 17:13:20.292794: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55eaa51abab0
[[[-0.3788082  -0.63152224]
  [ 0.69478613 -0.15805985]]

 [[-0.12801631 -1.0104226 ]
  [ 1.         -0.25424612]]]
 hzhangxyz@zhanghao  ~/Documents/test  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py -
2018-02-17 17:13:26.235761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-17 17:13:26.236224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2018-02-17 17:13:26.236250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0
2018-02-17 17:13:26.438962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-17 17:13:26.439018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-02-17 17:13:26.439028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-02-17 17:13:26.439205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
2018-02-17 17:13:26.629210: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x564cf97d4cf0
[[[-0.95954746 -0.42915264]
  [-0.46691963 -0.69855815]]

 [[-0.7818373   0.17119625]
  [ 1.          0.19870493]]]
 hzhangxyz@zhanghao  ~/Documents/test  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - -
2018-02-17 17:13:30.150087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-17 17:13:30.150570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2018-02-17 17:13:30.150595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0
2018-02-17 17:13:30.354220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-17 17:13:30.354264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-02-17 17:13:30.354271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-02-17 17:13:30.354448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
2018-02-17 17:13:30.413176: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7f05400021d0 executing computations on platform CUDA. Devices:
2018-02-17 17:13:30.413215: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0
2018-02-17 17:13:30.555655: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.0.176, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.
2018-02-17 17:13:30.945780: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55a05a6aa980
2018-02-17 17:13:31.100741: F tensorflow/compiler/xla/util.cc:187] Check failed: p1.size() == p2.size() (3 vs. 0)
[1]    17796 abort (core dumped)  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - -
 ✘ hzhangxyz@zhanghao  ~/Documents/test  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - - -
2018-02-17 17:13:36.566031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-17 17:13:36.566533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2018-02-17 17:13:36.566560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0
2018-02-17 17:13:36.780285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-17 17:13:36.780327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2018-02-17 17:13:36.780334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2018-02-17 17:13:36.780575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
2018-02-17 17:13:36.838113: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7f4490001fd0 executing computations on platform CUDA. Devices:
2018-02-17 17:13:36.838151: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0
2018-02-17 17:13:36.856927: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.0.176, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.
2018-02-17 17:13:37.011879: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x557af19c7660
[[[-4.4751650e-01 -2.8206566e-01]
  [ 8.2662034e-01 -5.5483520e-01]]

 [[ 1.0000000e+00 -8.6649950e-04]
  [ 2.1385331e-01  4.9290603e-01]]]
```

# system info

### output of tf_env_collect.sh
```

== cat /etc/issue ===============================================
Linux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux
LSB_VERSION=1.4

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.5.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0-rc0
tf.GIT_VERSION = v1.6.0-rc1-167-g56422034fe
tf.COMPILER_VERSION = v1.6.0-rc1-167-g56422034fe
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /opt/cuda-9.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Feb 17 18:25:00 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.25                 Driver Version: 390.25                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |
| N/A   50C    P0    N/A /  N/A |      0MiB /  4046MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
```

tf is compiled from source (56422034fe)
with cuda, with xla, with opt, with mkl, without anything else

### OS is archlinux: 
```
Linux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux
```

### python
```
Python 2.7.14 (default, Jan  5 2018, 10:41:29)
[GCC 7.2.1 20171224] on linux2
```

### bazel
```
..............................................................
Build label: 0.10.0- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jun 18 09:24:24 +50064 (1517714702664)
Build timestamp: 1517714702664
Build timestamp as int: 1517714702664
```

### GPU
cuda 9.0, cudnn 7.0.5
GPU: GeForce GTX 950M

### Describe the problem
core dump when use xla with gpu

@jlebar @bixia1 "
17089, How to output * LIB static library,I want to develop on Windows with VS 2017
17088,Can't stop TF from printing probabilities ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using the tutorial code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**:  3.6.4
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: python cnn_mnist.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I can't stop TF from printing the ""INFO:tensorflow:probabilities = []"" part, where in the angle brackets is a list that spans for several hundred lines. Trying to suppress the verbosity either does nothing or causes lines like ""INFO:tensorflow:loss = 2.314889, step = 2"" to stop appearing as well. 

### Source code / logs

I used the source code from this page: https://www.tensorflow.org/tutorials/layers
"
17087,TF Lite example segmentation fault,"
### System information

OS Platform and Distribution - Ubuntu 16.04.2 LTS
TensorFlow installed from: https://github.com/tensorflow/tensorflow.git
TensorFlow version: last commit f66e9f92820804b7c2b4698147d07d5d2277c62f 
Bazel version: Build label: 0.8.1- (@non-git)
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce: bazel build --verbose_failures --config opt --cxxopt=-std=c++11 --config monolithic //tensorflow/contrib/lite/examples/label_image:label_image 
./label_image

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
== uname -a =====================================================
Linux qds101 4.4.65 https://github.com/tensorflow/tensorflow/issues/1 SMP PREEMPT Mon Sep 18 13:34:00 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux
== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)
== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc1-1607-gbe4e5ef
tf.COMPILER_VERSION = v1.3.0-rc1-1607-gbe4e5ef


### Describe the problem
I compile example/label_image nad when I run it I get segmentation fault.
Error comes from the following line in resize_bilinear.c (called from label_image.c resize<float>(interpreter->typed_tensor<float>(input), in, image_height,
                    image_width, image_channels, wanted_height, wanted_width,
                    wanted_channels, s);)
 

type::ResizeBilinear(GetTensorData<float>(input), GetTensorDims(input),   \
                       GetTensorData<int32>(size), GetTensorDims(size),     \
                       GetTensorData<float>(output), GetTensorDims(output), \
                       params->align_corners)

because params is NULL
I looked at code and i do not believe that this variable (builtin_data) ever initialized in this case because resize is called not within graph


### Source code / logs
user@qds101:~/ml/tf_lite/tensorflow/tensorflow/contrib/lite/examples/label_image/exec$ ./label_image -v 1
nnapi error: unable to open library libneuralnetworks.so
Loaded model ./vgg16_conv1_opt.tflite
resolved reporter
tensors size: 6
nodes size: 1
inputs: 1
input(0) name: Placeholder
0: Placeholder, 602112, 1, 0, 0
1: conv1_1, 12845056, 1, 0, 0
2: conv1_1/Conv2D_bias, 256, 1, 0, 0
3: conv1_1/weights, 6912, 1, 0, 0
len: 150666
width, height, channels: 224, 224, 3
input: 0
number of inputs: 1
number of outputs: 1
Interpreter has 6 tensors and 1 nodes
Inputs: 0
Outputs: 1

Tensor   0 kTfLiteFloat32  kTfLiteArenaRw     602112 bytes ( 0.6 MB)  1 224 224 3

Tensor   1 kTfLiteFloat32  kTfLiteArenaRw   12845056 bytes (12.2 MB)  1 224 224 64

Tensor   2 kTfLiteFloat32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64

Tensor   3 kTfLiteFloat32   kTfLiteMmapRo       6912 bytes ( 0.0 MB)  64 3 3 3

Tensor   4 kTfLiteFloat32  kTfLiteArenaRw    5419008 bytes ( 5.2 MB)  1 224 224 27

Tensor   5 kTfLiteFloat32  kTfLiteDynamic       6912 bytes ( 0.0 MB)  27 64

Node   0 Operator Builtin Code   3
  Inputs: 0 3 2
  Outputs: 1
params = (nil) //my print
Segmentation fault
"
17085,Spanish Translation,"
Hello! I would like to translate the project into Spanish! is it already translated?
"
17082,Feature request: Add mode argument to discriminator function in tfgans GANEstimator,"Can we add mode as an optional argument to the discriminator function in the tfgan GANEstimator? This would allow the discriminator function to perform batch normalisation and dropout.
Posted this on stack overflow:

https://stackoverflow.com/questions/48837537/why-cant-i-pass-mode-to-the-discriminator-function-in-tensorflows-ganestimator"
17081,keras multi_gpu_model broken going from 1.6.0-rc0 to 1.6.0-rc1,"Have I written custom code: Yes
OS Platform and Distribution: Linux Ubuntu 17.04
TensorFlow installed from: source
TensorFlow version: 1.6.0-rc1
Python version: 3.6 
Bazel version: 0.10
GCC/Compiler version: 6.0
CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
GPU model and memory: NVIDIA Titan Z 12GB
`Exact` command to reproduce: multi_gpu_model(model, gpus=2)

Just upgraded from rc0 to rc1 of release 1.6.0 and I'm now getting the following crash when running the multi_gpu_model function (was working fine with rc0):

    parallel_model = multi_gpu_model(model, gpus=2)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/utils/training_utils.py"", line 207, in multi_gpu_model

    return Model(model.inputs, merged)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 694, in __init__

    self._init_graph_network(*args, **kwargs)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 733, in _init_graph_network

    if layer.is_placeholder:

AttributeError: 'Lambda' object has no attribute 'is_placeholder'

I rolled back to the 1.5 branch and I'm not having any issues running multi_gpu_model there.


"
17080,The method tf.graph_util.remove_training_nodes() is broken,"On Windows 7 + tensorflow 1.4 the following code:

tf.graph_util.remove_training_nodes(tf.get_default_graph())

throws out the following error message:

......
  File ""C:\Program Files\Python3\lib\site-packages\tensorflow\python\framework\graph_util_impl.py"", line 278, in remove_training_nodes
    input_nodes = input_graph.node
AttributeError: 'Graph' object has no attribute 'node'
.....
"
17079,tf.zeros should raise an error when passed a DType as shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab, colab.research.google.com
- **TensorFlow installed from (source or binary)**:  unsure
- **TensorFlow version (use command below)**: 1.6.0-rc1
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:
```python
import tensorflow as tf
tf.InteractiveSession()
tf.zeros(tf.string).eval()
```

### Describe the problem

This returns a NumPy array with seven elements?!?
```python
array([0., 0., 0., 0., 0., 0., 0.], dtype=float32)
```

Of course, I mixed up the shape and dtype arguments to tf.zeros, The proper usage is `tf.zeros((), tf.string))`. So this should really raise TypeError (or maybe ValueError)."
17076,Sample for report_tensor_allocations_upon_oom and RunOptions,"This is a feature request.

Please add some example to the docs describing how to use report_tensor_allocations_upon_oom and other options of RunOptions

All I could find is this file:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/model_analyzer_test.py

But it is not obvious. For example, it contains:

    from tensorflow.core.protobuf import config_pb2

and then

    with session.Session() as sess:
        sess.run(c, options=config_pb2.RunOptions(
            report_tensor_allocations_upon_oom=True))

And more questions arise like: ""What is config_pb2?"" etc.

Thanks."
17075,Compiling TensorFlow on Windows with NVIDIA 9.1 GPU?,"Does anyone have this working, prior to 1.3.0 the nightly builds at tensorflow had GPU builds, these no longer appear to be available.

I have the build running and makes it through all tests until the 'end', There are multiple errors shown below.  After reviewing other posts it appears that the GPU build with Windows is not working? 

cmake .. ^
-A x64 ^
-DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=%SWIGEXE% ^
-DPYTHON_EXECUTABLE=%PYEXE% ^
-DPYTHON_LIBRARIES=%PYLIB% ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=%CUDNNH% ^
-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^
-Dtensorflow_BUILD_CC_TESTS=OFF ^
-Dtensorflow_TF_NIGHTLY=OFF ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^
-Dtensorflow_CUDA_VERSION=9.1 ^
-Dtensorflow_ENABLE_GPU=ON

msbuild.exe ^
/p:Configuration=Release ^
/maxcpucount ^
/verbosity:minimal ^
/fileLogger ^
/fileloggerparameters:logfile=%TFDIR%\tf_msbuild.log ^
tf_python_build_pip_package.vcxproj


     3>Done building target ""FinalizeBuildStatus"" in project ""pywrap_tensorflow_internal_static.vcxproj"".
     3>Target ""Build"" in file ""C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.BuildSteps.Targets"" from project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal_static.vcxproj"" (entry point):
     3>Done building target ""Build"" in project ""pywrap_tensorflow_internal_static.vcxproj"".
     3>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal_static.vcxproj"" (default targets).
     2>Done executing task ""MSBuild"" -- FAILED.
     2>Done building target ""ResolveProjectReferences"" in project ""pywrap_tensorflow_internal.vcxproj"" -- FAILED.
     2>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
   235>Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_periodic_resample_op.vcxproj"" (235) is building ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (2:2) on node 1 (default targets).
   2:2>Building with tools version ""14.0"".
       Target ""_CheckForInvalidConfigurationAndPlatform"" skipped. Previously built successfully.
       Target ""Build"" skipped. Previously built unsuccessfully.
   2:2>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
   234>Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_nearest_neighbor_ops.vcxproj"" (234) is building ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (2:3) on node 1 (default targets).
   2:3>Building with tools version ""14.0"".
       Target ""_CheckForInvalidConfigurationAndPlatform"" skipped. Previously built successfully.
       Target ""Build"" skipped. Previously built unsuccessfully.
   2:3>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
   233>Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_lstm_ops.vcxproj"" (233) is building ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (2:4) on node 1 (default targets).
   2:4>Building with tools version ""14.0"".
       Target ""_CheckForInvalidConfigurationAndPlatform"" skipped. Previously built successfully.
       Target ""Build"" skipped. Previously built unsuccessfully.
   2:4>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
   232>Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_gru_ops.vcxproj"" (232) is building ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (2:5) on node 1 (default targets).
   2:5>Building with tools version ""14.0"".
       Target ""_CheckForInvalidConfigurationAndPlatform"" skipped. Previously built successfully.
       Target ""Build"" skipped. Previously built unsuccessfully.
   2:5>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
     9>Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_beam_search_ops.vcxproj"" (9) is building ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (2:6) on node 1 (default targets).
   2:6>Building with tools version ""14.0"".
       Target ""_CheckForInvalidConfigurationAndPlatform"" skipped. Previously built successfully.
       Target ""Build"" skipped. Previously built unsuccessfully.
   2:6>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
   235>Done executing task ""MSBuild"" -- FAILED.
   235>Done building target ""ResolveProjectReferences"" in project ""_periodic_resample_op.vcxproj"" -- FAILED.
   235>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_periodic_resample_op.vcxproj"" (default targets) -- FAILED.
   234>Done executing task ""MSBuild"" -- FAILED.
   234>Done building target ""ResolveProjectReferences"" in project ""_nearest_neighbor_ops.vcxproj"" -- FAILED.
   234>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_nearest_neighbor_ops.vcxproj"" (default targets) -- FAILED.
   233>Done executing task ""MSBuild"" -- FAILED.
   233>Done building target ""ResolveProjectReferences"" in project ""_lstm_ops.vcxproj"" -- FAILED.
   233>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_lstm_ops.vcxproj"" (default targets) -- FAILED.
   232>Done executing task ""MSBuild"" -- FAILED.
   232>Done building target ""ResolveProjectReferences"" in project ""_gru_ops.vcxproj"" -- FAILED.
   232>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_gru_ops.vcxproj"" (default targets) -- FAILED.
     9>Done executing task ""MSBuild"" -- FAILED.
     9>Done building target ""ResolveProjectReferences"" in project ""_beam_search_ops.vcxproj"" -- FAILED.
     9>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\_beam_search_ops.vcxproj"" (default targets) -- FAILED.
     7>Done executing task ""MSBuild"" -- FAILED.
     7>Done building target ""ResolveProjectReferences"" in project ""tf_extension_ops.vcxproj"" -- FAILED.
     7>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_extension_ops.vcxproj"" (default targets) -- FAILED.
     1>Done executing task ""MSBuild"" -- FAILED.
     1>Done building target ""ResolveProjectReferences"" in project ""tf_python_build_pip_package.vcxproj"" -- FAILED.
     1>Done Building Project ""C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default targets) -- FAILED.
"
17071,"Windows Build of Tensorflow, CMakeLists.txt error","The Cmake error is displayed when check for native architecture. The following change reports success for the same test on Windows.

```
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Success
```

```
:: EDIT %CMAKED%\CMakeLists.txt to replace -march=native with /arch:AVX2 or if older CPU with one of [AVX2|FMA|SSE4.2|FPMATH|MMX] 
if (tensorflow_OPTIMIZE_FOR_NATIVE_ARCH)
  include(CheckCXXCompilerFlag)
  CHECK_CXX_COMPILER_FLAG(""/arch:AVX2"" COMPILER_OPT_ARCH_NATIVE_SUPPORTED)
  if (COMPILER_OPT_ARCH_NATIVE_SUPPORTED)
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} /arch:AVX2"")
  endif()
endif()

```"
17069,tf.cast() can't cast string to number,"OS: Win10 64bit
Tensorflow version: 1.5.0

It looks like tf.cast() can't cast string to number,  but the documents doesn't explain it.
I found this problem when I migrated older versions of tensorflow code to a new version.

When I used tf.cast() to cast string to float, I got error ""Unimplemented: Cast string to float is not supported"". 
And when I used tf.string_to_number () instead of tf.cast (), the problem was solved.

"
17068,Get variable mapping (dictionary) after checkpoint restore,"It is a feature request.

Please provide a way to get the list of Tensorflow variables restored from a checkpoint and corresponding Python variables.

More info:

Consider that you have this code:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]], name=""matrix1"")
    matrix_2 = tf.Variable([[5, 6], [7, 8]], name=""matrix2"")
```

When a tensorflow checkpoint is created it has matrix1 and matrix2 variables saved. It is easy to understand that matrix1 (in TF namespace) corresponds to matrix_1 (in Python namespace).

Now consider that the code is:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]])
    matrix_2 = tf.Variable([[5, 6], [7, 8]])
```

A tensorflow checkpoint will have Variable_1:0 and Variable_2:0 variables saved. And it is not obvious at all that Variable_1:0 (in TF namespace) corresponds to matrix_1 (in Python namespace). Especially if there are dozens of variables.

There should be a way to get a list of corresponding variables. For example:
```
Variable_1 = matrix_1
Variable_2 = matrix_2
...
Variable_100 = relu_1_weights
```


"
17067,"Build Error Windows, No results found for more than one instance of overloaded function ""google::protobuf::Arena::CreateM essageInternal"".","No results found for more than one instance of overloaded function ""google::protobuf::Arena::CreateM essageInternal"".

:: - MSVC Community 2015
:: - ANACONDA 4.4.4 (Python 3.5.5)
:: - CMake 3.10.2
:: - SWIG 3.0.12
:: - GIT 2.15.1.windows.2
:: - NVIDIA CUDA 9.1, CUDNN 7.05

TensorFlow pulled from git repo on 2/12/2018
==

C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/arena.h(719): error : more than one instance of overloaded function ""google::protobuf::Arena::CreateM
essageInternal"" matches the argument list: [C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)""
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)""
              argument types are: (google::protobuf::Arena *)
            detected during:
              instantiation of ""Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::TensorShapeProto_Dim]""
  (729): here
              instantiation of ""T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::TensorShapeProto_Dim]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(648): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::TensorShapeProto_Dim]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(675): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::
  TensorShapeProto_Dim]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(1554): here
              instantiation of ""TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo
  w::TensorShapeProto_Dim>::TypeHandler]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(2001): here
              instantiation of ""Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::TensorShapeProto_Dim]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\tensorflow/core/framework/tensor_shape.pb.h(471): here

C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/arena.h(719): error : more than one instance of overloaded function ""google::protobuf::Arena::CreateM
essageInternal"" matches the argument list: [C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)""
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)""
  cwise_op_bitwise_and.cc
              argument types are: (google::protobuf::Arena *)
            detected during:
              instantiation of ""Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::ResourceHandleProto]""
  (729): here
              instantiation of ""T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::ResourceHandleProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(648): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::ResourceHandleProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(675): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::
  ResourceHandleProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(1554): here
              instantiation of ""TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo
  w::ResourceHandleProto>::TypeHandler]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(2001): here
              instantiation of ""Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::ResourceHandleProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\tensorflow/core/framework/tensor.pb.h(1091): here

C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/arena.h(719): error : more than one instance of overloaded function ""google::protobuf::Arena::CreateM
essageInternal"" matches the argument list: [C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)""
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)""
              argument types are: (google::protobuf::Arena *)
            detected during:
              instantiation of ""Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::VariantTensorDataProto]""
  (729): here
              instantiation of ""T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::VariantTensorDataProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(648): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::VariantTensorDataProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(675): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::
  VariantTensorDataProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(1554): here
              instantiation of ""TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo
  w::VariantTensorDataProto>::TypeHandler]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(2001): here
              instantiation of ""Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::VariantTensorDataProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\tensorflow/core/framework/tensor.pb.h(1121): here

C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/arena.h(719): error : more than one instance of overloaded function ""google::protobuf::Arena::CreateM
essageInternal"" matches the argument list: [C:\g\tensorflow\tensorflow\tensorflow\contrib\cmake\build\tf_core_gpu_kernels.vcxproj]
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)""
              function template ""T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)""
              argument types are: (google::protobuf::Arena *)
            detected during:
              instantiation of ""Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::TensorProto]""
  (729): here
              instantiation of ""T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::TensorProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(648): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::TensorProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(675): here
              instantiation of ""GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::
  TensorProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(1554): here
              instantiation of ""TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo
  w::TensorProto>::TypeHandler]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\google/protobuf/repeated_field.h(2001): here
              instantiation of ""Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::TensorProto]""
  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\tensorflow/core/framework/tensor.pb.h(1365): here

  4 errors detected in the compilation of ""C:/Users/user/AppData/Local/Temp/tmpxft_0002aa34_00000000-12_adjust_contrast_op_gpu.cu.compute_52.cpp1.ii""."
17065,ImportError: cannot import name pywrap_dlopen_global_flags,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
17064,Allocating C++ types instead of Tensors in a new Op - Feature Request,"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installed from (source or binary): No
- TensorFlow version: 1.5
- Python version: 2.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A

### Feature Request
##### Feature requested: the ability to allocate memory in a new op as C++ types and not only as tensors;

I have been implementing a new op on Tensorflow following the [guide](https://www.tensorflow.org/extend/adding_an_op) and have noticed what could be a useful feature for people implementing new ops in C++/CUDA.

Currently using `OpKernelConstruction* context` it is only possible to allocate memory (CPU or GPU) in the form of tensors. For basic  C++ types you can obtain  a pointer for that type easily, for example:

```
Tensor tensor;
OP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT,  TensorShape({5}), &tensor));
float * ptr = tensor.flat<float>.data();
```

However, for more complex types like `structs` this is not possible (or at least not direct). Why not have something like:

```
struct A {
    int a;
    int b;
};
A * a = nullptr;
OP_REQUIRES_OK(context, context->allocate_bytes(n_bytes=sizeof(A), address = a));
```

I think this could be useful when porting C++ code from elsewhere and simplifies memory allocation of non-tensor types in C++.
Could this be a useful feature or is there a good reason it's not implemented?

### Note:
One possible work-around with the current system is to allocate a tensor of type `UINT_8`with the number of bytes required and then used `reinterpret_cast `:

```
struct A {
    int a;
    int b;
};
A * a = nullptr;
Tensor tensor;
OP_REQUIRES_OK(context, context->allocate_temp(DT_UINT8, TensorShape({sizeof(A)}), &tensor));
a = reinterpret_cast<A*>(tensor.flat<unsigned char>().data());
```
This kind of feels like cheating and over complex just for allocating memory.
"
17062,Change Kernel dimension(height * width * input_channel * output_channel) to NCHW format,"First of all, it seems that no one tries this due to the fact that I couldn't get any answers from stackoverflow or googling.

I'm trying to use cudnn inside the custom op.

Basically, custom op receives kernel layout (height * width * input_channel * output_channel).

But, according to cudnn policy, kernel layout has to be (output_channel * input_channel * height * width) inside custom op. But, I couldn't find anything regarding this issues.

Are there any solutions for converting layout like this(HWCN to NCHW)?"
17060,OSError: [Errno 13] Permission denied.. TensorFlow couldn't install,"Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 316, in clobber
    ensure_dir(destdir)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py"", line 83, in ensure_dir
    os.makedirs(path)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-3.1.1.dist-info'"
17059,Dataset API does not pass dimensionality information for its output tensor,"SYSTEM INFO
python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

----------------------------------------------------------------UPDATE-------------------------------------------------------------------

Please directly go to my third post which reproduces my issue with the minimum amount of code

----------------------------------------------------------UPDATE FINISHED----------------------------------------------------------

[https://github.com/tensorflow/tensorflow/issues/13348](url)

My problem is very similar to the issue above but I did not find solution in that post.

```
def image_parser(image_name):
    im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)
    im = cv2.imread(im_file)
    blobs, im_scales = _get_blobs(im)
    assert len(im_scales) == 1, ""Only single-image batch implemented""
    im_blob = blobs['data']
    blobs['im_info'] = np.array([im_blob.shape[1], im_blob.shape[2], im_scales[0]], dtype=np.float32)
    return blobs['data'], blobs['im_info'], im_scales, im

def _get_blobs(im):
  """"""Convert an image and RoIs within that image into network inputs.""""""
  blobs = {}
  blobs['data'], im_scale_factors = _get_image_blob(im)

  return blobs, im_scale_factors

def _get_image_blob(im):
  """"""Converts an image into a network input.
  Arguments:
    im (ndarray): a color image in BGR order
  Returns:
    blob (ndarray): a data blob holding an image pyramid
    im_scale_factors (list): list of image scales (relative to im) used
      in the image pyramid
  """"""
  im_orig = im.astype(np.float32, copy=True)
  im_orig -= cfg.PIXEL_MEANS

  im_shape = im_orig.shape
  im_size_min = np.min(im_shape[0:2])
  im_size_max = np.max(im_shape[0:2])

  processed_ims = []
  im_scale_factors = []

  for target_size in cfg.TEST.SCALES:
    im_scale = float(target_size) / float(im_size_min)
    # Prevent the biggest axis from being more than MAX_SIZE
    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:
      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)
    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,
            interpolation=cv2.INTER_LINEAR)
    im_scale_factors.append(im_scale)
    processed_ims.append(im)

  # Create a blob to hold the input images
  blob = im_list_to_blob(processed_ims)
  return blob, np.array(im_scale_factors)


def im_list_to_blob(ims):
  """"""Convert a list of images into a network input.

  Assumes images are already prepared (means subtracted, BGR order, ...).
  """"""
  max_shape = np.array([im.shape for im in ims]).max(axis=0)
  num_images = len(ims)
  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),
                  dtype=np.float32)
  for i in range(num_images):
    im = ims[i]
    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im

  return blob
```

I defined a function `image_parser` which will parse image file name into 4 numpy arrays (it has three subsequent function calls,  ` _get_blobs`, ` _get_image_blob`,  `im_list_to_blob`)

Then I construct a dataset with `py_func` mapping to form the input pipeline:

   ```
    images = []
    for root, dirs, files in os.walk('./data/demo/'):
        for file in files:
            if file.endswith('jpg'):
                images.append(file)

    # dataset construction
    im_dataset = tf.data.Dataset.from_tensor_slices(images)
    im_dataset = im_dataset.map(lambda image:
                                tuple(tf.py_func(image_parser, [image], [tf.float32, tf.float32, tf.float64, tf.uint8])),
                                num_parallel_calls = 2)
    im_dataset = im_dataset.prefetch(4)
    print(""output data type is "", im_dataset.output_types)
    print(""output data shape is "", im_dataset.output_shapes)
    iterator = im_dataset.make_initializable_iterator()
    with tf.Session() as sess:
        sess.run(iterator.initializer)
        a = sess.run(iterator.get_next())
    print(""shape of the run results are: "")
    print(a[0].shape)
    print(a[1].shape)
    print(a[2].shape)
    print(a[3].shape)
```

When I print the shape of my output tensors from dataset right after my dataset construction, the print is:
```
output data type is  (tf.float32, tf.float32, tf.float64, tf.uint8)
output data shape is  (TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))
```
the shape of tensors is None.

However I could print out the output tensor shape after my sess.run. 
```
shape of the run results are: 
(1, 600, 800, 3)
(3,)
(1,)
(375, 500, 3)
```

I need the shape of the output tensor from dataset to be defined to feed into my graph. Thanks!"
17056,"tf.reduce_min([inf, nan]).eval() == inf, should be nan","### Describe the problem

`tf.reduce_min` and `tf.reduce_max` do not propagate nans correctly.  E.g.,

    >>> tf.reduce_min([inf, nan]).eval()
    inf

but the correct answer (the one most useful for debugging) is `nan`.  This is presumably due to a backwards comparison, similar to the old `tf.nn.relu(nan) == 0` bug that @alexalemi found.

### Source code / logs
Here's a colab illustrating the problem with TensorFlow 1.6.0-rc1: https://drive.google.com/file/d/1nDA0Q48PveBlx_D5Zurchbw8l7eczRSB/view?usp=sharing"
17053,[Feature Request] Support grayscale images for classifier on Android,"How can one modify ClassifierActivity.java (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java) to convert images obtained from the camera to grayscale and then pass them to the inferenceInterface?

**Have I written custom code:** I want to use the ClassifierActivity with another neural network that was trained on grayscale images
**OS Platform and Distribution:** N/A
**TensorFlow installed from:** pip
**TensorFlow version:** 1.2.0
**Python version:** 3.6
**Bazel version (if compiling from source):** N/A
**GCC/Compiler version (if compiling from source):** N/A
**CUDA/cuDNN version:** N/A
**GPU model and memory:** N/A
**Exact command to reproduce:** N/A"
17050,Distributed FIFOQueue with shared_name is not shared,"### System information
- **Environment**:
Shared Cluster
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
RHEL Server 7.2
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
v1.5.0-0-g37aa430d84 1.5.0
- **Python version**:
3.6
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
N/A -- GPU not allocated

I am attempting to use a `FIFOQueue` to signal the parameter servers to shut down on a multi-machine shared cluster, based on [this](https://stackoverflow.com/questions/39810356/shut-down-server-in-tensorflow/40186129#40186129) [example](https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca). After some testing, I believe that `shared_name` simply doesn't seem to do anything--even after removing the `dequeue()` operations, the number of elements in the `FIFOQueue` don't correlate to the number of workers.

Minimum Reproducible Code
```
# for example
cluster = tf.train.ClusterSpec({
    'ps': ['192.168.1.1:36598'],
    'worker': ['192.168.1.2:40596', '192.168.1.3:47324', '192.168.1.4:38923']
})
# ... #
server = tf.train.Server(cluster, job_name=job_name, task_index=task)

# using server.join() causes cluster management headaches
# use a FIFOQueue to tell the parameter server to shutdown
if job_name == 'ps':
    with tf.device('/job:ps/task:%d' % task):
        queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue')
    with tf.Session(server.target) as sess:
        sess.run(queue.dequeue())
        print('ps %d: quitting' % task)

# MonitoredTrainingSession with FinalOpsHook not shown
elif job_name == 'worker':
    with tf.device('/job:worker/task:%d' % task):
        with tf.name_scope('done_queue'):
            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue')
    with tf.Session(server.target) as sess:
        _, size = sess.run([queue.enqueue(1), queue.size()])
        print('Worker:%d sending done to ps:%d [elements=%d]' % (task, 0, size))
```"
17048,Tensorflow or cuda not giving back gpu memory after session closes,"I am tying to install tensorflow correctly and I am getting memory allocation erros.
I am using:

Ubuntu 16.04
tf = 1.5.0 from pip install tensorflow-gpu
CUDA 9.0
CUDNN 7.0.5

starting python in a command terminal and running the following commands:
import tensorflow as tf
sess = tf.Session()
sess.close()

If I start a session it is fine the first time it says total memory: 7.72Gib free Memory: 7.50GiB

The next time in the same terminal I start python again ti says freeMemory: 279.44MiB

and finally if I start again it says:

freeMemory 122.50MiB

failed to allocate 72.50M from device: CUDA_ERROR_OUT_OF_MEMORY

What can I do to fix this? 

I have pasted the entire sequence below:

teves@teves:~$ python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-15 11:06:55.708721: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-15 11:06:55.846202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-15 11:06:55.846656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.50GiB
2018-02-15 11:06:55.846685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
>>> sess.close()
>>> 
[1]+  Stopped                 python
teves@teves:~$ python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-15 11:07:34.144528: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-15 11:07:34.351426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-15 11:07:34.351699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 279.44MiB
2018-02-15 11:07:34.351732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
>>> sess.close()
>>> 
[2]+  Stopped                 python
teves@teves:~$ python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-15 11:08:43.527818: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-15 11:08:43.877301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-15 11:08:43.877577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 122.50MiB
2018-02-15 11:08:43.877610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-02-15 11:08:44.047980: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 72.50M (76021760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
>>> tf.__version__
'1.5.0'
>>> 
"
17047,Failing assertion when building with MKL and using Xception,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
v1.6.0-rc0-19-gecec1d8
- **Python version**: 
Python 2.7.12
- **Bazel version (if compiling from source)**:
Bazel 0.8.0
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
None
- **Exact command to reproduce**:
1. Get the Dockerfile from here: https://gist.github.com/abiro/c155a8107879d9c4e0368f3c3e94ea81
2. Build with the MKL library: `docker build -t tf_mkl -f Dockerfile.devel-cpu-mkl .`
3. Build without the MKL library: `docker build -t tf_nomkl -f Dockerfile.devel-cpu-mkl --build-arg MKL_FLAG=0 .`
3. Run image built with the MKL library: `docker run tf_mkl`
Result: error (see output below)
4. Run image built without the MKL library: `docker run tf_nomkl`
Result: no error

### Describe the problem
When building TensorFlow 1.6 with the MKL library, inference with the Xception model results in a failing assertion and the program exits. See the error message and the code below please. The Dockerfile linked above contains the source code and can be used to easily reproduce the problem.

### Source code / logs
#### Source code
```python
import tensorflow as tf
import numpy as np

x = tf.keras.applications.xception.Xception(weights=""xception_weights.h5"")
x.predict(np.zeros((1, 299, 299, 3)), batch_size=1)
```

#### Logs
Step 4 from above outputs the following:
>2018-02-15 15:04:26.610461: F tensorflow/core/kernels/mkl_input_conversion_op.cc:448] Check failed: tf_input.CheckReorderToOpMem( memory::primitive_desc(output_mkl_md, cpu_engine), tensor_out, &net) == true (0 vs. 1)
Aborted (core dumped)

#### Misc
The host machine used to build and run the Docker containers was an EC2 c4.2xlarge instance with an Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz processor.

Related to #16982"
17046,Problem compiling on mac os x TF 1.6,"Hello I am on Mac Os X

Darwin fcamacbook.dyndns.cern.ch 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64

and I have the latest gcc

Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin17.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
Found CUDA installation: /usr/local/cuda, version 8.0

I have python 3.6.4 from homebrew

When I try to compile the master of TF from github with mkl support and -march=native I have


ERROR: /usr/local/tensorflow/tensorflow/core/BUILD:1574:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed (Exit 1)
clang: error: unsupported option '-fopenmp'
Target //tensorflow/tools/pip_package:build_pip_package failed to build

Thanks for help. 
"
17045,Tensorflow 1.5.0 breaking previously built models,"Hello!

I've recently updated to tensorflow version 1.5.0, and suddenly receive an error, that I can't decipher, for code that worked before (in version 1.4.1): `Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop`

I've also tried using the softmax_cross_entropy_with_logits function, but that still produced the same error. Here's the [stackoverflow](https://stackoverflow.com/questions/48713335/tensorflow-strided-slice-slicing-error-with-while-loop) post, in case its a coding mistake on my part.
The model is a seq2seq variation.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5.0 (previous 1.4.1)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Copy, paste and run the code


``` python
import tensorflow as tf
from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
from tensorflow.python.layers import core as layers_core

# NOTE: Time major

# ---------------- Constants Manager ----------------------------
class ConstantsManager(object):
    def __init__(self, input_dimensions, input_embedding_size, inputs_embedded, encoder_hidden_units,
                 transducer_hidden_units, vocab_ids, input_block_size, beam_width):
        assert transducer_hidden_units == encoder_hidden_units, 'Encoder and transducer have to have the same amount' \
                                                                'of hidden units'
        self.input_dimensions = input_dimensions
        self.vocab_ids = vocab_ids
        self.E_SYMBOL = len(self.vocab_ids)
        self.vocab_ids.append('E_SYMBOL')
        self.GO_SYMBOL = len(self.vocab_ids)
        self.vocab_ids.append('GO_SYMBOL')
        self.vocab_size = len(self.vocab_ids)
        self.input_embedding_size = input_embedding_size
        self.inputs_embedded = inputs_embedded
        self.encoder_hidden_units = encoder_hidden_units
        self.transducer_hidden_units = transducer_hidden_units
        self.input_block_size = input_block_size
        self.beam_width = beam_width
        self.batch_size = 1  # Cannot be increased, see paper
        self.log_prob_init_value = 0

# ----------------- Model ---------------------------------------


class Model(object):
    def __init__(self, cons_manager):
        self.var_list = []
        self.cons_manager = cons_manager
        self.max_blocks, self.inputs_full_raw, self.transducer_list_outputs, self.start_block, self.encoder_hidden_init,\
            self.trans_hidden_init, self.logits, self.encoder_hidden_state_new, \
            self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()

        self.targets, self.train_op, self.loss = self.build_training_step()

    def build_full_transducer(self):
        with tf.variable_scope('transducer_training'):

            embeddings = tf.Variable(tf.random_uniform([self.cons_manager.vocab_size,
                                                        self.cons_manager.input_embedding_size], -1.0, 1.0),
                                     dtype=tf.float32,
                                     name='embedding')
            # Inputs
            max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')  # total amount of blocks to go through
            if self.cons_manager.inputs_embedded is True:
                input_type = tf.float32
            else:
                input_type = tf.int32
            inputs_full_raw = tf.placeholder(shape=(None, self.cons_manager.batch_size,
                                                    self.cons_manager.input_dimensions), dtype=input_type,
                                             name='inputs_full_raw')  # shape [max_time, 1, input_dims]
            transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                     name='transducer_list_outputs')  # amount to output per block
            start_block = tf.placeholder(dtype=tf.int32, name='transducer_start_block')  # where to start the input

            encoder_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.encoder_hidden_units), dtype=tf.float32,
                                                 name='encoder_hidden_init')
            trans_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.transducer_hidden_units), dtype=tf.float32,
                                               name='trans_hidden_init')

            # Temporary constants, maybe changed during inference
            end_symbol = tf.get_variable(name='end_symbol',
                                         initializer=tf.constant_initializer(self.cons_manager.vocab_size),
                                         shape=(), dtype=tf.int32)

            # Turn inputs into tensor which is easily readable#

            inputs_full = tf.reshape(inputs_full_raw, shape=[-1, self.cons_manager.input_block_size,
                                                             self.cons_manager.batch_size,
                                                             self.cons_manager.input_dimensions])

            # Outputs
            outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)

            init_state = (start_block, outputs_ta, encoder_hidden_init, trans_hidden_init)

            # Initiate cells, NOTE: if there is a future error, put these back inside the body function
            encoder_cell = tf.contrib.rnn.LSTMCell(num_units=self.cons_manager.encoder_hidden_units)
            transducer_cell = tf.contrib.rnn.LSTMCell(self.cons_manager.transducer_hidden_units)

            def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
                return current_block < start_block + max_blocks

            def body(current_block, outputs_int, encoder_hidden, trans_hidden):

                # --------------------- ENCODER ----------------------------------------------------------------------
                encoder_inputs = inputs_full[current_block]
                encoder_inputs_length = [tf.shape(encoder_inputs)[0]]
                encoder_hidden_state = encoder_hidden

                if self.cons_manager.inputs_embedded is True:
                    encoder_inputs_embedded = encoder_inputs
                else:
                    encoder_inputs = tf.reshape(encoder_inputs, shape=[-1, self.cons_manager.batch_size])
                    encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)

                # Build model

                # Build previous state
                encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
                encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, self.cons_manager.encoder_hidden_units])
                encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, self.cons_manager.encoder_hidden_units])
                encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)

                #   encoder_outputs: [max_time, batch_size, num_units]
                encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                    encoder_cell, encoder_inputs_embedded,
                    sequence_length=encoder_inputs_length, time_major=True,
                    dtype=tf.float32, initial_state=encoder_hidden_state_t)

                # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
                encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
                encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new,
                                                      shape=[2, -1, self.cons_manager.encoder_hidden_units])

                # --------------------- TRANSDUCER --------------------------------------------------------------------
                encoder_raw_outputs = encoder_outputs
                # Save/load the state as one tensor, use encoder state as init if this is the first block
                trans_hidden_state = tf.cond(current_block > 0, lambda: trans_hidden, lambda: encoder_hidden_state_new)
                transducer_amount_outputs = transducer_list_outputs[current_block - start_block]

                # Model building
                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                    embedding=embeddings,
                    start_tokens=tf.tile([self.cons_manager.GO_SYMBOL],
                                         [self.cons_manager.batch_size]),  # TODO: check if this looks good
                    end_token=end_symbol)  # vocab size, so that it doesn't prematurely end the decoding

                attention_states = tf.transpose(encoder_raw_outputs,
                                                [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]

                attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                    self.cons_manager.encoder_hidden_units, attention_states)

                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                    transducer_cell,
                    attention_mechanism,
                    attention_layer_size=self.cons_manager.transducer_hidden_units)

                projection_layer = layers_core.Dense(self.cons_manager.vocab_size, use_bias=False)

                # Build previous state
                trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
                trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, self.cons_manager.transducer_hidden_units])
                trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, self.cons_manager.transducer_hidden_units])
                trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)

                decoder = tf.contrib.seq2seq.BasicDecoder(
                    decoder_cell, helper,
                    decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                    output_layer=projection_layer)

                outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                            output_time_major=True,
                                                                                            maximum_iterations=transducer_amount_outputs)
                logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
                decoder_prediction = outputs.sample_id  # For debugging

                # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
                transducer_hidden_state_new = tf.concat(
                    [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                    axis=0)
                transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                         shape=[2, -1, self.cons_manager.transducer_hidden_units])


                # Note the outputs
                outputs_int = outputs_int.write(current_block - start_block, logits)

                return current_block + 1, outputs_int, encoder_hidden_state_new, transducer_hidden_state_new

            _, outputs_final, encoder_hidden_state_new, transducer_hidden_state_new = \
                tf.while_loop(cond, body, init_state, parallel_iterations=1)

            # Process outputs
            outputs = outputs_final.concat()
            logits = tf.reshape(
                outputs,
                shape=(-1, 1, self.cons_manager.vocab_size))  # And now its [max_output_time, batch_size, vocab]

            # For loading the model later on
            logits = tf.identity(logits, name='logits')
            encoder_hidden_state_new = tf.identity(encoder_hidden_state_new, name='encoder_hidden_state_new')
            transducer_hidden_state_new = tf.identity(transducer_hidden_state_new, name='transducer_hidden_state_new')

        train_saver = tf.train.Saver()  # For now save everything

        return max_blocks, inputs_full_raw, transducer_list_outputs, start_block, encoder_hidden_init,\
            trans_hidden_init, logits, encoder_hidden_state_new, transducer_hidden_state_new, train_saver

    def build_training_step(self):
        targets = tf.placeholder(shape=(None,), dtype=tf.int32, name='targets')
        targets_one_hot = tf.one_hot(targets, depth=self.cons_manager.vocab_size, dtype=tf.float32)

        targets_one_hot = tf.Print(targets_one_hot, [targets], message='Targets: ', summarize=10)
        targets_one_hot = tf.Print(targets_one_hot, [tf.argmax(self.logits, axis=2)], message='Argmax: ', summarize=10)

        stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot,
                                                                         logits=self.logits)
        loss = tf.reduce_mean(stepwise_cross_entropy)
        train_op = tf.train.AdamOptimizer().minimize(loss)
        return targets, train_op, loss


constants_manager = ConstantsManager(input_dimensions=1, input_embedding_size=11, inputs_embedded=False,
                                     encoder_hidden_units=100, transducer_hidden_units=100, vocab_ids=[0, 1, 2],
                                     input_block_size=1, beam_width=5)
model = Model(cons_manager=constants_manager)
```
I can try and make a smaller fail case if needed.

Thanks!
Nikita"
17044,"Hello, I am interested in collaborating with your project, serving as a translator to Spanish since it is my native language. I can translate any document in .md","
"
17043,seg fault in 1.6rc0 and master on skylake cpu (avx512 related probably),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Built from source
- **TensorFlow version (use command below)**:
v1.6.0-rc0-19-gecec1d8 1.6.0-rc1
master
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.1.85/7.0.5.15
- **GPU model and memory**:
GeForce GTX 1080 Ti
totalMemory: 10.91GiB freeMemory: 8.36GiB

### Describe the problem

If tf1.6 is compiled with `--march=native` then running inference on a large model ends up with crash with cryptic stack trace, 100% reproducible, no matter with or without CUDA (whether CUDA_VISIBLE_DEVICES="""" or absent).

If compiled **without** avx512 support (i.e. `-O3 -msse4.2 -mavx2 -mfma` only), then everything works fine.
TF emits a warning though:
```
2018-02-15 14:03:54.237530: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
```

I checked another issue with 64 byte alignment (https://github.com/tensorflow/tensorflow/issues/15588), but it does not help.

Here is a crash trace, if it may help.
```
(gdb) bt
#0  0x00007fdea3c5c9d5 in Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> const&, long, long, long, long) () from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fdea3cd0fc4 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 48, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool) ()
   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fdea06232d1 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fdea06210e7 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fde96afbc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007fded77d76ba in start_thread (arg=0x7fdeb87d8700) at pthread_create.c:333
#6  0x00007fded750d41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
(gdb)
```"
17041,"Saving the model throws ""op_kernel.cc"" No such file or directory error","```
Epoch: 50 Train Perplexity: 37.819
Epoch: 50 Valid Perplexity: 60.281
Test Perplexity: 44.138
Saving model to output.
2018-02-15 14:05:42.473937: W tensorflow/core/framework/op_kernel.cc:1198] Not found: ; No such file or directory
Traceback (most recent call last):
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_call
    return fn(*args)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1329, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory
	 [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_109, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_111, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_113, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_115, Model/Model/embedding/Adam/_117, Model/Model/embedding/Adam_1/_119, Model/Model/softmax_b/Adam/_121, Model/Model/softmax_b/Adam_1/_123, Model/Model/softmax_w/Adam/_125, Model/Model/softmax_w/Adam_1/_127, Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/_129, Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/_131, Model/embedding/_133, Model/global_step, Model/softmax_b/_135, Model/softmax_w/_137, Train/Model/Variable/_139, Train/Model/beta1_power/_141, Train/Model/beta2_power/_143)]]
```
### System information
- **Have I written custom code**: [PTB model](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py) from official RNN tutorial (using custom data and hyperparams slightly modified)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux 4.14.16-1 Manjaro
- **TensorFlow installed from**: pip
- **TensorFlow version**: 1.5.0
- **Python version**: 3.6 
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA GTX 1070 Mobile (8GB, driver 384.111)
- **Exact command to reproduce**: just running the training file `main.py` with default FLAG params

### Describe the problem
Training goes fine and I can observe loss decreasing. I can also run Tensorboard on the log dir and see the model graph. When the max_epoch is reached the code throws the above mentioned error.

### Source code / logs
--
"
17037,Bug: tf.contrib.learn.Experiment.continuous_train_and_eval does not release GPU memory switching between training and evaluation phases,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0rc0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: 1080, 8GB
- **Exact command to reproduce**: -

The documentation of [`continuous_train_and_eval`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment#continuous_train_and_eval) says:

> the resources (e.g., memory) used by training will be released before evaluation (train_and_evaluate takes double resources)

From the execution logs, however, this doesn't seem to be the case:

**Starting training**:
```
2018-02-15 11:06:37.745151: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1208] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835
pciBusID: 0000:02:00.0
totalMemory: 8.00GiB freeMemory: 6.60GiB
2018-02-15 11:06:37.764258: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-15 11:06:39.084140: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6381 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
```

**Switching from training to evaluation**:
```
2018-02-15 09:12:54.871561: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-15 09:12:54.871717: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 175 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
```

**Switching back from evaluation to training**:
```
2018-02-14 19:56:50.513663: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-14 19:56:50.513819: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 175 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
```

It seems the function's loop fails to release memory after the first training phase is complete, so the subsequent devices only have 175MBs of memory available. Interestingly, training does experience a reduction in speed, but not as high as I would have expected (with 6GBs of memory available I have about 1.1 steps/second, with 175MBs it's 1.0 steps/second).

I'm training the standard `inception_resnet_v2` from `slim`'s model zoo (batch size is 32, if that information is of any use)."
17034,Quantized graphs produce unusable output on Android,"When running a quantized graph on linux/ios/android the first 2 are correct, android though produces only garbage output. Does only occur with newer trained graphs (TF 1.4/5 i suppose), never had this issue before.

`softfp` problem?
"
17033,cuda error on import,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Fedora 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.4.0-19-ga52c8d9, 1.4.1
- **Python version**: 2.7
- **Bazel version**: N/A
- **CUDA/cuDNN version**: cuda_8.0.61,  cudnnv5
- **GPU model and memory**: GeForce GTX TITAN X ,   12207MiB
- **Exact command to reproduce**:

We have a computer cluster that some of the machines have GPU and others don't. I have installed tensorflow-gpu-1.4 from wheel file in a virtualenv in a folder on the file-server which means that it is accessible on all the machines in the cluster. 
My program is a distributed software which means that some of the tasks are done on all the cluster nodes. (data generation and configuration) and the machine learning part is only done on the machines with GPU. I activate the aforementioned virtualenv before running the servers on nodes of the cluster so all the nodes are running inside the same virtual environment.  
On the machines that have a GPU when I import tensorflow everything works fine, but when I import the tensorflow on the machines that do not have the gpu (and Cuda is not installed on them) I get following error:   

> In [1]: import tensorflow as tf
> ImportError                               Traceback (most recent call last)
> <ipython-input-1-64156d691fe5> in <module>()
> ----> 1 import tensorflow as tf
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()
>      22 
>      23 # pylint: disable=wildcard-import
> ---> 24 from tensorflow.python import *
>      25 # pylint: enable=wildcard-import
>      26 
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()
>      47 import numpy as np
>      48 
> ---> 49 from tensorflow.python import pywrap_tensorflow
>      50 
>      51 # Protocol buffers
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
>      70 for some common reasons and solutions.  Include the entire stack trace
>      71 above this error message when asking for help."""""" % traceback.format_exc()
> ---> 72   raise ImportError(msg)
>      73 
>      74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long
> 
> ImportError: Traceback (most recent call last):
>   File ""virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/install_sources#common_installation_problems
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.

I am aware that tensorflow-gpu is statically linked to the Cuda libraries and I installed a local version of Cuda using the runfile in a folder on the file-server (accessible to all the nodes) and added its path to the LD_LIBRARY_PATH and PATH and now importing tf gives me the following error:

> 2018-02-15 01:41:58.519693: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU 

> supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
> 2018-02-15 01:41:58.519972: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)
> 2018-02-15 01:41:58.520007: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: maserati
> 2018-02-15 01:41:58.520017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: maserati
> 2018-02-15 01:41:58.520140: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
> 2018-02-15 01:41:58.520167: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.111  Tue Dec 19 23:51:45 PST 2017
> GCC version:  gcc version 7.2.1 20170915 (Red Hat 7.2.1-2) (GCC) 
> """"""
> 2018-02-15 01:41:58.520191: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.111.0

And when i run a hello world script it gives me the following error (which clearly means it cant run anything on gpu because of previous error):

> In [5]: with tf.device(""/GPU:0""):
>    ...:     hello = tf.constant('Hello, TensorFlow!')
>    ...:     sess = tf.Session()
>    ...:     print(sess.run(hello))
>    ...:     
> InvalidArgumentError                      Traceback (most recent call last)
> <ipython-input-5-4fc1d9ca141a> in <module>()
>       2     hello = tf.constant('Hello, TensorFlow!')
>       3     sess = tf.Session()
> ----> 4     print(sess.run(hello))
>       5 
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
>     887     try:
>     888       result = self._run(None, fetches, feed_dict, options_ptr,
> --> 889                          run_metadata_ptr)
>     890       if run_metadata:
>     891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
>    1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
>    1119       results = self._do_run(handle, final_targets, final_fetches,
> -> 1120                              feed_dict_tensor, options, run_metadata)
>    1121     else:
>    1122       results = []
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
>    1315     if handle is None:
>    1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
> -> 1317                            options, run_metadata)
>    1318     else:
>    1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)
> 
> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
>    1334         except KeyError:
>    1335           pass
> -> 1336       raise type(e)(node_def, op, message)
>    1337 
>    1338   def _extend_graph(self):
> 
> InvalidArgumentError: Cannot assign a device for operation 'Const_1': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
> 	 [[Node: Const_1 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: Hello, TensorFlow!>, _device=""/device:GPU:0""]()]]
> 
> Caused by op u'Const_1', defined at:
>   File ""/virtualenv/bin/ipython"", line 11, in <module>
>     sys.exit(start_ipython())
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/__init__.py"", line 119, in start_ipython
>     return launch_new_instance(argv=argv, **kwargs)
>   File ""/virtualenv/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
>     app.start()
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/terminal/ipapp.py"", line 355, in start
>     self.shell.mainloop()
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py"", line 493, in mainloop
>     self.interact()
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py"", line 484, in interact
>     self.run_cell(code, store_history=True)
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
>     interactivity=interactivity, compiler=compiler, result=result)
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
>     if self.run_code(code, result):
>   File ""/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
>     exec(code_obj, self.user_global_ns, self.user_ns)
>   File ""<ipython-input-5-4fc1d9ca141a>"", line 2, in <module>
>     hello = tf.constant('Hello, TensorFlow!')
>   File ""/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 214, in constant
>     name=name).outputs[0]
>   File ""/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
>     op_def=op_def)
>   File ""/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
> 
> InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Const_1': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
> 	 [[Node: Const_1 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: Hello, TensorFlow!>, _device=""/device:GPU:0""]()]]

I dont wan't/ can't install cuda on non-gpu machines is there any work around for this issue?"
17031,program crashes on tf1.5 when creating a tensor object with CAPI,"The following function is used to create a tensor object feeded into session's run function for inference at evaluation.    The evaluation test is wrapped in a loop for maximum 500 iterations, it crashes at iteration of 200-300 each time with log ""Process finished with exit code 139 (interrupted by signal 11: SIGSEGV"".   It works well in previous versions.  Thanks for any correction.

```
static void Deallocator(void *data, size_t, void *arg) {
  tensorflow::cpu_allocator()->DeallocateRaw(data);
  *reinterpret_cast<bool *>(arg) = true;
}

TF_Tensor *DlTensorUtil<float>::Feature2TF_Tensor(float feature[], int64_t dims[], int nDims, TF_DataType type) {
  size_t len = 1;
  for (int i = 0; i < nDims; i++)
    len *= dims[i];
  len *= sizeof(float);
  bool deallocator_called = false;
  TF_Tensor *ts = TF_NewTensor(type, dims, nDims, feature, len, &Deallocator, &deallocator_called);
  return ts;
}

void DlTensorUtil<float>::Feature2Tensor(float feature[], int64_t dims[], int nDims, TF_DataType type, Tensor &out_tensor) {
  TF_Tensor *tftensor = Feature2TF_Tensor(feature, dims, nDims, type);
  out_tensor = tensorflow::TensorCApi::MakeTensor(tftensor->dtype, tftensor->shape, tftensor->buffer);
}
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3
- **TensorFlow installed from (source or binary)**: From Source
- **TensorFlow version (use command below)**:  1.5
- **Python version**:  Not used
- **Bazel version (if compiling from source)**:  0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/6.1
- **GPU model and memory**: gtx1080/8g
- **Exact command to reproduce**:"
17028,MLP prediction is 3~4x slower than theano/pytorch,"(Moving from keras-team/keras#9388)
For a simple 2-hidden-layer MLP, TF is 3~4x slower than pytorch and theano. This is only for **prediction**, not training, and it is only on **CPU**. Please see this [GitHub gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/eb4eec9056b02b1ac2e0e039f646347c02885309) for timing.

For reproducibility, the test was done on AWS EC2 c5.large. Two different builds were tested and showed similar results. One is the pre-built [AWS deep learning AMI](https://aws.amazon.com/machine-learning/amis/) (ami-e07e779a). Another is installing conda on a fresh Ubuntu machine and then installing TF with `pip`.

My questions are:
1. Is this performance difference expected? I assume that TF shouldn't be that slow. 
2. @fchollet suggested that the TF installation was broken. If so, what's the correct way to install TensorFlow to ensure good performance? I also tried [other installation methods on the official docs](https://www.tensorflow.org/install/install_linux), as such native Python and pip, but didn't get better performance.

### System information
- **Have I written custom code**: Almost all built-in functions
- **OS Platform and Distribution**: Linux Ubuntu 16.04 
Test env 1: AWS deep learning AMI Ubuntu Version (ami-e07e779a)
Test env 2: AWS base Ubuntu AMI (ami-66506c1c)
- **TensorFlow installed from**: 
Test env 1: TensorFlow/Keras/PyTorch are all provided by that AMI.
Test env 2: Installed from binary, i.e. `pip install tensorflow` and `pip install keras`. PyTorch was installed by `conda install pytorch`.
- **TensorFlow version**: 1.5.0
- **Python version**: 3.6
- **Bazel version**: N/A (from binary)
- **CUDA/cuDNN version**: CPU-only
- **GPU model and memory**: CPU-only
- **Exact command to reproduce**: Please follow this [GitHub gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/eb4eec9056b02b1ac2e0e039f646347c02885309)."
17024,Script train_image_classifier.py has no option use_nesterov in the MomentumOptimizer,"**Have I written custom code:** No
**OS Platform and Distribution:** Ubuntu 17.04
**TensorFlow installed from:** Pip
**TensorFlow version:** 1.4.0
**Bazel version:** N/A
**CUDA/cuDNN version:** CUDA 8.0.61, CUDNN v6
**GPU model and memory:** Tesla K80
**Exact command to reproduce:** N/A

In Tensorflow, the `MomentumOptimizer` has `use_nesterov` disabled by default. In the generic classifier training script - `train_image_classifier.py` - the flags do not have `use_nesterov` option (see [here](https://github.com/tensorflow/models/blob/4c05414826e87f3b8ef0534862748e4b7fcd1ec7/research/slim/train_image_classifier.py#L297) and [here](https://github.com/tensorflow/models/blob/4c05414826e87f3b8ef0534862748e4b7fcd1ec7/research/slim/train_image_classifier.py#L116)), which makes it real easy to miss the fact that it is not being used. I think an additional flag should be introduced in the optimizer section of `train_image_classifier.py`.

P.S. Why was the decision taken to disable `use_nesterov`? I tried searching online if there were any disadvantages associated with Nesterov momentum but did not come up with much. [The paper referenced in Tensorflow documentation](http://proceedings.mlr.press/v28/sutskever13.pdf) only encourages the use of Nesterov momentum versus classical momentum. Am I missing something?"
17022,Is python 3.7.x supported with Tensorflow,"Ive been trying to install Tensorflow on my computer which currently runs python 3.7, however I keep running into some common issues... And each time i try to use the solutions provided, nothing works. 

Im not sure, but Im guessing python 3.7 might not be supported considering the official Tensorflow page has no link to python 3.7, that maybe this is the reason I havent been able to correctly install Tensorflow."
17021,sigmoid_cross_entropy Docstring bug,"### Describe the problem
Tf.losses.sigmoid_cross_entory parameter description of label indicates it has to be ""integer"" and in range (0,1). There are no integers between (0,1) so that seems really difficult to abide by. 
It seems that in the code it does not need to be integer and the range should be [0, 1]. 
https://www.tensorflow.org/api_docs/python/tf/losses/sigmoid_cross_entropy

P.S. Why does softmax_cross_entropy assume one-hot encoding? For instance in distillation your labels are softmax outputs (floats in [0, 1]). "
17020,"""DeprecationWarning: The binary mode of fromstring is deprecated"" warning appears in some cases","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary, tf-nightly
- **TensorFlow version (use command below)**: 1.7.0.dev20180214, git version b2a0f1c
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
from tensorflow.contrib.image.ops import gen_distort_image_ops
from tensorflow.python.framework import tensor_util
tensor_util.constant_value(tf.convert_to_tensor([1., 1.]))
```

### Describe the problem
When I run the program above, I get the warning
```
/home/reedwm/venvs/tfnightlycpu/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)
```

This may not seem so bad, but when running [`tf_cnn_benchmarks`](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), I get hundreds of such warnings.

What's very strange is that if I comment the line `from tensorflow.contrib.image.ops import gen_distort_image_ops`, I don't get the warning.

This is the same issue as ppwwyyxx/tensorpack#641. @yaroslavvb, did you file a TensorFlow bug for this? If so, this can be marked as a duplicate.

Not really sure who to triage this to. /CC @mrry, can you address this or retriage?
"
17018,Feature/PR Idea - mean IoU for vector of thresholds,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: `False`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.0.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: `None`
- **GCC/Compiler version (if compiling from source)**: `None`
- **CUDA/cuDNN version**: `None`
- **GPU model and memory**: Radeon Pro 560 4096 MB - Intel HD Graphics 630 1536 MB
- **Exact command to reproduce**: `None`

### Summary
I'd love to have a convenient, clean API for expressing the calculation of mean IoU for a set of thresholds instead of a single value. 
Like the way it's described here: https://www.kaggle.com/c/data-science-bowl-2018#evaluation

I'm planning on submitting a PR with a solution unless I get told here that the idea is explicitly being opposed by the decision-makers. Please let me know if you'd consider merging something like this in and I'll move to the next phase.

I'm assuming that the implementation is possible without having to get knee deep in CUDA C/C++ given that there's already a mean IoU feature here and I would (hopefully) just need to extend that in a backward compatible way so that it takes some optional extra information in the form of a vector of thresholds =>
 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/metrics_impl.py#L948:

Tips and guidance are most welcome, thank you in advance for any help and the verdict as well.
"
17017,"Integration of ""Tensor Comprehensions""?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Not relevant
- **TensorFlow installed from (source or binary)**: Not relevant
- **TensorFlow version (use command below)**: Not relevant
- **Python version**: Not relevant
- **Bazel version (if compiling from source)**: Not relevant
- **GCC/Compiler version (if compiling from source)**: Not relevant
- **CUDA/cuDNN version**: Not relevant
- **GPU model and memory**: Not relevant
- **Exact command to reproduce**: Not relevant

### Describe the problem
FAIR just released an initial version of their [Tensor Comprehension Framework](https://research.fb.com/announcing-tensor-comprehensions/) which I think is a really clever concept. The Tensor comprehension library allows to define functions with a syntax similar to einstein-notation and then compiles these functions into fast GPU code via evolutionary search. Is this something you would consider including into the core or would you rather favor an integration as a separate framework?

Cheers,
Phil
"
17016,Error in `tfe.implicit_gradients(loss)` in eager mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:
---

I was trying to run MNIST model in `Eager` mode on Kaggle Kernels but as I am passing data to my model, the optimizer is throwing this particular error :
`ValueError: No trainable variables were accessed while the function was being computed.` I can confirm that the data being passed to the model are non-zero and are in the correct shape. I don't understand why is the model is throwing the error then. Here is my code:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

class MNIST(object):
    def __init__(self, data_format):
        # Set the input shape according to the availability of GPU 
        if data_format == 'channels_first':
            self._input_shape = [-1, 1, 28, 28]
        else:
            self._input_shape = [-1, 28, 28, 1]
        
        self.conv1 = tf.layers.Conv2D(32, 3, 
                                      activation=tf.nn.relu, 
                                      padding='same', 
                                      data_format=data_format)
        
        self.maxpool = tf.layers.MaxPooling2D((2,2), (2,2), 
                                            padding='same', 
                                            data_format=data_format)
        
        self.conv2 = tf.layers.Conv2D(64, 3, 
                                      activation=tf.nn.relu, 
                                      padding='same', 
                                      data_format=data_format)
        
        self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)
        self.dropout = tf.layers.Dropout(0.5)
        self.dense2 = tf.layers.Dense(10)
        
        
 
    def predict(self, inputs):
        x = tf.reshape(inputs, self._input_shape)
        x = self.conv1(x)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = self.maxpool(x)
        x = tf.layers.flatten(x)
        x = self.dense1(x)
        x = self.dropout(x) #enable at training and disable at testing
        x = self.dense2(x)
        return x

# Define loss functions
def loss(model, inputs, targets):
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
                          logits=model.predict(inputs), labels=targets))

# Calculate accuracy
def compute_accuracy(predictions, labels):
    model_pred = tf.argmax(predictions, axis=1,output_type=tf.int64)
    actual_labels = tf.argmax(labels, axis=1, output_type=tf.int64)
    return tf.reduce_sum(tf.cast(tf.equal(model_pred, actual_labels)),dtype=tf.float32) / float(predictions.shape[0].value)

device = ""gpu:0"" if tfe.num_gpus() else ""cpu:0""
model = MNIST('channels_first' if tfe.num_gpus() else 'channels_last')
optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
grad = tfe.implicit_gradients(loss)

batch_size = 8
train_batches = len(X_train) // batch_size
valid_batches = len(X_valid) // batch_size
nb_epochs = 5

for i in range(nb_epochs):
    with tf.device(device):
        for j in range(train_batches):
            inputs, targets = next(train_data_gen)
            optimizer.apply_gradients(grad(model, inputs, targets))
            if j % 10 == 0:
                print(""Step %d: Loss on training set : %f"" %(i, loss(model, inputs, targets).numpy()))
        
"
17015,tfcompile tf.cond not dominated by switch nodes,"Using the following example:

```
graph = tf.Graph()
with graph.as_default():
  x = tf.placeholder(name='x', shape=(2,), dtype=tf.float64)
  y = tf.cond(x[0] > x[1], partial(lambda x: x[0], x), partial(lambda x: x[1], x))
```

Compiling with:
`
tfcompile --graph=test_graph.pb --config=test_config.pb --entry_point=test_func --cpp_class=test --out_object=test.o --out_header=test.hpp --gen_program_shape=true --target_cpu=haswell
`

Gives the following error:
`
2018-02-13 16:32:27.484808: F tensorflow/compiler/aot/tfcompile_main.cc:140] Non-OK-status: status status: Failed precondition: Value {name:'cond/strided_slice_1' id:20 op device:{/device:XLA_CPU_JIT} def:{cond/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_DOUBLE, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](cond/strided_slice_1/Switch, cond/strided_slice_1/stack, cond/strided_slice_1/stack_1, cond/strided_slice_1/stack_2)}} not dominated by switch nodes.
`

Would expect code to be generated similar to:
```
double test_func( double* x )
{
  if (x[0] > x[1] )
    return x[0];
  else
    return x[1];
}
```

tensorflow version 1.5.0
RHEL 7.3 64bit
tensorflow built from source 
python 2.7
bazel 0.7
gcc 4.8.5
"
17014,Importing graph with tf.contrib.resampler.resampler fails,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSx High Sierra
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
CPU
- **Bazel version (if compiling from source)**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Importing a graph def with a  `tf.contrib.resampler.resampler` op fails iff `tf.contrib` is not imported first.

Execute:
```
import tensorflow as tf

def export_model(filename, sess, output_node_names):
    from tensorflow.python.framework import graph_util
    output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                                 sess.graph.as_graph_def(add_shapes=True),
                                                                 output_node_names)
    with tf.gfile.GFile(filename, ""wb"") as f:
        f.write(output_graph_def.SerializeToString())
        
def read_frozen_protobuf(path):
    with tf.gfile.FastGFile(str(path), 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        return graph_def

    
def export(filename):
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = tf.placeholder(dtype=tf.float64, shape=[32, 32], name='images')
        points = tf.placeholder(dtype=tf.float64, shape=[32, 2], name='points')
        resampled = tf.contrib.resampler.resampler(images, points, name='resampled')
        output_node_names = ['resampled/Resampler']
        export_model(filename, sess, output_node_names)
        
def load(filename):
    import numpy as np
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = np.zeros((32, 32), dtype=np.float64)
        points = np.zeros((32, 2), dtype=np.float64)
        graph_def = read_frozen_protobuf(filename)
        tf.import_graph_def(graph_def, 
                            input_map={'images': images,
                                       'points': points},
                            return_elements=['resampled/Resampler:0'])
        
######################################################
frozen_graph_def = '/tmp/test.frozen'
export(frozen_graph_def)
load(frozen_graph_def)
```

Then, in a new interpreter (where the load(..) function is defined), execute:
```
frozen_graph_def = '/tmp/test.frozen'
load(frozen_graph_def)
```

This give the error message:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-64251e160f7d> in <module>()
     42 frozen_graph_def = '/tmp/test.frozen'
     43 # export(frozen_graph_def)
---> 44 load(frozen_graph_def)

<ipython-input-1-64251e160f7d> in load(filename)
     37                             input_map={'images': images,
     38                                        'points': points},
---> 39                             return_elements=['resampled/Resampler:0'])
     40 
     41 ######################################################################

/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    314                 'in a future version' if date is None else ('after %s' % date),
    315                 instructions)
--> 316       return func(*args, **kwargs)
    317     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    318                                        _add_deprecated_arg_notice_to_docstring(

/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    539         # Set any default attr values that aren't present.
    540         if node.op not in op_dict:
--> 541           raise ValueError('No op named %s in defined operations.' % node.op)
    542         op_def = op_dict[node.op]
    543         for attr_def in op_def.attr:

ValueError: No op named Resampler in defined operations.
```"
17013,[Android Studio + TensorFlow] Error with some Kernel's Operations,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: Source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.12
- **Bazel version**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **GPU model and memory**: GTX 1050 TI 256MB
- **Exact command to reproduce**: Build on Android Studio

## Describe the problem
Hello guys,

My team and I are attempting to import and use a specific model on Android Studio. This functionality is pretty easy, we feed this model with an image in bitmap format and we receive a matrix points with results.
But on **Run** step this model shows different errors according to **libtensorflow_inference.so**.

I create an image to better explain my problem to you. See below:
![issue](https://user-images.githubusercontent.com/29979626/36210004-7405189e-1184-11e8-80b4-1f77044c100a.png)


## Source code / logs
### Initially, I want to load my model:
```
public static Detector testCreate(AssetManager assetManager, String modelFileName, String labelFileName, int inputWidth, int inputHeight, int inputDepth, String inputName, String outputName) throws IOException {

	TensorFlowImageDetector d = new TensorFlowImageDetector();
        d.inputName = inputName;
        d.outputName = outputName;
        d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFileName);

        d.inputWidth = inputWidth;
        d.inputHeight = inputHeight;
        d.inputDepth = inputDepth;

        d.outputNames = new String[]{outputName};
        // start outputs
        // fix labelfilename != null

        return d;
}
```

### Before I need to detect element on image (feed, run and fetch):
```
public Detection testDetectImage(final float[] pixels) {

        // Log this method so that it can be analyzed with systrace.
        TraceCompat.beginSection(""detectImage"");

        // Copy the input data into TensorFlow.
        TraceCompat.beginSection(""feed"");
        inferenceInterface.feed(inputName, pixels, 1, inputWidth, inputHeight, inputDepth);
        TraceCompat.endSection();

        // Run the inference call.
        TraceCompat.beginSection(""run"");
        inferenceInterface.run(outputNames, Boolean.parseBoolean(null));
        TraceCompat.endSection();

        // Copy the output Tensor back into the output array.
        TraceCompat.beginSection(""fetch"");
        //inferenceInterface.fetch(outputName, outputs);
        TraceCompat.endSection();

        return null;
}
```
### Comments
1. Errors occurring on **Run** function
2. We already tried to solve the problem with 3 different ways mentioned in the image
3. My model has an input size of 1x256x256x3 and for test, I'm using a matrix of zeros
4. To generate a custom library we used this tutorial: https://medium.com/@daj/how-to-shrink-the-tensorflow-android-inference-library-cb698facf758
5. To generate a full library of Android, on Bazel, we used `--copt=-D__ANDROID_TYPES_FULL__` flag
6. We used JAR created by Bazel when generated custom libraries

### Logs for errors
>**Op Sin**
```
02-14 13:23:08.584 8090-8090/? E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
02-14 13:23:08.584 8090-8090/? I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
02-14 13:23:08.685 8090-8090/? I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
02-14 13:23:11.083 8090-8090/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: Model load took 518ms, TensorFlow version: 1.6.0-rc0
02-14 13:23:11.092 8090-8090/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/frozen_pose3d.pb'
02-14 13:23:11.092 8090-8090/com.example.lis.zitronenkuchen D/MainActivity: Load Success!
02-14 13:23:11.215 8090-8090/com.example.lis.zitronenkuchen E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input_image], outputs:[MatMul]
02-14 13:23:11.216 8090-8090/com.example.lis.zitronenkuchen D/AndroidRuntime: Shutting down VM
02-14 13:23:11.217 8090-8090/com.example.lis.zitronenkuchen E/AndroidRuntime: FATAL EXCEPTION: main
                                                                              Process: com.example.lis.zitronenkuchen, PID: 8090
                                                                              java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.lis.zitronenkuchen/com.example.lis.zitronenkuchen.MainActivity}: java.lang.RuntimeException: Error initializing TensorFlow!
                                                                                  at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2659)
                                                                                  at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724)
                                                                                  at android.app.ActivityThread.-wrap12(ActivityThread.java)
                                                                                  at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473)
                                                                                  at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                                  at android.os.Looper.loop(Looper.java:154)
                                                                                  at android.app.ActivityThread.main(ActivityThread.java:6123)
                                                                                  at java.lang.reflect.Method.invoke(Native Method)
                                                                                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867)
                                                                                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757)
                                                                               Caused by: java.lang.RuntimeException: Error initializing TensorFlow!
                                                                                  at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:44)
                                                                                  at android.app.Activity.performCreate(Activity.java:6672)
                                                                                  at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1140)
                                                                                  at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2612)
                                                                                  at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724) 
                                                                                  at android.app.ActivityThread.-wrap12(ActivityThread.java) 
                                                                                  at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473) 
                                                                                  at android.os.Handler.dispatchMessage(Handler.java:102) 
                                                                                  at android.os.Looper.loop(Looper.java:154) 
                                                                                  at android.app.ActivityThread.main(ActivityThread.java:6123) 
                                                                                  at java.lang.reflect.Method.invoke(Native Method) 
                                                                                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867) 
                                                                                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757) 
                                                                               Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Sin' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                                <no registered kernels>
                                                                              
                                                                              	 [[Node: ViewpointNet/get_rot_mat/Sin = Sin[T=DT_FLOAT](ViewpointNet/get_rot_mat/Sqrt)]]
                                                                                  at org.tensorflow.Session.run(Native Method)
                                                                                  at org.tensorflow.Session.access$100(Session.java:48)
                                                                                  at org.tensorflow.Session$Runner.runHelper(Session.java:298)
                                                                                  at org.tensorflow.Session$Runner.run(Session.java:248)
                                                                                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)
                                                                                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                                                                                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)
                                                                                  at com.example.lis.zitronenkuchen.TensorFlowImageDetector.detectImage(TensorFlowImageDetector.java:73)
                                                                                  at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:41)
                                                                                  	... 12 more
```

>**Op Identity**
```
E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
D/AndroidRuntime: Shutting down VM


                  --------- beginning of crash
E/AndroidRuntime: FATAL EXCEPTION: main
                  Process: com.example.lis.zitronenkuchen, PID: 6521
                  java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.lis.zitronenkuchen/com.example.lis.zitronenkuchen.MainActivity}: java.lang.RuntimeException: Error initializing TensorFlow!
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2659)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724)
                      at android.app.ActivityThread.-wrap12(ActivityThread.java)
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473)
                      at android.os.Handler.dispatchMessage(Handler.java:102)
                      at android.os.Looper.loop(Looper.java:154)
                      at android.app.ActivityThread.main(ActivityThread.java:6123)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757)
                   Caused by: java.lang.RuntimeException: Error initializing TensorFlow!
                      at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:44)
                      at android.app.Activity.performCreate(Activity.java:6672)
                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1140)
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2612)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724) 
                      at android.app.ActivityThread.-wrap12(ActivityThread.java) 
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473) 
                      at android.os.Handler.dispatchMessage(Handler.java:102) 
                      at android.os.Looper.loop(Looper.java:154) 
                      at android.app.ActivityThread.main(ActivityThread.java:6123) 
                      at java.lang.reflect.Method.invoke(Native Method) 
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867) 
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757) 
                   Caused by: org.tensorflow.TensorFlowException: Op type not registered 'Identity' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.
                      at org.tensorflow.Graph.importGraphDef(Native Method)
                      at org.tensorflow.Graph.importGraphDef(Graph.java:130)
                      at org.tensorflow.Graph.importGraphDef(Graph.java:114)
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                      at com.example.lis.zitronenkuchen.TensorFlowImageDetector.create(TensorFlowImageDetector.java:47)
                      at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:29)
                      at android.app.Activity.performCreate(Activity.java:6672) 
                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1140) 
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2612) 
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724) 
                      at android.app.ActivityThread.-wrap12(ActivityThread.java) 
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473) 
                      at android.os.Handler.dispatchMessage(Handler.java:102) 
                      at android.os.Looper.loop(Looper.java:154) 
                      at android.app.ActivityThread.main(ActivityThread.java:6123) 
                      at java.lang.reflect.Method.invoke(Native Method) 
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867) 
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757) 
```

>**Op Switch**
```
02-14 13:09:36.334 26040-26040/com.example.lis.zitronenkuchen E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
02-14 13:09:36.334 26040-26040/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
02-14 13:09:36.406 26040-26040/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
02-14 13:09:37.835 26040-26040/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: Model load took 1382ms, TensorFlow version: 1.3.1
02-14 13:09:37.843 26040-26040/com.example.lis.zitronenkuchen I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/frozen_pose3d.pb'
02-14 13:09:37.843 26040-26040/com.example.lis.zitronenkuchen D/MainActivity: Load Success!
02-14 13:09:37.892 26040-26040/com.example.lis.zitronenkuchen E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input_image], outputs:[MatMul]
02-14 13:09:37.892 26040-26040/com.example.lis.zitronenkuchen D/AndroidRuntime: Shutting down VM
02-14 13:09:37.893 26040-26040/com.example.lis.zitronenkuchen E/AndroidRuntime: FATAL EXCEPTION: main
                                                                                Process: com.example.lis.zitronenkuchen, PID: 26040
                                                                                java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.lis.zitronenkuchen/com.example.lis.zitronenkuchen.MainActivity}: java.lang.RuntimeException: Error initializing TensorFlow!
                                                                                    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2426)
                                                                                    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2490)
                                                                                    at android.app.ActivityThread.-wrap11(ActivityThread.java)
                                                                                    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1354)
                                                                                    at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                                    at android.os.Looper.loop(Looper.java:148)
                                                                                    at android.app.ActivityThread.main(ActivityThread.java:5443)
                                                                                    at java.lang.reflect.Method.invoke(Native Method)
                                                                                    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:728)
                                                                                    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:618)
                                                                                 Caused by: java.lang.RuntimeException: Error initializing TensorFlow!
                                                                                    at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:44)
                                                                                    at android.app.Activity.performCreate(Activity.java:6245)
                                                                                    at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1130)
                                                                                    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2379)
                                                                                    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2490) 
                                                                                    at android.app.ActivityThread.-wrap11(ActivityThread.java) 
                                                                                    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1354) 
                                                                                    at android.os.Handler.dispatchMessage(Handler.java:102) 
                                                                                    at android.os.Looper.loop(Looper.java:148) 
                                                                                    at android.app.ActivityThread.main(ActivityThread.java:5443) 
                                                                                    at java.lang.reflect.Method.invoke(Native Method) 
                                                                                    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:728) 
                                                                                    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:618) 
                                                                                 Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                                  device='GPU'; T in [DT_STRING]
                                                                                  device='GPU'; T in [DT_BOOL]
                                                                                  device='GPU'; T in [DT_INT32]
                                                                                  device='GPU'; T in [DT_INT64]
                                                                                  device='GPU'; T in [DT_FLOAT]
                                                                                  device='GPU'; T in [DT_HALF]
                                                                                  device='CPU'; T in [DT_QINT32]
                                                                                  device='CPU'; T in [DT_QUINT8]
                                                                                  device='CPU'; T in [DT_QINT8]
                                                                                  device='CPU'; T in [DT_FLOAT]
                                                                                  device='CPU'; T in [DT_HALF]
                                                                                  device='CPU'; T in [DT_INT32]
                                                                                  device='CPU'; T in [DT_INT64]
                                                                                
                                                                                	 [[Node: PosePrior/dropout/cond/Switch = Switch[T=DT_BOOL](PlaceholderWithDefault, PlaceholderWithDefault)]]
                                                                                    at org.tensorflow.Session.run(Native Method)
                                                                                    at org.tensorflow.Session.access$100(Session.java:48)
                                                                                    at org.tensorflow.Session$Runner.runHelper(Session.java:295)
                                                                                    at org.tensorflow.Session$Runner.run(Session.java:245)
                                                                                    at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:143)
                                                                                    at com.example.lis.zitronenkuchen.TensorFlowImageDetector.detectImage(TensorFlowImageDetector.java:73)
                                                                                    at com.example.lis.zitronenkuchen.MainActivity.onCreate(MainActivity.java:41)
                                                                                    	... 12 more
```

Thank you! :grin:"
17012,LNK2019	unresolved external symbol __std_reverse_trivially_swappable_8  when compiling proto_text.vcxproj,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: Cmake 3.10.2, swigwin 3.0.12, Visual studio 2017, but toolset 2015 v140
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

I'm following the CMake guide as described here: https://github.com/tensorflow/tensorflow/blob/fbddebee0bf07dadfb2b15ec678291dd5730ca99/tensorflow/contrib/cmake/README.md.

Error appears after following command:
`MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`

### Describe the problem
Error when compiling the proto_text.vcxproj

### Source code / logs
```
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj"" (default target) (1) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_framework.vcxproj"" (default target) (3) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj"" (default target) (4) ->
""C:\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj"" (default target) (5) ->
(Link target) ->
  libprotobuf.lib(text_format.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function ""void __cdecl std::_Buffered_inplace_merge_d
ivide_and_conquer2<class google::protobuf::Message const * *,__int64,class google::protobuf::Message const *,class google::protobuf::MapEntryMessageComparator>(class google::protobuf
::Message const * *,class google::protobuf::Message const * *,class google::protobuf::Message const * *,__int64,__int64,struct std::_Temporary_buffer<class google::protobuf::Message
const *> &,class google::protobuf::MapEntryMessageComparator,class google::protobuf::Message const * *,class google::protobuf::Message const * *,__int64,__int64)"" (??$_Buffered_inpla
ce_merge_divide_and_conquer2@PEAPEBVMessage@protobuf@google@@_JPEBV123@VMapEntryMessageComparator@23@@std@@YAXPEAPEBVMessage@protobuf@google@@00_J1AEAU?$_Temporary_buffer@PEBVMessage
@protobuf@google@@@0@VMapEntryMessageComparator@23@0011@Z) [C:\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
  C:\tensorflow\tensorflow\contrib\cmake\build\Release\proto_text.exe : fatal error LNK1120: 1 unresolved externals [C:\tensorflow\tensorflow\contrib\cmake\build\proto_text.vcxproj]
```

"
17011,[Feature Request] Specify output size for tf.contrib.image.transform,"## Status Quo
Right now the output size of [tf.contrib.image.transform](https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform) is equal to the input size.

## Request
Specify output size for [tf.contrib.image.transform](https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform) by argument.

## Usage example:
This would be very useful for custom augmentations where the input images do not have a fixed size, but the output images shall have a constant size. This operation can be simulated by padding the input images (such that no information is lost when e.g. rotation the image), then transforming them and finally cropping/padding them to the desired size, but it'd be much more efficient and a lot easier if the op could support setting the desired output size.

Thanks!"
17009,Failed to call cudnnRNNBackwardData: CUDNN_STATUS_INTERNAL_ERROR,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.4.1
- **Python version**: 3.5.2
- **Bazel version:** Not compiled from source
- **GCC/Compiler version**: Not compiled from source
- **CUDA/cuDNN version**: 8.0/v6
- **GPU model and memory**: GeForce GTX 1080 (8GB x 4)
- **Exact command to reproduce**: Code as per below

The code starts training and after an arbitrary number of iterations on batches, it gives the following error:

```
2018-02-14 23:51:31.591963: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
2018-02-14 23:51:31.592000: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1
2018-02-14 23:51:31.592023: E tensorflow/stream_executor/cuda/cuda_dnn.cc:1679] Failed to call cudnnRNNBackwardData: CUDNN_STATUS_INTERNAL_ERROR
Aborted (core dumped)
```

I'm using `cudnn_gru` in a `tf.while_loop` control flow operation and using initializers from outside the `tf.while` scope since variables aren't allowed to be instantiated within the `tf.while` scope:

```
import tensorflow as tf

gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=150, input_size=500)
gru_fw_1 = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=150, input_size=1800)
e = tf.random_uniform([gru_fw.params_size()], -0.1, 0.1)
f = tf.random_uniform([gru_fw.params_size()], -0.1, 0.1)
g = tf.zeros([1, 4, 150])
h = tf.zeros([1, 4, 150])
zeros_i = tf.zeros([4, 150])

class cudnn_gru:
	def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):
		self.num_layers = num_layers
		self.grus = []
		self.params = []
		self.inits = []
		self.dropout_mask = []
		for layer in range(num_layers):
			input_size_ = input_size if layer == 0 else 2 * num_units
			gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(
				num_layers=1, num_units=num_units, input_size=input_size_)
			gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(
				num_layers=1, num_units=num_units, input_size=input_size_)
			with tf.variable_scope('CUDNN_GRU', reuse=tf.AUTO_REUSE):
				param_fw = tf.get_variable(""param_fw"",initializer=e,validate_shape=False)
				param_bw = tf.get_variable(""param_bw"",initializer=f,validate_shape=False)
				init_fw = tf.get_variable(""init_fw"", initializer=g)
				init_bw = tf.get_variable(""init_bw"", initializer=h)
	def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):
		outputs = [tf.transpose(inputs, [1, 0, 2])]
		for layer in range(self.num_layers):
			gru_fw, gru_bw = self.grus[layer]
			param_fw, param_bw = self.params[layer]
			init_fw, init_bw = self.inits[layer]
			mask_fw, mask_bw = self.dropout_mask[layer]
			with tf.variable_scope(""fw""):
				out_fw, _ = gru_fw(outputs[-1] * mask_fw, init_fw, param_fw)
			with tf.variable_scope(""bw""):
				inputs_bw = tf.reverse_sequence(
					outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)
				out_bw, _ = gru_bw(inputs_bw, init_bw, param_bw)
				out_bw = tf.reverse_sequence(
					out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)
			outputs.append(tf.concat([out_fw, out_bw], axis=2))
		if concat_layers:
			res = tf.concat(outputs[1:], axis=2)
		else:
			res = outputs[-1]
		res = tf.transpose(res, [1, 0, 2])
		return res

class native_gru:
	def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=""native_gru""):
		self.num_layers = num_layers
		self.grus = []
		self.inits = []
		self.dropout_mask = []
		self.scope = scope
		for layer in range(num_layers):
			input_size_ = input_size if layer == 0 else 2 * num_units
			gru_fw = tf.contrib.rnn.GRUCell(num_units)
			gru_bw = tf.contrib.rnn.GRUCell(num_units)
			with tf.variable_scope('native_GRU', reuse=tf.AUTO_REUSE):

				init_fw = tf.get_variable(""init_fw"", initializer=zeros_i)
				init_bw = tf.get_variable(""init_bw"", initializer=zeros_i)
				
			#init_fw = tf.Variable(tf.zeros([batch_size, num_units]))
			#init_bw = tf.Variable(tf.zeros([batch_size, num_units]))
			mask_fw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),
							  keep_prob=keep_prob, is_train=is_train, mode=None)
			mask_bw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),
							  keep_prob=keep_prob, is_train=is_train, mode=None)
			self.grus.append((gru_fw, gru_bw, ))
			self.inits.append((init_fw, init_bw, ))
			self.dropout_mask.append((mask_fw, mask_bw, ))

	def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):
		outputs = [inputs]
		with tf.variable_scope(self.scope):
			for layer in range(self.num_layers):
				gru_fw, gru_bw = self.grus[layer]
				init_fw, init_bw = self.inits[layer]
				mask_fw, mask_bw = self.dropout_mask[layer]
				with tf.variable_scope(""fw_{}"".format(layer)):
					out_fw, _ = tf.nn.dynamic_rnn(
						gru_fw, outputs[-1] * mask_fw, seq_len, initial_state=init_fw, dtype=tf.float32)
				with tf.variable_scope(""bw_{}"".format(layer)):
					inputs_bw = tf.reverse_sequence(
						outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)
					out_bw, _ = tf.nn.dynamic_rnn(
						gru_fw, inputs_bw, seq_len, initial_state=init_bw, dtype=tf.float32)
					out_bw = tf.reverse_sequence(
						out_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)
				outputs.append(tf.concat([out_fw, out_bw], axis=2))
		if concat_layers:
			res = tf.concat(outputs[1:], axis=2)
		else:
			res = outputs[-1]
		return res
```
This has not happened before, when the project structure was sequential and not within a control flow mechanism. Also, if I change the implementation to use native GRU instead of cudnn_gru, it works perfectly fine."
17007,Installation Tensorflow from source stuck at Downloading grpc ,"Installation Tensorflow 1.4.0-rc0 from source on RHEL 7.4 

Installation went fine with Python3.6, CUDA 9.1,  CUDNN 7.0.5

cd tensorflow-1.4.0-rc0
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

However, it is not moving from 
INFO: Downloading https://mirror.bazel.build/github.com/grpc/grpc/archive/781fd/6f6ea03645a520cd5c675da67ab61f87e4b.tar.gz
"
17006,Batch normalization acting weird?!,"### System Information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7.1
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Exact command to reproduce**: See code below
- **Bazel version**: Not building from source
- **CUDA/cuDNN version**: Version 6
- **GPU model and memory**: Nvidia Quadro k2200 with 4GB


### Describe the problem
Hello everyone,

so theres the method tf.layers.batch_normalization and i am trying to understand what is Happening behind the curtains and in how far it is manually reproducible and in conformance of the paper by Ioffe&Szegedy. After running a small script which can only Train the two Parameters Gamma and beta to minimize the loss it appears that there is not normalization taking place(activation reduces by the mean over all activations of one Batch divided by the square-root of the variance of all activations of one batch). It is more or less always considering the Initial activation and simply multiplies this with Gamma plus beta. And that is especially interesting if the Batch size is reduced to one(not even remotely sure what is Happening then; yet it seems like this does not affect the computation at all)...

Thank you for your time and consideration!

### Source code / logs
```Python
import numpy as np
import tensorflow as tf

test_img = np.array([[[[50],[100]],
                   [[150],[2000]]],
                   [[[0],[300]],
                   [[140],[5000]]],
                   [[[0],[300]],
                   [[140],[5500]]],
                   [[[0],[200]],
                   [[1400],[5000]]],
                   [[[0],[300]],
                   [[140],[5000]]]], np.float32)
gt_img = np.array([[[[60],[130]],
                [[180],[225]]],
                [[[60],[130]],
                [[180],[225]]],
                [[[600],[130]],
                [[1800],[225]]],
                [[[60],[100]],
                [[180],[205]]],
                [[[60],[100]],
                [[180],[205]]]], np.float32)
test_img_op = tf.convert_to_tensor(test_img, tf.float32)
norm_op = tf.layers.batch_normalization(test_img_op, momentum=0)

loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = gt_img,
                                                             logits = norm_op))
count = 0
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer_obj = tf.train.AdamOptimizer(0.01).minimize(loss_op)
with tf.Session() as sess:
    sess.run(tf.group(tf.global_variables_initializer(), 
                      tf.local_variables_initializer()))
    print(test_img)
    while True:
        count += 1
        if count < 100:
            new_img, op, lossy, trainable = sess.run([norm_op, 
                                                      optimizer_obj, 
                                                      loss_op, 
                                                      tf.trainable_variables()])
            print(trainable)
            print(new_img)
        else:
            break
```"
17003,Missing files and errors during Windows C++ Visual studio build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: Cmake 3.10.2, swigwin 3.0.12, Visual studio 2017, but toolset 2015 v140
- **CUDA/cuDNN version**: N/A, trying to build cpu version
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

**1. Cmake build**:
```
C:\tensorflow\tensorflow\contrib\cmake\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
More? -DSWIG_EXECUTABLE=C:\swigwin-3.0.12\swig.exe ^
More? -DPYTHON_EXECUTABLE=C:\Users\User\Anaconda3\python.exe  ^
More? -DPYTHON_LIBRARIES=C:\Users\User\Anaconda3\libs\python35.lib
```
**2. Visual studio 2017**:

For each project I set in properties:
- target platform version: 10.0.16299.0 -> 8.1
- platform toolset: Visual studio 2017 (v141) -> Visual studio 2015 (v140)

Then in Configuration Manager I select Release in Active Resolution Configuration.

### Describe the problem
I want to use Tensorflow in my Windows C++ application. Therefore I'm trying to build in Visual studio on a Windows system.
However during the build I receive multiple errors, e.g. missing files and multiple projects cannot be built.

### Source code / logs

Multiple times I receive this missing file error:
```
...
C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\captured_function.cc)
...
```

Failed project builds:
```
79>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
79>Done building project ""stateless_random_ops_gen_cc.vcxproj"" -- FAILED.
80>------ Build started: Project: state_ops_gen_cc, Configuration: Release x64 ------
80>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
80>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
80>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
80>Done building project ""state_ops_gen_cc.vcxproj"" -- FAILED.
81>------ Build started: Project: bitwise_ops_gen_cc, Configuration: Release x64 ------
81>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
81>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
81>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
81>Done building project ""bitwise_ops_gen_cc.vcxproj"" -- FAILED.
82>------ Build started: Project: candidate_sampling_ops_gen_cc, Configuration: Release x64 ------
82>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
82>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
82>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
82>Done building project ""candidate_sampling_ops_gen_cc.vcxproj"" -- FAILED.
83>------ Build started: Project: checkpoint_ops_gen_cc, Configuration: Release x64 ------
83>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
83>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
83>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
83>Done building project ""checkpoint_ops_gen_cc.vcxproj"" -- FAILED.
84>------ Build started: Project: spectral_ops_gen_cc, Configuration: Release x64 ------
84>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
77>tf_array_ops.vcxproj -> C:\tensorflow\tensorflow\contrib\cmake\build\tf_array_ops.dir\Release\tf_array_ops.lib
85>------ Build started: Project: array_ops_gen_cc, Configuration: Release x64 ------
84>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
85>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
84>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
84>Done building project ""spectral_ops_gen_cc.vcxproj"" -- FAILED.
86>------ Build started: Project: io_ops_gen_cc, Configuration: Release x64 ------
85>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
85>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
86>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
85>Done building project ""array_ops_gen_cc.vcxproj"" -- FAILED.
87>------ Build started: Project: set_ops_gen_cc, Configuration: Release x64 ------
86>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
86>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
86>Done building project ""io_ops_gen_cc.vcxproj"" -- FAILED.
88>------ Build started: Project: linalg_ops_gen_cc, Configuration: Release x64 ------
87>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
88>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
87>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
88>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
88>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
88>Done building project ""linalg_ops_gen_cc.vcxproj"" -- FAILED.
89>------ Build started: Project: resource_variable_ops_gen_cc, Configuration: Release x64 ------
87>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
87>Done building project ""set_ops_gen_cc.vcxproj"" -- FAILED.
90>------ Build started: Project: math_ops_gen_cc, Configuration: Release x64 ------
89>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
89>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
89>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
89>Done building project ""resource_variable_ops_gen_cc.vcxproj"" -- FAILED.
91>------ Build started: Project: logging_ops_gen_cc, Configuration: Release x64 ------
90>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
90>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
90>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
90>Done building project ""math_ops_gen_cc.vcxproj"" -- FAILED.
92>------ Build started: Project: random_ops_gen_cc, Configuration: Release x64 ------
91>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
91>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
91>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
91>Done building project ""logging_ops_gen_cc.vcxproj"" -- FAILED.
93>------ Build started: Project: lookup_ops_gen_cc, Configuration: Release x64 ------
92>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
93>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
92>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
93>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
93>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
93>Done building project ""lookup_ops_gen_cc.vcxproj"" -- FAILED.
94>------ Build started: Project: sdca_ops_gen_cc, Configuration: Release x64 ------
94>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
92>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
92>Done building project ""random_ops_gen_cc.vcxproj"" -- FAILED.
95>------ Build started: Project: image_ops_gen_cc, Configuration: Release x64 ------
94>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
95>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
94>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
94>Done building project ""sdca_ops_gen_cc.vcxproj"" -- FAILED.
96>------ Build started: Project: nn_ops_gen_cc, Configuration: Release x64 ------
95>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
95>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
95>Done building project ""image_ops_gen_cc.vcxproj"" -- FAILED.
97>------ Build started: Project: remote_fused_graph_ops_gen_cc, Configuration: Release x64 ------
96>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
96>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
96>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
96>Done building project ""nn_ops_gen_cc.vcxproj"" -- FAILED.
98>------ Build started: Project: no_op_gen_cc, Configuration: Release x64 ------
97>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
98>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
97>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
98>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
98>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
98>Done building project ""no_op_gen_cc.vcxproj"" -- FAILED.
99>------ Build started: Project: control_flow_ops_gen_cc, Configuration: Release x64 ------
99>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
97>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
97>Done building project ""remote_fused_graph_ops_gen_cc.vcxproj"" -- FAILED.
100>------ Build started: Project: parsing_ops_gen_cc, Configuration: Release x64 ------
99>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
99>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
99>Done building project ""control_flow_ops_gen_cc.vcxproj"" -- FAILED.
101>------ Build started: Project: ctc_ops_gen_cc, Configuration: Release x64 ------
100>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
101>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
100>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
101>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
101>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
101>Done building project ""ctc_ops_gen_cc.vcxproj"" -- FAILED.
102>------ Build started: Project: script_ops_gen_cc, Configuration: Release x64 ------
102>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
100>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
100>Done building project ""parsing_ops_gen_cc.vcxproj"" -- FAILED.
103>------ Build started: Project: data_flow_ops_gen_cc, Configuration: Release x64 ------
102>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
102>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
102>Done building project ""script_ops_gen_cc.vcxproj"" -- FAILED.
104>------ Build started: Project: dataset_ops_gen_cc, Configuration: Release x64 ------
103>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
103>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
103>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
103>Done building project ""data_flow_ops_gen_cc.vcxproj"" -- FAILED.
105>------ Build started: Project: sparse_ops_gen_cc, Configuration: Release x64 ------
104>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
105>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
104>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
105>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
105>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
105>Done building project ""sparse_ops_gen_cc.vcxproj"" -- FAILED.
106>------ Build started: Project: functional_ops_gen_cc, Configuration: Release x64 ------
104>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
104>Done building project ""dataset_ops_gen_cc.vcxproj"" -- FAILED.
107>------ Build started: Project: string_ops_gen_cc, Configuration: Release x64 ------
106>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
106>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
106>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
106>Done building project ""functional_ops_gen_cc.vcxproj"" -- FAILED.
108>------ Build started: Project: sendrecv_ops_gen_cc, Configuration: Release x64 ------
107>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
108>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
107>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
108>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
108>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
108>Done building project ""sendrecv_ops_gen_cc.vcxproj"" -- FAILED.
109>------ Build started: Project: training_ops_gen_cc, Configuration: Release x64 ------
107>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
107>Done building project ""string_ops_gen_cc.vcxproj"" -- FAILED.
110>------ Build started: Project: user_ops_gen_cc, Configuration: Release x64 ------
109>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
109>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
109>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
109>Done building project ""training_ops_gen_cc.vcxproj"" -- FAILED.
111>------ Build started: Project: tf_core_cpu, Configuration: Release x64 ------
110>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
111>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
110>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
111>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
111>loader.cc
110>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_cc_op_gen_main.dir\Release\cc_op_gen.obj'
...
```

```
152>------ Build started: Project: contrib_tensor_forest_stats_ops_gen_python, Configuration: Release x64 ------
121>padded_batch_dataset_op.cc
121>padding_fifo_queue.cc
121>padding_fifo_queue_op.cc
152>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
152>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>parallel_map_dataset_op.cc
152>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
152>Done building project ""contrib_tensor_forest_stats_ops_gen_python.vcxproj"" -- FAILED.
153>------ Build started: Project: contrib_text_skip_gram_ops_gen_python, Configuration: Release x64 ------
121>parameterized_truncated_normal_op.cc
121>parse_tensor_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\parallel_map_dataset_op.cc)
121>pooling_ops_3d.cc
121>pooling_ops_common.cc
121>population_count_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\pooling_ops_common.cc)
121>prefetch_dataset_op.cc
153>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
153>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
153>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>priority_queue.cc
153>Done building project ""contrib_text_skip_gram_ops_gen_python.vcxproj"" -- FAILED.
154>------ Build started: Project: control_flow_ops_gen_python, Configuration: Release x64 ------
121>priority_queue_op.cc
154>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>qr_op_complex128.cc
121>qr_op_complex64.cc
121>qr_op_double.cc
154>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
154>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
154>Done building project ""control_flow_ops_gen_python.vcxproj"" -- FAILED.
155>------ Build started: Project: ctc_ops_gen_python, Configuration: Release x64 ------
121>qr_op_float.cc
155>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
155>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
155>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
155>Done building project ""ctc_ops_gen_python.vcxproj"" -- FAILED.
```

```
150>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory
150>Done building project ""tf_core_direct_session.vcxproj"" -- FAILED.
151>------ Build started: Project: tf_contrib_tensor_forest_stats_ops, Configuration: Release x64 ------
121>mirror_pad_op_cpu_impl_3.cc
121>mirror_pad_op_cpu_impl_4.cc
121>mirror_pad_op_cpu_impl_5.cc
121>mkl_aggregate_ops.cc
121>mkl_avgpooling_op.cc
121>mkl_concat_op.cc
121>mkl_conv_grad_bias_ops.cc
121>mkl_conv_grad_filter_ops.cc
121>mkl_conv_grad_input_ops.cc
121>mkl_conv_ops.cc
121>mkl_cwise_ops_common.cc
121>mkl_fused_batch_norm_op.cc
121>mkl_identity_op.cc
121>mkl_input_conversion_op.cc
121>mkl_lrn_op.cc
121>mkl_matmul_op.cc
121>mkl_maxpooling_op.cc
121>mkl_pooling_ops_common.cc
121>mkl_relu_op.cc
121>mkl_reshape_op.cc
121>mkl_transpose_op.cc
121>multinomial_op.cc
121>no_op.cc
121>non_max_suppression_op.cc
121>one_hot_op.cc
121>ops_util.cc
121>pack_op.cc
121>pad_op.cc
152>------ Build started: Project: contrib_tensor_forest_stats_ops_gen_python, Configuration: Release x64 ------
121>padded_batch_dataset_op.cc
121>padding_fifo_queue.cc
121>padding_fifo_queue_op.cc
152>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
152>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>parallel_map_dataset_op.cc
152>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
152>Done building project ""contrib_tensor_forest_stats_ops_gen_python.vcxproj"" -- FAILED.
153>------ Build started: Project: contrib_text_skip_gram_ops_gen_python, Configuration: Release x64 ------
121>parameterized_truncated_normal_op.cc
121>parse_tensor_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\parallel_map_dataset_op.cc)
121>pooling_ops_3d.cc
121>pooling_ops_common.cc
121>population_count_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\pooling_ops_common.cc)
121>prefetch_dataset_op.cc
153>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
153>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
153>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>priority_queue.cc
153>Done building project ""contrib_text_skip_gram_ops_gen_python.vcxproj"" -- FAILED.
154>------ Build started: Project: control_flow_ops_gen_python, Configuration: Release x64 ------
121>priority_queue_op.cc
154>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>qr_op_complex128.cc
121>qr_op_complex64.cc
121>qr_op_double.cc
154>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
154>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
154>Done building project ""control_flow_ops_gen_python.vcxproj"" -- FAILED.
155>------ Build started: Project: ctc_ops_gen_python, Configuration: Release x64 ------
121>qr_op_float.cc
155>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
155>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
155>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
155>Done building project ""ctc_ops_gen_python.vcxproj"" -- FAILED.
156>------ Build started: Project: data_flow_ops_gen_python, Configuration: Release x64 ------
121>queue_base.cc
121>queue_ops.cc
121>random_crop_op.cc
121>random_op.cc
121>random_poisson_op.cc
121>random_shuffle_op.cc
157>------ Build started: Project: dataset_ops_gen_python, Configuration: Release x64 ------
121>random_shuffle_queue_op.cc
157>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>range_dataset_op.cc
157>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>range_sampler.cc
121>reader_dataset_ops.cc
121>reader_ops.cc
157>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
157>Done building project ""dataset_ops_gen_python.vcxproj"" -- FAILED.
158>------ Build started: Project: debug_ops_gen_python, Configuration: Release x64 ------
121>record_input_op.cc
121>record_yielder.cc
121>reduce_join_op.cc
121>reduction_ops_all.cc
121>reduction_ops_any.cc
121>reduction_ops_common.cc
121>reduction_ops_max.cc
158>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
158>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>reduction_ops_mean.cc
158>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
158>Done building project ""debug_ops_gen_python.vcxproj"" -- FAILED.
159>------ Build started: Project: functional_ops_gen_python, Configuration: Release x64 ------
121>reduction_ops_min.cc
121>reduction_ops_prod.cc
121>reduction_ops_sum.cc
159>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
159>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
159>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
159>Done building project ""functional_ops_gen_python.vcxproj"" -- FAILED.
160>------ Build started: Project: image_ops_gen_python, Configuration: Release x64 ------
160>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
160>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
160>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
160>Done building project ""image_ops_gen_python.vcxproj"" -- FAILED.
161>------ Build started: Project: io_ops_gen_python, Configuration: Release x64 ------
121>relu_op.cc
121>remote_fused_graph_execute_op.cc
161>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
161>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>remote_fused_graph_execute_utils.cc
121>C:\tensorflow\tensorflow\core\kernels\remote_fused_graph_execute_op.cc(74): warning C4373: 'tensorflow::RemoteFusedGraphExecuteOp::Compute': virtual function overrides 'tensorflow::OpKernel::Compute', previous versions of the compiler did not override when parameters only differed by const/volatile qualifiers
121>C:\tensorflow\tensorflow/core/framework/op_kernel.h(102): note: see declaration of 'tensorflow::OpKernel::Compute'
121>repeat_dataset_op.cc
121>reshape_op.cc
161>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
161>Done building project ""io_ops_gen_python.vcxproj"" -- FAILED.
162>------ Build started: Project: linalg_ops_gen_python, Configuration: Release x64 ------
121>resize_area_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\remote_fused_graph_execute_utils.cc)
121>resize_bicubic_op.cc
121>resize_bilinear_op.cc
121>resize_nearest_neighbor_op.cc
121>resource_variable_ops.cc
121>restore_op.cc
121>reverse_op.cc
121>reverse_sequence_op.cc
121>C:\tensorflow\tensorflow/core/util/tensor_slice_writer.h(36): fatal error C1083: Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\restore_op.cc)
121>sample_distorted_bounding_box_op.cc
121>save_op.cc
121>save_restore_tensor.cc
163>------ Build started: Project: logging_ops_gen_python, Configuration: Release x64 ------
163>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>C:\tensorflow\tensorflow/core/util/tensor_slice_writer.h(36): fatal error C1083: Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\save_op.cc)
121>save_restore_v2_ops.cc
163>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>C:\tensorflow\tensorflow/core/util/tensor_slice_writer.h(36): fatal error C1083: Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\save_restore_tensor.cc)
121>scan_ops.cc
163>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
163>Done building project ""logging_ops_gen_python.vcxproj"" -- FAILED.
164>------ Build started: Project: lookup_ops_gen_python, Configuration: Release x64 ------
121>C:\tensorflow\tensorflow/core/util/tensor_slice_writer.h(36): fatal error C1083: Cannot open include file: 'tensorflow/core/util/saved_tensor_slice.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\save_restore_v2_ops.cc)
121>scatter_functor.cc
121>scatter_nd_op.cc
164>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
164>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>scatter_nd_op_cpu_impl_0.cc
121>scatter_nd_op_cpu_impl_1.cc
164>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
164>Done building project ""lookup_ops_gen_python.vcxproj"" -- FAILED.
165>------ Build started: Project: math_ops_gen_python, Configuration: Release x64 ------
121>scatter_nd_op_cpu_impl_2.cc
121>scatter_nd_op_cpu_impl_3.cc
121>scatter_nd_op_cpu_impl_4.cc
121>scatter_nd_op_cpu_impl_5.cc
121>scatter_op.cc
166>------ Build started: Project: nn_ops_gen_python, Configuration: Release x64 ------
121>sdca_internal.cc
121>sdca_ops.cc
121>segment_reduction_ops.cc
121>self_adjoint_eig_op.cc
121>self_adjoint_eig_v2_op_complex128.cc
166>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
166>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>self_adjoint_eig_v2_op_complex64.cc
166>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>self_adjoint_eig_v2_op_double.cc
166>Done building project ""nn_ops_gen_python.vcxproj"" -- FAILED.
167>------ Build started: Project: parsing_ops_gen_python, Configuration: Release x64 ------
167>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
167>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>self_adjoint_eig_v2_op_float.cc
121>self_adjoint_eig_v2_op_gpu.cc
121>sendrecv_ops.cc
121>sequence_ops.cc
121>serialize_sparse_op.cc
167>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
167>Done building project ""parsing_ops_gen_python.vcxproj"" -- FAILED.
168>------ Build started: Project: random_ops_gen_python, Configuration: Release x64 ------
121>session_ops.cc
121>set_kernels.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\session_ops.cc)
121>shape_ops.cc
121>shuffle_dataset_op.cc
121>skip_dataset_op.cc
168>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
168>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>slice_op.cc
121>slice_op_cpu_impl_1.cc
121>slice_op_cpu_impl_2.cc
121>slice_op_cpu_impl_3.cc
121>slice_op_cpu_impl_4.cc
121>slice_op_cpu_impl_5.cc
168>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
168>Done building project ""random_ops_gen_python.vcxproj"" -- FAILED.
169>------ Build started: Project: remote_fused_graph_ops_gen_python, Configuration: Release x64 ------
169>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
169>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>slice_op_cpu_impl_6.cc
121>slice_op_cpu_impl_7.cc
121>sloppy_interleave_dataset_op.cc
121>softmax_op.cc
169>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
169>Done building project ""remote_fused_graph_ops_gen_python.vcxproj"" -- FAILED.
170>------ Build started: Project: resource_variable_ops_gen_python, Configuration: Release x64 ------
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\sloppy_interleave_dataset_op.cc)
121>softplus_op.cc
121>softsign_op.cc
121>spacetobatch_functor.cc
170>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>spacetobatch_op.cc
170>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
170>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
170>Done building project ""resource_variable_ops_gen_python.vcxproj"" -- FAILED.
171>------ Build started: Project: script_ops_gen_python, Configuration: Release x64 ------
121>C:\tensorflow\tensorflow\core\kernels\spacetobatch_op.cc(196): warning C4002: too many actual parameters for macro 'TF_SPACETOBATCH_BLOCK_DIMS_CASE'
121>spacetodepth_op.cc
121>sparse_add_grad_op.cc
121>sparse_add_op.cc
121>sparse_concat_op.cc
171>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
171>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sparse_conditional_accumulator_op.cc
121>sparse_cross_op.cc
171>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
171>Done building project ""script_ops_gen_python.vcxproj"" -- FAILED.
172>------ Build started: Project: sdca_ops_gen_python, Configuration: Release x64 ------
121>sparse_dense_binary_op_shared.cc
172>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>sparse_fill_empty_rows_op.cc
172>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sparse_matmul_op.cc
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\sparse_matmul_op.cc)
121>sparse_reduce_op.cc
121>sparse_reorder_op.cc
172>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>sparse_reshape_op.cc
172>Done building project ""sdca_ops_gen_python.vcxproj"" -- FAILED.
173>------ Build started: Project: contrib_image_ops_gen_python, Configuration: Release x64 ------
173>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>sparse_slice_op.cc
173>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sparse_softmax_op.cc
121>sparse_sparse_binary_op_shared.cc
173>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
173>Done building project ""contrib_image_ops_gen_python.vcxproj"" -- FAILED.
174>------ Build started: Project: set_ops_gen_python, Configuration: Release x64 ------
121>sparse_split_op.cc
121>sparse_tensor_dense_add_op.cc
174>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
174>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sparse_tensor_dense_matmul_op.cc
121>sparse_tensor_slice_dataset_op.cc
174>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
174>Done building project ""set_ops_gen_python.vcxproj"" -- FAILED.
175>------ Build started: Project: sparse_ops_gen_python, Configuration: Release x64 ------
121>sparse_tensors_map_ops.cc
121>sparse_to_dense_op.cc
121>sparse_xent_op.cc
175>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>spectrogram.cc
121>spectrogram_op.cc
121>split_lib_cpu.cc
175>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
175>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
175>Done building project ""sparse_ops_gen_python.vcxproj"" -- FAILED.
121>split_op.cc
176>------ Build started: Project: spectral_ops_gen_python, Configuration: Release x64 ------
121>split_v_op.cc
121>driver_manager.cc
176>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
176>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sqlite_query_connection.cc
176>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
176>Done building project ""spectral_ops_gen_python.vcxproj"" -- FAILED.
177>------ Build started: Project: state_ops_gen_python, Configuration: Release x64 ------
121>sql_dataset_ops.cc
121>stack_ops.cc
177>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>stage_op.cc
121>stateless_random_ops.cc
177>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\core\kernels\stack_ops.cc)
121>strided_slice_op.cc
121>strided_slice_op_define_grad.cc
121>strided_slice_op_inst_0.cc
177>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>strided_slice_op_inst_1.cc
177>Done building project ""state_ops_gen_python.vcxproj"" -- FAILED.
178>------ Build started: Project: stateless_random_ops_gen_python, Configuration: Release x64 ------
121>strided_slice_op_inst_2.cc
178>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
178>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>strided_slice_op_inst_3.cc
178>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
178>Done building project ""stateless_random_ops_gen_python.vcxproj"" -- FAILED.
179>------ Build started: Project: array_ops_gen_python, Configuration: Release x64 ------
179>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
179>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
179>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
179>Done building project ""array_ops_gen_python.vcxproj"" -- FAILED.
180>------ Build started: Project: string_ops_gen_python, Configuration: Release x64 ------
180>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
180>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>strided_slice_op_inst_4.cc
180>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
180>Done building project ""string_ops_gen_python.vcxproj"" -- FAILED.
181>------ Build started: Project: bitwise_ops_gen_python, Configuration: Release x64 ------
181>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
181>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
181>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
181>Done building project ""bitwise_ops_gen_python.vcxproj"" -- FAILED.
182>------ Build started: Project: candidate_sampling_ops_gen_python, Configuration: Release x64 ------
121>strided_slice_op_inst_5.cc
182>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
182>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
182>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
182>Done building project ""candidate_sampling_ops_gen_python.vcxproj"" -- FAILED.
183>------ Build started: Project: checkpoint_ops_gen_python, Configuration: Release x64 ------
121>strided_slice_op_inst_6.cc
121>strided_slice_op_inst_7.cc
121>string_join_op.cc
183>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
183>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
183>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
183>Done building project ""checkpoint_ops_gen_python.vcxproj"" -- FAILED.
121>string_split_op.cc
121>string_to_hash_bucket_op.cc
121>string_to_number_op.cc
121>substr_op.cc
121>summary_audio_op.cc
121>summary_image_op.cc
121>summary_interface.cc
184>------ Build started: Project: contrib_bigquery_reader_ops_gen_python, Configuration: Release x64 ------
121>summary_kernels.cc
121>summary_op.cc
121>summary_tensor_op.cc
121>svd_op_complex128.cc
121>svd_op_complex64.cc
121>svd_op_double.cc
184>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
184>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>svd_op_float.cc
184>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>take_dataset_op.cc
184>Done building project ""contrib_bigquery_reader_ops_gen_python.vcxproj"" -- FAILED.
185>------ Build started: Project: contrib_boosted_trees_model_ops_gen_python, Configuration: Release x64 ------
121>tensor_array.cc
185>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>tensor_array_ops.cc
185>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>tensor_dataset_op.cc
185>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
185>Done building project ""contrib_boosted_trees_model_ops_gen_python.vcxproj"" -- FAILED.
186>------ Build started: Project: contrib_boosted_trees_prediction_ops_gen_python, Configuration: Release x64 ------
186>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
186>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>tensor_slice_dataset_op.cc
186>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
186>Done building project ""contrib_boosted_trees_prediction_ops_gen_python.vcxproj"" -- FAILED.
187>------ Build started: Project: contrib_boosted_trees_quantiles_ops_gen_python, Configuration: Release x64 ------
187>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>text_line_reader_op.cc
187>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>tf_record_reader_op.cc
187>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
187>Done building project ""contrib_boosted_trees_quantiles_ops_gen_python.vcxproj"" -- FAILED.
188>------ Build started: Project: contrib_boosted_trees_split_handler_ops_gen_python, Configuration: Release x64 ------
188>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
188>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>tile_functor_cpu.cc
121>tile_ops.cc
188>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
188>Done building project ""contrib_boosted_trees_split_handler_ops_gen_python.vcxproj"" -- FAILED.
189>------ Build started: Project: contrib_boosted_trees_stats_accumulator_ops_gen_python, Configuration: Release x64 ------
121>tile_ops_cpu_impl_1.cc
189>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
189>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
189>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
189>Done building project ""contrib_boosted_trees_stats_accumulator_ops_gen_python.vcxproj"" -- FAILED.
190>------ Build started: Project: contrib_boosted_trees_training_ops_gen_python, Configuration: Release x64 ------
190>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
190>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
190>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>tile_ops_cpu_impl_2.cc
190>Done building project ""contrib_boosted_trees_training_ops_gen_python.vcxproj"" -- FAILED.
191>------ Build started: Project: contrib_cudnn_rnn_ops_gen_python, Configuration: Release x64 ------
121>c:\tensorflow\tensorflow\contrib\cmake\build\external\eigen_archive\eigen\src/Core/products/GeneralMatrixVector.h(131): fatal error C1060: compiler is out of heap space (compiling source file C:\tensorflow\tensorflow\core\kernels\svd_op_complex128.cc)
121>tile_ops_cpu_impl_3.cc
121>tile_ops_cpu_impl_4.cc
121>tile_ops_cpu_impl_5.cc
191>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
191>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
191>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
191>Done building project ""contrib_cudnn_rnn_ops_gen_python.vcxproj"" -- FAILED.
192>------ Build started: Project: contrib_factorization_clustering_ops_gen_python, Configuration: Release x64 ------
121>tile_ops_cpu_impl_6.cc
121>tile_ops_cpu_impl_7.cc
121>topk_op.cc
121>training_op_helpers.cc
192>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
192>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>transpose_functor_cpu.cc
121>transpose_op.cc
121>unique_op.cc
192>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
192>Done building project ""contrib_factorization_clustering_ops_gen_python.vcxproj"" -- FAILED.
193>------ Build started: Project: contrib_factorization_factorization_ops_gen_python, Configuration: Release x64 ------
121>unpack_op.cc
121>warn_about_ints.cc
121>where_op.cc
121>whole_file_read_ops.cc
193>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
193>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>window_dataset.cc
121>word2vec_kernels.cc
121>xent_op.cc
121>xsmm_conv2d.cc
121>zip_dataset_op.cc
121>batch_features.cc
121>dropout_utils.cc
121>examples_iterable.cc
193>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
193>Done building project ""contrib_factorization_factorization_ops_gen_python.vcxproj"" -- FAILED.
194>------ Build started: Project: contrib_framework_variable_ops_gen_python, Configuration: Release x64 ------
121>parallel_for.cc
121>sparse_column_iterable.cc
121>tensor_utils.cc
121>example_partitioner.cc
121>bias-feature-column-handler.cc
121>categorical-feature-column-handler.cc
121>dense-quantized-feature-column-handler.cc
194>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
194>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>sparse-quantized-feature-column-handler.cc
121>multiple_additive_trees.cc
121>decision_tree.cc
121>masked_matmul_ops.cc
194>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
194>Done building project ""contrib_framework_variable_ops_gen_python.vcxproj"" -- FAILED.
195>------ Build started: Project: contrib_image_distort_image_ops_gen_python, Configuration: Release x64 ------
121>wals_solver_ops.cc
121>factorization_ops.cc
196>------ Build started: Project: contrib_image_sirds_ops_gen_python, Configuration: Release x64 ------
121>zero_initializer_op.cc
196>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
196>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>adjust_hsv_in_yiq_op.cc
121>bipartite_match_op.cc
196>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
196>Done building project ""contrib_image_sirds_ops_gen_python.vcxproj"" -- FAILED.
121>distort_image_ops.cc
121>sparse_feature_cross_kernel.cc
121>sparse_feature_cross_op.cc
121>resampler_ops.cc
197>------ Build started: Project: contrib_input_pipeline_ops_gen_python, Configuration: Release x64 ------
121>tensor_forest_ops.cc
121>reinterpret_string_to_float_op.cc
197>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
197>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>scatter_add_ndim_op.cc
121>tree_utils.cc
121>hard_routing_function_op.cc
121>k_feature_gradient_op.cc
121>k_feature_routing_function_op.cc
197>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
197>Done building project ""contrib_input_pipeline_ops_gen_python.vcxproj"" -- FAILED.
198>------ Build started: Project: contrib_layers_sparse_feature_cross_ops_gen_python, Configuration: Release x64 ------
121>routing_function_op.cc
121>routing_gradient_op.cc
121>stochastic_hard_routing_function_op.cc
121>stochastic_hard_routing_gradient_op.cc
198>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
198>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>unpack_path_op.cc
121>utils.cc
198>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
198>Done building project ""contrib_layers_sparse_feature_cross_ops_gen_python.vcxproj"" -- FAILED.
199>------ Build started: Project: contrib_memory_stats_ops_gen_python, Configuration: Release x64 ------
121>skip_gram_kernels.cc
199>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
121>skip_gram_ops.cc
199>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>cross_replica_ops.cc
121>infeed_ops.cc
121>outfeed_ops.cc
199>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
121>replication_ops.cc
199>Done building project ""contrib_memory_stats_ops_gen_python.vcxproj"" -- FAILED.
200>------ Build started: Project: contrib_nccl_ops_gen_python, Configuration: Release x64 ------
200>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
200>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
121>tpu_configuration_ops.cc
200>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
200>Done building project ""contrib_nccl_ops_gen_python.vcxproj"" -- FAILED.
201>------ Build started: Project: contrib_nearest_neighbor_ops_gen_python, Configuration: Release x64 ------
201>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
201>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
201>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
201>Done building project ""contrib_nearest_neighbor_ops_gen_python.vcxproj"" -- FAILED.
202>------ Build started: Project: contrib_resampler_ops_gen_python, Configuration: Release x64 ------
202>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
202>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
202>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
202>Done building project ""contrib_resampler_ops_gen_python.vcxproj"" -- FAILED.
203>------ Build started: Project: contrib_rnn_gru_ops_gen_python, Configuration: Release x64 ------
203>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
203>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
203>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
203>Done building project ""contrib_rnn_gru_ops_gen_python.vcxproj"" -- FAILED.
204>------ Build started: Project: contrib_rnn_lstm_ops_gen_python, Configuration: Release x64 ------
204>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
204>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
204>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
204>Done building project ""contrib_rnn_lstm_ops_gen_python.vcxproj"" -- FAILED.
205>------ Build started: Project: contrib_seq2seq_beam_search_ops_gen_python, Configuration: Release x64 ------
205>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
205>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
205>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
205>Done building project ""contrib_seq2seq_beam_search_ops_gen_python.vcxproj"" -- FAILED.
206>------ Build started: Project: training_ops_gen_python, Configuration: Release x64 ------
206>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
206>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
206>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
206>Done building project ""training_ops_gen_python.vcxproj"" -- FAILED.
207>------ Build started: Project: contrib_tensor_forest_hybrid_ops_gen_python, Configuration: Release x64 ------
207>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
207>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
207>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
207>Done building project ""contrib_tensor_forest_hybrid_ops_gen_python.vcxproj"" -- FAILED.
208>------ Build started: Project: user_ops_gen_python, Configuration: Release x64 ------
208>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
208>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
208>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
208>Done building project ""user_ops_gen_python.vcxproj"" -- FAILED.
209>------ Build started: Project: contrib_tensor_forest_model_ops_gen_python, Configuration: Release x64 ------
209>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
209>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
209>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
209>Done building project ""contrib_tensor_forest_model_ops_gen_python.vcxproj"" -- FAILED.
210>------ Build started: Project: contrib_tensor_forest_ops_gen_python, Configuration: Release x64 ------
210>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
210>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
210>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_eager_op_gen.obj'
210>Done building project ""contrib_tensor_forest_ops_gen_python.vcxproj"" -- FAILED.
211>------ Build started: Project: tf_python_ops, Configuration: Release x64 ------
211>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
211>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
211>Generating tf_python/tensorflow/python/ops/gen_array_ops.py
211>'Release\array_ops_gen_python.exe' is not recognized as an internal or external command,
211>operable program or batch file.
211>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 9009.
211>Done building project ""tf_python_ops.vcxproj"" -- FAILED.
212>------ Build started: Project: tf_contrib_reduce_slice_ops_ops, Configuration: Release x64 ------
212>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
212>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
212>reduce_slice_ops.cc
212>tf_contrib_reduce_slice_ops_ops.vcxproj -> C:\tensorflow\tensorflow\contrib\cmake\build\tf_contrib_reduce_slice_ops_ops.dir\Release\tf_contrib_reduce_slice_ops_ops.lib
213>------ Build started: Project: tf_contrib_tpu_ops, Configuration: Release x64 ------
213>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
213>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
213>cross_replica_ops.cc
213>infeed_ops.cc
213>outfeed_ops.cc
213>replication_ops.cc
213>tpu_configuration_ops.cc
213>tf_contrib_tpu_ops.vcxproj -> C:\tensorflow\tensorflow\contrib\cmake\build\tf_contrib_tpu_ops.dir\Release\tf_contrib_tpu_ops.lib
121>Done building project ""tf_core_kernels.vcxproj"" -- FAILED.
214>------ Build started: Project: tf_tools_transform_graph_lib, Configuration: Release x64 ------
215>------ Build started: Project: benchmark_model, Configuration: Release x64 ------
214>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
215>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
214>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
215>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
215>benchmark_model.cc
215>benchmark_model_main.cc
214>add_default_attributes.cc
214>backports.cc
214>fake_quantize_training.cc
214>file_utils.cc
214>flatten_atrous.cc
214>fold_batch_norms.cc
214>fold_constants_lib.cc
214>fold_old_batch_norms.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\backports.cc)
214>freeze_requantization_ranges.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\fold_constants_lib.cc)
214>fuse_convolutions.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\fold_old_batch_norms.cc)
214>insert_logging.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\fold_batch_norms.cc)
214>obfuscate_names.cc
214>remove_attribute.cc
214>remove_device.cc
214>remove_ema.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\remove_attribute.cc)
214>remove_nodes.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\fuse_convolutions.cc)
214>rename_attribute.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\remove_device.cc)
214>rename_op.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\obfuscate_names.cc)
214>set_device.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\insert_logging.cc)
214>sort_by_execution_order.cc
214>sparsify_gather.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\remove_nodes.cc)
214>strip_unused_nodes.cc
214>transform_graph.cc
214>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\sparsify_gather.cc)
214>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\sparsify_gather.cc)
214>transform_utils.cc
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\rename_op.cc)
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\rename_attribute.cc)
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\sort_by_execution_order.cc)
214>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\tools\graph_transforms\strip_unused_nodes.cc)
215>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_core_cpu.dir\Release\accumulate_n_optimizer.obj'
215>Done building project ""benchmark_model.vcxproj"" -- FAILED.
216>------ Build started: Project: tf_tutorials_example_trainer, Configuration: Release x64 ------
216>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
216>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
216>example_trainer.cc
216>C:\tensorflow\tensorflow/cc/ops/standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory
216>Done building project ""tf_tutorials_example_trainer.vcxproj"" -- FAILED.
217>------ Build started: Project: tf_label_image_example, Configuration: Release x64 ------
217>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
217>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
217>main.cc
217>C:\tensorflow\tensorflow\examples\label_image\main.cc(42): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/image_ops.h': No such file or directory
214>Done building project ""tf_tools_transform_graph_lib.vcxproj"" -- FAILED.
218>------ Build started: Project: pywrap_tensorflow_internal_static, Configuration: Release x64 ------
217>Done building project ""tf_label_image_example.vcxproj"" -- FAILED.
219>------ Build started: Project: compare_graphs, Configuration: Release x64 ------
219>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
219>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
218>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
219>compare_graphs.cc
218>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
218>Generating __force_rebuild
218>
218>Running SWIG to generate Python wrappers
218>print_model_analysis.cc
218>pywrap_tensor.cc
218>pywrap_tfe_src.cc
218>tf_session_helper.cc
218>python_eager_op_gen.cc
218>cpp_shape_inference.cc
218>python_op_gen.cc
218>numpy.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\client\tf_session_helper.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\client\tf_session_helper.cc)
218>ndarray_tensor.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
218>C:\tensorflow\tensorflow\python\framework\python_op_gen.cc(23): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
218>ndarray_tensor_bridge.cc
218>py_func.cc
218>C:\tensorflow\tensorflow\python\eager\python_eager_op_gen.cc(22): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
218>py_seq_tensor.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
219>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_tools_transform_graph_lib.dir\Release\backports.obj'
218>safe_ptr.cc
219>Done building project ""compare_graphs.vcxproj"" -- FAILED.
218>py_record_reader.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
218>py_record_writer.cc
220>------ Build started: Project: summarize_graph, Configuration: Release x64 ------
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
218>kernel_registry.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
218>ops.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_func.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_func.cc)
218>scope.cc
218>pywrap_tensorflow_internal.cc
218>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
218>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
218>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\cc\framework\scope.cc)
218>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
221>------ Build started: Project: transform_graph, Configuration: Release x64 ------
221>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
221>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
221>transform_graph_main.cc
218>Done building project ""pywrap_tensorflow_internal_static.vcxproj"" -- FAILED.
222>------ Build started: Project: pywrap_tensorflow_internal, Configuration: Release x64 ------
222>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
222>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
222>Generating __force_rebuild
222>
222>Running SWIG to generate Python wrappers
222>print_model_analysis.cc
222>pywrap_tensor.cc
222>pywrap_tfe_src.cc
222>tf_session_helper.cc
222>python_eager_op_gen.cc
222>cpp_shape_inference.cc
222>python_op_gen.cc
222>numpy.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tfe_src.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\client\tf_session_helper.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\client\tf_session_helper.cc)
222>ndarray_tensor.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\eager\pywrap_tensor.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\framework\cpp_shape_inference.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor.cc)
221>LINK : fatal error LNK1181: cannot open input file 'C:\tensorflow\tensorflow\contrib\cmake\build\tf_tools_transform_graph_lib.dir\Release\backports.obj'
221>Done building project ""transform_graph.vcxproj"" -- FAILED.
222>C:\tensorflow\tensorflow\python\eager\python_eager_op_gen.cc(22): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
222>ndarray_tensor_bridge.cc
222>C:\tensorflow\tensorflow\python\framework\python_op_gen.cc(23): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/op_def.pb_text.h': No such file or directory
222>py_func.cc
222>py_seq_tensor.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\ndarray_tensor_bridge.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\core\profiler\internal\print_model_analysis.cc)
222>safe_ptr.cc
222>py_record_reader.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_reader.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\safe_ptr.cc)
222>py_record_writer.cc
222>kernel_registry.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\io\py_record_writer.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_seq_tensor.cc)
222>ops.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_func.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\python\lib\core\py_func.cc)
222>scope.cc
222>pywrap_tensorflow_internal.cc
222>C:\tensorflow\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
222>C:\tensorflow\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
222>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\cc\framework\scope.cc)
222>C:\tensorflow\tensorflow/core/common_runtime/device.h(37): fatal error C1083: Cannot open include file: 'tensorflow/core/framework/device_attributes.pb_text.h': No such file or directory (compiling source file C:\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.cc)
222>Done building project ""pywrap_tensorflow_internal.vcxproj"" -- FAILED.
223>------ Build started: Project: _nearest_neighbor_ops, Configuration: Release x64 ------
224>------ Build started: Project: _lstm_ops, Configuration: Release x64 ------
224>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
223>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
223>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
224>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
224>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
224>blas_gemm.cc
223>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
223>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
223>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
223>hyperplane_lsh_probes.cc
223>nearest_neighbor_ops.cc
224>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
224>lstm_ops.cc
223>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
223>Done building project ""_nearest_neighbor_ops.vcxproj"" -- FAILED.
225>------ Build started: Project: _gru_ops, Configuration: Release x64 ------
225>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
225>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
225>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
225>blas_gemm.cc
225>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
225>gru_ops.cc
224>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
224>lstm_ops.cc
225>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
225>gru_ops.cc
224>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
224>Done building project ""_lstm_ops.vcxproj"" -- FAILED.
226>------ Build started: Project: _beam_search_ops, Configuration: Release x64 ------
226>Building Custom Rule C:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
226>CMake does not need to re-run because C:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to-date.
226>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
226>beam_search_ops.cc
225>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
225>Done building project ""_gru_ops.vcxproj"" -- FAILED.
226>cl : Command line warning D9025: overriding '/DTF_COMPILE_LIBRARY' with '/UTF_COMPILE_LIBRARY'
226>beam_search_ops.cc
226>LINK : fatal error LNK1181: cannot open input file 'Release\pywrap_tensorflow_internal.lib'
226>Done building project ""_beam_search_ops.vcxproj"" -- FAILED.
```


"
17002,Please add a working example for tf.contrib.factorization.KMeansClustering in tutorial example,"### Describe the problem
Hi, 

I have been trying to use tf.contrib.factorization.KMeansClustering for clustering. I got the documentation in [link](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering). But it did not specify how to give input. There is no input argument here : 

__init__(
    num_clusters,
    model_dir=None,
    initial_clusters=RANDOM_INIT,
    distance_metric=SQUARED_EUCLIDEAN_DISTANCE,
    random_seed=0,
    use_mini_batch=True,
    mini_batch_steps_per_iteration=1,
    kmeans_plus_plus_num_retries=2,
    relative_tolerance=None,
    config=None
)

So I request you to please add a working example to train and predict using kmeansClustring in tutorial example.

Thank You
"
17001,Minor issue with TF Lite building natively on aarch64,"Linux qds101 4.4.65 #1 SMP PREEMPT Mon Sep 18 13:34:00 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609


== uname -a =====================================================
Linux qds101 4.4.65 #1 SMP PREEMPT Mon Sep 18 13:34:00 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc1-1607-gbe4e5ef
tf.COMPILER_VERSION = v1.3.0-rc1-1607-gbe4e5ef

### Describe the problem
When I try to compile TF Lite example on my aarch64 machine, i get an error from gcc. 
The problem, i think, is that kernels/internal/BUILD file does not support aarch64 properly:
NEON_FLAGS_IF_APPLICABLE = select({
    "":arm"": [
        ""-O3"",
        ""-mfpu=neon"",
        ""-mfloat-abi=softfp"",

But for aarch64 gcc those options do not exist (https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html#AArch64-Options). If I remove those two options, compilation is successful.

### Source code / logs
user@qds101:~/ml/tf_lite/tensorflow$ bazel build --verbose_failures --config opt  --cxxopt=-std=c++11 --config monolithic  //tensorflow/contrib/lite/examples/label_image:label_image 
WARNING: /home/user/.cache/bazel/_bazel_user/5b7d3a270696b210c6b4e035929a0ce1/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/5b7d3a270696b210c6b4e035929a0ce1/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions
INFO: Analysed target //tensorflow/contrib/lite/examples/label_image:label_image (1 packages loaded).
INFO: Found 1 target...
ERROR: /home/user/ml/tf_lite/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:308:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1): gcc failed: error executing command 
  (cd /home/user/.cache/bazel/_bazel_user/5b7d3a270696b210c6b4e035929a0ce1/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/user/bin:/home/user/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=native' '-std=c++0x' '-std=c++11' -MD -MF bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.d '-frandom-seed=bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o' -iquote . -iquote bazel-out/arm-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/arm-opt/genfiles/external/bazel_tools -iquote external/arm_neon_2_x86_sse -iquote bazel-out/arm-opt/genfiles/external/arm_neon_2_x86_sse -iquote external/gemmlowp -iquote bazel-out/arm-opt/genfiles/external/gemmlowp -isystem external/bazel_tools/tools/cpp/gcc3 -O3 '-mfpu=neon' '-mfloat-abi=softfp' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/contrib/lite/kernels/internal/tensor_utils.cc -o bazel-out/arm-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o)
gcc: error: unrecognized command line option '-mfpu=neon'
gcc: error: unrecognized command line option '-mfloat-abi=softfp'
Target //tensorflow/contrib/lite/examples/label_image:label_image failed to build
INFO: Elapsed time: 1.052s, Critical Path: 0.03s
FAILED: Build did NOT complete successfully
"
16999,Feature Request: Add Edit Distance metric_fn to tf.metrics or Make a way to Pass Custom metric_fn outside tf.metrics,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.4 CPU
- **Python version**: 
3.6
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A


### Describe the problem
I would like another metric (edit_distance) to be added into tf.metrics for `label_error_rate` calculation. I'm having trouble passing metric dicts for `eval_metric_ops` to be used by `ModelFnOps` that are not part of  tf.metrics. 

If that isn't possible, would it be possible for us we can pass our own `metric_fn`s that are outside tf.metrics? 

### Source code / logs
This is the metric dict I would like to pass as the `eval_metric_ops` for `ModelFnOps`:

`metric = {""label_error_rate"": tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true), name=""label_error_rate"")}`

Where `y_pred` and `y_true` are sparse.
"
16997,permission request,"Greetings team
Can I volunteer to be a member of the tensorflow / tensorflow team
if yes I am very happy to join and help, I speak Indonesian, maybe I can help a team that does not understand Indonesian or so forth.

Greetings : raja-rizki

thank you :)"
16995,tf.fake_quant_with_min_max_vars returns wrong answer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA-8.0 CuDNN 6.0
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**: 


### Describe the problem
tf.fake_quant_with_min_max_vars returns wrong answer.


### Source code / logs
```python
import tensorflow as tf

a =tf.Variable([ 0.09504107, 0.0748544, 0.09333218, 0.106306, 0.09921047, 0.0930253, 0.09277194, 0.08704954, 0.12734564, 0.11479893], dtype=tf.float32)


sess = tf.InteractiveSession()
sess.run(tf.initialize_variables([a]))

print a.eval()
print tf.fake_quant_with_min_max_vars(inputs=a, min=tf.reduce_min(a), max=tf.reduce_max(a), num_bits=8).eval()
print tf.reduce_min(a).eval()
print tf.reduce_max(a).eval()
```

it prints like below
```
[ 0.09504107  0.0748544   0.09333218  0.106306    0.09921047  0.0930253
  0.09277194  0.08704954  0.12734564  0.11479893]
[ 0.05249124  0.05249124  0.05249124  0.05249124  0.05249124  0.05249124
  0.05249124  0.05249124  0.05249124  0.05249124]
0.0748544
0.127346
```"
16990,[Feature request] Adding scaffold parameter estimator heads,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
When creating custom estimators, using the Heads defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py) reduces an important part of the boilerplate as well as guarantee uniform evaluation against canned estimator instances. However, one key parameter is missing in in `create_estimator_spec` function, which is the `scaffold` that one can pass into an `EstimatorSpec`. One example where that is needed is if you want to initialize a large tensor with a numpy array, for example for loading an embedding file from word2vec. This [StackOverflow question](https://stackoverflow.com/questions/44680769/loading-pre-trained-word2vec-to-initialise-embedding-lookup-in-the-estimator-mod) also describes the issue.

### Source code / logs
This is my normal code, currently impossible to use with heads. Actually, it might not be impossible, but this seems to me like the cleaner solution. I have the [tiny PR](https://github.com/eisenjulian/tensorflow/commit/9e6c004f07d2ba5c17d58007092f513b54077198) ready if you think this is a valuable change. Thanks a lot for your time!

```python
def model_fn(mode, features, labels, hparams):
  embed_ph = tf.placeholder(
      shape=[hparams.vocab_size, hparams.embedding_size], 
      dtype=tf.float32)
  embeddings = tf.Variable(embed_ph)
  # Define your model
  return tf.estimator.EstimatorSpec(
      ..., # normal EstimatorSpec args
      scaffold=tf.train.Scaffold(init_feed_dict={embed_ph: my_embedding_numpy_array})
  )
```
"
16988,TensorFlow compile from source with GPU support error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master version or 1.6.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: NVIDIA Tesla K80
- **Exact command to reproduce**: bazel build  --config=opt --config=cuda tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""

### Describe the problem
I have tried to compile the master version from source, I've added all the env variables, been through stackoverflow and github issues, nothing works, I think it's a bug.

### Source code / logs
ERROR: /home/ubuntu/work/master/tensorflow/tensorflow/contrib/periodic_resample/BUILD:40:1: Linking of rule '//tensorflow/contrib/periodic_resample:gen_gen_periodic_resample_op_py_py_wrappers_cc' failed (Exit 1)
/usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'

and a lot more going"
16986,ParseSingleExample op is missing from op_def_registry.get_registered_ops(),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip install
- **TensorFlow version (use command below)**: cpu 1.5.0
- **Python version**: 3.0

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

**ParseSingleSample** is available as tf.parse_single_example() method call, but it is not listed in the op_def_registry.get_registered_ops()

Which cause freeze_graph.py call to fail with following error:
ValueError: No op named ParseSingleExample in defined operations.

When converting mobilenet model with training information.

"
16985,undefined symbol: PyUnicodeUCS4_FromString   ,"I tried to install tensorflow cpu version using python 2.7 on Ubuntu16.04 under virtualenv. but when I want to import tensorflow, the error is :

`(My_python2) yuan@ubuntu:~$ python
Python 2.7.13 (default, Feb 13 2018, 14:17:11) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_FromString


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Please help! Many thanks"
16983,Importing graph with tf.contrib.resampler.resampler fails,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSx High Sierra
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
CPU
- **Bazel version (if compiling from source)**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Importing a graph def with a  `tf.contrib.resampler.resampler` op fails iff `tf.contrib` is not imported first.

Execute:
```
import tensorflow as tf

def export_model(filename, sess, output_node_names):
    from tensorflow.python.framework import graph_util
    output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                                 sess.graph.as_graph_def(add_shapes=True),
                                                                 output_node_names)
    with tf.gfile.GFile(filename, ""wb"") as f:
        f.write(output_graph_def.SerializeToString())
        
def read_frozen_protobuf(path):
    with tf.gfile.FastGFile(str(path), 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        return graph_def

    
def export(filename):
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = tf.placeholder(dtype=tf.float64, shape=[32, 32], name='images')
        points = tf.placeholder(dtype=tf.float64, shape=[32, 2], name='points')
        resampled = tf.contrib.resampler.resampler(images, points, name='resampled')
        output_node_names = ['resampled/Resampler']
        export_model(filename, sess, output_node_names)
        
def load(filename):
    import numpy as np
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = np.zeros((32, 32), dtype=np.float64)
        points = np.zeros((32, 2), dtype=np.float64)
        graph_def = read_frozen_protobuf(filename)
        tf.import_graph_def(graph_def, 
                            input_map={'images': images,
                                       'points': points},
                            return_elements=['resampled/Resampler:0'])
        
######################################################
frozen_graph_def = '/tmp/test.frozen'
export(frozen_graph_def)
load(frozen_graph_def)
```

Then, in a new interpreter (where the load(..) function is defined), execute:
```
frozen_graph_def = '/tmp/test.frozen'
load(frozen_graph_def)
```

This give the error message:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-64251e160f7d> in <module>()
     42 frozen_graph_def = '/tmp/test.frozen'
     43 # export(frozen_graph_def)
---> 44 load(frozen_graph_def)

<ipython-input-1-64251e160f7d> in load(filename)
     37                             input_map={'images': images,
     38                                        'points': points},
---> 39                             return_elements=['resampled/Resampler:0'])
     40 
     41 ######################################################################

/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    314                 'in a future version' if date is None else ('after %s' % date),
    315                 instructions)
--> 316       return func(*args, **kwargs)
    317     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    318                                        _add_deprecated_arg_notice_to_docstring(

/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    539         # Set any default attr values that aren't present.
    540         if node.op not in op_dict:
--> 541           raise ValueError('No op named %s in defined operations.' % node.op)
    542         op_def = op_dict[node.op]
    543         for attr_def in op_def.attr:

ValueError: No op named Resampler in defined operations.
```"
16982,dnnConversionCreate_F32 fails when running TF with optimized MKL,"### System information
- **Have I written custom code**: Yes, (https://github.com/jakubkarczewski/AlexNetF/blob/master/alexnet.py)
- **OS Platform and Distribution**: Linux Centos 7
- **TensorFlow installed from**: compiled from source from https://github.com/tensorflow/tensorflow/releases
- **TensorFlow version**: 1.6.0-rc0
- **Python version**:  2.7
- **Bazel version**: 0.10.0
- **GCC/Compiler version**: stock Centos 7 gcc
- **Compilation command**: ```bazel build --config=mkl --copt=""-DINTEL_MKL_ML"" --copt=""-mfma"" --copt=""-mavx2"" --copt=""-march=broadwell"" --copt=""-O3"" -s -c opt //tensorflow/tools/pip_package:build_pip_package;;```
- **Exact command to reproduce**: python alexnet.py --training_epoch=1 --model_version=1 output/
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A


### Describe the problem
Running following training with Tensorflow compiled with command specified above results in error: ```2018-02-12 23:40:38.088756: F tensorflow/core/kernels/mkl_lrn_op.cc:595] Check failed: dnnConversionCreate_F32( &convert_input, static_cast<dnnLayout_t>(inimage_shape.GetCurLayout()), lt_internal_input) == E_SUCCESS (-1 vs. 0) ``` as opossed to running without any error and training properly on Tensorflow version available under ```pip install tensorflow```.
For training data I used Imagenet 60gb dataset (http://www.image-net.org/challenges/LSVRC/2012/) with 1000 classes.
What's more - following error can be found when running with Tensorflow from precompiled wheel files for both versions of python. This makes me think that the way I compile TF is not the problem here."
16981,module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions',"I have followed the TensorFlow tutorial, ""[Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition)""

When I was running **train.py**, I got this error message:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-4-61266873bc5f> in <module>()
     77 import numpy as np
     78 from six.moves import xrange  # pylint: disable=redefined-builtin
---> 79 import tensorflow as tf
     80 
     81 import input_data

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     56     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__
     60   from tensorflow.python.pywrap_tensorflow_internal import __git_version__

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
    102 def TFE_NewContextOptions():
    103     return _pywrap_tensorflow_internal.TFE_NewContextOptions()
--> 104 TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions
    105 
    106 def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):

AttributeError: module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'
```
So, I checked out **pywrap_tensorflow_internal.py** in the pyhton directory. 
This is the part of **pywrap_tensorflow_internal.py** that defines TFE_NewContextOptions.

```
def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):
    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig(options, proto, proto_len, status)
TFE_ContextOptionsSetConfig = _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig
TFE_DEVICE_PLACEMENT_EXPLICIT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT
TFE_DEVICE_PLACEMENT_WARN = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN
TFE_DEVICE_PLACEMENT_SILENT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT
TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32
```"
16980,"Tensorboard Error 404, path /[[_dataImageSrc]] not found","I am using the object detection API
and training a new ssd_mobilenet from scratch with my dataset.
I already did a successful retraining, but now i want to try a complete new training without checkpoint.

The training and evaluation job seem to run normal, but when i start tensorboard and open port 6006 in my browser,  the terminal where its running shows following error:
`
W0213 11:04:23.869396 Thread-2 application.py:273] path /[[_dataImageSrc]] not found, sending 404
`
and no scalars, except of the learning rate, are visualized in tensorboard.
In contrast to that Evaluation images, the graph, distributions and histogram are all shown correctly."
16979,"[Critical some questions] Tensorflow-lite, Neural Networks API","Hi.

**Some question.**

 **1.**

> Current i use  tensorflow library `  compile 'org.tensorflow:tensorflow-lite:+'`
> and i want to use Neural Networks Api, to reduce inference time.
> 
> but wrapper function is not existing in `Interpreter.java` class 
> `  private static native void useNNAPI(long var0, boolean var2);`
> so i can't use it..
> 
> If I create a wrapper function, can I use Neural Networks API? (my device level >  8.1)

What is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java
?

**2.**

> i'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb) 
> inference speed about 290 ms -> 73ms decreased

> Command line(mobileNet)

<pre><code>
bazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \
  //tensorflow/contrib/lite/toco:toco -- \
  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \
  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=1,224,224,3 \
  --input_array=input \
  --output_array=MobilenetV1/Predictions/Reshape_1 \
  --default_ranges_min=0 \
  --default_ranges_max=6 \
  --mean_value=127.5 \
  --std_value=127.5
</pre></code>

> In a similar way,
> But frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)
> inference speed about 11000 ms -> 11000ms. 
>  **If the model size is reduced, does not the inference time generally decrease?**

"
16976,df982b8de - Split gpu_id.h and GpuIdManager out from build target breaks build for verbs and GDR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: TF not compiling. (master)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: Any
- **Exact command to reproduce**: 

1. ./configure ... with GDR (and/or verbs)
2. bazel build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Commit df982b8de breaks the build for GDR and verbs. 
> ERROR: /home/eladw/google/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/contrib/gdr:gdr_memory_manager':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/gdr/gdr_memory_manager.cc':
  '/home/eladw/google/tensorflow/tensorflow/core/common_runtime/gpu/gpu_id.h'.
"
16975,libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX},"python
Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) 
[GCC 7.2.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems
"
16972,Tensorflow inference interface cannot be compiled from build.gradle,"When I enter the dependency named `compile 'org.tensorflow:tensorflow-android:+'` it produces the error named 

> Error:(44, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler

Even after adding all the required libraries and dependencies, the error still persists."
16971,Go API - graph.get_tensor_by_name,"
What is the equivalent of the Python graph.get_tensor_by_name in Go?

thanks
"
16969,Tensorflow in tornado,"The process stop in calculating and did not give any response.
just like
`[->] restore model
2018-02-13 15:27:49.235484: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 566c31076b9c7519 with config:`"
16967,No package nasm,"On Release version v1.5.0.
In the file tensorflow/workspace.bzl.
At the line 208.

The link [http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2]() is not exist any more.

"
16965,"Bug - Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)  Aborted (core dumped)","Hello ~

I got this error when use `tf.layers.conv2d`.
I use systems as following.

ubuntu 16.04
tensorflow 1.5.0
cuda 9.0
cudnn 7

and I executed my code on nvidia-docker conteiner.
(nvidia/cuda:9.0-cudnn7-runtime-ubuntu16.04)

"
16964,[Build Failed] Convolutional pose machine converts tflite format,"Hello.

I converting CPM(convolutional pose machine) PB file to TFLITE file, encounter error(s).

I success, converting from MobileNet v1 224 PB file to uint8 TFLITE file format (17.2 MB -> 4.3 MB)

**Just only this log.**
<pre><code>
WARNING: Config values are not defined in any .rc file: opt
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.205s, Critical Path: 0.00s
INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/danshin/tensorflow_lite/lite_model/frozen_cpm.pb' '--output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint8_cpm.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shape=1,368,368,3' '--input_array=images/Placeholder' '--output_array=stage6/confidence_maps/BiasAdd' '--default_ranges_min=0' '--default_ranges_max=6' '--mean_value=127.5' '--std_value=127.5' '--v=1'
2018-02-13 11:07:56.107251: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 324 operators, 444 arrays (0 quantized)
</code></pre>

**Command line**
<pre><code>
- bazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \
- //tensorflow/contrib/lite/toco:toco -- \
- --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_cpm.pb \
- --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint8_cpm.tflite \
- --input_format=TENSORFLOW_GRAPHDEF \
- --output_format=TFLITE \
- --inference_type=QUANTIZED_UINT8 \
- --inference_input_type=QUANTIZED_UINT8 \
- --input_shape=1,368,368,3 \
- --input_array=images/Placeholder \
- --output_array=stage6/confidence_maps/BiasAdd \
- --default_ranges_min=0 \
- --default_ranges_max=6 \
- --mean_value=127.5 \
- --std_value=127.5 \
- --v=1
</code></pre>

**Input node name description**
 Dim is -1, could this be a problem?
```
name: ""images/Placeholder""
op: ""Placeholder""
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: -1
      }
      dim {
        size: 368
      }
      dim {
        size: 368
      }
      dim {
        size: 3
      }
    }
  }
}
```"
16963,Improve transform_graph tool dependencies,"### System information
- **TensorFlow installed from (source or binary)**: source

Is there any way we can improve this dependency chain? The transform graph tool really shoudn't need to know about kernel implementations as far as I can tell.  Perhaps the appropriate place to cut this is at @org_tensorflow//tensorflow/core:tensorflow_opensource. Maintainer thoughts?
```
bazel query 'somepath(@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph,@org_tensorflow//tensorflow/core/kernels:strided_slice_op)'
@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph
@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph_main_lib
@org_tensorflow//tensorflow/tools/graph_transforms:transforms_lib
@org_tensorflow//tensorflow/core:tensorflow
@org_tensorflow//tensorflow/core:tensorflow_opensource
@org_tensorflow//tensorflow/core:all_kernels
@org_tensorflow//tensorflow/core/kernels:array
@org_tensorflow//tensorflow/core/kernels:strided_slice_op
```
"
16960,Build error introduced by 1baac78627. Can't build sources (master).,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: GCC 4.9.2-10
- **CUDA/cuDNN version**: Cuda8 / Cudnn7
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:
```
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
Failure to build sources:
```
ERROR: /opt/tensorflow/tensorflow/core/BUILD:2077:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)
In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddAllocVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:123:17: error: 'class tensorflow::Allocator' has no member named 'AddAllocVisitor'
     allocator_->AddAllocVisitor(visitor);
                 ^
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddFreeVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:127:17: error: 'class tensorflow::Allocator' has no member named 'AddFreeVisitor'
     allocator_->AddFreeVisitor(visitor);
                 ^
```
Class `MklCPUAllocator` has a member `allocator_` which is of type `tensorflow::Allocator` which does not have the member functions accessed. "
16955,Row wise lookup table in Tensorflow,"Currently I have a matrix in which each row is a lookup table. Corresponding to it I have a coded matrix with the same number of rows as the lookup table. e.g.

> LookupTable (matrix size 100, 32)

> CodedMatrix (matrix size 100, 1000)

So the lookup table values match to the corresponding row of the coded matrix. The code matrix contains numbers from 0 to 31 which have a corresponding value in the Lookup table for that specific row.

The final output of this should be 

> DecodedMatrix(matrix size 100, 1000)

In which each value of the row is replaced with it's corresponding lookup output.

Currently in numpy I use a for loop, because at the end I sum up the decoded matrix along the row axis for the final output. The code looks like this

       out = sum([C[L] for C,L in zip(CodedMatrix, LookupTable)])

which is a still inefficient. But in Tensorflow I use

     nRows = tf.constant(100, name=""nRows"")
     n     = tf.Variable(tf.constant(0))

     def cond(n, out):
         return n < nRows

     def body(n, out):  
         out = out + tf.gather(LookupTable[m,:], CodedMatrix[m,:])
         return n+1, out

     out = tf.while_loop(cond, body, [n, out])[1]

This execution takes a lot of time, because each time a new tensor is created and using the loop isn't very efficient.

Is there a way to do this without using while loop? Does tf.gather have any setup to do lookup like this?

Have I written custom code - Yes
OS Platform and Distribution - Mac OS X High Sierra
TensorFlow installed from - Source
TensorFlow version - 1.5.0-rc0
Bazel version - 0.5.4
CUDA/cuDNN version - N/A
GPU model and memory - N/A
Exact command to reproduce - Provided aboce"
16954,Iterator.get_next() documentation improvement request,"### System information
N/A
### Describe the problem
Recently I've written some code using Dataset API and I would like to request a problem with documentation (IMO).  Instead of hardcoding comments [here](https://github.com/tensorflow/tensorflow/blob/3ee1721b46d0e61097d0ee72f01e3de9739f0b6f/tensorflow/python/data/ops/iterator_ops.py#L31), please, move @mrry's annotation about `Iterator.get_next()` and `GET_NEXT_CALL_WARNING_THRESHOLD` into `get_next()` method documentation. 

I don't know why I didn't get that beautiful warning on my console output but I think I'm not the first person with funny thread-bomb running and consuming system resources. :) You know about that also (see comment).
So... It would be great if you could move all critical annotation into main documentation. I'm thinking now about all ML newcomers rather than me (Yeah, I actually found solution by myself :)) 
That's all. 
### Source code / logs
N/A"
16951,"TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'","### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.4.1
- **Python version**: 3.5.2
- **Bazel version:** Not compiled from source
- **GCC/Compiler version**: Not compiled from source
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1080 (8GB x 4)
- **Exact command to reproduce**: N/A

I'm trying to convert a the initializer form from `tf.Variable` to `tf.get_variable` for `Cudnn_GRU` but I keep getting this error. I have to convert because tensorflow does not allow initializing in loop/control-flow functions and only allow lambda initializers or through `tf.get_variable`

I have reduced the problem into the following minimal example:
```
import tensorflow as tf
e = tf.random_uniform_initializer(-0.1, 0.1)
i = tf.constant(0)
def func():
    gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=75, input_size=25)
    # original line: commented out and working if not under a control flow mechanism
    # param_fw = tf.Variable(tf.random_uniform([gru_fw.params_size()], -0.1, 0.1), validate_shape=False)
    # converted line
    param_fw = tf.get_variable(""abcd"", shape=[gru_fw.params_size()],initializer=e, validate_shape=False)
    return param_fw

def func2():
    ### repeat the same thing from func1
    pass

result = tf.cond(tf.equal(i, tf.constant(0)),func,func2)
```
The traceback is as follows:
```
Traceback (most recent call last):
	File ""test_run_error.py"", line 16, in <module>
		result = tf.cond(tf.equal(i, tf.constant(0)),func,func2)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
		return func(*args, **kwargs)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1855, in cond
		orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1725, in BuildCondBranch
		original_result = fn()
	File ""test_run_error.py"", line 9, in func
		param_fw = tf.get_variable(""abcd"", shape=[gru_fw.params_size()],initializer=e, validate_shape=False)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py"", line 1203, in get_variable
		constraint=constraint)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py"", line 1092, in get_variable
		constraint=constraint)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py"", line 425, in get_variable
		constraint=constraint)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py"", line 394, in _true_getter
		use_resource=use_resource, constraint=constraint)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py"", line 730, in _get_single_variable
		shape = tensor_shape.as_shape(shape)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 849, in as_shape
		return TensorShape(shape)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 455, in __init__
		self._dims = [as_dimension(d) for d in dims_iter]
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 455, in <listcomp>
		self._dims = [as_dimension(d) for d in dims_iter]
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 397, in as_dimension
		return Dimension(value)
	File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 32, in __init__
		self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'
```"
16950,"Windows Installation tutorial has wrong cuDNN version requirement, 7 required for latest version.","**Inaccuracies in the documentation**.

In the installation documentation Tensorflow in Windows says that the system must be running a version of CUDA 9.0 (correctly) and cuDNN 6.0 (not correct) because on the Nvidia website we can download and install only:
- **cuDNN** v7.0.5 , for **CUDA** 8.0/9.0/9.1 ;
- **cuDNN** v7.0.4 , for **CUDA** 8.0/9.0 ;
- **cuDNN** v6.0 , for **CUDA** 8.0 .

In https://developers.googleblog.com/2018/01/announcing-tensorflow-15.html said that 

> If you are using GPU Acceleration on **Windows** or Linux, TensorFlow 1.5 now has CUDA 9 and **cuDNN 7** support built-in. 

In issue #16477 (https://github.com/tensorflow/tensorflow/issues/16477)  as @gunan  said 
> Windows also requires cuDNN 7, looks like we missed that.

In proof of his words let me give you output logs:

(tensorflow15) C:\Windows\system32>python
Python 3.5.4 |Anaconda, Inc.| (default, Nov  8 2017, 14:34:30) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
      import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\site-packages\tensorflow\python\platform\self_check.py"", line 87, in preload_check
    ctypes.WinDLL(build_info.cudnn_dll_name)
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\ctypes\__init__.py"", line 351, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] Не найден указанный модуль (The specified module was not found)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\User\AppData\Local\conda\conda\envs\tensorflow15\lib\site-packages\tensorflow\python\platform\self_check.py"", line 97, in preload_check
    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))
ImportError: Could not find **'cudnn64_7.dll'**. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading **cuDNN 7** from this URL: https://developer.nvidia.com/cudnn


In conclusion.

Need to replace:

> **cuDNN  v6.0**. For details, see NVIDIA's documentation. Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.

to

> **cuDNN v7.0**. For details, see NVIDIA's documentation. Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.

on the web page https://www.tensorflow.org/install/install_windows
"
16948,tf.QueueBase.dequeue_many returns list instead of tuple,"The dequeue_many operation returns a **list** of Tensors, while the documentation states that it should be a **tuple** (which is more sensible).

> Returns:
> The tuple of concatenated tensors that was dequeued.
(https://www.tensorflow.org/api_docs/python/tf/QueueBase)

Version: 1.5.0 from PIP, Python3, on OS/X"
16947,Using P100 on different generations of CPUs causes training to slow down on TF1.5,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RedHat 7.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: From yum installation.
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 7.0.5
- **GPU model and memory**: P100-PCIe 16GB
- **Exact command to reproduce**: bazel build options: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 


### Source code / logs
[Alexnet_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716272/Alexnet_PSCPU_GPU1_fp32_18_02_08_18_01.log)
[Inception3_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716273/Inception3_PSCPU_GPU1_fp32_18_02_08_18_01.log)
[Resnet50_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716274/Resnet50_PSCPU_GPU1_fp32_18_02_08_18_01.log)

Hi all,

When I use tensorflow benchmark to the model performance will see the decreasing performance.

The following is the performance result of alexnet, we can observe weird results in the performance will continue to decrease on Broadwell CPU.
Step	Img/sec	total_loss
1	images/sec: 2935.5 +/- 0.0 (jitter = 0.0)	7.198
10	images/sec: 2934.3 +/- 0.9 (jitter = 2.8)	7.201
20	images/sec: 2936.5 +/- 0.8 (jitter = 1.9)	7.199
30	images/sec: 2936.5 +/- 0.6 (jitter = 2.2)	7.201
40	images/sec: 2936.4 +/- 0.5 (jitter = 2.3)	7.200
50	images/sec: 2935.4 +/- 1.1 (jitter = 2.5)	7.199
60	images/sec: 2935.4 +/- 0.9 (jitter = 2.7)	7.200
70	images/sec: 2935.4 +/- 0.8 (jitter = 3.0)	7.202
80	images/sec: 2935.5 +/- 0.7 (jitter = 3.0)	7.201
90	images/sec: 2935.7 +/- 0.7 (jitter = 3.6)	7.200
100	images/sec: 2935.8 +/- 0.6 (jitter = 3.2)	7.200
110	images/sec: 2936.0 +/- 0.6 (jitter = 3.7)	7.198
120	images/sec: 2936.0 +/- 0.5 (jitter = 3.6)	7.198
130	images/sec: 2936.0 +/- 0.5 (jitter = 3.3)	7.199
140	images/sec: 2935.9 +/- 0.5 (jitter = 3.2)	7.198
150	images/sec: 2936.2 +/- 0.4 (jitter = 3.2)	7.198
160	images/sec: 2936.3 +/- 0.4 (jitter = 3.2)	7.198
170	images/sec: 2936.1 +/- 0.4 (jitter = 3.3)	7.199
180	images/sec: 2936.1 +/- 0.4 (jitter = 3.2)	7.199
190	images/sec: 2936.1 +/- 0.4 (jitter = 3.2)	7.199
200	images/sec: 2936.0 +/- 0.4 (jitter = 3.1)	7.200
210	images/sec: 2936.1 +/- 0.4 (jitter = 3.1)	7.199
220	images/sec: 2936.0 +/- 0.3 (jitter = 3.0)	7.199
230	images/sec: 2935.9 +/- 0.3 (jitter = 3.1)	7.199
240	images/sec: 2935.9 +/- 0.3 (jitter = 3.2)	7.199
250	images/sec: 2935.9 +/- 0.3 (jitter = 3.2)	7.199
260	images/sec: 2936.0 +/- 0.3 (jitter = 3.2)	7.198
270	images/sec: 2936.0 +/- 0.3 (jitter = 3.1)	7.199
280	images/sec: 2936.1 +/- 0.3 (jitter = 3.4)	7.197
290	images/sec: 2936.3 +/- 0.3 (jitter = 3.6)	7.198
300	images/sec: 2936.3 +/- 0.3 (jitter = 3.4)	7.200
310	images/sec: 2936.4 +/- 0.3 (jitter = 3.4)	7.199
320	images/sec: 2936.4 +/- 0.3 (jitter = 3.5)	7.199
330	images/sec: 2936.4 +/- 0.3 (jitter = 3.5)	7.198
340	images/sec: 2936.5 +/- 0.3 (jitter = 3.6)	7.200
350	images/sec: 2936.5 +/- 0.3 (jitter = 3.7)	7.200
360	images/sec: 2936.5 +/- 0.2 (jitter = 3.6)	7.200
370	images/sec: 2936.5 +/- 0.2 (jitter = 3.7)	7.200
380	images/sec: 2936.5 +/- 0.2 (jitter = 3.7)	7.199
390	images/sec: 2936.6 +/- 0.2 (jitter = 3.7)	7.199
400	images/sec: 2936.6 +/- 0.2 (jitter = 3.6)	7.198
410	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.200
420	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.197
430	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.198
440	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.198
450	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.198
460	images/sec: 2936.7 +/- 0.2 (jitter = 3.6)	7.199
470	images/sec: 2936.6 +/- 0.2 (jitter = 3.5)	7.200
480	images/sec: 2936.6 +/- 0.2 (jitter = 3.6)	7.199
490	images/sec: 2936.6 +/- 0.2 (jitter = 3.6)	7.196
500	images/sec: 2936.6 +/- 0.2 (jitter = 3.6)	7.198
510	images/sec: 2936.6 +/- 0.2 (jitter = 3.6)	7.199
520	images/sec: 2935.8 +/- 0.3 (jitter = 3.7)	7.198
530	images/sec: 2933.9 +/- 0.7 (jitter = 3.8)	7.200
540	images/sec: 2931.4 +/- 1.0 (jitter = 3.9)	7.197
550	images/sec: 2925.7 +/- 1.9 (jitter = 4.0)	7.197
560	images/sec: 2920.8 +/- 2.4 (jitter = 4.2)	7.198
570	images/sec: 2916.1 +/- 2.7 (jitter = 4.3)	7.199
580	images/sec: 2911.0 +/- 3.0 (jitter = 4.4)	7.200
590	images/sec: 2903.0 +/- 3.7 (jitter = 4.6)	7.196
600	images/sec: 2894.5 +/- 4.3 (jitter = 4.7)	7.196
610	images/sec: 2887.1 +/- 4.7 (jitter = 4.8)	7.200
620	images/sec: 2879.7 +/- 5.0 (jitter = 5.0)	7.200
630	images/sec: 2872.6 +/- 5.3 (jitter = 5.2)	7.199
640	images/sec: 2865.7 +/- 5.6 (jitter = 5.4)	7.198
650	images/sec: 2852.2 +/- 6.5 (jitter = 5.6)	7.197
660	images/sec: 2832.6 +/- 7.7 (jitter = 5.7)	7.200
670	images/sec: 2814.1 +/- 8.6 (jitter = 5.8)	7.198
680	images/sec: 2799.0 +/- 9.2 (jitter = 6.0)	7.198
690	images/sec: 2785.8 +/- 9.7 (jitter = 6.1)	7.200
700	images/sec: 2773.1 +/- 10.0 (jitter = 6.3)	7.199
710	images/sec: 2760.9 +/- 10.4 (jitter = 6.5)	7.198
....
9940	images/sec: 1442.7 +/- 4.2 (jitter = 84.0)	7.197
9950	images/sec: 1442.6 +/- 4.2 (jitter = 83.8)	7.196
9960	images/sec: 1442.6 +/- 4.2 (jitter = 83.6)	7.197
9970	images/sec: 1442.6 +/- 4.2 (jitter = 83.4)	7.197
9980	images/sec: 1442.6 +/- 4.2 (jitter = 83.3)	7.197
9990	images/sec: 1442.6 +/- 4.2 (jitter = 83.0)	7.197
10000	images/sec: 1442.3 +/- 4.2 (jitter = 83.0)	7.198


But we test the same environment on Purley platform, we could get the normal performance on training result.

Tensorflow (img/sec)         | AlexNet | InceptionV3 | RenNet50
 SKL   -   P100 GPU x 1       | 2919.01 | 139.12 | 219.91
BWD -    P100GPU x 1        | 1441.96 | 77.87   | 120.8

What is the major problem on this strange results? 

"
16946,tensorflow lite converter(toco) build error ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10(64bit)
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: tensorflow 1.5.0
- **Python version**: Python 2.7/3.6
- **Bazel version (if compiling from source)**:  bazel 0.9.0
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: No GPU model

### Describe the problem
I try to build the toco that is tensorflow lite converter. 
But I can not success to build. please see below for the details.

### Source code / logs
`C:\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco`
The following error message appears.
> ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 88
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 88
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""C:/tensorflow/third_party/repo.bzl"", line 59, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(3) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Stderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
INFO: Elapsed time: 27.852s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/contrib/lite/toco

- plus info.
The following message appears when I input like this in command line. (for test)
`patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch`
> patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354
This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.

`patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary`
> patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).
Hunk #2 succeeded at 825 (offset 169 lines).
Hunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).

I don't know how to add --binary option to script...
ref. https://github.com/tensorflow/tensorflow/issues/10435
"
16944,Tensorflow (0.11.0) returns error while running training and testing scripts. ,"Due to some constraint i am using Tensorflow version 0.11.0, which i installed using bellow commands:
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
pip install --ignore-installed --upgrade $TF_BINARY_URL

After installation i am successfully able to import Tensorflow in python:
```
cvsion@cvsion:~$ python -c 'import tensorflow as tf; print(tf.__version__)'
0.11.0

cvsion@cvsion:~/Deeplearning/DeepTensorFlow/tensorlayer$ python 
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> 
```

but when i am running my training script its giving error:

> Traceback (most recent call last):
>   File ""./testing_nvidia.py"", line 8, in <module>
>     import tensorlayer as tl
>   File ""/home/cvsion/Deeplearning/DeepTensorFlow/tensorlayer/tensorlayer/__init__.py"", line 23, in <module>
>     from . import distributed
>   File ""/home/cvsion/Deeplearning/DeepTensorFlow/tensorlayer/tensorlayer/distributed.py"", line 9, in <module>
>     from tensorflow.python.training import session_run_hook
> ImportError: cannot import name session_run_hook

Any idea to resolve above issue"
16943,"tensorflow serving, add_meta_graph_and_variables() legacy_init_op doesn't surpport a tf.group of multi ops. ","the new nmt model in tutorial [https://github.com/tensorflow/nmt](url) using index_table_from_file and the datasets API make_initializable_iterator. So when I want to deploy the model as serving,  in builder.add_meta_graph_and_variables(), I assigned legacy_init_op with an tf.group(tf.tables_initializer(), infer_model.iterator.initializer) or use tf.control_dependencies, when I export the model and run it, I met a error: Failed precondition: Table not initialized.

```# Build the signature_def_map.
    builder = tf.saved_model.builder.SavedModelBuilder(export_path)
    tensor_info_src = tf.saved_model.utils.build_tensor_info(
        infer_model.src_placeholder)
    tensor_info_sample_words = tf.saved_model.utils.build_tensor_info(
        loaded_infer_model.sample_words)

    prediction_signature = (
        tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'src': tensor_info_src},
            outputs={'sample_words': tensor_info_sample_words},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

    with tf.control_dependencies([tf.tables_initializer()]):
         op1 = tf.no_op()
    with tf.control_dependencies([op1, infer_model.iterator.initializer]):
         op2 = tf.no_op(name='legacy_init_op')

    legacy_init_op = tf.group(tf.tables_initializer(), infer_model.iterator.initializer,
                              name='legacy_init_op')
    print(legacy_init_op)

    table_init_op = tf.group(tf.tables_initializer(),
                             name='legacy_init_op')
    print(table_init_op)

    builder.add_meta_graph_and_variables(
        session, [tf.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict_chat': prediction_signature
        },
        legacy_init_op=op2)
    builder.save()
    print('Done exporting.')```"
16942,"wrapping op in while_loop makes it run faster, even with single iteration","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430 (1.5.0)
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.1, CuDNN 7
- **GPU model and memory**: GeForce GTX 1050, 4042MiB
- **Exact command to reproduce**: See below

### Describe the problem

I am working on an object detector. I have one entry point to my code that detects objects in a single image, and another that reads in a batch of images. The batch version uses `tf.while_loop` to apply the same inference code to each image in the batch. I can't put all the images into a single tensor because they are all likely to be different dimensions, so I am using a `TensorArray`. The final results work as expected, but the timing behavior is odd.

The first `sess.run` call in the loop over the dataset is always slower than the rest, presumably due to caching, memory allocation, etc. However, the first call in the non-batch version is WAY slower than the first call in the batch version. I distilled this down to the minimal examples below. The first version does a dot product in a straightforward way. On my machine, it takes about 13 seconds to run. The second version does the same dot product, but wrapped in a `while_loop` and writing the result to an intermediate `TensorArray` and then reading it out again. On my machine, it takes about 4 seconds to run. Presumably, version 2 has to do the same data copying, memory allocation, etc. as version 1, in addition to the overhead of the `while_loop` and `TensorArray` calls. So, the timing behavior is unexpected.

Version 2 can even be modified to compute the same dot product 10 (or more) times, and it will still run faster. This behavior holds even if the loop body is modified to use a random vector every time, so I don't think it is due to caching. It also holds even if `parallel_iterations` is held to `1`, so I don't think it is due to parallelism. Is this a bug, or is something else going on under the hood?

### Source code / logs

```python
# version 1 (~13 seconds on my machine)
graph = tf.Graph()
with graph.as_default(), tf.device('/gpu:0'):
    nums = tf.range(200000000, dtype=tf.float32)
    dot_product = tf.reduce_sum((nums/2) * (nums-5))
with tf.Session(graph=graph) as sess:
    print(sess.run(dot_product))
```

```python
# version 2 (~4 seconds on my machine)
graph = tf.Graph()
with graph.as_default(), tf.device('/gpu:0'):
    arr = tf.TensorArray(size=1, dtype=tf.float32)
    nums = tf.range(200000000, dtype=tf.float32)
    i = tf.constant(0)
    def body(i, arr):
        arr = arr.write(i, tf.reduce_sum((nums/2) * (nums-5)))
        return i+1, arr
    i, arr = tf.while_loop(
        cond=lambda i, x: i < 1,
        body=body,
        loop_vars=[i, arr],
        parallel_iterations=1)
    dot_product = arr.read(0)
with tf.Session(graph=graph) as sess:
    print(sess.run(dot_product))
```
"
16941,Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed /usr/bin/ld.gold: fatal error: bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/quantize_and_dequantize_op/tensorflow/core/kernels/quantize_and_dequantize_op.pic.o: read: Input/output error,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**:
2.7.12 
- **Bazel version (if compiling from source)**:
0.10.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **Exact command to reproduce**:
bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=-march=native //tensorflow/tools/pip_package:build_pip_package
### Describe the problem
I am trying to install Tensorflow on my laptop which has i5 and 8GB RAM.

When I am building the Tensorflow from source 
I use

    bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=-march=native //tensorflow/tools/pip_package:build_pip_package

Everything seems to go well but at the end I get this error

    `ERROR: /home/siladittya/tensorflow/tensorflow/python/BUILD:3166:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)

    /usr/bin/ld.gold: fatal error: bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/quantize_and_dequantize_op/tensorflow/core/kernels/quantize_and_dequantize_op.pic.o: read: Input/output error

    collect2: error: ld returned 1 exit status

    Target //tensorflow/tools/pip_package:build_pip_package failed to build
`

I looked up a few posts in some forums, but they are saying that it can be solved using `--local_resources` but when I used that argument,

    `bazel build --local_resources 8000,2.0,2.0 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt //tensorflow/tools/pip_package:build_pip_package`

I am getting the same error again.
 
But it doesn't seem to be a error which can be solved using `--local_resources` because it doesn't work.

What else can be the solution to this problem?
"
16940,"TypeError: ('Keyword argument not understood:', 'adjustment') when passing slim.batch_norm as normalizer_fn to slim.conv2d","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version**:
1.4.0 CPU
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A


### Describe the problem
I encountered a problem using batch norm in `slim.conv2d`. Whenever I pass `slim.batch_norm` as the `normalizer_fn` for `slim.conv2d`, I encounter this error:

```
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 1059, in convolution
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 650, in batch_norm
    fused=fused)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 118, in __init__
    name=name, trainable=trainable, **kwargs)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 98, in __init__
    raise TypeError('Keyword argument not understood:', kwarg)
TypeError: ('Keyword argument not understood:', 'adjustment')
```"
16939,"tensorflow 1.3,1.4,1.5,1.6 DLL load failed with CUDA 9.1, CUDnn-7.05, Windows 10","import tensorflow strack trace

```
> py lib\_learn\tensorflow\versions\versiontest.py
Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib\_learn\tensorflow\versions\versiontest.py"", line 4, in <module>
    import tensorflow as tf
  File ""lib/tensorflow_gpu_130\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
16937,there is no idct implementation in tensorflow,"Hi
I need idct implementation in tensorflow, anyone aware of any implementation on this?
thanks
"
16935,TensorFlow1.5.0 absl.flags._exceptions.UnparsedFlagAccessError in Jupyter Notebook,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**:GeForce 950M(4Gb)
- **Exact command to reproduce**:

### Describe the problem
I run the function `cifar10.maybe_download_and_extract()` of `tensorflow/models/tutorials/image/cifar10.py` file in python command and jupyter notebook .The code runs successfully in python command,  but it causes `absl.flags._exceptions.UnparsedFlagAccessError`from `cifar10.FLAGS.data_dir` in jupyter notebook.

The jupyter notebook has already **configured the tensorflow kernel**, other tensorflow examples could run smoothly on jupyter notebook.

Significant code in `cifar10.py` file is:

    import tensorflow as tf
    import cifar10_input
    FLAGS = tf.app.flags.FLAGS

    # Basic model parameters.
    tf.app.flags.DEFINE_integer('batch_size', 128,
                            """"""Number of images to process in a batch."""""")
    tf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',
                           """"""Path to the CIFAR-10 data directory."""""")
    tf.app.flags.DEFINE_boolean('use_fp16', False,
                            """"""Train the model using fp16."""""")
    # .....
    def maybe_download_and_extract():
        """"""Download and extract the tarball from Alex's website.""""""
        dest_directory = FLAGS.data_dir
        if not os.path.exists(dest_directory):
            os.makedirs(dest_directory)
    # ......

The exception happened from the code statement `dest_directory = FLAGS.data_dir`.

The successful output in python command is as follows:

    >>> cifar10.maybe_download_and_extract()
    >> Downloading cifar-10-binary.tar.gz 100.0%
    Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.


The detailed error in jupyter notebook is as follows:

    UnrecognizedFlagError                     Traceback (most recent call last)
    <ipython-input-6-ddaee26bfd14> in <module>()
    ----> 1 cifar10.maybe_download_and_extract()
    ~/models/tutorials/image/cifar10/cifar10.py in maybe_download_and_extract()
    381 def maybe_download_and_extract():
    382   """"""Download and extract the tarball from Alex's website.""""""
    --> 383   dest_directory = FLAGS.data_dir
    384   if not os.path.exists(dest_directory):
    385     os.makedirs(dest_directory)
    ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/flags.py in 
    __getattr__(self, name)
    82     # a flag.
    83     if not wrapped.is_parsed():
    ---> 84       wrapped(_sys.argv)
    85     return wrapped.__getattr__(name)
    86 
    ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)
    628       suggestions = _helpers.get_flag_suggestions(name, list(self))
    629       raise _exceptions.UnrecognizedFlagError(
    --> 630           name, value, suggestions=suggestions)
    631 
    632     self.mark_as_parsed()
    UnrecognizedFlagError: Unknown command line flag 'f'

### Source code / logs

    #git clone https://github.com/tensorflow/models.git
    #cd models/tutorials/image/cifar10
    import cifar10
    cifar10.maybe_download_and_extract()"
16934,Can not create confusion_matrix,"This is my code for the network. I am using tensorflow 1.4 and I am using my nvidia for training. The code is working. I want to create a confusion matrix and I do it like this 



    with tf.device('/device:GPU:0'):    
        images = tf.placeholder(tf.float32, [None, IMAGE_WIDTH, IMAGE_LENGTH, 3])
        labels = tf.placeholder(tf.int64, [None])

        flat = tf.layers.flatten(labels)
        logits = tf.layers.dense(flat, len(set(train_labels_arr)), tf.nn.relu)

        predicted_labels = tf.argmax(tf.nn.softmax(logits), 1)

        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,
                                                                   labels = labels)
        loss = tf.reduce_mean(cross_entropy)

        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)
        correct_prediction = tf.equal(predicted_labels, labels)
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        init = tf.global_variables_initializer()

I want to create a confusion matrix and I do it like this:

     confusion_matrix = tf.confusion_matrix(labels=labels, predictions=predicted_labels,
                                          num_classes=len(set(train_labels_arr)),dtype=tf.int64)
When I call: 

    session = tf.Session()
    session.run(init) 

I got very weird error, but if remove the line with the confusion_matrix everything is ok: InvalidArgumentError: Cannot assign a device for operation 'confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT64], summarize=3, _device=""/device:GPU:0""](confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/Switch, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_0, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_1, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_2, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/Switch_1)]]

Caused by op u'confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert', defined at:
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/lachaka/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-27-64238c75dfbf>"", line 41, in <module>
    num_classes=len(set(train_labels_arr)),dtype=tf.int64)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/confusion_matrix.py"", line 162, in confusion_matrix
    labels, message='`labels` contains negative values')],
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py"", line 237, in assert_non_negative
    return assert_less_equal(zero, x, data=data, summarize=summarize)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py"", line 443, in assert_less_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py"", line 107, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 134, in Assert
    condition, no_op, true_assert, name=""AssertGuard"")
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1864, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1725, in BuildCondBranch
    original_result = fn()
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 132, in true_assert
    condition, data, summarize, name=""Assert"")
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 47, in _assert
    name=name)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access"
16933,Converting numpy array to TFRecord is slow,"### FloatList and Feature is slow for numpy array.
Saving numpy arrays with np.load and np.save is much faster than Converting to TFRecord and reading it back.
while profiling the code, I found that half of the time is spent in _floats_feature.
tf.train.FloatList is taking 1/3 of the time.
How to speed this up? 

### System information
- **Below snippet of code to convert numpy array is much slow compared  np.save, np.load**:
- **OS Platform and Distribution: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 2.7.12


### Source code / logs

```
import tensorflow as tf
import numpy as np



def floatme(value):
    return tf.train.FloatList(value=value)

def _floats_feature(value):
    return tf.train.Feature(float_list=floatme(value))

tfr_filename = ""deleteme.tfr""
data = ["" "".join(np.random.randint(0, 1000, size=4005).astype(str)) for i in range(10000)]
with tf.python_io.TFRecordWriter(tfr_filename) as writer:
    print('Converting to vectors')
    vectors = [np.fromstring(line, dtype=int, sep=' ', count=4004+1) for line in data]
    print('Converting to examples')
    for i, vec in enumerate(vectors):
        # Create an example protocol buffer
        example = tf.train.Example(features=tf.train.Features(feature={
            'label': _floats_feature([vec[4004], vec[4004]<1.0]),
            'data' : _floats_feature(vec[:4004]),
            }))
        writer.write(example.SerializeToString())


```


ncalls | tottime | percall | cumtime | percall | filename:lineno(function)
-- | -- | -- | -- | -- | --
232810 | 49.887 | 0 | 49.887 | 0 | convert_train_dataset_tfrecord.py:76(floatme)
116405 | 20.095 | 0 | 20.095 | 0 | {numpy.core.multiarray.fromstring}
232810 | 13.328 | 0 | 63.216 | 0 | convert_train_dataset_tfrecord.py:79(_floats_feature)

"
16932,Clarify RNNCell documentation on state_size and zero_state,"## TF Version
Documentation of current head r1.5

### Describe the problem
In the [RNNCell documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell#zero_state) for `zero_state` it is unclear if the state returned from an RNNCell should always be a 1-D Tensor or if it can be an N-D Tensor. The second is actually the case.

The current documentation says
> If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size, state_size] filled with zeros.

This is mostly true, however if Rank(TensorShape) is > 1 then it should say `[batch_size] + state_size.shape.as_list()` or some variant, to properly inform the user that arbitrary dimensioned Tensors can be passed through.

This is contradicted in the next sentence with:
> If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size.

Here 2-D tensors should be N-D tensors and `[batch_size, s]`  should be `[batch_size] + s.shape.as_list()`.

Failure to understand that N-D states can be passed back might force some users to flatten and then reshape states, which will result in performance penalties. An example where such N-D states are required might be for RNNCell's with external memory devices (like the DNC from Deepmind).
The documentation for `state_size` is adequate but could be made to explicitly mention N-D states are allowed.

### Proof
Example RNNCell with structure state tuple containing N-D state
```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import rnn_cell_impl

class NDState(rnn_cell_impl.RNNCell):
    def __init__(self,
            output_size):
        self._output_size=output_size

    @property
    def state_size(self):
       return (1, (tf.TensorShape([4,4]), 2) )
    @property
    def output_size(self):
        return self._output_size


    
    def __call__(self, inputs, state):
        """"""
        inputs : batch_size x input_size
        state : tuple of 2D [batch_size x s]
        """"""
        print(""State:"",state)

        return tf.layers.dense(inputs,self.output_size), state
        


def test():
    test_kwargs = { 'output_size':1}

    inputs = tf.random_normal(shape=(100,10,2),dtype=tf.float32)
    cell = NDState(**test_kwargs)
    zero_state = cell.zero_state(10, tf.float32)
    print(""Zero_state:"",zero_state)
    output,state = tf.nn.dynamic_rnn(cell,inputs, time_major=True,dtype=tf.float32)
    

    
if __name__ == ""__main__"":
    test()
```

Prints:
```
Zero_state: (<tf.Tensor 'NDStateZeroState/zeros:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'NDStateZeroState/zeros_1:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'NDStateZeroState/zeros_2:0' shape=(10, 2) dtype=float32>))
State: (<tf.Tensor 'rnn/while/Identity_3:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'rnn/while/Identity_4:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'rnn/while/Identity_5:0' shape=(10, 2) dtype=float32>))
```

## Template
Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: git master
TensorFlow version: r1.5 and previous
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A"
16931,how do `TensorFlowInferenceInterface` process image whose height not equal to width,"I'm writing a java service using Spring boot and my tensorflow predict model was trained by python Tensorflo Object Detection API. The model is FasterRCNN (coco-resnet),the model can accept any shape(whose height not equal to width,such as  600x800,1280x720) of image in python while thing cannot be done like this in Java. 

I read an example code written in Java from [TensorFlowImageClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java),the pre-allocate buffers is  $inputSize \* inputSize$. If i changed that to **inputHeight\*inputWidth** and some other nessary code, the inference process still went fine but recognize accuracy decrease dramatically.

Can anybody give some tips?

Here is my coding envirnoment:

+ Window 10
+ Tensorflow 1.5 build from whl 
+ CUDA 8.0 with cudnn 5.1
+ GPU  nvidia 1080 ti 
+ 16GB Memory


"
16930,Gradient function of tile operator failed with sparse Tensor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4/1.5
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I found an issue when I use a _tile_ operator followed by a _gather_ operator. It seems that there is a `isinstance(grad, ops.Tensor)` assert in the gradient function of _tile_ operator, while _gather_ operator will result in a sparse gradient with the type of IndexedSlices.

Please see below example for details.

### Source code / logs
Here is a small example to re-produce this issue:
```python
x = tf.Variable([[0.0], [1.0], [2.0], [3.0], [4.0]], name='x')
x = tf.tile(x, [1, 2])
i = tf.Variable(list(range(0, 5)), name='i')
x = tf.gather(x, i)
s = tf.reduce_sum(x)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train_op = optimizer.minimize(s)
```

You will get below errors:
```
ValueError: No attr named '_XlaCompile' in name: ""Tile""
op: ""Tile""
input: ""x/read""
input: ""Tile/multiples""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""Tmultiples""
  value {
    type: DT_INT32
  }
}

During handling of the above exception, another exception occurred:
Traceback
 (most recent call last):
  File ""D:/test/test_gather.py"", line 12, in <module>
    train_op = optimizer.minimize(s)
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\training\optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\training\optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""D:\PyEnv\tf1.5\lib\site-packages\tensorflow\python\ops\array_grad.py"", line 552, in _TileGrad
    assert isinstance(grad, ops.Tensor)
AssertionError
```

I think this is caused by the assertion in gradient function of _tile_ operator:
```python
@ops.RegisterGradient(""Tile"")
def _TileGrad(op, grad):
  """"""Sum reduces grad along the tiled dimensions.""""""
  assert isinstance(grad, ops.Tensor)
  input_shape = array_ops.shape(op.inputs[0])
```

Could we remove this assert or change the type to ops._TensorLike?"
16929,Tensorflow Lite toco convert a quantized model to tflite format crash error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.5 / r1.6
- **Python version**: python2.7 / python 3.6
- **Bazel version (if compiling from source)**: bazel release 0.9.0
- **GCC/Compiler version (if compiling from source)**: GCC/G++ 7.2.0, 
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: detail description in below

### Describe the problem

I use facenet framework which base on Inception_resnet_v1 to recognize face, and i use below command to quantize and convert a freeze graph pb file to tflite format:

quantize cmd:

bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
   --in_graph=/home/andy/datasets/facenet/facenet_model.pb \
   --out_graph=/home/andy/datasets/facenet/facenet_model_quantized.pb \
   --inputs=input:0 \
   --outputs=embeddings:0 \
   --transforms='strip_unused_nodes(type=float, shape=""1,160,160,3"")
    remove_nodes(op=Identity, op=CheckNumerics)
    fold_old_batch_norms
    quantize_weights
    strip_unused_nodes 
    sort_by_execution_order'

toco convert cmd:

bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_format=TENSORFLOW_GRAPHDEF \
  --input_file=/home/andy/datasets/facenet/facenet_model_quantized.pb \
  --output_format=TFLITE \
  --output_file=/home/andy/datasets/facenet/facenet_model_quantized.lite \
  --inference_type=QUANTIZED_UINT8 \
  --inference_input_type=QUANTIZED_UINT8 \
  --input_arrays=input \
  --output_arrays=embeddings \
  --input_shapes=1,160,160,3\
  --mean_values=128 \
  --std_values=128 \
  --default_ranges_min=0 \
  --default_ranges_max=6

I sure facenet_model.pb and facenet_model_quantized.pb can be run normal, but when toco convert the quantized model to tensorflow lite format it crash below error:

`2018-02-11 16:52:44.901519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Dequantize
2018-02-11 16:52:44.901544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Dequantize
2018-02-11 16:52:44.906789: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: RandomUniform
2018-02-11 16:52:45.171561: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 3003 operators, 5620 arrays (0 quantized)
2018-02-11 16:52:45.309297: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2890 operators, 5394 arrays (0 quantized)
2018-02-11 16:52:45.486937: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2890 operators, 5394 arrays (0 quantized)
2018-02-11 16:52:45.489630: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:48] Check failed: predicate_array.data_type == ArrayDataType::kBool 
Aborted (core dumped)
`

for resolve the issue i add some debug code before the line of resolve_tensorflow_switch.cc:48, it's like that:

LOG(INFO) << ""###### predicate_name: "" << LogName(*switch_op);
LOG(INFO) << ""###### predicate_name: "" << predicate_name;
switch (predicate_array.data_type) {
case ArrayDataType::kBool:
  LOG(INFO) << ""###### predicate_array.data_type kBool"";
  break;
case ArrayDataType::kFloat:
  LOG(INFO) << ""###### predicate_array.data_type kFloat"";
  break;
case ArrayDataType::kUint8:
  LOG(INFO) << ""###### predicate_array.data_type kUint8"";
  break;
case ArrayDataType::kInt32:
  LOG(INFO) << ""###### predicate_array.data_type kInt32"";
  break;
case ArrayDataType::kInt64:
  LOG(INFO) << ""###### predicate_array.data_type kInt64"";
  break;
case ArrayDataType::kNone:
  LOG(INFO) << ""###### predicate_array.data_type kNone"";
  break;
default:
  LOG(INFO) << ""###### predicate_array.data_type not know"";
}

then, i rebuild the toco, and rerun the convert cmd, log show that:

`2018-02-11 17:01:34.481749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Dequantize
2018-02-11 17:01:34.486969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: RandomUniform
2018-02-11 17:01:34.752556: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 3003 operators, 5620 arrays (0 quantized)
2018-02-11 17:01:34.888735: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2890 operators, 5394 arrays (0 quantized)
2018-02-11 17:01:35.063875: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2890 operators, 5394 arrays (0 quantized)
2018-02-11 17:01:35.066550: I tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:48] ###### predicate_name: {TensorFlowSwitch operator with output InceptionResnetV1/Bottleneck/BatchNorm/cond/FusedBatchNorm_1/Switch_4}
2018-02-11 17:01:35.066558: I tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:49] ###### predicate_name: Const_1
2018-02-11 17:01:35.066562: I tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:67] ###### predicate_array.data_type kNone
2018-02-11 17:01:35.066565: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:72] Check failed: predicate_array.data_type == ArrayDataType::kBool 
Aborted (core dumped)
`

i don't know why the ""predicate_name: Const_1"" of op ""InceptionResnetV1/Bottleneck/BatchNorm/cond/FusedBatchNorm_1/Switch_4"" it's data type is KNone, but the function ResolveTensorFlowSwitch::Run request it's data type should be KBool. The Inception_resnet_v1 Bottleneck is in a fully connect layer, and relation code show below:
 
`
end_points = {}
  
    with tf.variable_scope(scope, 'InceptionResnetV1', [inputs], reuse=reuse):
        with slim.arg_scope([slim.batch_norm, slim.dropout],
                            is_training=is_training):
            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                                stride=1, padding='SAME'):
      
                # 149 x 149 x 32
                net = slim.conv2d(inputs, 32, 3, stride=2, padding='VALID',
                                  scope='Conv2d_1a_3x3')
                end_points['Conv2d_1a_3x3'] = net
                # 147 x 147 x 32
                net = slim.conv2d(net, 32, 3, padding='VALID',
                                  scope='Conv2d_2a_3x3')
                end_points['Conv2d_2a_3x3'] = net
                # 147 x 147 x 64
                net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')
                end_points['Conv2d_2b_3x3'] = net
                # 73 x 73 x 64
                net = slim.max_pool2d(net, 3, stride=2, padding='VALID',
                                      scope='MaxPool_3a_3x3')
                end_points['MaxPool_3a_3x3'] = net
                # 73 x 73 x 80
                net = slim.conv2d(net, 80, 1, padding='VALID',
                                  scope='Conv2d_3b_1x1')
                end_points['Conv2d_3b_1x1'] = net
                # 71 x 71 x 192
                net = slim.conv2d(net, 192, 3, padding='VALID',
                                  scope='Conv2d_4a_3x3')
                end_points['Conv2d_4a_3x3'] = net
                # 35 x 35 x 256
                net = slim.conv2d(net, 256, 3, stride=2, padding='VALID',
                                  scope='Conv2d_4b_3x3')
                end_points['Conv2d_4b_3x3'] = net
                
                # 5 x Inception-resnet-A
                net = slim.repeat(net, 5, block35, scale=0.17)
                end_points['Mixed_5a'] = net
        
                # Reduction-A
                with tf.variable_scope('Mixed_6a'):
                    net = reduction_a(net, 192, 192, 256, 384)
                end_points['Mixed_6a'] = net
                
                # 10 x Inception-Resnet-B
                net = slim.repeat(net, 10, block17, scale=0.10)
                end_points['Mixed_6b'] = net
                
                # Reduction-B
                with tf.variable_scope('Mixed_7a'):
                    net = reduction_b(net)
                end_points['Mixed_7a'] = net
                
                # 5 x Inception-Resnet-C
                net = slim.repeat(net, 5, block8, scale=0.20)
                end_points['Mixed_8a'] = net
                
                net = block8(net, activation_fn=None)
                end_points['Mixed_8b'] = net
                
                with tf.variable_scope('Logits'):
                    end_points['PrePool'] = net
                    #pylint: disable=no-member
                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',
                                          scope='AvgPool_1a_8x8')
                    net = slim.flatten(net)
          
                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                                       scope='Dropout')
          
                    end_points['PreLogitsFlatten'] = net
                
                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, 
                        scope='Bottleneck', reuse=False)
  
    return net, end_points
`

how can i solve the issue, and which step of convert cmd is it wrong?
i hold someone can help me, thanks!
"
16927,image_retraining/retrain.py warning: Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization(),"If I download https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py and run it I get the following warning:

`2018-02-10 20:48:38.928435: W C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_def_util.cc:343] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().`

Here is a screenshot if that helps:

![untitled](https://user-images.githubusercontent.com/5672876/36068513-456022f4-0e8c-11e8-8b81-d0d5fb5199b0.png)

After looking at some old TensorFlow issues, for example these:

https://github.com/tensorflow/tensorflow/issues/2164

https://github.com/tensorflow/tensorflow/issues/4128

It seems that this concern has existed for quite a while and that it seems likely related to using the Inception model from 2015:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L893

Can anybody update this to resolve this warning please?

-- Edit ------------------------

provided additional requested information in separate response below"
16926,Dataset to TFRecord,"The dataset API is excellent, I am even using it to perform data augmentation before training. However, I notice that when converting my dataset to tfrecord I follow a pattern that might easily be automated as a `to_records` method, in fact I think this is somewhat similar to using the `cache` method. Anyway, it would make it very straight forward for dataset users to create tfrecords."
16925,Using bidirectional_dynamic_rnn with Grid3LSTMCell Gives Outputs with Different Shapes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Microsoft Windows 8.1
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.4 cpu
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A


### Describe the problem
I'm not sure if that's how using `bidirectional_dynamic_rnn` with Grid3LSTMCell works but it's giving outputs that are of different shapes: the first one has three dimensions and the second one has four dimensions.

### Source code / logs
Here's the code:

```
import tensorflow as tf
from tensorflow.contrib import grid_rnn

class BidirectionalGridRNNCellTest(tf.test.TestCase):
    def setUp(self):
        self.num_features = 1
        self.time_steps = 1
        self.batch_size = 1
        tf.reset_default_graph()
        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])
        self.cell_fw = grid_rnn.Grid3LSTMCell(num_units=8)
        self.cell_bw = grid_rnn.Grid3LSTMCell(num_units=8)

    def test_bidirectional_dynamic_grid_rnn(self):
        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)
        print(outputs)


if __name__ == '__main__':
    tf.test.main()
```

And here's the output of the print statement:

`((<tf.Tensor 'bidirectional_rnn/fw/fw/transpose:0' shape=(1, 1, 8) dtype=float32>,), <tf.Tensor 'ReverseV2:0' shape=(1, 1, 1, 8) dtype=float32>)`"
16924,typeid broken across shared boundary makes a271c36b5ead4686b72d972b193bf1f534a92ffd not work ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom shared object linking to 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: a271c36b5ead4686b72d972b193bf1f534a92ffd
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: P4000 (8 GB)
- **Exact command to reproduce**: This is a little involved.

### Describe the problem

@mrry First of all, thanks for quickly responding to my issue #16682, and for following up with the fixes in a271c36b5ead4686b72d972b193bf1f534a92ffd without my even mentioning the problem to you. Very impressive.

I know that in that commit, you mention ""A subsequent change will move `tf.contrib.data` kernel implementations to a custom op library."" When you say custom op library, I assume you mean a  distinct shared object file.

Unfortunately, unless you use --config=monolithic to build, this will not work because the typeid of DatasetVariantWrapper will be different between libtensorflow_framework.so and the custom op library shared object, because they are loaded with RTLD_LOCAL. --config=monolithic avoids the problem because _python_framework_internal.so is loaded with RTLD_GLOBAL in that case. This will override the custom op library's ""weak"" (I am talking about weak linkage of symbols here) typeid of DatasetVariantWrapper. Otherwise, `variant::get<DatasetVariantWrapper>()` will fail in dataset.cc's GetDatasetFromVariantTensor, because the two typeids that get compared have two separate pointers.

I first found this problem documented [here](https://svn.boost.org/trac10/ticket/754). [This](https://stackoverflow.com/questions/23383102/dynamic-cast-troubles-over-shared-libraries) stack overflow answer was also helpful.

I'm not sure what the right solution is to this yet. It seems it may be possible to change from pointer equality for type info to checking the equality of mangled strings with strcmp, based on my reading of libstdc++'s typeinfo header file.

Happy to answer any questions,s ince this is rather involved.

### Source code / logs

```
2018-02-10 13:53:14.169478: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-10 13:53:14.422106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Found device 0 with properties:
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:02:00.0
totalMemory: 7.91GiB freeMemory: 6.98GiB
2018-02-10 13:53:14.422134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Adding visible gpu devices: 0
2018-02-10 13:54:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-10 13:54:11.579230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:912]      0
2018-02-10 13:54:11.579254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:925] 0:   N
2018-02-10 13:54:11.579420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1016] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6740 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-02-10 13:54:11.619971: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:827 : Invalid argument: Tensor must be a Dataset object.
```"
16919,libhdfs3 instead of libhdfs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see diff below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

Current HDFS support in TensorFlow is based on [`libhdfs`](https://wiki.apache.org/hadoop/LibHDFS), a JNI wrapper for the HDFS client implemented in Java. Due to this design choice, users of `libhdfs` (and transitively TensorFlow) 

* need to keep in mind an implicit dependency on the JVM runtime,
* have to do quite a bit of [trickery](https://www.tensorflow.org/deploy/hadoop) to setup the environment expected by `libhdfs`.

[`libhdfs3`](http://pivotal-data-attic.github.io/attic-c-hdfs-client) is a native implementation of an HDFS client, which could potentially free the users from the two issues mentioned above. The documentation of `libhdfs3` claims that its C API is ""almost the same"" as that of `libhdfs`. I've verified this on the `master` version of TensorFlow and the diff is indeed negligible:

```diff
index 5f2b222622..0f168fe0dd 100644
--- a/tensorflow/core/platform/hadoop/hadoop_file_system.h
+++ b/tensorflow/core/platform/hadoop/hadoop_file_system.h
@@ -19,8 +19,8 @@ limitations under the License.
 #include ""tensorflow/core/platform/env.h""

 extern ""C"" {
-struct hdfs_internal;
-typedef hdfs_internal* hdfsFS;
+struct HdfsFileSystemInternalWrapper;
+typedef HdfsFileSystemInternalWrapper* hdfsFS;
 }
```

What do you think?"
16918,TensorFlow demo keeps stopping for flowers_images on Android TFlite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:1080Ti 11gb
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am following Tensorflow for poets 2(https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#7) session8(Run the customised app).So far, I have successfully ran the demo by Google for Android, but when I train custom flowers data and try to run the demo again by following session 8 in the tutorial in which I replace output name by final result as follows: private static final String OUTPUT_NAME = ""final_result"";

### Source code / logs
02/08 20:48:30: Launching tfmobile
$ adb install-multiple -r -t C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_7.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_2.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_6.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_8.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_9.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\dep\dependencies.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_5.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_4.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_3.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_1.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\intermediates\split-apk\debug\slices\slice_0.apk C:\Users\Ajinkya\Tensorflow_for_poets\tensorflow-for-poets-2\android\tfmobile\gradleBuild\outputs\apk\debug\tfmobile-debug.apk 
Split APKs installed
$ adb shell am start -n ""org.tensorflow.demo/org.tensorflow.demo.ClassifierActivity"" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER
Client not ready yet..Connected to process 32013 on device motorola-moto_g__4_-ZY223WW626
Capturing and displaying logcat messages from application. This behavior can be disabled in the ""Logcat output"" section of the ""Debugger"" settings page.
I/InstantRun: starting instant run server: is main process
D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@3c0dd38
D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@3c0dd38
D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@3c0dd38
I/Adreno: QUALCOMM build                   : 7d18700, I8ee426a9a2
          Build Date                       : 10/07/16
          OpenGL ES Shader Compiler Version: XE031.09.00.03
          Local Branch                     : mybranch22308589
          Remote Branch                    : quic/LA.BR.1.3.6_rb1.6
          Remote Branch                    : NONE
          Reconstruct Branch               : NOTHING
I/OpenGLRenderer: Initialized EGL, version 1.4
D/OpenGLRenderer: Swap behavior 1
I/CameraManagerGlobal: Connecting to camera service
I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480
I/tensorflow: CameraConnectionFragment: Valid preview sizes: [1280x960, 1280x720, 960x720, 960x540, 864x480, 720x480, 640x480]
I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [768x432, 320x240, 176x144]
I/tensorflow: CameraConnectionFragment: Exact size match found.
I/TensorFlowImageClassifier: Reading labels from: labels.txt
I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
D/AndroidRuntime: Shutting down VM
E/AndroidRuntime: FATAL EXCEPTION: main
                  Process: org.tensorflow.demo, PID: 32013
                  java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph.pb'
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:100)
                      at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
                      at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:130)
                      at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:159)
                      at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:421)
                      at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:428)
                      at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)
                      at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)
                      at android.view.TextureView.getHardwareLayer(TextureView.java:387)
                      at android.view.TextureView.draw(TextureView.java:325)
                      at android.view.View.updateDisplayListIfDirty(View.java:16065)
                      at android.view.View.draw(View.java:16849)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3768)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3554)
                      at android.view.View.updateDisplayListIfDirty(View.java:16060)
                      at android.view.View.draw(View.java:16849)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3768)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3554)
                      at android.view.View.draw(View.java:17086)
                      at android.view.View.updateDisplayListIfDirty(View.java:16065)
                      at android.view.View.draw(View.java:16849)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3768)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3554)
                      at android.view.View.updateDisplayListIfDirty(View.java:16060)
                      at android.view.View.draw(View.java:16849)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3768)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3554)
                      at android.view.View.updateDisplayListIfDirty(View.java:16060)
                      at android.view.View.draw(View.java:16849)
                      at android.view.ViewGroup.drawChild(ViewGroup.java:3768)
                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3554)
                      at android.view.View.draw(View.java:17086)
                      at com.android.internal.policy.DecorView.draw(DecorView.java:751)
                      at android.view.View.updateDisplayListIfDirty(View.java:16065)
                      at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:657)
                      at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:663)
                      at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:771)
                      at android.view.ViewRootImpl.draw(ViewRootImpl.java:2808)
                      at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2616)
                      at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2223)
                      at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1258)
                      at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6348)
                      at android.view.Choreographer$CallbackRecord.run(Choreographer.java:871)
                      at android.view.Choreographer.doCallbacks(Choreographer.java:683)
                      at android.view.Choreographer.doFrame(Choreographer.java:619)
                      at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:857)
                      at android.os.Handler.handleCallback(Handler.java:751)
                      at android.os.Handler.dispatchMessage(Handler.java:95)
                      at android.os.Looper.loop(Looper.java:154)
                      at android.app.ActivityThread.main(ActivityThread.java:6123)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757)
E/AndroidRuntime: Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_0/convolution = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input, MobilenetV1/Conv2d_0/weights). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:392)
                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:96)
                        ... 52 more
Application terminated.

And the application is terminated, what should I do to resolve this
"
16917,"ValueError: No attr named '_XlaCompile' in name: ""while/attention/cond/fw/CudnnRNN/Enter"" and AttributeError: 'NoneType' object has no attribute 'back_prop'","### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.4.1
- **Python version**: 3.5.2
- **Bazel version:** Not compiled from source
- **GCC/Compiler version**: Not compiled from source
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1080 (8GB x 4)
- **Exact command to reproduce**: N/A

I have a similar issue faced in [this thread](https://github.com/tensorflow/tensorflow/issues/12420) since I've started using tf.while_loop and the error is causing on `grads = self.opt.compute_gradients(self.loss)`

I'm initializing a class for gru layers outside the `tf.while_loop` since I can't initialize variables within the `tf.while_loop` without using `tf.get_variable` and then use the `__call__` of the class variable multiple times to use it within the `tf.while_loop` fn body. The sample codes are provided as per below:

```
Traceback (most recent call last):
  File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"",
 line 348, in _MaybeCompile
    xla_compile = op.get_attr(""_XlaCompile"")
  File ""/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line
 2003, in get_attr
    raise ValueError(""No attr named '"" + name + ""' in "" + str(self._node_def))
ValueError: No attr named '_XlaCompile' in name: ""while/attention/cond/fw/CudnnRNN/Enter""
op: ""Enter""
input: ""Variable_14/read""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""frame_name""
  value {
    s: ""while/while_context""
  }
}
attr {
  key: ""is_constant""
  value {
    b: true
  }
}
attr {
  key: ""parallel_iterations""
  value {
    i: 10
  }
}

# also, error in the same line while handling the above error:

AttributeError: 'NoneType' object has no attribute 'back_prop'
```

The Cudnn_gru custom class:
```
class cudnn_gru:
	def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):
		self.num_layers = num_layers
		self.grus = []
		self.params = []
		self.inits = []
		self.dropout_mask = []
		for layer in range(num_layers):
			input_size_ = input_size if layer == 0 else 2 * num_units
			gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(
				num_layers=1, num_units=num_units, input_size=input_size_)
			gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(
				num_layers=1, num_units=num_units, input_size=input_size_)
>>>>>>>>>>>Error over the following 4 lines for initializing within the while_loop
			param_fw = tf.Variable(tf.random_uniform(
				[gru_fw.params_size()], -0.1, 0.1), validate_shape=False)
			param_bw = tf.Variable(tf.random_uniform(
				[gru_bw.params_size()], -0.1, 0.1), validate_shape=False)
			init_fw = tf.Variable(tf.zeros([1, batch_size, num_units]))
			init_bw = tf.Variable(tf.zeros([1, batch_size, num_units]))
			mask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),
							  keep_prob=keep_prob, is_train=is_train, mode=None)
			mask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),
							  keep_prob=keep_prob, is_train=is_train, mode=None)
			self.grus.append((gru_fw, gru_bw, ))
			self.params.append((param_fw, param_bw, ))
			self.inits.append((init_fw, init_bw, ))
			self.dropout_mask.append((mask_fw, mask_bw, ))

	def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):
		outputs = [tf.transpose(inputs, [1, 0, 2])]
		for layer in range(self.num_layers):
			gru_fw, gru_bw = self.grus[layer]
			param_fw, param_bw = self.params[layer]
			init_fw, init_bw = self.inits[layer]
			mask_fw, mask_bw = self.dropout_mask[layer]
			with tf.variable_scope(""fw""):
				out_fw, _ = gru_fw(outputs[-1] * mask_fw, init_fw, param_fw)
			with tf.variable_scope(""bw""):
				inputs_bw = tf.reverse_sequence(
					outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)
				out_bw, _ = gru_bw(inputs_bw, init_bw, param_bw)
				out_bw = tf.reverse_sequence(
					out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)
			outputs.append(tf.concat([out_fw, out_bw], axis=2))
		if concat_layers:
			res = tf.concat(outputs[1:], axis=2)
		else:
			res = outputs[-1]
		res = tf.transpose(res, [1, 0, 2])
		return res
```
and the model:
```
class Model(object):
        def __init__(...):
            ....
            ....
            self.ready()
            if trainable:
			self.lr = tf.get_variable(
			""lr"", shape=[], dtype=tf.float32, trainable=False)
			self.opt = tf.train.AdadeltaOptimizer(
				learning_rate=self.lr, epsilon=1e-6)
>>>>>>>>>>>Compile time ERROR over this line:
                	grads = self.opt.compute_gradients(self.loss)
			gradients, variables = zip(*grads)
			capped_grads, _ = tf.clip_by_global_norm(gradients, config.grad_clip)
			self.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)

	def get_vP(self,i):
		....
	        ....
		with tf.variable_scope(""encoding""):
>>>>>>>>>>> def f1():
                # used to initialize the cudnn_gru class over here,
                # but shifted outside the tf.while_loop
                # due to initializing errors in tf.Variable in cudnn_gru __init__
                return self.rnn1(c_emb, seq_len=self.c_len)
            def f2():
                return self.rnn1(c_emb, seq_len=self.c_len)
            c = tf.cond(tf.equal(i, zero), f1, f2)
            q = self.rnn1(q_emb, seq_len=self.q_len)
            self.q_enc = q
        with tf.variable_scope(""attention""):
                qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,
                    keep_prob=config.keep_prob, is_train=self.is_train,
		name_scope=""attention_layer"")
>>>>>>>>>>> def f3():
                # same situation as f1()
                return self.rnn2(qc_att, seq_len=self.c_len)
            def f4():
				return self.rnn2(qc_att, seq_len=self.c_len)
            att = tf.cond(tf.equal(self.i, zero), f3, f4)
            def f5():
                return att
            def f6():
                return tf.concat([att, att], axis=1)
            self.att_vP = tf.cond(tf.equal(i, zero), f5, f6)

            return tf.add(i,tf.constant(1, dtype=tf.int64))
	
	def condition(self,i):
		return tf.less(i, self.para_count[0])
	
	def ready(self):
		config = self.config
		N, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, \
			config.char_limit, config.hidden, config.char_dim, config.char_hidden
		gru = cudnn_gru if config.use_cudnn else native_gru

		self.cell_fw = tf.contrib.rnn.GRUCell(dg)
		self.cell_bw = tf.contrib.rnn.GRUCell(dg)

>>>>>>>># initializing here instead of within tf.while_loop body
		self.rnn1 = gru(num_layers=3, num_units=150, batch_size=N, input_size=500,\
			keep_prob=config.keep_prob, is_train=self.is_train)
		self.rnn2 = gru(num_layers=1, num_units=d, batch_size=N, input_size=1800,\
			keep_prob=config.keep_prob, is_train=self.is_train)
		
		result = tf.while_loop(self.condition, self.get_vP, loop_vars=[self.i])

                ....
                ...."
16914,"Bazel can't find cudnn.h, ignores cudnn directory specified in configuration","
[cuda-inst.txt](https://github.com/tensorflow/tensorflow/files/1713644/cuda-inst.txt)
### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27 x64
- **TensorFlow installed from (source or binary)**: Source/Release
- **TensorFlow version (use command below)**: 1.6.0-rc0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 1.10
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: CUDA 9.1, cudNN 7.0.5
- **GPU model and memory**: GTX 1060
- **Exact command to reproduce**:

`bazel build --config=opt --config=cuda  --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package`

Relevant cudnn files are located at:
```
/usr/include/cuda/cudnn.h
/usr/lib64/libcudnn.so
/usr/lib64/libcudnn.so.7
/usr/lib64/libcudnn.so.7.0.5
```
and should be included:
`export LD_LIBRARY_PATH=""/usr/include/cuda/cupti:/usr/include/cuda:/usr/lib64""`

### Describe the problem
```
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 1063
		_create_local_cuda_repository(repository_ctx)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 917, in _create_local_cuda_repository
		_get_cuda_config(repository_ctx)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 672, in _get_cuda_config
		_cudnn_version(repository_ctx, cudnn_install_base..., ...)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 397, in _cudnn_version
		_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 646, in _find_cudnn_header_dir
		auto_configure_fail((""Cannot find cudnn.h under %s"" ...))
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 152, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find cudnn.h under /lib64
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 1063
		_create_local_cuda_repository(repository_ctx)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 917, in _create_local_cuda_repository
		_get_cuda_config(repository_ctx)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 672, in _get_cuda_config
		_cudnn_version(repository_ctx, cudnn_install_base..., ...)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 397, in _cudnn_version
		_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 646, in _find_cudnn_header_dir
		auto_configure_fail((""Cannot find cudnn.h under %s"" ...))
	File ""/home/torstein/progs/tensorflow-1.6.0-rc0/third_party/gpus/cuda_configure.bzl"", line 152, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find cudnn.h under /lib64
INFO: Elapsed time: 0.060s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```
I have tried specifying `/usr`, `/usr/include` and `/usr/include/cuda` as cudnn-path when running `./configure`. The configurator detects `cudnn.h` and does not complain.
If I specify `/usr`, bazel complains it can't find cudnn.h under `/usr`.
If I specify anything else, bazel seems intent to look under `lib64` for whatever reason and does not find it. I did bazel clean between reconfigurations. Have also tried bazel with `--action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`

Have attached full install paths for cuda, cudnn, cupti (rpm -ql):
[cuda-inst.txt](https://github.com/tensorflow/tensorflow/files/1713645/cuda-inst.txt)

Configuration log:
```
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.10.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Found possible Python library paths:
  /usr/lib/python3.6/site-packages
  /usr/lib64/python3.6/site-packages
  /usr/local/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.6/site-packages]
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y
jemalloc as malloc support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n
No Apache Kafka Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.
Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1
Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 
Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr]:/usr/include/cuda
Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.1
Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/cuda-gcc
Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=tensorrt    	# Build with TensorRT support.
Configuration finished
```"
16913,Eager: tf.linalg.inv(tf.transpose(mat)) has undefined shape in function with tfe.defun,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.6.0dev20180126(GPU)
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Source code / logs
* No problem in graph mode
```Python
import tensorflow as tf
import numpy as np
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
@tfe.defun
def test(rect):
    rect_T = tf.transpose(rect)
    print(rect_T.shape)
    print(tf.linalg.inv(rect_T).shape)
    print(tf.linalg.inv(rect).shape)
    print(tf.linalg.inv(tf.transpose(rect)).shape)
rect_tf = tf.constant(np.random.uniform(size=[4, 4]))
test(rect_tf)
```
Output:
```
(4, 4)
(?, ?)
(4, 4)
(?, ?)
```"
16911,VS2017 / Windows build error tfcompile: no user-defined conversion for 'xla::HloInstruction::Identical::<lambda_6c9857087f6484280d6d6ec01ce267b9>,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: afc30a0dc00e13d5f0ed522e98ba7074f21d2813
- **Python version**: Python 3.6.2 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: 0.10
- **GCC/Compiler version (if compiling from source)**: Microsoft (R) C/C++ Optimizing Compiler Version 19.12.25835 for x64
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/aot:tfcompile`

### Describe the problem
After applying #16904 to fix #16882 compilation continues until it fails with:
```bash
.\tensorflow/compiler/xla/service/hlo_instruction.h(579): error C2446: ':': no conversion from 'xla::HloInstruction::Identical::<lambda_6c9857087f6484280d6d6ec01ce267b9>' to 'xla::HloInstruction::Identical::<lambda_687e181a7b8d05356bae3b6704b3fe49>'
```

### Source code / logs
Full error log
```bash
.\tensorflow/compiler/xla/service/hlo_instruction.h(579): error C2446: ':': no conversion from 'xla::HloInstruction::Identical::<lambda_6c9857087f6484280d6d6ec01ce267b9>' to 'xla::HloInstruction::Identical::<lambda_687e181a7b8d05356bae3b6704b3fe49>'
.\tensorflow/compiler/xla/service/hlo_instruction.h(579): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called
.\tensorflow/compiler/xla/service/hlo_instruction.h(580): error C3536: 'eq_shapes': cannot be used before it is initialized
.\tensorflow/compiler/xla/service/hlo_instruction.h(580): error C2064: term does not evaluate to a function taking 2 arguments
.\tensorflow/compiler/xla/service/hlo_instruction.h(595): error C2664: 'bool xla::HloInstruction::IdenticalSlowPath(const xla::HloInstruction &,const std::function<bool (const xla::HloComputation *,const xla::HloComputation *)> &,const std::function<bool (const xla::Shape &,const xla::Shape &)> &) const': cannot convert argument 3 from 'int' to 'const std::function<bool (const xla::Shape &,const xla::Shape &)> &'
.\tensorflow/compiler/xla/service/hlo_instruction.h(595): note: Reason: cannot convert from 'int' to 'const std::function<bool (const xla::Shape &,const xla::Shape &)>'
.\tensorflow/compiler/xla/service/hlo_instruction.h(595): note: No constructor could take the source type, or constructor overload resolution was ambiguous
Target //tensorflow/compiler/aot:tfcompile failed to build
```

See [attached file](https://github.com/tensorflow/tensorflow/files/1713411/build.log) for the complete build log.
"
16910,Saver is saving empty .meta and .data files,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu
- **TensorFlow installed from (source or binary)**: I don't know , it's on google colaboratory
- **TensorFlow version (use command below)**:1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: i don't know, it's on google colaboratory
- **GPU model and memory**: Tesla K40 12 gb
- **Exact command to reproduce**: summarize_K40.py
I'm saving my model using:
```
saver = tf.train.Saver()
saver.save(sess,checkpoint + str(round(update_loss / update_check, 3)) + '_e' +str(epoch_i),global_step=epoch_i)
```
but tensorflow creates all the files, but they are always empty i.e. 0 bytes
I'm attaching the source code, but I don't think it's my fault since once every 200 runs it actually creates non-empty files. I'm using google colaboratory
[summarize_K40 (copy).py.txt](https://github.com/tensorflow/tensorflow/files/1713321/summarize_K40.copy.py.txt)
"
16907,Multi-GPU could not provide performance improve with dataset API,"I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.

```
import tensorflow as tf
import numpy as np
import time
import os

#dataset with 1000 vectors
dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []


for i in range(500):
    for j in range(2):
        with tf.device(""/gpu:%d"" % j):
            with tf.name_scope(""Tower_%d"" % j) as scope:
                operand = iterator.get_next()
                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
                tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```
I have 2 GPUs and this program takes 0.68 seconds to finish.
When I change to a single GPU execution mode:

```
import tensorflow as tf
import numpy as np
import time
import os

os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []

with tf.device(""/gpu:0""):
    for i in range(1000):
        operand = iterator.get_next()
        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
        tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```

It takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?

python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

Thanks!
"
16906,"python 3.6 + latest tf = infinity of ""msgpack_numpy.py:142 PendingDeprecationWarning: encoding is deprecated."" ","Symptom, trying to train resnet-50 imagenet @ppwwyyxx 's [imagenet-resnet.py](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py) script fails to start after 5 minutes, because of some thread being blocked trying to flush a gazillion of warning messages above

Since this problem is solved by downgrading from ""tf-nightly-gpu"" (`1.7.0-dev20180208`) to tensorflow-gpu 1.5, TensorFlow must be blame for introducing a new usage of this deprecated method. Unfortunately the warning messages are quite uninformative, with no indication of who is calling this :/

Note that Python 3.6 is the only Python 3 version supported on Amazon Deep Learning conda AMI images.

Warning messages look like this:

```
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)

```"
16900,Restoring a model trained with tf.estimator and feeding input through feed_dict,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5
- **Python version**: 
3.6.2
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
NVIDIA 1050 4 GB

I trained a resnet with tf.estimator, the model was saved during the training process. The saved files consist of `.data`, `.index` and `.meta`. I'd like to load this model back and get predictions for new images. The data was fed to the model during training using `tf.data.Dataset`.  I have closely followed the resnet implementation given [here][1].

I would like to restore the model and feed inputs to the nodes using a feed_dict. 
**My attempt**
      #rebuild input pipeline
      images, labels = input_fn(data_dir, batch_size=32, num_epochs=1)
        
      #rebuild graph
      prediction= imagenet_model_fn(images,labels,{'batch_size':32,'data_format':'channels_first','resnet_size':18},mode = tf.estimator.ModeKeys.EVAL).predictions 
    
      saver  = tf.train.Saver()
      with tf.Session() as sess:
        ckpt = tf.train.get_checkpoint_state(r'./model')
        saver.restore(sess, ckpt.model_checkpoint_path)
        while True:
        try:
            pred,im= sess.run([prediction,images])
            print(pred)
        except tf.errors.OutOfRangeError:
          break

I fed a dataset which was evaluated on the same model using `classifier.evaluate`, but the above method gives wrong predictions. The model gives same class and probability, 1.0, for all images.

The code I used for training and building the model is as below:

Specification for parsing the dataset:

    def parse_record(raw_record, is_training):
      keys_to_features = {
          'image/encoded':
              tf.FixedLenFeature((), tf.string, default_value=''),
          'image/class/label':
              tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),
      }
      parsed = tf.parse_single_example(raw_record, keys_to_features)
      image = tf.image.decode_image(
          tf.reshape(parsed['image/encoded'], shape=[]),3)
      image = tf.image.convert_image_dtype(image, dtype=tf.float32)
      label = tf.cast(
          tf.reshape(parsed['image/class/label'], shape=[]),
          dtype=tf.int32)
      return image, tf.one_hot(label,2)

The following function parses the data and creates batches for training

    def input_fn(is_training, data_dir, batch_size, num_epochs=1):
      dataset = tf.data.Dataset.from_tensor_slices(
          filenames(is_training, data_dir))
      if is_training:
         dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
      dataset = dataset.flat_map(tf.data.TFRecordDataset)
      dataset = dataset.map(lambda value: parse_record(value, is_training),
                            num_parallel_calls=5)
      dataset = dataset.prefetch(batch_size)
      if is_training:
          dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
      dataset = dataset.repeat(num_epochs)
      dataset = dataset.batch(batch_size)
    
      iterator = dataset.make_one_shot_iterator()
      images, labels = iterator.get_next()
      return images, labels

A classifier is created as below for training on train set and evaluation on validation set

    classifier = tf.estimator.Estimator(
          model_fn=model_function, model_dir=flags.model_dir, config=run_config,
          params={
              'resnet_size': flags.resnet_size,
              'data_format': flags.data_format,
              'batch_size': flags.batch_size,
          })
    
        #Training cycle
         classifier.train(
             input_fn=lambda: input_function(
                 training_phase=True, flags.data_dir, flags.batch_size, flags.epochs_per_eval),
             hooks=[logging_hook])
        # Evaluate the model 
        eval_results = classifier.evaluate(input_fn=lambda: input_function(
            training_phase=False, flags.data_dir, flags.batch_size))

This is how I tried to load and get predictions from the model.
I'd like to feed images through a `feed_dict` so that I can see the model's performance on individual images.
What is the right way to restore a saved model and perform inference on it. I want to feed images directly without using `tf.data.Dataset`.  I had opened a [question ](https://stackoverflow.com/questions/48679622/restoring-a-model-trained-with-tf-estimator-and-feeding-input-through-feed-dict)on stackoverflow, but didn't get any response. I'm wondering if there really is a feature that can help with restoring a model trained with `tf.estimator` and feeding images through a `feed_dict`. 


  [1]: https://github.com/deepaksuresh/models/blob/master/official/resnet/resnet.py

"
16899,TF 1.5.0 Java API broken in Ubuntu 14.04: `GLIBCXX_3.4.20' not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The 1.5.0 Java API fails at runtime on Ubuntu 14.04 (after working okay for 1.3/1.4) on the hello world example. I originally noticed this behavior in CentOS 7; I'm reporting this as a bug for Ubuntu 14.04 because it is officially supported in the [docs](https://www.tensorflow.org/install/install_java#supported_platforms) and the error appears to be the same.

Possibly related to #15777, in which case the solution may just be to build `libtensorflow_jni.so` on Ubuntu 14. Thanks in advance for looking into this!

### Source code / logs
1. (Optionally) use docker container with Java 8 and Maven: 
`docker pull goyalzz/ubuntu-java-8-maven-docker-image`
`docker run -it goyalzz/ubuntu-java-8-maven-docker-image:latest bash`
2. Follow instructions in [official Java API Maven example](https://www.tensorflow.org/install/install_java#example).
3. Upon Step 3 of above (` mvn -q compile exec:java`), observe the following error:

```
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:java (default-cli) on project hellotf: An exception occured while executing the Java class. /tmp/tensorflow_native_libraries-1518205710861-0/libtensorflow_jni.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /tmp/tensorflow_native_libraries-1518205710861-0/libtensorflow_jni.so) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
```"
16898,Tensorflow build with MKL-DNN produces garbage results,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (running a keras example)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.6.0-rc0-0-gaaf367e, 1.6.0-rc0
- **Python version**: 3.6.4 (Anaconda)
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:  see below

### Describe the problem
I have built Tensorflow 1.6.0-rc0 from source in Ubuntu 16.04 with MKL-DNN support following [this guide][1]. The build proceeds without any problem. Testing it with keras 2.1.3 on a simple convnet from [this example][2] ""as is"" is two times slower than with the non-MKL build.

Now, tuning the MKL parameters as recommended in [the guide][1] leads to almost 2 times speedup over the non-MKL build. But produces complete nonsense in terms of accuracy (and loss):

[![Erroneous results from Tensorflow MKL-DNN build][3]][3]

This comes with no errors or warnings from the console. The CPU is an i7-4790K.

For reference, results obtained from a run without tuning the MKL parameters are as expected:

[![Correct results from Tensorflow][4]][4]

  [1]: https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn
  [2]: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb
  [3]: https://i.stack.imgur.com/7EUmX.png
  [4]: https://i.stack.imgur.com/8s8jn.png

### Source code / logs
The MKL parameters were tuned as follows:
~~~
from keras import backend as K
K.set_session(K.tf.Session(config=K.tf.ConfigProto(inter_op_parallelism_threads=1)))
os.environ[""KMP_BLOCKTIME""] = ""0""
os.environ[""KMP_AFFINITY""] = ""granularity=fine,verbose,compact,1,0""
~~~
and following settings were printed in the console:
~~~
OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7}
OMP: Info #156: KMP_AFFINITY: 8 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)
OMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5323 thread 0 bound to OS proc set {0}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5334 thread 1 bound to OS proc set {1}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5335 thread 2 bound to OS proc set {2}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5336 thread 3 bound to OS proc set {3}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5337 thread 4 bound to OS proc set {4}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5338 thread 5 bound to OS proc set {5}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5339 thread 6 bound to OS proc set {6}
OMP: Info #247: KMP_AFFINITY: pid 5271 tid 5340 thread 7 bound to OS proc set {7}
~~~"
16895,Variables disappearing when freezing graph with (fused) BatchNorm,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm using an inception_resnet_v2 from the slim model zoo
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6.0rc0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: 1080, 8GB
- **Exact command to reproduce**: - 

### Describe the problem
There seem to be a bug in handling fused BatchNorm's variables when freezing a graph.
Both `moving_mean` and `moving_average` disappear from the graph after `inference_graph = extract_sub_graph(input_graph_def, output_node_names)` in `graph_util.convert_variables_to_constants` and this breaks afterwards the `optimize_for_inference` script, that searches for those two variables-turned-const.

From inspecting the input and output GraphDef, what I think is the culprit is the definition of the FusedBatchNorm node:

<details>
  <summary>Node GraphDef</summary>

```
node {
  name: ""InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm""
  op: ""FusedBatchNorm""
  input: ""InceptionResnetV2/Conv2d_1a_3x3/Conv2D""
  input: ""InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const""
  input: ""InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta/read""
  input: ""InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_1""
  input: ""InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_output_shapes""
    value {
      list {
        shape {
          dim {
            size: -1
          }
          dim {
            size: 149
          }
          dim {
            size: 149
          }
          dim {
            size: 32
          }
        }
        shape {
          dim {
            size: 32
          }
        }
        shape {
          dim {
            size: 32
          }
        }
        shape {
          dim {
            size: 32
          }
        }
        shape {
          dim {
            size: 32
          }
        }
      }
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""epsilon""
    value {
      f: 0.001
    }
  }
  attr {
    key: ""is_training""
    value {
      b: true
    }
  }
}
```

</details>

The `input`s don't mention neither the mean or average, my suspicion is that `extract_sub_graph` does not detect them as part of the graph to be kept and strips the nodes.

"
16894,`control_dependencies` unexpected behavior with tensorflow>=1.3,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: from binary
- **TensorFlow version (use command below)**: 1.1.0, 1.3.0, 1.5.0
- **Python version**:  2.7.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**:
gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) for compiling custom ops.
- **CUDA/cuDNN version**: do not matter
- **GPU model and memory**: do not matter 
- **Exact command to reproduce**: As following

### Describe the problem

For the code below: (the complete sample follows later)
```python
first_op = custom_ops_module.custom_first(features_v)
with tf.control_dependencies([first_op]):
    second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

for i in range(5):
    sess.run(second_op)
```
According to [the doc of control dependencies](https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies), `second_op` will only run after `first_op` has executed.
However, this is not true for tensorflow 1.3+ when the ops are customized cpp codes.

* tensorflow 1.1
The code runs exactly what I want: a `first_op`-`second_op` loop.
* tensorflow 1.3
`second_op` **runs first**, followed the `first_op`-`second_op` loop.
* tensorflow 1.5
`second_op` runs **first and only once globally**, then `first_op` runs again and again...

### Source code / logs

To reproduce, a customized op is need (C++ source and compile script attached). The Python part loads the compiled `custom_ops.so` and behaves as described above, for TensorFlow 1.1, 1.3 and 1.5, each with log provided.

```python
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

from __future__ import print_function

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
custom_ops_module = tf.load_op_library('./custom_ops.so')

def run():
    features_v = tf.Variable(
            tf.zeros(shape=[480, 256]), 
            name='features_v',
            trainable=False,
            collections=[tf.GraphKeys.LOCAL_VARIABLES])
    first_op = custom_ops_module.custom_first(features_v)
    with tf.control_dependencies([first_op]):
        second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

    sess_config = tf.ConfigProto()
    sess_config.allow_soft_placement = True 

    sess = tf.Session(config=sess_config)

    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    print('init vars done')

    print('start loop')
    for i in range(5):
        print('loop: ', i)
        sess.run(second_op)

if __name__ == '__main__':
    print(""tensorflow"", tf.__version__)
    run()

```

The custom ops
```cpp
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

#include <glog/logging.h>

///////////////////////////////////////// NameSpace ////////////////////////////////////////////

using tensorflow::DEVICE_CPU;
using tensorflow::Tensor;

///////////////////////////////////////// CustomFirstOp ////////////////////////////////////////
REGISTER_OP(""CustomFirst"")
    .Input(""features: float32"")
    ;
class CustomFirstOp : public tensorflow::OpKernel {
 public:
  explicit CustomFirstOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      const Tensor& input_tensor = context->input(0);
      LOG(INFO) << ""FirstOp enter with "" << input_tensor.DebugString(); 
      LOG(INFO) << ""FirstOp leave"";
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomFirst"").Device(DEVICE_CPU), CustomFirstOp);
///////////////////////////////////////// CustomSecondOp ///////////////////////////////////////
REGISTER_OP(""CustomSecond"")
    .Output(""images: uint8"")
    ;
class CustomSecondOp : public tensorflow::OpKernel {
 public:
  explicit CustomSecondOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      LOG(INFO) << ""SecondOp enter"";
      tensorflow::Tensor* output_tensor = nullptr;
      OP_REQUIRES_OK(context, context->allocate_output(0, tensorflow::TensorShape({480, 220, 220, 1}), &output_tensor));
      auto output_tensor_buffer = output_tensor->shaped<tensorflow::uint8, 4>({480, 220, 220, 1});
      LOG(INFO) << ""SecondOp leave with "" << output_tensor->DebugString();
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomSecond"").Device(DEVICE_CPU), CustomSecondOp);
//////////////////////////////////////// Op End //////////////////////////////////////////////////////

```
Compile scripts for tensorflow 1.1 and 1.3
```
#!/bin/bash
TF_INC=( $(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC -I $TF_INC -lglog
```
Compile scripts for tensorflow 1.5
```
#!/bin/bash
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -lglog
```
Log with tensorflow 1.1
```
tensorflow 1.1.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:04:19.464854 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.464900 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.464946 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.486101 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:04:19.507359 31266 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.507411 31266 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.507489 31269 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.507637 31269 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:04:19.547608 31269 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.547644 31269 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.547683 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.547745 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:04:19.579887 31267 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.579923 31267 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.579952 31270 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.580021 31270 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:04:19.874367 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.874406 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.874454 31267 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.874563 31267 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.3
```
tensorflow 1.3.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:03:34.893167 28884 custom_ops.cc:36] SecondOp enter
I0210 00:03:34.893406 28884 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:03:35.004695 29283 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.004729 29283 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.004763 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.025548 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:03:35.031692 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.031735 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.031818 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.051908 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:03:35.056428 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.056473 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.056516 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.056560 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:03:35.061014 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.061050 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.061085 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.061139 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:03:35.065387 29282 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.065429 29282 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.065500 29281 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.065654 29281 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.5
```
tensorflow 1.5.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:39:03.166337 27046 custom_ops.cc:36] SecondOp enter
I0210 00:39:03.166424 27046 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:39:03.868782 27965 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.868820 27965 custom_ops.cc:23] FirstOp leave
loop:  1
I0210 00:39:03.879077 27964 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.879094 27964 custom_ops.cc:23] FirstOp leave
loop:  2
I0210 00:39:03.893448 27962 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.893474 27962 custom_ops.cc:23] FirstOp leave
loop:  3
I0210 00:39:03.906409 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.906440 27963 custom_ops.cc:23] FirstOp leave
loop:  4
I0210 00:39:03.917815 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.917836 27963 custom_ops.cc:23] FirstOp leave
```"
16893,Building TensorFlow v1.5.0 from sources does not detect bazel v0.10.0 correctly,"It looks like build script uses strings for version comparison, i.e. tuple `(""0"", ""10"", ""0"") < (""0"", ""4"", ""5"")`, hence Bazel version 0.10.0 is considered less then required version 0.4.5:
```
user@server:~/tensorflow-1.5.0$ bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
ERROR: /home/user/tensorflow-1.5.0/WORKSPACE:15:1: Traceback (most recent call last):
        File ""/home/user/tensorflow-1.5.0/WORKSPACE"", line 15
                closure_repositories()
        File ""/home/user/.cache/bazel/_bazel_user/999dc7eb04d16a3dd0103e4cb7e7e45c/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
                _check_bazel_version(""Closure Rules"", ""0.4.5"")
        File ""/home/user/.cache/bazel/_bazel_user/999dc7eb04d16a3dd0103e4cb7e7e45c/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
                fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
```
OS Platform and Distribution: SuSE Linux 12.3
TensorFlow installed from: [this source](https://github.com/tensorflow/tensorflow/archive/v1.5.0.zip)
TensorFlow version: 1.5.0
Bazel version: 0.10.0, compiled from [this source](https://github.com/bazelbuild/bazel/releases/download/0.10.0/bazel-0.10.0-dist.zip)
CUDA/cuDNN version: N/A / was not enabled
GPU model and memory: N/A / was not enabled
Exact command to reproduce: `bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`"
16891,How show detecting image.,"When I start a object_detection_tutorial script via ipython, it's didn't show me any image.  My console:
![image](https://user-images.githubusercontent.com/17855733/36030750-0f96f458-0db1-11e8-867c-7d545c6e3170.png)
But something is starting in process because starting some python program, but before it's  open, the program is closed.
![image](https://user-images.githubusercontent.com/17855733/36030857-713182fa-0db1-11e8-9b25-c208d0979d12.png)
code is here: https://pastebin.com/eh0SWjyU"
16889,Problems Getting TensorFlow to behave Deterministically,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes, the code I am working with is company proprietary. If you need an example I will need to try to extract something that I can share. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Mac OS 10.13.2

- **TensorFlow installed from (source or binary)**:

binary

- **TensorFlow version (use command below)**:

v1.3.0-rc2-20-g0787eee 1.3.0

- **Python version**: 

3.6.4

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:

Running on CPU.

- **GPU model and memory**:

- **Exact command to reproduce**:

Please see description of problem. I don't currently have an example I can give you but I am hoping that the information I am providing is enough to get at least some idea about what is going on.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am having a major struggle trying to get TensorFlow to behave in a deterministic manner. I started out trying to compare two different versions of my source code, but the numeric differences from one execution to the next were large enough to make comparisons problematic.

So I went to trying to run exactly the same code in two different shells. And this is where things got weird.

I went through the code, and everywhere where TensorFlow is imported, everywhere I did operations using the form ""with graph as default:"", and prior to each call to run I added in a call to     tf.set_random_seed(42) [and yes, I used 42 in all of the calls].

I also placed breakpoints before anything in my code which was a tf call with 'random' in the name (I even searched the docs to see if there were random calls that didn't have random in the name, I found a couple but I am not using them). In all of the random calls that I am using I added in a seed=42 argument. So as far as I can tell I have covered all of the bases.

So what happens? I run the code in one shell, get a set of numbers. Run code in a second shell, get identical numbers. Good. Run the code a few more times, keep getting the same numbers. Better.
Run the code again, get completely different numbers!!! If I keep running the code in the different shells sometimes I get the same numbers from execution to execution. Sometimes I get different numbers. Sometimes it goes on to completely new numbers. Sometimes it goes back to an older set of numbers.

So I am getting something that is consistent enough that it feels like I haven't missed any places where I need to set the seed, but not consistent enough to say that it is deterministic.

I really want to be able to run the code over and over again and get exactly the same numeric result. Is this an unreasonable expectation? 

Can anyone offer an explanation as to why the code would generate exactly the same result (and I'm looking down to 8 decimal places so this is fairly precise) five times in a row, and then switch to a completely different result and then repeat that result multiple times in a row?

I can certainly try to reduce this into an example I can share (the model I am using is a toy model we use for unit testing). I wanted to see if anyone had any insight into the general problem before I put the time into that.




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

I will include two examples of the numeric output I am getting. This data is part of the returned result from Session.run().

Example A:

out_node_weights [[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184
   9.10322666  9.09527016  9.10811996  9.09029293]
 [ 9.34178257  9.34915924  9.34216785  9.34740925  9.32617569  9.34794617
   9.34459496  9.35203457  9.34202194  9.34494209]
 [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184
   9.10322666  9.09527016  9.10811996  9.09029293]
 [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184
   9.10322666  9.09527016  9.10811996  9.09029293]
 [ 9.59115124  9.59841442  9.59205914  9.59892082  9.58935356  9.59801197
   9.59655094  9.5970974   9.5989027   9.59136868]
 [ 9.39457417  9.39968395  9.39309788  9.4028101   9.40468502  9.39697838
   9.39671326  9.39805126  9.39696026  9.39565849]]

Example B:

out_node_weights [[ 9.04592514  9.05451775  9.05065632  9.05263042  9.06681919  9.05208111
   9.05834484  9.05059719  9.06315136  9.04525948]
 [ 9.25311852  9.26151276  9.25516319  9.25806046  9.23816681  9.26067734
   9.25860214  9.26416397  9.25700569  9.25596619]
 [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269
   9.27913475  9.27859116  9.28033924  9.27534485]
 [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269
   9.27913475  9.27859116  9.28033924  9.27534485]
 [ 9.50198936  9.51004601  9.50419044  9.50922203  9.50067139  9.50993729
   9.50946426  9.5085659   9.51255226  9.50196838]
 [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269
   9.27913475  9.27859116  9.28033924  9.27534485]]"
16888,tfcompile fails with Executor failed to create kernel. Not found: No registered 'ResizeNearestNeighbor' ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, very simple CNN (attached)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 926fc13
- **Python version**: python 3.6.4
- **Bazel version (if compiling from source)**: 0.10
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: ./bazel-bin/tensorflow/compiler/aot/tfcompile  --graph=""/data/models/tensorflow/testmodel.pb"" --config=""/data/models/tensorflow/testmodel.pbtxt""

### Describe the problem
Trying to compile a trivial model, consisting of a single nearest-neighbour resize layer, I get an abort trap 6. The model graph and config can be [found here](https://github.com/tensorflow/tensorflow/files/1710673/testmodel.zip).

### Source code / logs
```bash
./bazel-bin/tensorflow/compiler/aot/tfcompile  --graph=""/data/models/tensorflow/testmodel.pb"" --
config=""/data/models/tensorflow/testmodel.pbtxt""
2018-02-09 11:45:13.215728: E tensorflow/compiler/tf2xla/graph_compiler.cc:115] Executor failed to create kernel. Not found: No registered 'ResizeNearestNeighbor' OpKernel for XLA_CPU_JIT devices compatible with node out/ResizeNearestNeighbor = ResizeNearestNeighbor[T=DT_FLOAT, align_corners=false](aot_feed_0/in, out/mul)
        .  Registered:  <no registered kernels>

         [[Node: out/ResizeNearestNeighbor = ResizeNearestNeighbor[T=DT_FLOAT, align_corners=false](aot_feed_0/in, out/mul)]]
2018-02-09 11:45:13.216094: F tensorflow/compiler/aot/tfcompile_main.cc:148] Non-OK-status: status status: Not found: No registered 'ResizeNearestNeighbor' OpKernel for XLA_CPU_JIT devices compatible with node out/ResizeNearestNeighbor = ResizeNearestNeighbor[T=DT_FLOAT,
align_corners=false](aot_feed_0/in, out/mul)
        .  Registered:  <no registered kernels>

         [[Node: out/ResizeNearestNeighbor = ResizeNearestNeighbor[T=DT_FLOAT, align_corners=false](aot_feed_0/in, out/mul)]]
Abort trap: 6
```"
16887,A problem about gcc 5.5.0. It does not compile TensorFlow.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Slackware Linux 14.2 64 bit

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.5.0

- **Python version**: 
3.6.4

- **Bazel version (if compiling from source)**:
0.5.4

- **GCC/Compiler version (if compiling from source)**:
5.5.0

- **CUDA/cuDNN version**:
9.0/7

- **GPU model and memory**:
1050Ti / 4Gb

- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.


#### gcc 5.5.0 does not compile TF source code.

First of all, sorry for that I'm NOT using Ubuntu Linux for TF. I know that Ubuntu Linux is the only supported Linux system for TF. Instead, I use an ancient distribution, i.e., Slackware Linux. Recently, I got a security update for Spectre. As a side-effect, I also got an updated gcc (5.3.0 -> 5.5.0).

I usually compile the TensorFlow source code for optimization. However, it can not be compiled with updated gcc (5.5.0), like this;


```
INFO: From Compiling tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.cc:
/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9220): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9231): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9244): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib64/gcc/x86_64-slackware-linux/5.5.0/include/avx512fintrin.h(9255): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""
```


So I googled a little bit, and found the following issue:

#10220 

in the middle of the thread, I saw

> I think the problem here is that gcc-5.5 shipped with avx512*intrin.h headers that switched to using void* and const void* (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=76731) but without switching the builtins to do the same. This is why 5.4 works but 5.5 breaks. 


so I tracked down the matter that I could see 

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=76731

> All of the scatter/gather intrinsics in avx512intrin.h use int/float/double pointers, which is incorrect. 

So, at first, I thought this problem is about gcc, but someone suggested that it's maybe related to CUDA.

> it's the GPU/CUDA code that doesn't support the new compiler so, if you want to build that, your only option is downgrade the compiler, until Nvidia releases a new CUDA sdk.

from https://www.linuxquestions.org/questions/showthread.php?p=5817669#post5817669

To wrap up, I was able to compile TF successfully with gcc 5.3.0. With gcc 5.5.0, I get error messages here and there, and yet I do not know what makes this errors. I suspect the combination of gcc and CUDA (and also TensorFlow) does not work well, but I still can not figure out which of them makes this fault.

Thank you for your help.


Best regards,
sungjin.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16886,Please support Cuda 9.1,"I tried cloning the alpha zero chess from https://github.com/Zeta36/chess-alpha-zero and almost got everything to work.  However when I try ""python src/chess_zero/run.py self"" I get the message 

ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit 

The problem is that I have CUDA 9.1 and cannot seem to get CUDA 9.0 on the website.  Would it be hard to make a version of TensorFlow-gpu that works with CUDA 9.1 ?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10  
 
- **TensorFlow installed from (source or binary)**:
Installed from https://www.tensorflow.org/  

- **TensorFlow version (use command below)**:
1.5

- **Python version**:
 3.6.3

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
NVIDIA GeForce GTX 1050 Ti  8GB Ram

- **Exact command to reproduce**:
python src/chess_zero/run.py self

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

e:\eDownloads>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\User\Anaconda3\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.  
I am requesting an update because I have CUDA 9.1 and can't get CUDA 9.0

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16884,The output_type of NodeDef should support DT_INT64,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem

Now we try to export the model to be load in Android and iOS devices. The exported operators includes `tf.argmax` which has the output_type of long(tf.DT_INT64). Once the model is used for inference it throws these error.

```
2018-02-09 10:45:15.690271: E /Users/tobe/code/tensorflow_template_application/ios_client/RunModelViewController.mm:194] Running model failed: Invalid argument: NodeDef mentions attr 'output_type' not in Op<name=ArgMax; signature=input:T, dimension:Tidx -> output:int64; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; 
```

The source code about this operator is in https://github.com/tobegit3hub/tensorflow_template_application/blob/master/dense_classifier.py#L422 .

I'm not really digging into the code why it supports float and int only. It would be great if it can support more output_types so that the models don't less modification for deployment.

### Source code / logs

None
"
16883,Feauture Request: Multidimensional RNN,"I would like to contribute a Multidimensional RNN feature in the contrib directory based on the implementation mentioned [here](https://github.com/tensorflow/tensorflow/issues/1453#issuecomment-194976468). 

> Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc. Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results. And maybe feed the result into another RNN.

By any chance, is it related to the [main paper](https://arxiv.org/abs/0705.2011) for it?"
16882,Compilation error when building tfcompile on Windows / Visual Studio 2017,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master / 926fc13f7378d14fa7980963c4fe774e5922e336
- **Python version**: Python 3.6.2 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: 0.10
- **GCC/Compiler version (if compiling from source)**: Microsoft (R) C/C++ Optimizing Compiler Version 19.12.25835 for x64
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/aot:tfcompile`


### Describe the problem
Trying to build tfcompile on Windows, using the instructions at https://github.com/rongjiecomputer/tensorflow-xla-aot-windows and https://github.com/tensorflow/tensorflow/issues/15213 results in a compilation error.

### Source code / logs
```
tensorflow/compiler/xla/literal_util.cc(237): error C2668: 'xla::Literal::data': ambiguous call to overloaded function
.\tensorflow/compiler/xla/literal_util.h(851): note: could be 'tensorflow::gtl::MutableArraySlice<tensorflow::uint8> xla::Literal::data<tensorflow::uint8>(const xla::ShapeIndex &)'
.\tensorflow/compiler/xla/literal_util.h(845): note: or       'tensorflow::gtl::ArraySlice<tensorflow::uint8> xla::Literal::data<tensorflow::uint8>(const xla::ShapeIndex &) const'
tensorflow/compiler/xla/literal_util.cc(237): note: while trying to match the argument list '()'
tensorflow/compiler/xla/literal_util.cc(450): note: see reference to function template instantiation 'tensorflow::Status xla::Literal::CopySliceFromInternal<tensorflow::uint8>(const xla::Literal &,tensorflow::gtl::ArraySlice<tensorflow::int64>,tensorflow::gtl::ArraySlice<tensorflow::int64>,tensorflow::gtl::ArraySlice<tensorflow::int64>)' being compiled
tensorflow/compiler/xla/literal_util.cc(237): error C2672: 'StridedCopy': no matching overloaded function found
Target //tensorflow/compiler/aot:tfcompile failed to build
```
See [attached file](https://github.com/tensorflow/tensorflow/files/1709297/bazel.log) for the complete bazel log."
16875,Update Dockerfile to install Python 3.6,"The Python 3 containers at gcr.io/tensorflow/tensorflow generated by [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) are currently built using Python 3.5.

It would be nice to update them to Python 3.6 instead, which I think would be as simple as adding `sudo apt-get install python3.6`.

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
"
16873,Undefined symbol when compiling a custom op,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: https://github.com/mozilla/DeepSpeech/blob/master/native_client/beam_search.cc
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430 1.5.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**: 9.1/7
- **GPU model and memory**: TitanX 12Gb x 2
- **Exact command to reproduce**:
Makefile:
```
CXX := g++
CFLAGS := -Wall -O3 -std=c++11

# KenLM flags.
SRC_DIR := kenlm
OBJ_DIR := kenlm

SRC_FILES := $(filter-out $(wildcard $(SRC_DIR)/util/*main*) $(wildcard $(SRC_DIR)/util/*test*), $(wildcard $(SRC_DIR)/util/*.cc))
SRC_FILES := $(SRC_FILES) $(filter-out $(wildcard $(SRC_DIR)/lm/*main*) $(wildcard $(SRC_DIR)/lm/*test*), $(wildcard $(SRC_DIR)/lm/*.cc))
SRC_FILES := $(SRC_FILES) $(filter-out $(wildcard $(SRC_DIR)/util/double-conversion/*main*) $(wildcard $(SRC_DIR)/util/double-conversion/*test*), $(wildcard $(SRC_DIR)/util/double-conversion/*.cc))

OBJ_FILES := $(patsubst $(SRC_DIR)/%.cc,$(OBJ_DIR)/%.o,$(SRC_FILES))

KENLM_CFLAGS := -I$(shell pwd)/kenlm -DNDEBUG -DKENLM_MAX_ORDER=6
KENLM_LFLAGS := -L$(shell pwd)/kenlm/build/lib -lkenlm -lkenlm_builder -lkenlm_filter -lkenlm_interpolate -lkenlm_util -lz -lbz2 -llzma

# TensorFlow flags.
TF_CFLAGS := -I$(shell pwd)/tensorflow

TF_CFLAGS := $(TF_CFLAGS) $(shell python3 -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))')
TF_LFLAGS := $(shell python3 -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))')

.PHONY: clean ctc_decoder.so generate_trie

default: clean ctc_decoder.so generate_trie

clean:
	rm -f $(OBJ_FILES)
	rm -f ctc_decoder.so
	rm -f generate_trie

ctc_decoder.so: $(OBJ_FILES)
	$(CXX) $(CFLAGS) $(KENLM_CFLAGS) $(TF_CFLAGS) -fPIC -shared -o $@ beam_search.cc $^ $(TF_LFLAGS)

$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cc
	$(CXX) $(CFLAGS) $(KENLM_CFLAGS) -fPIC -c -o $@ $<

generate_trie:
	$(CXX) $(CFLAGS) $(KENLM_CFLAGS) generate_trie.cc -o $@ $(KENLM_LFLAGS)

```

Python:
```
import tensorflow as tf
ctc_module = tf.load_op_library('/home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc_decoder.so')
```
### Describe the problem
I compiled a custom op (mozilla's ctc decoder with lm) using the make file I pasted above and it compiles without a problem but when I try to load it I get an undefined symbol error.

### Source code / logs
```
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> ctc_module = tf.load_op_library('/home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc-decoder.so')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /home/thomas/projects/deepspeech2/deepspeech2/ctc/ctc-decoder.so: undefined symbol: _ZN10tensorflow15OpKernelContext10CtxFailureEPKciRKNS_6StatusE
```
"
16872,Tensorflow 1.5 tf.dynamic_partition on GPU calculates wrong gradient shapes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
None
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
pip install tensorflow-gpu==1.5.0

- **TensorFlow version (use command below)**:
- **Python version**: 
v1.5.0-0-g37aa430d84 1.5.0
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7_7.0.5.15-1
- **GPU model and memory**:
GTX 1080 ti
- **Exact command to reproduce**:
See source code below


### Describe the problem
Using Tensorflow 1.5 with GPU Cuda 9.0 loss can not back propagate through tf.dynamic_partition.  This happens only for very specific partition lists and I have not figured out what exactly causes it based on the partition lists.  Attached is a partition list that does have the issue.

[membership.json.txt](https://github.com/tensorflow/tensorflow/files/1708232/membership.json.txt)



### Source code / logs
```python
import tensorflow as tf
import numpy as np
import json


def main(session):
  partitions_np = np.array(json.load(open('membership.json')))
  data_np = np.random.random(size=(len(partitions_np), 1))
  num_partitions = int(np.max(partitions_np)) + 1
  labels_np = np.zeros(shape=(num_partitions,))

  data = tf.placeholder(tf.float32, shape=data_np.shape)
  partitions = tf.placeholder(tf.int32, shape=partitions_np.shape)
  labels = tf.placeholder(tf.float32, shape=labels_np.shape)
  feed_dict = {
    data: data_np,
    partitions: partitions_np,
    labels: labels_np
  }

  data = tf.layers.Dense(1)(data)
  activated_par = tf.dynamic_partition(data, partitions, num_partitions)

  sparse_reps = [
    tf.reduce_mean(activated, 0, keepdims=True)
    for activated in activated_par
  ]

  output = tf.reduce_sum(sparse_reps, axis=1)
  loss = tf.reduce_sum(output - labels)
  session.run(tf.global_variables_initializer())

  momentum_train_op = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.1).minimize(loss=loss)
  adam_train_op = tf.train.AdamOptimizer().minimize(loss=loss)
  session.run(tf.global_variables_initializer())
  print(""Attempting to run momentum optimizer"")
  try:
    session.run(momentum_train_op, feed_dict=feed_dict)
  except Exception as e:
    print(e)

  print(""Attempting to run adam optimizer"")
  try:
    session.run(adam_train_op, feed_dict=feed_dict)
  except Exception as e:
    print(e)


if __name__ == ""__main__"":
  config = tf.ConfigProto(
    # If you lock to CPU it will not throw exception
    # device_count={'GPU': 0}
  )
  with tf.Session(config=config) as session:
    main(session)
```

Output
```
Attempting to run momentum optimizer
data[255].shape = [41,1] does not start with indices[255].shape = [28]
	 [[Node: gradients/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/DynamicPartition_grad/DynamicPartition/_2057, gradients/DynamicPartition_grad/DynamicPartition/_2059, gradients/DynamicPartition_grad/DynamicPartition/_2061, gradients/DynamicPartition_grad/DynamicPartition/_2063, gradients/DynamicPartition_grad/DynamicPartition/_2065, gradients/DynamicPartition_grad/DynamicPartition/_2067, gradients/DynamicPartition_grad/DynamicPartition/_2069, gradients/DynamicPartition_grad/DynamicPartition/_2071, gradients/DynamicPartition_grad/DynamicPartition/_2073, gradients/DynamicPartition_grad/DynamicPartition/_2075, gradients/DynamicPartition_grad/DynamicPartition/_2077, gradients/DynamicPartition_grad/DynamicPartition/_2079, gradients/DynamicPartition_grad/DynamicPartition/_2081, gradients/DynamicPartition_grad/DynamicPartition/_2083, gradients/DynamicPartition_grad/DynamicPartition/_2085, gradients/DynamicPartition_grad/DynamicPartition/_2087, gradients/DynamicPartition_grad/DynamicPartition/_2089, gradients/DynamicPartition_grad/DynamicPartition/_2091, gradients/DynamicPartition_grad/DynamicPartition/_2093, gradients/DynamicPartition_grad/DynamicPartition/_2095, gradients/DynamicPartition_grad/DynamicPartition/_2097, gradients/DynamicPartition_grad/DynamicPartition/_2099, gradients/DynamicPartition_grad/DynamicPartition/_2101, gradients/DynamicPartition_grad/DynamicPartition/_2103, gradients/DynamicPartition_grad/DynamicPartition/_2105, gradients/DynamicPartition_grad/DynamicPartition/_2107, gradients/DynamicPartition_grad/DynamicPartition/_2109, gradients/DynamicPartition_grad/DynamicPartition/_2111, gradients/DynamicPartition_grad/DynamicPartition/_2113, gradients/DynamicPartition_grad/DynamicPartition/_2115, gradients/DynamicPartition_grad/DynamicPartition/_2117, gradients/DynamicPartition_grad/DynamicPartition/_2119, gradients/DynamicPartition_grad/DynamicPartition/_2121, gradients/DynamicPartition_grad/DynamicPartition/_2123, gradients/DynamicPartition_grad/DynamicPartition/_2125, gradients/DynamicPartition_grad/DynamicPartition/_2127, gradients/DynamicPartition_grad/DynamicPartition/_2129, gradients/DynamicPartition_grad/DynamicPartition/_2131, gradients/DynamicPartition_grad/DynamicPartition/_2133, gradients/DynamicPartition_grad/DynamicPartition/_2135, gradients/DynamicPartition_grad/DynamicPartition/_2137, gradients/DynamicPartition_grad/DynamicPartition/_2139, gradients/DynamicPartition_grad/DynamicPartition/_2141, gradients/DynamicPartition_grad/DynamicPartition/_2143, gradients/DynamicPartition_grad/DynamicPartition/_2145, gradients/DynamicPartition_grad/DynamicPartition/_2147, gradients/DynamicPartition_grad/DynamicPartition/_2149, gradients/DynamicPartition_grad/DynamicPartition/_2151, gradients/DynamicPartition_grad/DynamicPartition/_2153, gradients/DynamicPartition_grad/DynamicPartition/_2155, gradients/DynamicPartition_grad/DynamicPartition/_2157, gradients/DynamicPartition_grad/DynamicPartition/_2159, gradients/DynamicPartition_grad/DynamicPartition/_2161, gradients/DynamicPartition_grad/DynamicPartition/_2163, gradients/DynamicPartition_grad/DynamicPartition/_2165, gradients/DynamicPartition_grad/DynamicPartition/_2167, gradients/DynamicPartition_grad/DynamicPartition/_2169, gradients/DynamicPartition_grad/DynamicPartition/_2171, gradients/DynamicPartition_grad/DynamicPartition/_2173, gradients/DynamicPartition_grad/DynamicPartition/_2175, gradients/DynamicPartition_grad/DynamicPartition/_2177, gradients/DynamicPartition_grad/DynamicPartition/_2179, gradients/DynamicPartition_grad/DynamicPartition/_2181, gradients/DynamicPartition_grad/DynamicPartition/_2183, gradients/DynamicPartition_grad/DynamicPartition/_2185, gradients/DynamicPartition_grad/DynamicPartition/_2187, gradients/DynamicPartition_grad/DynamicPartition/_2189, gradients/DynamicPartition_grad/DynamicPartition/_2191, gradients/DynamicPartition_grad/DynamicPartition/_2193, gradients/DynamicPartition_grad/DynamicPartition/_2195, gradients/DynamicPartition_grad/DynamicPartition/_2197, gradients/DynamicPartition_grad/DynamicPartition/_2199, gradients/DynamicPartition_grad/DynamicPartition/_2201, gradients/DynamicPartition_grad/DynamicPartition/_2203, gradients/DynamicPartition_grad/DynamicPartition/_2205, gradients/DynamicPartition_grad/DynamicPartition/_2207, gradients/DynamicPartition_grad/DynamicPartition/_2209, gradients/DynamicPartition_grad/DynamicPartition/_2211, gradients/DynamicPartition_grad/DynamicPartition/_2213, gradients/DynamicPartition_grad/DynamicPartition/_2215, gradients/DynamicPartition_grad/DynamicPartition/_2217, gradients/DynamicPartition_grad/DynamicPartition/_2219, gradients/DynamicPartition_grad/DynamicPartition/_2221, gradients/DynamicPartition_grad/DynamicPartition/_2223, gradients/DynamicPartition_grad/DynamicPartition/_2225, gradients/DynamicPartition_grad/DynamicPartition/_2227, gradients/DynamicPartition_grad/DynamicPartition/_2229, gradients/DynamicPartition_grad/DynamicPartition/_2231, gradients/DynamicPartition_grad/DynamicPartition/_2233, gradients/DynamicPartition_grad/DynamicPartition/_2235, gradients/DynamicPartition_grad/DynamicPartition/_2237, gradients/DynamicPartition_grad/DynamicPartition/_2239, gradients/DynamicPartition_grad/DynamicPartition/_2241, gradients/DynamicPartition_grad/DynamicPartition/_2243, gradients/DynamicPartition_grad/DynamicPartition/_2245, gradients/DynamicPartition_grad/DynamicPartition/_2247, gradients/DynamicPartition_grad/DynamicPartition/_2249, gradients/DynamicPartition_grad/DynamicPartition/_2251, gradients/DynamicPartition_grad/DynamicPartition/_2253, gradients/DynamicPartition_grad/DynamicPartition/_2255, gradients/DynamicPartition_grad/DynamicPartition/_2257, gradients/DynamicPartition_grad/DynamicPartition/_2259, gradients/DynamicPartition_grad/DynamicPartition/_2261, gradients/DynamicPartition_grad/DynamicPartition/_2263, gradients/DynamicPartition_grad/DynamicPartition/_2265, gradients/DynamicPartition_grad/DynamicPartition/_2267, gradients/DynamicPartition_grad/DynamicPartition/_2269, gradients/DynamicPartition_grad/DynamicPartition/_2271, gradients/DynamicPartition_grad/DynamicPartition/_2273, gradients/DynamicPartition_grad/DynamicPartition/_2275, gradients/DynamicPartition_grad/DynamicPartition/_2277, gradients/DynamicPartition_grad/DynamicPartition/_2279, gradients/DynamicPartition_grad/DynamicPartition/_2281, gradients/DynamicPartition_grad/DynamicPartition/_2283, gradients/DynamicPartition_grad/DynamicPartition/_2285, gradients/DynamicPartition_grad/DynamicPartition/_2287, gradients/DynamicPartition_grad/DynamicPartition/_2289, gradients/DynamicPartition_grad/DynamicPartition/_2291, gradients/DynamicPartition_grad/DynamicPartition/_2293, gradients/DynamicPartition_grad/DynamicPartition/_2295, gradients/DynamicPartition_grad/DynamicPartition/_2297, gradients/DynamicPartition_grad/DynamicPartition/_2299, gradients/DynamicPartition_grad/DynamicPartition/_2301, gradients/DynamicPartition_grad/DynamicPartition/_2303, gradients/DynamicPartition_grad/DynamicPartition/_2305, gradients/DynamicPartition_grad/DynamicPartition/_2307, gradients/DynamicPartition_grad/DynamicPartition/_2309, gradients/DynamicPartition_grad/DynamicPartition/_2311, gradients/DynamicPartition_grad/DynamicPartition/_2313, gradients/DynamicPartition_grad/DynamicPartition/_2315, gradients/DynamicPartition_grad/DynamicPartition/_2317, gradients/DynamicPartition_grad/DynamicPartition/_2319, gradients/DynamicPartition_grad/DynamicPartition/_2321, gradients/DynamicPartition_grad/DynamicPartition/_2323, gradients/DynamicPartition_grad/DynamicPartition/_2325, gradients/DynamicPartition_grad/DynamicPartition/_2327, gradients/DynamicPartition_grad/DynamicPartition/_2329, gradients/DynamicPartition_grad/DynamicPartition/_2331, gradients/DynamicPartition_grad/DynamicPartition/_2333, gradients/DynamicPartition_grad/DynamicPartition/_2335, gradients/DynamicPartition_grad/DynamicPartition/_2337, gradients/DynamicPartition_grad/DynamicPartition/_2339, gradients/DynamicPartition_grad/DynamicPartition/_2341, gradients/DynamicPartition_grad/DynamicPartition/_2343, gradients/DynamicPartition_grad/DynamicPartition/_2345, gradients/DynamicPartition_grad/DynamicPartition/_2347, gradients/DynamicPartition_grad/DynamicPartition/_2349, gradients/DynamicPartition_grad/DynamicPartition/_2351, gradients/DynamicPartition_grad/DynamicPartition/_2353, gradients/DynamicPartition_grad/DynamicPartition/_2355, gradients/DynamicPartition_grad/DynamicPartition/_2357, gradients/DynamicPartition_grad/DynamicPartition/_2359, gradients/DynamicPartition_grad/DynamicPartition/_2361, gradients/DynamicPartition_grad/DynamicPartition/_2363, gradients/DynamicPartition_grad/DynamicPartition/_2365, gradients/DynamicPartition_grad/DynamicPartition/_2367, gradients/DynamicPartition_grad/DynamicPartition/_2369, gradients/DynamicPartition_grad/DynamicPartition/_2371, gradients/DynamicPartition_grad/DynamicPartition/_2373, gradients/DynamicPartition_grad/DynamicPartition/_2375, gradients/DynamicPartition_grad/DynamicPartition/_2377, gradients/DynamicPartition_grad/DynamicPartition/_2379, gradients/DynamicPartition_grad/DynamicPartition/_2381, gradients/DynamicPartition_grad/DynamicPartition/_2383, gradients/DynamicPartition_grad/DynamicPartition/_2385, gradients/DynamicPartition_grad/DynamicPartition/_2387, gradients/DynamicPartition_grad/DynamicPartition/_2389, gradients/DynamicPartition_grad/DynamicPartition/_2391, gradients/DynamicPartition_grad/DynamicPartition/_2393, gradients/DynamicPartition_grad/DynamicPartition/_2395, gradients/DynamicPartition_grad/DynamicPartition/_2397, gradients/DynamicPartition_grad/DynamicPartition/_2399, gradients/DynamicPartition_grad/DynamicPartition/_2401, gradients/DynamicPartition_grad/DynamicPartition/_2403, gradients/DynamicPartition_grad/DynamicPartition/_2405, gradients/DynamicPartition_grad/DynamicPartition/_2407, gradients/DynamicPartition_grad/DynamicPartition/_2409, gradients/DynamicPartition_grad/DynamicPartition/_2411, gradients/DynamicPartition_grad/DynamicPartition/_2413, gradients/DynamicPartition_grad/DynamicPartition/_2415, gradients/DynamicPartition_grad/DynamicPartition/_2417, gradients/DynamicPartition_grad/DynamicPartition/_2419, gradients/DynamicPartition_grad/DynamicPartition/_2421, gradients/DynamicPartition_grad/DynamicPartition/_2423, gradients/DynamicPartition_grad/DynamicPartition/_2425, gradients/DynamicPartition_grad/DynamicPartition/_2427, gradients/DynamicPartition_grad/DynamicPartition/_2429, gradients/DynamicPartition_grad/DynamicPartition/_2431, gradients/DynamicPartition_grad/DynamicPartition/_2433, gradients/DynamicPartition_grad/DynamicPartition/_2435, gradients/DynamicPartition_grad/DynamicPartition/_2437, gradients/DynamicPartition_grad/DynamicPartition/_2439, gradients/DynamicPartition_grad/DynamicPartition/_2441, gradients/DynamicPartition_grad/DynamicPartition/_2443, gradients/DynamicPartition_grad/DynamicPartition/_2445, gradients/DynamicPartition_grad/DynamicPartition/_2447, gradients/DynamicPartition_grad/DynamicPartition/_2449, gradients/DynamicPartition_grad/DynamicPartition/_2451, gradients/DynamicPartition_grad/DynamicPartition/_2453, gradients/DynamicPartition_grad/DynamicPartition/_2455, gradients/DynamicPartition_grad/DynamicPartition/_2457, gradients/DynamicPartition_grad/DynamicPartition/_2459, gradients/DynamicPartition_grad/DynamicPartition/_2461, gradients/DynamicPartition_grad/DynamicPartition/_2463, gradients/DynamicPartition_grad/DynamicPartition/_2465, gradients/DynamicPartition_grad/DynamicPartition/_2467, gradients/DynamicPartition_grad/DynamicPartition/_2469, gradients/DynamicPartition_grad/DynamicPartition/_2471, gradients/DynamicPartition_grad/DynamicPartition/_2473, gradients/DynamicPartition_grad/DynamicPartition/_2475, gradients/DynamicPartition_grad/DynamicPartition/_2477, gradients/DynamicPartition_grad/DynamicPartition/_2479, gradients/DynamicPartition_grad/DynamicPartition/_2481, gradients/DynamicPartition_grad/DynamicPartition/_2483, gradients/DynamicPartition_grad/DynamicPartition/_2485, gradients/DynamicPartition_grad/DynamicPartition/_2487, gradients/DynamicPartition_grad/DynamicPartition/_2489, gradients/DynamicPartition_grad/DynamicPartition/_2491, gradients/DynamicPartition_grad/DynamicPartition/_2493, gradients/DynamicPartition_grad/DynamicPartition/_2495, gradients/DynamicPartition_grad/DynamicPartition/_2497, gradients/DynamicPartition_grad/DynamicPartition/_2499, gradients/DynamicPartition_grad/DynamicPartition/_2501, gradients/DynamicPartition_grad/DynamicPartition/_2503, gradients/DynamicPartition_grad/DynamicPartition/_2505, gradients/DynamicPartition_grad/DynamicPartition/_2507, gradients/DynamicPartition_grad/DynamicPartition/_2509, gradients/DynamicPartition_grad/DynamicPartition/_2511, gradients/DynamicPartition_grad/DynamicPartition/_2513, gradients/DynamicPartition_grad/DynamicPartition/_2515, gradients/DynamicPartition_grad/DynamicPartition/_2517, gradients/DynamicPartition_grad/DynamicPartition/_2519, gradients/DynamicPartition_grad/DynamicPartition/_2521, gradients/DynamicPartition_grad/DynamicPartition/_2523, gradients/DynamicPartition_grad/DynamicPartition/_2525, gradients/DynamicPartition_grad/DynamicPartition/_2527, gradients/DynamicPartition_grad/DynamicPartition/_2529, gradients/DynamicPartition_grad/DynamicPartition/_2531, gradients/DynamicPartition_grad/DynamicPartition/_2533, gradients/DynamicPartition_grad/DynamicPartition/_2535, gradients/DynamicPartition_grad/DynamicPartition/_2537, gradients/DynamicPartition_grad/DynamicPartition/_2539, gradients/DynamicPartition_grad/DynamicPartition/_2541, gradients/DynamicPartition_grad/DynamicPartition/_2543, gradients/DynamicPartition_grad/DynamicPartition/_2545, gradients/DynamicPartition_grad/DynamicPartition/_2547, gradients/DynamicPartition_grad/DynamicPartition/_2549, gradients/DynamicPartition_grad/DynamicPartition/_2551, gradients/DynamicPartition_grad/DynamicPartition/_2553, gradients/DynamicPartition_grad/DynamicPartition/_2555, gradients/DynamicPartition_grad/DynamicPartition/_2557, gradients/DynamicPartition_grad/DynamicPartition/_2559, gradients/DynamicPartition_grad/DynamicPartition/_2561, gradients/DynamicPartition_grad/DynamicPartition/_2563, gradients/DynamicPartition_grad/DynamicPartition/_2565, gradients/DynamicPartition_grad/DynamicPartition/_2567, gradients/Mean_grad/truediv, gradients/Mean_1_grad/truediv, gradients/Mean_2_grad/truediv, gradients/Mean_3_grad/truediv, gradients/Mean_4_grad/truediv, gradients/Mean_5_grad/truediv, gradients/Mean_6_grad/truediv, gradients/Mean_7_grad/truediv, gradients/Mean_8_grad/truediv, gradients/Mean_9_grad/truediv, gradients/Mean_10_grad/truediv, gradients/Mean_11_grad/truediv, gradients/Mean_12_grad/truediv, gradients/Mean_13_grad/truediv, gradients/Mean_14_grad/truediv, gradients/Mean_15_grad/truediv, gradients/Mean_16_grad/truediv, gradients/Mean_17_grad/truediv, gradients/Mean_18_grad/truediv, gradients/Mean_19_grad/truediv, gradients/Mean_20_grad/truediv, gradients/Mean_21_grad/truediv, gradients/Mean_22_grad/truediv, gradients/Mean_23_grad/truediv, gradients/Mean_24_grad/truediv, gradients/Mean_25_grad/truediv, gradients/Mean_26_grad/truediv, gradients/Mean_27_grad/truediv, gradients/Mean_28_grad/truediv, gradients/Mean_29_grad/truediv, gradients/Mean_30_grad/truediv, gradients/Mean_31_grad/truediv, gradients/Mean_32_grad/truediv, gradients/Mean_33_grad/truediv, gradients/Mean_34_grad/truediv, gradients/Mean_35_grad/truediv, gradients/Mean_36_grad/truediv, gradients/Mean_37_grad/truediv, gradients/Mean_38_grad/truediv, gradients/Mean_39_grad/truediv, gradients/Mean_40_grad/truediv, gradients/Mean_41_grad/truediv, gradients/Mean_42_grad/truediv, gradients/Mean_43_grad/truediv, gradients/Mean_44_grad/truediv, gradients/Mean_45_grad/truediv, gradients/Mean_46_grad/truediv, gradients/Mean_47_grad/truediv, gradients/Mean_48_grad/truediv, gradients/Mean_49_grad/truediv, gradients/Mean_50_grad/truediv, gradients/Mean_51_grad/truediv, gradients/Mean_52_grad/truediv, gradients/Mean_53_grad/truediv, gradients/Mean_54_grad/truediv, gradients/Mean_55_grad/truediv, gradients/Mean_56_grad/truediv, gradients/Mean_57_grad/truediv, gradients/Mean_58_grad/truediv, gradients/Mean_59_grad/truediv, gradients/Mean_60_grad/truediv, gradients/Mean_61_grad/truediv, gradients/Mean_62_grad/truediv, gradients/Mean_63_grad/truediv, gradients/Mean_64_grad/truediv, gradients/Mean_65_grad/truediv, gradients/Mean_66_grad/truediv, gradients/Mean_67_grad/truediv, gradients/Mean_68_grad/truediv, gradients/Mean_69_grad/truediv, gradients/Mean_70_grad/truediv, gradients/Mean_71_grad/truediv, gradients/Mean_72_grad/truediv, gradients/Mean_73_grad/truediv, gradients/Mean_74_grad/truediv, gradients/Mean_75_grad/truediv, gradients/Mean_76_grad/truediv, gradients/Mean_77_grad/truediv, gradients/Mean_78_grad/truediv, gradients/Mean_79_grad/truediv, gradients/Mean_80_grad/truediv, gradients/Mean_81_grad/truediv, gradients/Mean_82_grad/truediv, gradients/Mean_83_grad/truediv, gradients/Mean_84_grad/truediv, gradients/Mean_85_grad/truediv, gradients/Mean_86_grad/truediv, gradients/Mean_87_grad/truediv, gradients/Mean_88_grad/truediv, gradients/Mean_89_grad/truediv, gradients/Mean_90_grad/truediv, gradients/Mean_91_grad/truediv, gradients/Mean_92_grad/truediv, gradients/Mean_93_grad/truediv, gradients/Mean_94_grad/truediv, gradients/Mean_95_grad/truediv, gradients/Mean_96_grad/truediv, gradients/Mean_97_grad/truediv, gradients/Mean_98_grad/truediv, gradients/Mean_99_grad/truediv, gradients/Mean_100_grad/truediv, gradients/Mean_101_grad/truediv, gradients/Mean_102_grad/truediv, gradients/Mean_103_grad/truediv, gradients/Mean_104_grad/truediv, gradients/Mean_105_grad/truediv, gradients/Mean_106_grad/truediv, gradients/Mean_107_grad/truediv, gradients/Mean_108_grad/truediv, gradients/Mean_109_grad/truediv, gradients/Mean_110_grad/truediv, gradients/Mean_111_grad/truediv, gradients/Mean_112_grad/truediv, gradients/Mean_113_grad/truediv, gradients/Mean_114_grad/truediv, gradients/Mean_115_grad/truediv, gradients/Mean_116_grad/truediv, gradients/Mean_117_grad/truediv, gradients/Mean_118_grad/truediv, gradients/Mean_119_grad/truediv, gradients/Mean_120_grad/truediv, gradients/Mean_121_grad/truediv, gradients/Mean_122_grad/truediv, gradients/Mean_123_grad/truediv, gradients/Mean_124_grad/truediv, gradients/Mean_125_grad/truediv, gradients/Mean_126_grad/truediv, gradients/Mean_127_grad/truediv, gradients/Mean_128_grad/truediv, gradients/Mean_129_grad/truediv, gradients/Mean_130_grad/truediv, gradients/Mean_131_grad/truediv, gradients/Mean_132_grad/truediv, gradients/Mean_133_grad/truediv, gradients/Mean_134_grad/truediv, gradients/Mean_135_grad/truediv, gradients/Mean_136_grad/truediv, gradients/Mean_137_grad/truediv, gradients/Mean_138_grad/truediv, gradients/Mean_139_grad/truediv, gradients/Mean_140_grad/truediv, gradients/Mean_141_grad/truediv, gradients/Mean_142_grad/truediv, gradients/Mean_143_grad/truediv, gradients/Mean_144_grad/truediv, gradients/Mean_145_grad/truediv, gradients/Mean_146_grad/truediv, gradients/Mean_147_grad/truediv, gradients/Mean_148_grad/truediv, gradients/Mean_149_grad/truediv, gradients/Mean_150_grad/truediv, gradients/Mean_151_grad/truediv, gradients/Mean_152_grad/truediv, gradients/Mean_153_grad/truediv, gradients/Mean_154_grad/truediv, gradients/Mean_155_grad/truediv, gradients/Mean_156_grad/truediv, gradients/Mean_157_grad/truediv, gradients/Mean_158_grad/truediv, gradients/Mean_159_grad/truediv, gradients/Mean_160_grad/truediv, gradients/Mean_161_grad/truediv, gradients/Mean_162_grad/truediv, gradients/Mean_163_grad/truediv, gradients/Mean_164_grad/truediv, gradients/Mean_165_grad/truediv, gradients/Mean_166_grad/truediv, gradients/Mean_167_grad/truediv, gradients/Mean_168_grad/truediv, gradients/Mean_169_grad/truediv, gradients/Mean_170_grad/truediv, gradients/Mean_171_grad/truediv, gradients/Mean_172_grad/truediv, gradients/Mean_173_grad/truediv, gradients/Mean_174_grad/truediv, gradients/Mean_175_grad/truediv, gradients/Mean_176_grad/truediv, gradients/Mean_177_grad/truediv, gradients/Mean_178_grad/truediv, gradients/Mean_179_grad/truediv, gradients/Mean_180_grad/truediv, gradients/Mean_181_grad/truediv, gradients/Mean_182_grad/truediv, gradients/Mean_183_grad/truediv, gradients/Mean_184_grad/truediv, gradients/Mean_185_grad/truediv, gradients/Mean_186_grad/truediv, gradients/Mean_187_grad/truediv, gradients/Mean_188_grad/truediv, gradients/Mean_189_grad/truediv, gradients/Mean_190_grad/truediv, gradients/Mean_191_grad/truediv, gradients/Mean_192_grad/truediv, gradients/Mean_193_grad/truediv, gradients/Mean_194_grad/truediv, gradients/Mean_195_grad/truediv, gradients/Mean_196_grad/truediv, gradients/Mean_197_grad/truediv, gradients/Mean_198_grad/truediv, gradients/Mean_199_grad/truediv, gradients/Mean_200_grad/truediv, gradients/Mean_201_grad/truediv, gradients/Mean_202_grad/truediv, gradients/Mean_203_grad/truediv, gradients/Mean_204_grad/truediv, gradients/Mean_205_grad/truediv, gradients/Mean_206_grad/truediv, gradients/Mean_207_grad/truediv, gradients/Mean_208_grad/truediv, gradients/Mean_209_grad/truediv, gradients/Mean_210_grad/truediv, gradients/Mean_211_grad/truediv, gradients/Mean_212_grad/truediv, gradients/Mean_213_grad/truediv, gradients/Mean_214_grad/truediv, gradients/Mean_215_grad/truediv, gradients/Mean_216_grad/truediv, gradients/Mean_217_grad/truediv, gradients/Mean_218_grad/truediv, gradients/Mean_219_grad/truediv, gradients/Mean_220_grad/truediv, gradients/Mean_221_grad/truediv, gradients/Mean_222_grad/truediv, gradients/Mean_223_grad/truediv, gradients/Mean_224_grad/truediv, gradients/Mean_225_grad/truediv, gradients/Mean_226_grad/truediv, gradients/Mean_227_grad/truediv, gradients/Mean_228_grad/truediv, gradients/Mean_229_grad/truediv, gradients/Mean_230_grad/truediv, gradients/Mean_231_grad/truediv, gradients/Mean_232_grad/truediv, gradients/Mean_233_grad/truediv, gradients/Mean_234_grad/truediv, gradients/Mean_235_grad/truediv, gradients/Mean_236_grad/truediv, gradients/Mean_237_grad/truediv, gradients/Mean_238_grad/truediv, gradients/Mean_239_grad/truediv, gradients/Mean_240_grad/truediv, gradients/Mean_241_grad/truediv, gradients/Mean_242_grad/truediv, gradients/Mean_243_grad/truediv, gradients/Mean_244_grad/truediv, gradients/Mean_245_grad/truediv, gradients/Mean_246_grad/truediv, gradients/Mean_247_grad/truediv, gradients/Mean_248_grad/truediv, gradients/Mean_249_grad/truediv, gradients/Mean_250_grad/truediv, gradients/Mean_251_grad/truediv, gradients/Mean_252_grad/truediv, gradients/Mean_253_grad/truediv, gradients/Mean_254_grad/truediv, gradients/Mean_255_grad/truediv)]]

Caused by op 'gradients/DynamicPartition_grad/DynamicStitch', defined at:
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 55, in <module>
    main(session)
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 33, in main
    momentum_train_op = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.1).minimize(loss=loss)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_grad.py"", line 42, in _DynamicPartitionGrads
    reconstructed = data_flow_ops.dynamic_stitch(partitioned_indices, grads)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 683, in dynamic_stitch
    ""DynamicStitch"", indices=indices, data=data, name=name)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'DynamicPartition', defined at:
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 55, in <module>
    main(session)
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 22, in main
    activated_par = tf.dynamic_partition(data, partitions, num_partitions)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 573, in dynamic_partition
    num_partitions=num_partitions, name=name)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): data[255].shape = [41,1] does not start with indices[255].shape = [28]
	 [[Node: gradients/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/DynamicPartition_grad/DynamicPartition/_2057, gradients/DynamicPartition_grad/DynamicPartition/_2059, gradients/DynamicPartition_grad/DynamicPartition/_2061, gradients/DynamicPartition_grad/DynamicPartition/_2063, gradients/DynamicPartition_grad/DynamicPartition/_2065, gradients/DynamicPartition_grad/DynamicPartition/_2067, gradients/DynamicPartition_grad/DynamicPartition/_2069, gradients/DynamicPartition_grad/DynamicPartition/_2071, gradients/DynamicPartition_grad/DynamicPartition/_2073, gradients/DynamicPartition_grad/DynamicPartition/_2075, gradients/DynamicPartition_grad/DynamicPartition/_2077, gradients/DynamicPartition_grad/DynamicPartition/_2079, gradients/DynamicPartition_grad/DynamicPartition/_2081, gradients/DynamicPartition_grad/DynamicPartition/_2083, gradients/DynamicPartition_grad/DynamicPartition/_2085, gradients/DynamicPartition_grad/DynamicPartition/_2087, gradients/DynamicPartition_grad/DynamicPartition/_2089, gradients/DynamicPartition_grad/DynamicPartition/_2091, gradients/DynamicPartition_grad/DynamicPartition/_2093, gradients/DynamicPartition_grad/DynamicPartition/_2095, gradients/DynamicPartition_grad/DynamicPartition/_2097, gradients/DynamicPartition_grad/DynamicPartition/_2099, gradients/DynamicPartition_grad/DynamicPartition/_2101, gradients/DynamicPartition_grad/DynamicPartition/_2103, gradients/DynamicPartition_grad/DynamicPartition/_2105, gradients/DynamicPartition_grad/DynamicPartition/_2107, gradients/DynamicPartition_grad/DynamicPartition/_2109, gradients/DynamicPartition_grad/DynamicPartition/_2111, gradients/DynamicPartition_grad/DynamicPartition/_2113, gradients/DynamicPartition_grad/DynamicPartition/_2115, gradients/DynamicPartition_grad/DynamicPartition/_2117, gradients/DynamicPartition_grad/DynamicPartition/_2119, gradients/DynamicPartition_grad/DynamicPartition/_2121, gradients/DynamicPartition_grad/DynamicPartition/_2123, gradients/DynamicPartition_grad/DynamicPartition/_2125, gradients/DynamicPartition_grad/DynamicPartition/_2127, gradients/DynamicPartition_grad/DynamicPartition/_2129, gradients/DynamicPartition_grad/DynamicPartition/_2131, gradients/DynamicPartition_grad/DynamicPartition/_2133, gradients/DynamicPartition_grad/DynamicPartition/_2135, gradients/DynamicPartition_grad/DynamicPartition/_2137, gradients/DynamicPartition_grad/DynamicPartition/_2139, gradients/DynamicPartition_grad/DynamicPartition/_2141, gradients/DynamicPartition_grad/DynamicPartition/_2143, gradients/DynamicPartition_grad/DynamicPartition/_2145, gradients/DynamicPartition_grad/DynamicPartition/_2147, gradients/DynamicPartition_grad/DynamicPartition/_2149, gradients/DynamicPartition_grad/DynamicPartition/_2151, gradients/DynamicPartition_grad/DynamicPartition/_2153, gradients/DynamicPartition_grad/DynamicPartition/_2155, gradients/DynamicPartition_grad/DynamicPartition/_2157, gradients/DynamicPartition_grad/DynamicPartition/_2159, gradients/DynamicPartition_grad/DynamicPartition/_2161, gradients/DynamicPartition_grad/DynamicPartition/_2163, gradients/DynamicPartition_grad/DynamicPartition/_2165, gradients/DynamicPartition_grad/DynamicPartition/_2167, gradients/DynamicPartition_grad/DynamicPartition/_2169, gradients/DynamicPartition_grad/DynamicPartition/_2171, gradients/DynamicPartition_grad/DynamicPartition/_2173, gradients/DynamicPartition_grad/DynamicPartition/_2175, gradients/DynamicPartition_grad/DynamicPartition/_2177, gradients/DynamicPartition_grad/DynamicPartition/_2179, gradients/DynamicPartition_grad/DynamicPartition/_2181, gradients/DynamicPartition_grad/DynamicPartition/_2183, gradients/DynamicPartition_grad/DynamicPartition/_2185, gradients/DynamicPartition_grad/DynamicPartition/_2187, gradients/DynamicPartition_grad/DynamicPartition/_2189, gradients/DynamicPartition_grad/DynamicPartition/_2191, gradients/DynamicPartition_grad/DynamicPartition/_2193, gradients/DynamicPartition_grad/DynamicPartition/_2195, gradients/DynamicPartition_grad/DynamicPartition/_2197, gradients/DynamicPartition_grad/DynamicPartition/_2199, gradients/DynamicPartition_grad/DynamicPartition/_2201, gradients/DynamicPartition_grad/DynamicPartition/_2203, gradients/DynamicPartition_grad/DynamicPartition/_2205, gradients/DynamicPartition_grad/DynamicPartition/_2207, gradients/DynamicPartition_grad/DynamicPartition/_2209, gradients/DynamicPartition_grad/DynamicPartition/_2211, gradients/DynamicPartition_grad/DynamicPartition/_2213, gradients/DynamicPartition_grad/DynamicPartition/_2215, gradients/DynamicPartition_grad/DynamicPartition/_2217, gradients/DynamicPartition_grad/DynamicPartition/_2219, gradients/DynamicPartition_grad/DynamicPartition/_2221, gradients/DynamicPartition_grad/DynamicPartition/_2223, gradients/DynamicPartition_grad/DynamicPartition/_2225, gradients/DynamicPartition_grad/DynamicPartition/_2227, gradients/DynamicPartition_grad/DynamicPartition/_2229, gradients/DynamicPartition_grad/DynamicPartition/_2231, gradients/DynamicPartition_grad/DynamicPartition/_2233, gradients/DynamicPartition_grad/DynamicPartition/_2235, gradients/DynamicPartition_grad/DynamicPartition/_2237, gradients/DynamicPartition_grad/DynamicPartition/_2239, gradients/DynamicPartition_grad/DynamicPartition/_2241, gradients/DynamicPartition_grad/DynamicPartition/_2243, gradients/DynamicPartition_grad/DynamicPartition/_2245, gradients/DynamicPartition_grad/DynamicPartition/_2247, gradients/DynamicPartition_grad/DynamicPartition/_2249, gradients/DynamicPartition_grad/DynamicPartition/_2251, gradients/DynamicPartition_grad/DynamicPartition/_2253, gradients/DynamicPartition_grad/DynamicPartition/_2255, gradients/DynamicPartition_grad/DynamicPartition/_2257, gradients/DynamicPartition_grad/DynamicPartition/_2259, gradients/DynamicPartition_grad/DynamicPartition/_2261, gradients/DynamicPartition_grad/DynamicPartition/_2263, gradients/DynamicPartition_grad/DynamicPartition/_2265, gradients/DynamicPartition_grad/DynamicPartition/_2267, gradients/DynamicPartition_grad/DynamicPartition/_2269, gradients/DynamicPartition_grad/DynamicPartition/_2271, gradients/DynamicPartition_grad/DynamicPartition/_2273, gradients/DynamicPartition_grad/DynamicPartition/_2275, gradients/DynamicPartition_grad/DynamicPartition/_2277, gradients/DynamicPartition_grad/DynamicPartition/_2279, gradients/DynamicPartition_grad/DynamicPartition/_2281, gradients/DynamicPartition_grad/DynamicPartition/_2283, gradients/DynamicPartition_grad/DynamicPartition/_2285, gradients/DynamicPartition_grad/DynamicPartition/_2287, gradients/DynamicPartition_grad/DynamicPartition/_2289, gradients/DynamicPartition_grad/DynamicPartition/_2291, gradients/DynamicPartition_grad/DynamicPartition/_2293, gradients/DynamicPartition_grad/DynamicPartition/_2295, gradients/DynamicPartition_grad/DynamicPartition/_2297, gradients/DynamicPartition_grad/DynamicPartition/_2299, gradients/DynamicPartition_grad/DynamicPartition/_2301, gradients/DynamicPartition_grad/DynamicPartition/_2303, gradients/DynamicPartition_grad/DynamicPartition/_2305, gradients/DynamicPartition_grad/DynamicPartition/_2307, gradients/DynamicPartition_grad/DynamicPartition/_2309, gradients/DynamicPartition_grad/DynamicPartition/_2311, gradients/DynamicPartition_grad/DynamicPartition/_2313, gradients/DynamicPartition_grad/DynamicPartition/_2315, gradients/DynamicPartition_grad/DynamicPartition/_2317, gradients/DynamicPartition_grad/DynamicPartition/_2319, gradients/DynamicPartition_grad/DynamicPartition/_2321, gradients/DynamicPartition_grad/DynamicPartition/_2323, gradients/DynamicPartition_grad/DynamicPartition/_2325, gradients/DynamicPartition_grad/DynamicPartition/_2327, gradients/DynamicPartition_grad/DynamicPartition/_2329, gradients/DynamicPartition_grad/DynamicPartition/_2331, gradients/DynamicPartition_grad/DynamicPartition/_2333, gradients/DynamicPartition_grad/DynamicPartition/_2335, gradients/DynamicPartition_grad/DynamicPartition/_2337, gradients/DynamicPartition_grad/DynamicPartition/_2339, gradients/DynamicPartition_grad/DynamicPartition/_2341, gradients/DynamicPartition_grad/DynamicPartition/_2343, gradients/DynamicPartition_grad/DynamicPartition/_2345, gradients/DynamicPartition_grad/DynamicPartition/_2347, gradients/DynamicPartition_grad/DynamicPartition/_2349, gradients/DynamicPartition_grad/DynamicPartition/_2351, gradients/DynamicPartition_grad/DynamicPartition/_2353, gradients/DynamicPartition_grad/DynamicPartition/_2355, gradients/DynamicPartition_grad/DynamicPartition/_2357, gradients/DynamicPartition_grad/DynamicPartition/_2359, gradients/DynamicPartition_grad/DynamicPartition/_2361, gradients/DynamicPartition_grad/DynamicPartition/_2363, gradients/DynamicPartition_grad/DynamicPartition/_2365, gradients/DynamicPartition_grad/DynamicPartition/_2367, gradients/DynamicPartition_grad/DynamicPartition/_2369, gradients/DynamicPartition_grad/DynamicPartition/_2371, gradients/DynamicPartition_grad/DynamicPartition/_2373, gradients/DynamicPartition_grad/DynamicPartition/_2375, gradients/DynamicPartition_grad/DynamicPartition/_2377, gradients/DynamicPartition_grad/DynamicPartition/_2379, gradients/DynamicPartition_grad/DynamicPartition/_2381, gradients/DynamicPartition_grad/DynamicPartition/_2383, gradients/DynamicPartition_grad/DynamicPartition/_2385, gradients/DynamicPartition_grad/DynamicPartition/_2387, gradients/DynamicPartition_grad/DynamicPartition/_2389, gradients/DynamicPartition_grad/DynamicPartition/_2391, gradients/DynamicPartition_grad/DynamicPartition/_2393, gradients/DynamicPartition_grad/DynamicPartition/_2395, gradients/DynamicPartition_grad/DynamicPartition/_2397, gradients/DynamicPartition_grad/DynamicPartition/_2399, gradients/DynamicPartition_grad/DynamicPartition/_2401, gradients/DynamicPartition_grad/DynamicPartition/_2403, gradients/DynamicPartition_grad/DynamicPartition/_2405, gradients/DynamicPartition_grad/DynamicPartition/_2407, gradients/DynamicPartition_grad/DynamicPartition/_2409, gradients/DynamicPartition_grad/DynamicPartition/_2411, gradients/DynamicPartition_grad/DynamicPartition/_2413, gradients/DynamicPartition_grad/DynamicPartition/_2415, gradients/DynamicPartition_grad/DynamicPartition/_2417, gradients/DynamicPartition_grad/DynamicPartition/_2419, gradients/DynamicPartition_grad/DynamicPartition/_2421, gradients/DynamicPartition_grad/DynamicPartition/_2423, gradients/DynamicPartition_grad/DynamicPartition/_2425, gradients/DynamicPartition_grad/DynamicPartition/_2427, gradients/DynamicPartition_grad/DynamicPartition/_2429, gradients/DynamicPartition_grad/DynamicPartition/_2431, gradients/DynamicPartition_grad/DynamicPartition/_2433, gradients/DynamicPartition_grad/DynamicPartition/_2435, gradients/DynamicPartition_grad/DynamicPartition/_2437, gradients/DynamicPartition_grad/DynamicPartition/_2439, gradients/DynamicPartition_grad/DynamicPartition/_2441, gradients/DynamicPartition_grad/DynamicPartition/_2443, gradients/DynamicPartition_grad/DynamicPartition/_2445, gradients/DynamicPartition_grad/DynamicPartition/_2447, gradients/DynamicPartition_grad/DynamicPartition/_2449, gradients/DynamicPartition_grad/DynamicPartition/_2451, gradients/DynamicPartition_grad/DynamicPartition/_2453, gradients/DynamicPartition_grad/DynamicPartition/_2455, gradients/DynamicPartition_grad/DynamicPartition/_2457, gradients/DynamicPartition_grad/DynamicPartition/_2459, gradients/DynamicPartition_grad/DynamicPartition/_2461, gradients/DynamicPartition_grad/DynamicPartition/_2463, gradients/DynamicPartition_grad/DynamicPartition/_2465, gradients/DynamicPartition_grad/DynamicPartition/_2467, gradients/DynamicPartition_grad/DynamicPartition/_2469, gradients/DynamicPartition_grad/DynamicPartition/_2471, gradients/DynamicPartition_grad/DynamicPartition/_2473, gradients/DynamicPartition_grad/DynamicPartition/_2475, gradients/DynamicPartition_grad/DynamicPartition/_2477, gradients/DynamicPartition_grad/DynamicPartition/_2479, gradients/DynamicPartition_grad/DynamicPartition/_2481, gradients/DynamicPartition_grad/DynamicPartition/_2483, gradients/DynamicPartition_grad/DynamicPartition/_2485, gradients/DynamicPartition_grad/DynamicPartition/_2487, gradients/DynamicPartition_grad/DynamicPartition/_2489, gradients/DynamicPartition_grad/DynamicPartition/_2491, gradients/DynamicPartition_grad/DynamicPartition/_2493, gradients/DynamicPartition_grad/DynamicPartition/_2495, gradients/DynamicPartition_grad/DynamicPartition/_2497, gradients/DynamicPartition_grad/DynamicPartition/_2499, gradients/DynamicPartition_grad/DynamicPartition/_2501, gradients/DynamicPartition_grad/DynamicPartition/_2503, gradients/DynamicPartition_grad/DynamicPartition/_2505, gradients/DynamicPartition_grad/DynamicPartition/_2507, gradients/DynamicPartition_grad/DynamicPartition/_2509, gradients/DynamicPartition_grad/DynamicPartition/_2511, gradients/DynamicPartition_grad/DynamicPartition/_2513, gradients/DynamicPartition_grad/DynamicPartition/_2515, gradients/DynamicPartition_grad/DynamicPartition/_2517, gradients/DynamicPartition_grad/DynamicPartition/_2519, gradients/DynamicPartition_grad/DynamicPartition/_2521, gradients/DynamicPartition_grad/DynamicPartition/_2523, gradients/DynamicPartition_grad/DynamicPartition/_2525, gradients/DynamicPartition_grad/DynamicPartition/_2527, gradients/DynamicPartition_grad/DynamicPartition/_2529, gradients/DynamicPartition_grad/DynamicPartition/_2531, gradients/DynamicPartition_grad/DynamicPartition/_2533, gradients/DynamicPartition_grad/DynamicPartition/_2535, gradients/DynamicPartition_grad/DynamicPartition/_2537, gradients/DynamicPartition_grad/DynamicPartition/_2539, gradients/DynamicPartition_grad/DynamicPartition/_2541, gradients/DynamicPartition_grad/DynamicPartition/_2543, gradients/DynamicPartition_grad/DynamicPartition/_2545, gradients/DynamicPartition_grad/DynamicPartition/_2547, gradients/DynamicPartition_grad/DynamicPartition/_2549, gradients/DynamicPartition_grad/DynamicPartition/_2551, gradients/DynamicPartition_grad/DynamicPartition/_2553, gradients/DynamicPartition_grad/DynamicPartition/_2555, gradients/DynamicPartition_grad/DynamicPartition/_2557, gradients/DynamicPartition_grad/DynamicPartition/_2559, gradients/DynamicPartition_grad/DynamicPartition/_2561, gradients/DynamicPartition_grad/DynamicPartition/_2563, gradients/DynamicPartition_grad/DynamicPartition/_2565, gradients/DynamicPartition_grad/DynamicPartition/_2567, gradients/Mean_grad/truediv, gradients/Mean_1_grad/truediv, gradients/Mean_2_grad/truediv, gradients/Mean_3_grad/truediv, gradients/Mean_4_grad/truediv, gradients/Mean_5_grad/truediv, gradients/Mean_6_grad/truediv, gradients/Mean_7_grad/truediv, gradients/Mean_8_grad/truediv, gradients/Mean_9_grad/truediv, gradients/Mean_10_grad/truediv, gradients/Mean_11_grad/truediv, gradients/Mean_12_grad/truediv, gradients/Mean_13_grad/truediv, gradients/Mean_14_grad/truediv, gradients/Mean_15_grad/truediv, gradients/Mean_16_grad/truediv, gradients/Mean_17_grad/truediv, gradients/Mean_18_grad/truediv, gradients/Mean_19_grad/truediv, gradients/Mean_20_grad/truediv, gradients/Mean_21_grad/truediv, gradients/Mean_22_grad/truediv, gradients/Mean_23_grad/truediv, gradients/Mean_24_grad/truediv, gradients/Mean_25_grad/truediv, gradients/Mean_26_grad/truediv, gradients/Mean_27_grad/truediv, gradients/Mean_28_grad/truediv, gradients/Mean_29_grad/truediv, gradients/Mean_30_grad/truediv, gradients/Mean_31_grad/truediv, gradients/Mean_32_grad/truediv, gradients/Mean_33_grad/truediv, gradients/Mean_34_grad/truediv, gradients/Mean_35_grad/truediv, gradients/Mean_36_grad/truediv, gradients/Mean_37_grad/truediv, gradients/Mean_38_grad/truediv, gradients/Mean_39_grad/truediv, gradients/Mean_40_grad/truediv, gradients/Mean_41_grad/truediv, gradients/Mean_42_grad/truediv, gradients/Mean_43_grad/truediv, gradients/Mean_44_grad/truediv, gradients/Mean_45_grad/truediv, gradients/Mean_46_grad/truediv, gradients/Mean_47_grad/truediv, gradients/Mean_48_grad/truediv, gradients/Mean_49_grad/truediv, gradients/Mean_50_grad/truediv, gradients/Mean_51_grad/truediv, gradients/Mean_52_grad/truediv, gradients/Mean_53_grad/truediv, gradients/Mean_54_grad/truediv, gradients/Mean_55_grad/truediv, gradients/Mean_56_grad/truediv, gradients/Mean_57_grad/truediv, gradients/Mean_58_grad/truediv, gradients/Mean_59_grad/truediv, gradients/Mean_60_grad/truediv, gradients/Mean_61_grad/truediv, gradients/Mean_62_grad/truediv, gradients/Mean_63_grad/truediv, gradients/Mean_64_grad/truediv, gradients/Mean_65_grad/truediv, gradients/Mean_66_grad/truediv, gradients/Mean_67_grad/truediv, gradients/Mean_68_grad/truediv, gradients/Mean_69_grad/truediv, gradients/Mean_70_grad/truediv, gradients/Mean_71_grad/truediv, gradients/Mean_72_grad/truediv, gradients/Mean_73_grad/truediv, gradients/Mean_74_grad/truediv, gradients/Mean_75_grad/truediv, gradients/Mean_76_grad/truediv, gradients/Mean_77_grad/truediv, gradients/Mean_78_grad/truediv, gradients/Mean_79_grad/truediv, gradients/Mean_80_grad/truediv, gradients/Mean_81_grad/truediv, gradients/Mean_82_grad/truediv, gradients/Mean_83_grad/truediv, gradients/Mean_84_grad/truediv, gradients/Mean_85_grad/truediv, gradients/Mean_86_grad/truediv, gradients/Mean_87_grad/truediv, gradients/Mean_88_grad/truediv, gradients/Mean_89_grad/truediv, gradients/Mean_90_grad/truediv, gradients/Mean_91_grad/truediv, gradients/Mean_92_grad/truediv, gradients/Mean_93_grad/truediv, gradients/Mean_94_grad/truediv, gradients/Mean_95_grad/truediv, gradients/Mean_96_grad/truediv, gradients/Mean_97_grad/truediv, gradients/Mean_98_grad/truediv, gradients/Mean_99_grad/truediv, gradients/Mean_100_grad/truediv, gradients/Mean_101_grad/truediv, gradients/Mean_102_grad/truediv, gradients/Mean_103_grad/truediv, gradients/Mean_104_grad/truediv, gradients/Mean_105_grad/truediv, gradients/Mean_106_grad/truediv, gradients/Mean_107_grad/truediv, gradients/Mean_108_grad/truediv, gradients/Mean_109_grad/truediv, gradients/Mean_110_grad/truediv, gradients/Mean_111_grad/truediv, gradients/Mean_112_grad/truediv, gradients/Mean_113_grad/truediv, gradients/Mean_114_grad/truediv, gradients/Mean_115_grad/truediv, gradients/Mean_116_grad/truediv, gradients/Mean_117_grad/truediv, gradients/Mean_118_grad/truediv, gradients/Mean_119_grad/truediv, gradients/Mean_120_grad/truediv, gradients/Mean_121_grad/truediv, gradients/Mean_122_grad/truediv, gradients/Mean_123_grad/truediv, gradients/Mean_124_grad/truediv, gradients/Mean_125_grad/truediv, gradients/Mean_126_grad/truediv, gradients/Mean_127_grad/truediv, gradients/Mean_128_grad/truediv, gradients/Mean_129_grad/truediv, gradients/Mean_130_grad/truediv, gradients/Mean_131_grad/truediv, gradients/Mean_132_grad/truediv, gradients/Mean_133_grad/truediv, gradients/Mean_134_grad/truediv, gradients/Mean_135_grad/truediv, gradients/Mean_136_grad/truediv, gradients/Mean_137_grad/truediv, gradients/Mean_138_grad/truediv, gradients/Mean_139_grad/truediv, gradients/Mean_140_grad/truediv, gradients/Mean_141_grad/truediv, gradients/Mean_142_grad/truediv, gradients/Mean_143_grad/truediv, gradients/Mean_144_grad/truediv, gradients/Mean_145_grad/truediv, gradients/Mean_146_grad/truediv, gradients/Mean_147_grad/truediv, gradients/Mean_148_grad/truediv, gradients/Mean_149_grad/truediv, gradients/Mean_150_grad/truediv, gradients/Mean_151_grad/truediv, gradients/Mean_152_grad/truediv, gradients/Mean_153_grad/truediv, gradients/Mean_154_grad/truediv, gradients/Mean_155_grad/truediv, gradients/Mean_156_grad/truediv, gradients/Mean_157_grad/truediv, gradients/Mean_158_grad/truediv, gradients/Mean_159_grad/truediv, gradients/Mean_160_grad/truediv, gradients/Mean_161_grad/truediv, gradients/Mean_162_grad/truediv, gradients/Mean_163_grad/truediv, gradients/Mean_164_grad/truediv, gradients/Mean_165_grad/truediv, gradients/Mean_166_grad/truediv, gradients/Mean_167_grad/truediv, gradients/Mean_168_grad/truediv, gradients/Mean_169_grad/truediv, gradients/Mean_170_grad/truediv, gradients/Mean_171_grad/truediv, gradients/Mean_172_grad/truediv, gradients/Mean_173_grad/truediv, gradients/Mean_174_grad/truediv, gradients/Mean_175_grad/truediv, gradients/Mean_176_grad/truediv, gradients/Mean_177_grad/truediv, gradients/Mean_178_grad/truediv, gradients/Mean_179_grad/truediv, gradients/Mean_180_grad/truediv, gradients/Mean_181_grad/truediv, gradients/Mean_182_grad/truediv, gradients/Mean_183_grad/truediv, gradients/Mean_184_grad/truediv, gradients/Mean_185_grad/truediv, gradients/Mean_186_grad/truediv, gradients/Mean_187_grad/truediv, gradients/Mean_188_grad/truediv, gradients/Mean_189_grad/truediv, gradients/Mean_190_grad/truediv, gradients/Mean_191_grad/truediv, gradients/Mean_192_grad/truediv, gradients/Mean_193_grad/truediv, gradients/Mean_194_grad/truediv, gradients/Mean_195_grad/truediv, gradients/Mean_196_grad/truediv, gradients/Mean_197_grad/truediv, gradients/Mean_198_grad/truediv, gradients/Mean_199_grad/truediv, gradients/Mean_200_grad/truediv, gradients/Mean_201_grad/truediv, gradients/Mean_202_grad/truediv, gradients/Mean_203_grad/truediv, gradients/Mean_204_grad/truediv, gradients/Mean_205_grad/truediv, gradients/Mean_206_grad/truediv, gradients/Mean_207_grad/truediv, gradients/Mean_208_grad/truediv, gradients/Mean_209_grad/truediv, gradients/Mean_210_grad/truediv, gradients/Mean_211_grad/truediv, gradients/Mean_212_grad/truediv, gradients/Mean_213_grad/truediv, gradients/Mean_214_grad/truediv, gradients/Mean_215_grad/truediv, gradients/Mean_216_grad/truediv, gradients/Mean_217_grad/truediv, gradients/Mean_218_grad/truediv, gradients/Mean_219_grad/truediv, gradients/Mean_220_grad/truediv, gradients/Mean_221_grad/truediv, gradients/Mean_222_grad/truediv, gradients/Mean_223_grad/truediv, gradients/Mean_224_grad/truediv, gradients/Mean_225_grad/truediv, gradients/Mean_226_grad/truediv, gradients/Mean_227_grad/truediv, gradients/Mean_228_grad/truediv, gradients/Mean_229_grad/truediv, gradients/Mean_230_grad/truediv, gradients/Mean_231_grad/truediv, gradients/Mean_232_grad/truediv, gradients/Mean_233_grad/truediv, gradients/Mean_234_grad/truediv, gradients/Mean_235_grad/truediv, gradients/Mean_236_grad/truediv, gradients/Mean_237_grad/truediv, gradients/Mean_238_grad/truediv, gradients/Mean_239_grad/truediv, gradients/Mean_240_grad/truediv, gradients/Mean_241_grad/truediv, gradients/Mean_242_grad/truediv, gradients/Mean_243_grad/truediv, gradients/Mean_244_grad/truediv, gradients/Mean_245_grad/truediv, gradients/Mean_246_grad/truediv, gradients/Mean_247_grad/truediv, gradients/Mean_248_grad/truediv, gradients/Mean_249_grad/truediv, gradients/Mean_250_grad/truediv, gradients/Mean_251_grad/truediv, gradients/Mean_252_grad/truediv, gradients/Mean_253_grad/truediv, gradients/Mean_254_grad/truediv, gradients/Mean_255_grad/truediv)]]

Attempting to run adam optimizer
data[255].shape = [41,1] does not start with indices[255].shape = [28]
	 [[Node: gradients_1/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_1/DynamicPartition_grad/DynamicPartition/_4625, gradients_1/DynamicPartition_grad/DynamicPartition/_4627, gradients_1/DynamicPartition_grad/DynamicPartition/_4629, gradients_1/DynamicPartition_grad/DynamicPartition/_4631, gradients_1/DynamicPartition_grad/DynamicPartition/_4633, gradients_1/DynamicPartition_grad/DynamicPartition/_4635, gradients_1/DynamicPartition_grad/DynamicPartition/_4637, gradients_1/DynamicPartition_grad/DynamicPartition/_4639, gradients_1/DynamicPartition_grad/DynamicPartition/_4641, gradients_1/DynamicPartition_grad/DynamicPartition/_4643, gradients_1/DynamicPartition_grad/DynamicPartition/_4645, gradients_1/DynamicPartition_grad/DynamicPartition/_4647, gradients_1/DynamicPartition_grad/DynamicPartition/_4649, gradients_1/DynamicPartition_grad/DynamicPartition/_4651, gradients_1/DynamicPartition_grad/DynamicPartition/_4653, gradients_1/DynamicPartition_grad/DynamicPartition/_4655, gradients_1/DynamicPartition_grad/DynamicPartition/_4657, gradients_1/DynamicPartition_grad/DynamicPartition/_4659, gradients_1/DynamicPartition_grad/DynamicPartition/_4661, gradients_1/DynamicPartition_grad/DynamicPartition/_4663, gradients_1/DynamicPartition_grad/DynamicPartition/_4665, gradients_1/DynamicPartition_grad/DynamicPartition/_4667, gradients_1/DynamicPartition_grad/DynamicPartition/_4669, gradients_1/DynamicPartition_grad/DynamicPartition/_4671, gradients_1/DynamicPartition_grad/DynamicPartition/_4673, gradients_1/DynamicPartition_grad/DynamicPartition/_4675, gradients_1/DynamicPartition_grad/DynamicPartition/_4677, gradients_1/DynamicPartition_grad/DynamicPartition/_4679, gradients_1/DynamicPartition_grad/DynamicPartition/_4681, gradients_1/DynamicPartition_grad/DynamicPartition/_4683, gradients_1/DynamicPartition_grad/DynamicPartition/_4685, gradients_1/DynamicPartition_grad/DynamicPartition/_4687, gradients_1/DynamicPartition_grad/DynamicPartition/_4689, gradients_1/DynamicPartition_grad/DynamicPartition/_4691, gradients_1/DynamicPartition_grad/DynamicPartition/_4693, gradients_1/DynamicPartition_grad/DynamicPartition/_4695, gradients_1/DynamicPartition_grad/DynamicPartition/_4697, gradients_1/DynamicPartition_grad/DynamicPartition/_4699, gradients_1/DynamicPartition_grad/DynamicPartition/_4701, gradients_1/DynamicPartition_grad/DynamicPartition/_4703, gradients_1/DynamicPartition_grad/DynamicPartition/_4705, gradients_1/DynamicPartition_grad/DynamicPartition/_4707, gradients_1/DynamicPartition_grad/DynamicPartition/_4709, gradients_1/DynamicPartition_grad/DynamicPartition/_4711, gradients_1/DynamicPartition_grad/DynamicPartition/_4713, gradients_1/DynamicPartition_grad/DynamicPartition/_4715, gradients_1/DynamicPartition_grad/DynamicPartition/_4717, gradients_1/DynamicPartition_grad/DynamicPartition/_4719, gradients_1/DynamicPartition_grad/DynamicPartition/_4721, gradients_1/DynamicPartition_grad/DynamicPartition/_4723, gradients_1/DynamicPartition_grad/DynamicPartition/_4725, gradients_1/DynamicPartition_grad/DynamicPartition/_4727, gradients_1/DynamicPartition_grad/DynamicPartition/_4729, gradients_1/DynamicPartition_grad/DynamicPartition/_4731, gradients_1/DynamicPartition_grad/DynamicPartition/_4733, gradients_1/DynamicPartition_grad/DynamicPartition/_4735, gradients_1/DynamicPartition_grad/DynamicPartition/_4737, gradients_1/DynamicPartition_grad/DynamicPartition/_4739, gradients_1/DynamicPartition_grad/DynamicPartition/_4741, gradients_1/DynamicPartition_grad/DynamicPartition/_4743, gradients_1/DynamicPartition_grad/DynamicPartition/_4745, gradients_1/DynamicPartition_grad/DynamicPartition/_4747, gradients_1/DynamicPartition_grad/DynamicPartition/_4749, gradients_1/DynamicPartition_grad/DynamicPartition/_4751, gradients_1/DynamicPartition_grad/DynamicPartition/_4753, gradients_1/DynamicPartition_grad/DynamicPartition/_4755, gradients_1/DynamicPartition_grad/DynamicPartition/_4757, gradients_1/DynamicPartition_grad/DynamicPartition/_4759, gradients_1/DynamicPartition_grad/DynamicPartition/_4761, gradients_1/DynamicPartition_grad/DynamicPartition/_4763, gradients_1/DynamicPartition_grad/DynamicPartition/_4765, gradients_1/DynamicPartition_grad/DynamicPartition/_4767, gradients_1/DynamicPartition_grad/DynamicPartition/_4769, gradients_1/DynamicPartition_grad/DynamicPartition/_4771, gradients_1/DynamicPartition_grad/DynamicPartition/_4773, gradients_1/DynamicPartition_grad/DynamicPartition/_4775, gradients_1/DynamicPartition_grad/DynamicPartition/_4777, gradients_1/DynamicPartition_grad/DynamicPartition/_4779, gradients_1/DynamicPartition_grad/DynamicPartition/_4781, gradients_1/DynamicPartition_grad/DynamicPartition/_4783, gradients_1/DynamicPartition_grad/DynamicPartition/_4785, gradients_1/DynamicPartition_grad/DynamicPartition/_4787, gradients_1/DynamicPartition_grad/DynamicPartition/_4789, gradients_1/DynamicPartition_grad/DynamicPartition/_4791, gradients_1/DynamicPartition_grad/DynamicPartition/_4793, gradients_1/DynamicPartition_grad/DynamicPartition/_4795, gradients_1/DynamicPartition_grad/DynamicPartition/_4797, gradients_1/DynamicPartition_grad/DynamicPartition/_4799, gradients_1/DynamicPartition_grad/DynamicPartition/_4801, gradients_1/DynamicPartition_grad/DynamicPartition/_4803, gradients_1/DynamicPartition_grad/DynamicPartition/_4805, gradients_1/DynamicPartition_grad/DynamicPartition/_4807, gradients_1/DynamicPartition_grad/DynamicPartition/_4809, gradients_1/DynamicPartition_grad/DynamicPartition/_4811, gradients_1/DynamicPartition_grad/DynamicPartition/_4813, gradients_1/DynamicPartition_grad/DynamicPartition/_4815, gradients_1/DynamicPartition_grad/DynamicPartition/_4817, gradients_1/DynamicPartition_grad/DynamicPartition/_4819, gradients_1/DynamicPartition_grad/DynamicPartition/_4821, gradients_1/DynamicPartition_grad/DynamicPartition/_4823, gradients_1/DynamicPartition_grad/DynamicPartition/_4825, gradients_1/DynamicPartition_grad/DynamicPartition/_4827, gradients_1/DynamicPartition_grad/DynamicPartition/_4829, gradients_1/DynamicPartition_grad/DynamicPartition/_4831, gradients_1/DynamicPartition_grad/DynamicPartition/_4833, gradients_1/DynamicPartition_grad/DynamicPartition/_4835, gradients_1/DynamicPartition_grad/DynamicPartition/_4837, gradients_1/DynamicPartition_grad/DynamicPartition/_4839, gradients_1/DynamicPartition_grad/DynamicPartition/_4841, gradients_1/DynamicPartition_grad/DynamicPartition/_4843, gradients_1/DynamicPartition_grad/DynamicPartition/_4845, gradients_1/DynamicPartition_grad/DynamicPartition/_4847, gradients_1/DynamicPartition_grad/DynamicPartition/_4849, gradients_1/DynamicPartition_grad/DynamicPartition/_4851, gradients_1/DynamicPartition_grad/DynamicPartition/_4853, gradients_1/DynamicPartition_grad/DynamicPartition/_4855, gradients_1/DynamicPartition_grad/DynamicPartition/_4857, gradients_1/DynamicPartition_grad/DynamicPartition/_4859, gradients_1/DynamicPartition_grad/DynamicPartition/_4861, gradients_1/DynamicPartition_grad/DynamicPartition/_4863, gradients_1/DynamicPartition_grad/DynamicPartition/_4865, gradients_1/DynamicPartition_grad/DynamicPartition/_4867, gradients_1/DynamicPartition_grad/DynamicPartition/_4869, gradients_1/DynamicPartition_grad/DynamicPartition/_4871, gradients_1/DynamicPartition_grad/DynamicPartition/_4873, gradients_1/DynamicPartition_grad/DynamicPartition/_4875, gradients_1/DynamicPartition_grad/DynamicPartition/_4877, gradients_1/DynamicPartition_grad/DynamicPartition/_4879, gradients_1/DynamicPartition_grad/DynamicPartition/_4881, gradients_1/DynamicPartition_grad/DynamicPartition/_4883, gradients_1/DynamicPartition_grad/DynamicPartition/_4885, gradients_1/DynamicPartition_grad/DynamicPartition/_4887, gradients_1/DynamicPartition_grad/DynamicPartition/_4889, gradients_1/DynamicPartition_grad/DynamicPartition/_4891, gradients_1/DynamicPartition_grad/DynamicPartition/_4893, gradients_1/DynamicPartition_grad/DynamicPartition/_4895, gradients_1/DynamicPartition_grad/DynamicPartition/_4897, gradients_1/DynamicPartition_grad/DynamicPartition/_4899, gradients_1/DynamicPartition_grad/DynamicPartition/_4901, gradients_1/DynamicPartition_grad/DynamicPartition/_4903, gradients_1/DynamicPartition_grad/DynamicPartition/_4905, gradients_1/DynamicPartition_grad/DynamicPartition/_4907, gradients_1/DynamicPartition_grad/DynamicPartition/_4909, gradients_1/DynamicPartition_grad/DynamicPartition/_4911, gradients_1/DynamicPartition_grad/DynamicPartition/_4913, gradients_1/DynamicPartition_grad/DynamicPartition/_4915, gradients_1/DynamicPartition_grad/DynamicPartition/_4917, gradients_1/DynamicPartition_grad/DynamicPartition/_4919, gradients_1/DynamicPartition_grad/DynamicPartition/_4921, gradients_1/DynamicPartition_grad/DynamicPartition/_4923, gradients_1/DynamicPartition_grad/DynamicPartition/_4925, gradients_1/DynamicPartition_grad/DynamicPartition/_4927, gradients_1/DynamicPartition_grad/DynamicPartition/_4929, gradients_1/DynamicPartition_grad/DynamicPartition/_4931, gradients_1/DynamicPartition_grad/DynamicPartition/_4933, gradients_1/DynamicPartition_grad/DynamicPartition/_4935, gradients_1/DynamicPartition_grad/DynamicPartition/_4937, gradients_1/DynamicPartition_grad/DynamicPartition/_4939, gradients_1/DynamicPartition_grad/DynamicPartition/_4941, gradients_1/DynamicPartition_grad/DynamicPartition/_4943, gradients_1/DynamicPartition_grad/DynamicPartition/_4945, gradients_1/DynamicPartition_grad/DynamicPartition/_4947, gradients_1/DynamicPartition_grad/DynamicPartition/_4949, gradients_1/DynamicPartition_grad/DynamicPartition/_4951, gradients_1/DynamicPartition_grad/DynamicPartition/_4953, gradients_1/DynamicPartition_grad/DynamicPartition/_4955, gradients_1/DynamicPartition_grad/DynamicPartition/_4957, gradients_1/DynamicPartition_grad/DynamicPartition/_4959, gradients_1/DynamicPartition_grad/DynamicPartition/_4961, gradients_1/DynamicPartition_grad/DynamicPartition/_4963, gradients_1/DynamicPartition_grad/DynamicPartition/_4965, gradients_1/DynamicPartition_grad/DynamicPartition/_4967, gradients_1/DynamicPartition_grad/DynamicPartition/_4969, gradients_1/DynamicPartition_grad/DynamicPartition/_4971, gradients_1/DynamicPartition_grad/DynamicPartition/_4973, gradients_1/DynamicPartition_grad/DynamicPartition/_4975, gradients_1/DynamicPartition_grad/DynamicPartition/_4977, gradients_1/DynamicPartition_grad/DynamicPartition/_4979, gradients_1/DynamicPartition_grad/DynamicPartition/_4981, gradients_1/DynamicPartition_grad/DynamicPartition/_4983, gradients_1/DynamicPartition_grad/DynamicPartition/_4985, gradients_1/DynamicPartition_grad/DynamicPartition/_4987, gradients_1/DynamicPartition_grad/DynamicPartition/_4989, gradients_1/DynamicPartition_grad/DynamicPartition/_4991, gradients_1/DynamicPartition_grad/DynamicPartition/_4993, gradients_1/DynamicPartition_grad/DynamicPartition/_4995, gradients_1/DynamicPartition_grad/DynamicPartition/_4997, gradients_1/DynamicPartition_grad/DynamicPartition/_4999, gradients_1/DynamicPartition_grad/DynamicPartition/_5001, gradients_1/DynamicPartition_grad/DynamicPartition/_5003, gradients_1/DynamicPartition_grad/DynamicPartition/_5005, gradients_1/DynamicPartition_grad/DynamicPartition/_5007, gradients_1/DynamicPartition_grad/DynamicPartition/_5009, gradients_1/DynamicPartition_grad/DynamicPartition/_5011, gradients_1/DynamicPartition_grad/DynamicPartition/_5013, gradients_1/DynamicPartition_grad/DynamicPartition/_5015, gradients_1/DynamicPartition_grad/DynamicPartition/_5017, gradients_1/DynamicPartition_grad/DynamicPartition/_5019, gradients_1/DynamicPartition_grad/DynamicPartition/_5021, gradients_1/DynamicPartition_grad/DynamicPartition/_5023, gradients_1/DynamicPartition_grad/DynamicPartition/_5025, gradients_1/DynamicPartition_grad/DynamicPartition/_5027, gradients_1/DynamicPartition_grad/DynamicPartition/_5029, gradients_1/DynamicPartition_grad/DynamicPartition/_5031, gradients_1/DynamicPartition_grad/DynamicPartition/_5033, gradients_1/DynamicPartition_grad/DynamicPartition/_5035, gradients_1/DynamicPartition_grad/DynamicPartition/_5037, gradients_1/DynamicPartition_grad/DynamicPartition/_5039, gradients_1/DynamicPartition_grad/DynamicPartition/_5041, gradients_1/DynamicPartition_grad/DynamicPartition/_5043, gradients_1/DynamicPartition_grad/DynamicPartition/_5045, gradients_1/DynamicPartition_grad/DynamicPartition/_5047, gradients_1/DynamicPartition_grad/DynamicPartition/_5049, gradients_1/DynamicPartition_grad/DynamicPartition/_5051, gradients_1/DynamicPartition_grad/DynamicPartition/_5053, gradients_1/DynamicPartition_grad/DynamicPartition/_5055, gradients_1/DynamicPartition_grad/DynamicPartition/_5057, gradients_1/DynamicPartition_grad/DynamicPartition/_5059, gradients_1/DynamicPartition_grad/DynamicPartition/_5061, gradients_1/DynamicPartition_grad/DynamicPartition/_5063, gradients_1/DynamicPartition_grad/DynamicPartition/_5065, gradients_1/DynamicPartition_grad/DynamicPartition/_5067, gradients_1/DynamicPartition_grad/DynamicPartition/_5069, gradients_1/DynamicPartition_grad/DynamicPartition/_5071, gradients_1/DynamicPartition_grad/DynamicPartition/_5073, gradients_1/DynamicPartition_grad/DynamicPartition/_5075, gradients_1/DynamicPartition_grad/DynamicPartition/_5077, gradients_1/DynamicPartition_grad/DynamicPartition/_5079, gradients_1/DynamicPartition_grad/DynamicPartition/_5081, gradients_1/DynamicPartition_grad/DynamicPartition/_5083, gradients_1/DynamicPartition_grad/DynamicPartition/_5085, gradients_1/DynamicPartition_grad/DynamicPartition/_5087, gradients_1/DynamicPartition_grad/DynamicPartition/_5089, gradients_1/DynamicPartition_grad/DynamicPartition/_5091, gradients_1/DynamicPartition_grad/DynamicPartition/_5093, gradients_1/DynamicPartition_grad/DynamicPartition/_5095, gradients_1/DynamicPartition_grad/DynamicPartition/_5097, gradients_1/DynamicPartition_grad/DynamicPartition/_5099, gradients_1/DynamicPartition_grad/DynamicPartition/_5101, gradients_1/DynamicPartition_grad/DynamicPartition/_5103, gradients_1/DynamicPartition_grad/DynamicPartition/_5105, gradients_1/DynamicPartition_grad/DynamicPartition/_5107, gradients_1/DynamicPartition_grad/DynamicPartition/_5109, gradients_1/DynamicPartition_grad/DynamicPartition/_5111, gradients_1/DynamicPartition_grad/DynamicPartition/_5113, gradients_1/DynamicPartition_grad/DynamicPartition/_5115, gradients_1/DynamicPartition_grad/DynamicPartition/_5117, gradients_1/DynamicPartition_grad/DynamicPartition/_5119, gradients_1/DynamicPartition_grad/DynamicPartition/_5121, gradients_1/DynamicPartition_grad/DynamicPartition/_5123, gradients_1/DynamicPartition_grad/DynamicPartition/_5125, gradients_1/DynamicPartition_grad/DynamicPartition/_5127, gradients_1/DynamicPartition_grad/DynamicPartition/_5129, gradients_1/DynamicPartition_grad/DynamicPartition/_5131, gradients_1/DynamicPartition_grad/DynamicPartition/_5133, gradients_1/DynamicPartition_grad/DynamicPartition/_5135, gradients_1/Mean_grad/truediv, gradients_1/Mean_1_grad/truediv, gradients_1/Mean_2_grad/truediv, gradients_1/Mean_3_grad/truediv, gradients_1/Mean_4_grad/truediv, gradients_1/Mean_5_grad/truediv, gradients_1/Mean_6_grad/truediv, gradients_1/Mean_7_grad/truediv, gradients_1/Mean_8_grad/truediv, gradients_1/Mean_9_grad/truediv, gradients_1/Mean_10_grad/truediv, gradients_1/Mean_11_grad/truediv, gradients_1/Mean_12_grad/truediv, gradients_1/Mean_13_grad/truediv, gradients_1/Mean_14_grad/truediv, gradients_1/Mean_15_grad/truediv, gradients_1/Mean_16_grad/truediv, gradients_1/Mean_17_grad/truediv, gradients_1/Mean_18_grad/truediv, gradients_1/Mean_19_grad/truediv, gradients_1/Mean_20_grad/truediv, gradients_1/Mean_21_grad/truediv, gradients_1/Mean_22_grad/truediv, gradients_1/Mean_23_grad/truediv, gradients_1/Mean_24_grad/truediv, gradients_1/Mean_25_grad/truediv, gradients_1/Mean_26_grad/truediv, gradients_1/Mean_27_grad/truediv, gradients_1/Mean_28_grad/truediv, gradients_1/Mean_29_grad/truediv, gradients_1/Mean_30_grad/truediv, gradients_1/Mean_31_grad/truediv, gradients_1/Mean_32_grad/truediv, gradients_1/Mean_33_grad/truediv, gradients_1/Mean_34_grad/truediv, gradients_1/Mean_35_grad/truediv, gradients_1/Mean_36_grad/truediv, gradients_1/Mean_37_grad/truediv, gradients_1/Mean_38_grad/truediv, gradients_1/Mean_39_grad/truediv, gradients_1/Mean_40_grad/truediv, gradients_1/Mean_41_grad/truediv, gradients_1/Mean_42_grad/truediv, gradients_1/Mean_43_grad/truediv, gradients_1/Mean_44_grad/truediv, gradients_1/Mean_45_grad/truediv, gradients_1/Mean_46_grad/truediv, gradients_1/Mean_47_grad/truediv, gradients_1/Mean_48_grad/truediv, gradients_1/Mean_49_grad/truediv, gradients_1/Mean_50_grad/truediv, gradients_1/Mean_51_grad/truediv, gradients_1/Mean_52_grad/truediv, gradients_1/Mean_53_grad/truediv, gradients_1/Mean_54_grad/truediv, gradients_1/Mean_55_grad/truediv, gradients_1/Mean_56_grad/truediv, gradients_1/Mean_57_grad/truediv, gradients_1/Mean_58_grad/truediv, gradients_1/Mean_59_grad/truediv, gradients_1/Mean_60_grad/truediv, gradients_1/Mean_61_grad/truediv, gradients_1/Mean_62_grad/truediv, gradients_1/Mean_63_grad/truediv, gradients_1/Mean_64_grad/truediv, gradients_1/Mean_65_grad/truediv, gradients_1/Mean_66_grad/truediv, gradients_1/Mean_67_grad/truediv, gradients_1/Mean_68_grad/truediv, gradients_1/Mean_69_grad/truediv, gradients_1/Mean_70_grad/truediv, gradients_1/Mean_71_grad/truediv, gradients_1/Mean_72_grad/truediv, gradients_1/Mean_73_grad/truediv, gradients_1/Mean_74_grad/truediv, gradients_1/Mean_75_grad/truediv, gradients_1/Mean_76_grad/truediv, gradients_1/Mean_77_grad/truediv, gradients_1/Mean_78_grad/truediv, gradients_1/Mean_79_grad/truediv, gradients_1/Mean_80_grad/truediv, gradients_1/Mean_81_grad/truediv, gradients_1/Mean_82_grad/truediv, gradients_1/Mean_83_grad/truediv, gradients_1/Mean_84_grad/truediv, gradients_1/Mean_85_grad/truediv, gradients_1/Mean_86_grad/truediv, gradients_1/Mean_87_grad/truediv, gradients_1/Mean_88_grad/truediv, gradients_1/Mean_89_grad/truediv, gradients_1/Mean_90_grad/truediv, gradients_1/Mean_91_grad/truediv, gradients_1/Mean_92_grad/truediv, gradients_1/Mean_93_grad/truediv, gradients_1/Mean_94_grad/truediv, gradients_1/Mean_95_grad/truediv, gradients_1/Mean_96_grad/truediv, gradients_1/Mean_97_grad/truediv, gradients_1/Mean_98_grad/truediv, gradients_1/Mean_99_grad/truediv, gradients_1/Mean_100_grad/truediv, gradients_1/Mean_101_grad/truediv, gradients_1/Mean_102_grad/truediv, gradients_1/Mean_103_grad/truediv, gradients_1/Mean_104_grad/truediv, gradients_1/Mean_105_grad/truediv, gradients_1/Mean_106_grad/truediv, gradients_1/Mean_107_grad/truediv, gradients_1/Mean_108_grad/truediv, gradients_1/Mean_109_grad/truediv, gradients_1/Mean_110_grad/truediv, gradients_1/Mean_111_grad/truediv, gradients_1/Mean_112_grad/truediv, gradients_1/Mean_113_grad/truediv, gradients_1/Mean_114_grad/truediv, gradients_1/Mean_115_grad/truediv, gradients_1/Mean_116_grad/truediv, gradients_1/Mean_117_grad/truediv, gradients_1/Mean_118_grad/truediv, gradients_1/Mean_119_grad/truediv, gradients_1/Mean_120_grad/truediv, gradients_1/Mean_121_grad/truediv, gradients_1/Mean_122_grad/truediv, gradients_1/Mean_123_grad/truediv, gradients_1/Mean_124_grad/truediv, gradients_1/Mean_125_grad/truediv, gradients_1/Mean_126_grad/truediv, gradients_1/Mean_127_grad/truediv, gradients_1/Mean_128_grad/truediv, gradients_1/Mean_129_grad/truediv, gradients_1/Mean_130_grad/truediv, gradients_1/Mean_131_grad/truediv, gradients_1/Mean_132_grad/truediv, gradients_1/Mean_133_grad/truediv, gradients_1/Mean_134_grad/truediv, gradients_1/Mean_135_grad/truediv, gradients_1/Mean_136_grad/truediv, gradients_1/Mean_137_grad/truediv, gradients_1/Mean_138_grad/truediv, gradients_1/Mean_139_grad/truediv, gradients_1/Mean_140_grad/truediv, gradients_1/Mean_141_grad/truediv, gradients_1/Mean_142_grad/truediv, gradients_1/Mean_143_grad/truediv, gradients_1/Mean_144_grad/truediv, gradients_1/Mean_145_grad/truediv, gradients_1/Mean_146_grad/truediv, gradients_1/Mean_147_grad/truediv, gradients_1/Mean_148_grad/truediv, gradients_1/Mean_149_grad/truediv, gradients_1/Mean_150_grad/truediv, gradients_1/Mean_151_grad/truediv, gradients_1/Mean_152_grad/truediv, gradients_1/Mean_153_grad/truediv, gradients_1/Mean_154_grad/truediv, gradients_1/Mean_155_grad/truediv, gradients_1/Mean_156_grad/truediv, gradients_1/Mean_157_grad/truediv, gradients_1/Mean_158_grad/truediv, gradients_1/Mean_159_grad/truediv, gradients_1/Mean_160_grad/truediv, gradients_1/Mean_161_grad/truediv, gradients_1/Mean_162_grad/truediv, gradients_1/Mean_163_grad/truediv, gradients_1/Mean_164_grad/truediv, gradients_1/Mean_165_grad/truediv, gradients_1/Mean_166_grad/truediv, gradients_1/Mean_167_grad/truediv, gradients_1/Mean_168_grad/truediv, gradients_1/Mean_169_grad/truediv, gradients_1/Mean_170_grad/truediv, gradients_1/Mean_171_grad/truediv, gradients_1/Mean_172_grad/truediv, gradients_1/Mean_173_grad/truediv, gradients_1/Mean_174_grad/truediv, gradients_1/Mean_175_grad/truediv, gradients_1/Mean_176_grad/truediv, gradients_1/Mean_177_grad/truediv, gradients_1/Mean_178_grad/truediv, gradients_1/Mean_179_grad/truediv, gradients_1/Mean_180_grad/truediv, gradients_1/Mean_181_grad/truediv, gradients_1/Mean_182_grad/truediv, gradients_1/Mean_183_grad/truediv, gradients_1/Mean_184_grad/truediv, gradients_1/Mean_185_grad/truediv, gradients_1/Mean_186_grad/truediv, gradients_1/Mean_187_grad/truediv, gradients_1/Mean_188_grad/truediv, gradients_1/Mean_189_grad/truediv, gradients_1/Mean_190_grad/truediv, gradients_1/Mean_191_grad/truediv, gradients_1/Mean_192_grad/truediv, gradients_1/Mean_193_grad/truediv, gradients_1/Mean_194_grad/truediv, gradients_1/Mean_195_grad/truediv, gradients_1/Mean_196_grad/truediv, gradients_1/Mean_197_grad/truediv, gradients_1/Mean_198_grad/truediv, gradients_1/Mean_199_grad/truediv, gradients_1/Mean_200_grad/truediv, gradients_1/Mean_201_grad/truediv, gradients_1/Mean_202_grad/truediv, gradients_1/Mean_203_grad/truediv, gradients_1/Mean_204_grad/truediv, gradients_1/Mean_205_grad/truediv, gradients_1/Mean_206_grad/truediv, gradients_1/Mean_207_grad/truediv, gradients_1/Mean_208_grad/truediv, gradients_1/Mean_209_grad/truediv, gradients_1/Mean_210_grad/truediv, gradients_1/Mean_211_grad/truediv, gradients_1/Mean_212_grad/truediv, gradients_1/Mean_213_grad/truediv, gradients_1/Mean_214_grad/truediv, gradients_1/Mean_215_grad/truediv, gradients_1/Mean_216_grad/truediv, gradients_1/Mean_217_grad/truediv, gradients_1/Mean_218_grad/truediv, gradients_1/Mean_219_grad/truediv, gradients_1/Mean_220_grad/truediv, gradients_1/Mean_221_grad/truediv, gradients_1/Mean_222_grad/truediv, gradients_1/Mean_223_grad/truediv, gradients_1/Mean_224_grad/truediv, gradients_1/Mean_225_grad/truediv, gradients_1/Mean_226_grad/truediv, gradients_1/Mean_227_grad/truediv, gradients_1/Mean_228_grad/truediv, gradients_1/Mean_229_grad/truediv, gradients_1/Mean_230_grad/truediv, gradients_1/Mean_231_grad/truediv, gradients_1/Mean_232_grad/truediv, gradients_1/Mean_233_grad/truediv, gradients_1/Mean_234_grad/truediv, gradients_1/Mean_235_grad/truediv, gradients_1/Mean_236_grad/truediv, gradients_1/Mean_237_grad/truediv, gradients_1/Mean_238_grad/truediv, gradients_1/Mean_239_grad/truediv, gradients_1/Mean_240_grad/truediv, gradients_1/Mean_241_grad/truediv, gradients_1/Mean_242_grad/truediv, gradients_1/Mean_243_grad/truediv, gradients_1/Mean_244_grad/truediv, gradients_1/Mean_245_grad/truediv, gradients_1/Mean_246_grad/truediv, gradients_1/Mean_247_grad/truediv, gradients_1/Mean_248_grad/truediv, gradients_1/Mean_249_grad/truediv, gradients_1/Mean_250_grad/truediv, gradients_1/Mean_251_grad/truediv, gradients_1/Mean_252_grad/truediv, gradients_1/Mean_253_grad/truediv, gradients_1/Mean_254_grad/truediv, gradients_1/Mean_255_grad/truediv)]]

Caused by op 'gradients_1/DynamicPartition_grad/DynamicStitch', defined at:
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 55, in <module>
    main(session)
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 34, in main
    adam_train_op = tf.train.AdamOptimizer().minimize(loss=loss)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 355, in minimize
    grad_loss=grad_loss)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 456, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 375, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 609, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_grad.py"", line 42, in _DynamicPartitionGrads
    reconstructed = data_flow_ops.dynamic_stitch(partitioned_indices, grads)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 683, in dynamic_stitch
    ""DynamicStitch"", indices=indices, data=data, name=name)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'DynamicPartition', defined at:
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 55, in <module>
    main(session)
  File ""/home/leswing/Documents/deepchem/contrib/leswing/tensorflow_bug.py"", line 22, in main
    activated_par = tf.dynamic_partition(data, partitions, num_partitions)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 573, in dynamic_partition
    num_partitions=num_partitions, name=name)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/home/leswing/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): data[255].shape = [41,1] does not start with indices[255].shape = [28]
	 [[Node: gradients_1/DynamicPartition_grad/DynamicStitch = DynamicStitch[N=256, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_1/DynamicPartition_grad/DynamicPartition/_4625, gradients_1/DynamicPartition_grad/DynamicPartition/_4627, gradients_1/DynamicPartition_grad/DynamicPartition/_4629, gradients_1/DynamicPartition_grad/DynamicPartition/_4631, gradients_1/DynamicPartition_grad/DynamicPartition/_4633, gradients_1/DynamicPartition_grad/DynamicPartition/_4635, gradients_1/DynamicPartition_grad/DynamicPartition/_4637, gradients_1/DynamicPartition_grad/DynamicPartition/_4639, gradients_1/DynamicPartition_grad/DynamicPartition/_4641, gradients_1/DynamicPartition_grad/DynamicPartition/_4643, gradients_1/DynamicPartition_grad/DynamicPartition/_4645, gradients_1/DynamicPartition_grad/DynamicPartition/_4647, gradients_1/DynamicPartition_grad/DynamicPartition/_4649, gradients_1/DynamicPartition_grad/DynamicPartition/_4651, gradients_1/DynamicPartition_grad/DynamicPartition/_4653, gradients_1/DynamicPartition_grad/DynamicPartition/_4655, gradients_1/DynamicPartition_grad/DynamicPartition/_4657, gradients_1/DynamicPartition_grad/DynamicPartition/_4659, gradients_1/DynamicPartition_grad/DynamicPartition/_4661, gradients_1/DynamicPartition_grad/DynamicPartition/_4663, gradients_1/DynamicPartition_grad/DynamicPartition/_4665, gradients_1/DynamicPartition_grad/DynamicPartition/_4667, gradients_1/DynamicPartition_grad/DynamicPartition/_4669, gradients_1/DynamicPartition_grad/DynamicPartition/_4671, gradients_1/DynamicPartition_grad/DynamicPartition/_4673, gradients_1/DynamicPartition_grad/DynamicPartition/_4675, gradients_1/DynamicPartition_grad/DynamicPartition/_4677, gradients_1/DynamicPartition_grad/DynamicPartition/_4679, gradients_1/DynamicPartition_grad/DynamicPartition/_4681, gradients_1/DynamicPartition_grad/DynamicPartition/_4683, gradients_1/DynamicPartition_grad/DynamicPartition/_4685, gradients_1/DynamicPartition_grad/DynamicPartition/_4687, gradients_1/DynamicPartition_grad/DynamicPartition/_4689, gradients_1/DynamicPartition_grad/DynamicPartition/_4691, gradients_1/DynamicPartition_grad/DynamicPartition/_4693, gradients_1/DynamicPartition_grad/DynamicPartition/_4695, gradients_1/DynamicPartition_grad/DynamicPartition/_4697, gradients_1/DynamicPartition_grad/DynamicPartition/_4699, gradients_1/DynamicPartition_grad/DynamicPartition/_4701, gradients_1/DynamicPartition_grad/DynamicPartition/_4703, gradients_1/DynamicPartition_grad/DynamicPartition/_4705, gradients_1/DynamicPartition_grad/DynamicPartition/_4707, gradients_1/DynamicPartition_grad/DynamicPartition/_4709, gradients_1/DynamicPartition_grad/DynamicPartition/_4711, gradients_1/DynamicPartition_grad/DynamicPartition/_4713, gradients_1/DynamicPartition_grad/DynamicPartition/_4715, gradients_1/DynamicPartition_grad/DynamicPartition/_4717, gradients_1/DynamicPartition_grad/DynamicPartition/_4719, gradients_1/DynamicPartition_grad/DynamicPartition/_4721, gradients_1/DynamicPartition_grad/DynamicPartition/_4723, gradients_1/DynamicPartition_grad/DynamicPartition/_4725, gradients_1/DynamicPartition_grad/DynamicPartition/_4727, gradients_1/DynamicPartition_grad/DynamicPartition/_4729, gradients_1/DynamicPartition_grad/DynamicPartition/_4731, gradients_1/DynamicPartition_grad/DynamicPartition/_4733, gradients_1/DynamicPartition_grad/DynamicPartition/_4735, gradients_1/DynamicPartition_grad/DynamicPartition/_4737, gradients_1/DynamicPartition_grad/DynamicPartition/_4739, gradients_1/DynamicPartition_grad/DynamicPartition/_4741, gradients_1/DynamicPartition_grad/DynamicPartition/_4743, gradients_1/DynamicPartition_grad/DynamicPartition/_4745, gradients_1/DynamicPartition_grad/DynamicPartition/_4747, gradients_1/DynamicPartition_grad/DynamicPartition/_4749, gradients_1/DynamicPartition_grad/DynamicPartition/_4751, gradients_1/DynamicPartition_grad/DynamicPartition/_4753, gradients_1/DynamicPartition_grad/DynamicPartition/_4755, gradients_1/DynamicPartition_grad/DynamicPartition/_4757, gradients_1/DynamicPartition_grad/DynamicPartition/_4759, gradients_1/DynamicPartition_grad/DynamicPartition/_4761, gradients_1/DynamicPartition_grad/DynamicPartition/_4763, gradients_1/DynamicPartition_grad/DynamicPartition/_4765, gradients_1/DynamicPartition_grad/DynamicPartition/_4767, gradients_1/DynamicPartition_grad/DynamicPartition/_4769, gradients_1/DynamicPartition_grad/DynamicPartition/_4771, gradients_1/DynamicPartition_grad/DynamicPartition/_4773, gradients_1/DynamicPartition_grad/DynamicPartition/_4775, gradients_1/DynamicPartition_grad/DynamicPartition/_4777, gradients_1/DynamicPartition_grad/DynamicPartition/_4779, gradients_1/DynamicPartition_grad/DynamicPartition/_4781, gradients_1/DynamicPartition_grad/DynamicPartition/_4783, gradients_1/DynamicPartition_grad/DynamicPartition/_4785, gradients_1/DynamicPartition_grad/DynamicPartition/_4787, gradients_1/DynamicPartition_grad/DynamicPartition/_4789, gradients_1/DynamicPartition_grad/DynamicPartition/_4791, gradients_1/DynamicPartition_grad/DynamicPartition/_4793, gradients_1/DynamicPartition_grad/DynamicPartition/_4795, gradients_1/DynamicPartition_grad/DynamicPartition/_4797, gradients_1/DynamicPartition_grad/DynamicPartition/_4799, gradients_1/DynamicPartition_grad/DynamicPartition/_4801, gradients_1/DynamicPartition_grad/DynamicPartition/_4803, gradients_1/DynamicPartition_grad/DynamicPartition/_4805, gradients_1/DynamicPartition_grad/DynamicPartition/_4807, gradients_1/DynamicPartition_grad/DynamicPartition/_4809, gradients_1/DynamicPartition_grad/DynamicPartition/_4811, gradients_1/DynamicPartition_grad/DynamicPartition/_4813, gradients_1/DynamicPartition_grad/DynamicPartition/_4815, gradients_1/DynamicPartition_grad/DynamicPartition/_4817, gradients_1/DynamicPartition_grad/DynamicPartition/_4819, gradients_1/DynamicPartition_grad/DynamicPartition/_4821, gradients_1/DynamicPartition_grad/DynamicPartition/_4823, gradients_1/DynamicPartition_grad/DynamicPartition/_4825, gradients_1/DynamicPartition_grad/DynamicPartition/_4827, gradients_1/DynamicPartition_grad/DynamicPartition/_4829, gradients_1/DynamicPartition_grad/DynamicPartition/_4831, gradients_1/DynamicPartition_grad/DynamicPartition/_4833, gradients_1/DynamicPartition_grad/DynamicPartition/_4835, gradients_1/DynamicPartition_grad/DynamicPartition/_4837, gradients_1/DynamicPartition_grad/DynamicPartition/_4839, gradients_1/DynamicPartition_grad/DynamicPartition/_4841, gradients_1/DynamicPartition_grad/DynamicPartition/_4843, gradients_1/DynamicPartition_grad/DynamicPartition/_4845, gradients_1/DynamicPartition_grad/DynamicPartition/_4847, gradients_1/DynamicPartition_grad/DynamicPartition/_4849, gradients_1/DynamicPartition_grad/DynamicPartition/_4851, gradients_1/DynamicPartition_grad/DynamicPartition/_4853, gradients_1/DynamicPartition_grad/DynamicPartition/_4855, gradients_1/DynamicPartition_grad/DynamicPartition/_4857, gradients_1/DynamicPartition_grad/DynamicPartition/_4859, gradients_1/DynamicPartition_grad/DynamicPartition/_4861, gradients_1/DynamicPartition_grad/DynamicPartition/_4863, gradients_1/DynamicPartition_grad/DynamicPartition/_4865, gradients_1/DynamicPartition_grad/DynamicPartition/_4867, gradients_1/DynamicPartition_grad/DynamicPartition/_4869, gradients_1/DynamicPartition_grad/DynamicPartition/_4871, gradients_1/DynamicPartition_grad/DynamicPartition/_4873, gradients_1/DynamicPartition_grad/DynamicPartition/_4875, gradients_1/DynamicPartition_grad/DynamicPartition/_4877, gradients_1/DynamicPartition_grad/DynamicPartition/_4879, gradients_1/DynamicPartition_grad/DynamicPartition/_4881, gradients_1/DynamicPartition_grad/DynamicPartition/_4883, gradients_1/DynamicPartition_grad/DynamicPartition/_4885, gradients_1/DynamicPartition_grad/DynamicPartition/_4887, gradients_1/DynamicPartition_grad/DynamicPartition/_4889, gradients_1/DynamicPartition_grad/DynamicPartition/_4891, gradients_1/DynamicPartition_grad/DynamicPartition/_4893, gradients_1/DynamicPartition_grad/DynamicPartition/_4895, gradients_1/DynamicPartition_grad/DynamicPartition/_4897, gradients_1/DynamicPartition_grad/DynamicPartition/_4899, gradients_1/DynamicPartition_grad/DynamicPartition/_4901, gradients_1/DynamicPartition_grad/DynamicPartition/_4903, gradients_1/DynamicPartition_grad/DynamicPartition/_4905, gradients_1/DynamicPartition_grad/DynamicPartition/_4907, gradients_1/DynamicPartition_grad/DynamicPartition/_4909, gradients_1/DynamicPartition_grad/DynamicPartition/_4911, gradients_1/DynamicPartition_grad/DynamicPartition/_4913, gradients_1/DynamicPartition_grad/DynamicPartition/_4915, gradients_1/DynamicPartition_grad/DynamicPartition/_4917, gradients_1/DynamicPartition_grad/DynamicPartition/_4919, gradients_1/DynamicPartition_grad/DynamicPartition/_4921, gradients_1/DynamicPartition_grad/DynamicPartition/_4923, gradients_1/DynamicPartition_grad/DynamicPartition/_4925, gradients_1/DynamicPartition_grad/DynamicPartition/_4927, gradients_1/DynamicPartition_grad/DynamicPartition/_4929, gradients_1/DynamicPartition_grad/DynamicPartition/_4931, gradients_1/DynamicPartition_grad/DynamicPartition/_4933, gradients_1/DynamicPartition_grad/DynamicPartition/_4935, gradients_1/DynamicPartition_grad/DynamicPartition/_4937, gradients_1/DynamicPartition_grad/DynamicPartition/_4939, gradients_1/DynamicPartition_grad/DynamicPartition/_4941, gradients_1/DynamicPartition_grad/DynamicPartition/_4943, gradients_1/DynamicPartition_grad/DynamicPartition/_4945, gradients_1/DynamicPartition_grad/DynamicPartition/_4947, gradients_1/DynamicPartition_grad/DynamicPartition/_4949, gradients_1/DynamicPartition_grad/DynamicPartition/_4951, gradients_1/DynamicPartition_grad/DynamicPartition/_4953, gradients_1/DynamicPartition_grad/DynamicPartition/_4955, gradients_1/DynamicPartition_grad/DynamicPartition/_4957, gradients_1/DynamicPartition_grad/DynamicPartition/_4959, gradients_1/DynamicPartition_grad/DynamicPartition/_4961, gradients_1/DynamicPartition_grad/DynamicPartition/_4963, gradients_1/DynamicPartition_grad/DynamicPartition/_4965, gradients_1/DynamicPartition_grad/DynamicPartition/_4967, gradients_1/DynamicPartition_grad/DynamicPartition/_4969, gradients_1/DynamicPartition_grad/DynamicPartition/_4971, gradients_1/DynamicPartition_grad/DynamicPartition/_4973, gradients_1/DynamicPartition_grad/DynamicPartition/_4975, gradients_1/DynamicPartition_grad/DynamicPartition/_4977, gradients_1/DynamicPartition_grad/DynamicPartition/_4979, gradients_1/DynamicPartition_grad/DynamicPartition/_4981, gradients_1/DynamicPartition_grad/DynamicPartition/_4983, gradients_1/DynamicPartition_grad/DynamicPartition/_4985, gradients_1/DynamicPartition_grad/DynamicPartition/_4987, gradients_1/DynamicPartition_grad/DynamicPartition/_4989, gradients_1/DynamicPartition_grad/DynamicPartition/_4991, gradients_1/DynamicPartition_grad/DynamicPartition/_4993, gradients_1/DynamicPartition_grad/DynamicPartition/_4995, gradients_1/DynamicPartition_grad/DynamicPartition/_4997, gradients_1/DynamicPartition_grad/DynamicPartition/_4999, gradients_1/DynamicPartition_grad/DynamicPartition/_5001, gradients_1/DynamicPartition_grad/DynamicPartition/_5003, gradients_1/DynamicPartition_grad/DynamicPartition/_5005, gradients_1/DynamicPartition_grad/DynamicPartition/_5007, gradients_1/DynamicPartition_grad/DynamicPartition/_5009, gradients_1/DynamicPartition_grad/DynamicPartition/_5011, gradients_1/DynamicPartition_grad/DynamicPartition/_5013, gradients_1/DynamicPartition_grad/DynamicPartition/_5015, gradients_1/DynamicPartition_grad/DynamicPartition/_5017, gradients_1/DynamicPartition_grad/DynamicPartition/_5019, gradients_1/DynamicPartition_grad/DynamicPartition/_5021, gradients_1/DynamicPartition_grad/DynamicPartition/_5023, gradients_1/DynamicPartition_grad/DynamicPartition/_5025, gradients_1/DynamicPartition_grad/DynamicPartition/_5027, gradients_1/DynamicPartition_grad/DynamicPartition/_5029, gradients_1/DynamicPartition_grad/DynamicPartition/_5031, gradients_1/DynamicPartition_grad/DynamicPartition/_5033, gradients_1/DynamicPartition_grad/DynamicPartition/_5035, gradients_1/DynamicPartition_grad/DynamicPartition/_5037, gradients_1/DynamicPartition_grad/DynamicPartition/_5039, gradients_1/DynamicPartition_grad/DynamicPartition/_5041, gradients_1/DynamicPartition_grad/DynamicPartition/_5043, gradients_1/DynamicPartition_grad/DynamicPartition/_5045, gradients_1/DynamicPartition_grad/DynamicPartition/_5047, gradients_1/DynamicPartition_grad/DynamicPartition/_5049, gradients_1/DynamicPartition_grad/DynamicPartition/_5051, gradients_1/DynamicPartition_grad/DynamicPartition/_5053, gradients_1/DynamicPartition_grad/DynamicPartition/_5055, gradients_1/DynamicPartition_grad/DynamicPartition/_5057, gradients_1/DynamicPartition_grad/DynamicPartition/_5059, gradients_1/DynamicPartition_grad/DynamicPartition/_5061, gradients_1/DynamicPartition_grad/DynamicPartition/_5063, gradients_1/DynamicPartition_grad/DynamicPartition/_5065, gradients_1/DynamicPartition_grad/DynamicPartition/_5067, gradients_1/DynamicPartition_grad/DynamicPartition/_5069, gradients_1/DynamicPartition_grad/DynamicPartition/_5071, gradients_1/DynamicPartition_grad/DynamicPartition/_5073, gradients_1/DynamicPartition_grad/DynamicPartition/_5075, gradients_1/DynamicPartition_grad/DynamicPartition/_5077, gradients_1/DynamicPartition_grad/DynamicPartition/_5079, gradients_1/DynamicPartition_grad/DynamicPartition/_5081, gradients_1/DynamicPartition_grad/DynamicPartition/_5083, gradients_1/DynamicPartition_grad/DynamicPartition/_5085, gradients_1/DynamicPartition_grad/DynamicPartition/_5087, gradients_1/DynamicPartition_grad/DynamicPartition/_5089, gradients_1/DynamicPartition_grad/DynamicPartition/_5091, gradients_1/DynamicPartition_grad/DynamicPartition/_5093, gradients_1/DynamicPartition_grad/DynamicPartition/_5095, gradients_1/DynamicPartition_grad/DynamicPartition/_5097, gradients_1/DynamicPartition_grad/DynamicPartition/_5099, gradients_1/DynamicPartition_grad/DynamicPartition/_5101, gradients_1/DynamicPartition_grad/DynamicPartition/_5103, gradients_1/DynamicPartition_grad/DynamicPartition/_5105, gradients_1/DynamicPartition_grad/DynamicPartition/_5107, gradients_1/DynamicPartition_grad/DynamicPartition/_5109, gradients_1/DynamicPartition_grad/DynamicPartition/_5111, gradients_1/DynamicPartition_grad/DynamicPartition/_5113, gradients_1/DynamicPartition_grad/DynamicPartition/_5115, gradients_1/DynamicPartition_grad/DynamicPartition/_5117, gradients_1/DynamicPartition_grad/DynamicPartition/_5119, gradients_1/DynamicPartition_grad/DynamicPartition/_5121, gradients_1/DynamicPartition_grad/DynamicPartition/_5123, gradients_1/DynamicPartition_grad/DynamicPartition/_5125, gradients_1/DynamicPartition_grad/DynamicPartition/_5127, gradients_1/DynamicPartition_grad/DynamicPartition/_5129, gradients_1/DynamicPartition_grad/DynamicPartition/_5131, gradients_1/DynamicPartition_grad/DynamicPartition/_5133, gradients_1/DynamicPartition_grad/DynamicPartition/_5135, gradients_1/Mean_grad/truediv, gradients_1/Mean_1_grad/truediv, gradients_1/Mean_2_grad/truediv, gradients_1/Mean_3_grad/truediv, gradients_1/Mean_4_grad/truediv, gradients_1/Mean_5_grad/truediv, gradients_1/Mean_6_grad/truediv, gradients_1/Mean_7_grad/truediv, gradients_1/Mean_8_grad/truediv, gradients_1/Mean_9_grad/truediv, gradients_1/Mean_10_grad/truediv, gradients_1/Mean_11_grad/truediv, gradients_1/Mean_12_grad/truediv, gradients_1/Mean_13_grad/truediv, gradients_1/Mean_14_grad/truediv, gradients_1/Mean_15_grad/truediv, gradients_1/Mean_16_grad/truediv, gradients_1/Mean_17_grad/truediv, gradients_1/Mean_18_grad/truediv, gradients_1/Mean_19_grad/truediv, gradients_1/Mean_20_grad/truediv, gradients_1/Mean_21_grad/truediv, gradients_1/Mean_22_grad/truediv, gradients_1/Mean_23_grad/truediv, gradients_1/Mean_24_grad/truediv, gradients_1/Mean_25_grad/truediv, gradients_1/Mean_26_grad/truediv, gradients_1/Mean_27_grad/truediv, gradients_1/Mean_28_grad/truediv, gradients_1/Mean_29_grad/truediv, gradients_1/Mean_30_grad/truediv, gradients_1/Mean_31_grad/truediv, gradients_1/Mean_32_grad/truediv, gradients_1/Mean_33_grad/truediv, gradients_1/Mean_34_grad/truediv, gradients_1/Mean_35_grad/truediv, gradients_1/Mean_36_grad/truediv, gradients_1/Mean_37_grad/truediv, gradients_1/Mean_38_grad/truediv, gradients_1/Mean_39_grad/truediv, gradients_1/Mean_40_grad/truediv, gradients_1/Mean_41_grad/truediv, gradients_1/Mean_42_grad/truediv, gradients_1/Mean_43_grad/truediv, gradients_1/Mean_44_grad/truediv, gradients_1/Mean_45_grad/truediv, gradients_1/Mean_46_grad/truediv, gradients_1/Mean_47_grad/truediv, gradients_1/Mean_48_grad/truediv, gradients_1/Mean_49_grad/truediv, gradients_1/Mean_50_grad/truediv, gradients_1/Mean_51_grad/truediv, gradients_1/Mean_52_grad/truediv, gradients_1/Mean_53_grad/truediv, gradients_1/Mean_54_grad/truediv, gradients_1/Mean_55_grad/truediv, gradients_1/Mean_56_grad/truediv, gradients_1/Mean_57_grad/truediv, gradients_1/Mean_58_grad/truediv, gradients_1/Mean_59_grad/truediv, gradients_1/Mean_60_grad/truediv, gradients_1/Mean_61_grad/truediv, gradients_1/Mean_62_grad/truediv, gradients_1/Mean_63_grad/truediv, gradients_1/Mean_64_grad/truediv, gradients_1/Mean_65_grad/truediv, gradients_1/Mean_66_grad/truediv, gradients_1/Mean_67_grad/truediv, gradients_1/Mean_68_grad/truediv, gradients_1/Mean_69_grad/truediv, gradients_1/Mean_70_grad/truediv, gradients_1/Mean_71_grad/truediv, gradients_1/Mean_72_grad/truediv, gradients_1/Mean_73_grad/truediv, gradients_1/Mean_74_grad/truediv, gradients_1/Mean_75_grad/truediv, gradients_1/Mean_76_grad/truediv, gradients_1/Mean_77_grad/truediv, gradients_1/Mean_78_grad/truediv, gradients_1/Mean_79_grad/truediv, gradients_1/Mean_80_grad/truediv, gradients_1/Mean_81_grad/truediv, gradients_1/Mean_82_grad/truediv, gradients_1/Mean_83_grad/truediv, gradients_1/Mean_84_grad/truediv, gradients_1/Mean_85_grad/truediv, gradients_1/Mean_86_grad/truediv, gradients_1/Mean_87_grad/truediv, gradients_1/Mean_88_grad/truediv, gradients_1/Mean_89_grad/truediv, gradients_1/Mean_90_grad/truediv, gradients_1/Mean_91_grad/truediv, gradients_1/Mean_92_grad/truediv, gradients_1/Mean_93_grad/truediv, gradients_1/Mean_94_grad/truediv, gradients_1/Mean_95_grad/truediv, gradients_1/Mean_96_grad/truediv, gradients_1/Mean_97_grad/truediv, gradients_1/Mean_98_grad/truediv, gradients_1/Mean_99_grad/truediv, gradients_1/Mean_100_grad/truediv, gradients_1/Mean_101_grad/truediv, gradients_1/Mean_102_grad/truediv, gradients_1/Mean_103_grad/truediv, gradients_1/Mean_104_grad/truediv, gradients_1/Mean_105_grad/truediv, gradients_1/Mean_106_grad/truediv, gradients_1/Mean_107_grad/truediv, gradients_1/Mean_108_grad/truediv, gradients_1/Mean_109_grad/truediv, gradients_1/Mean_110_grad/truediv, gradients_1/Mean_111_grad/truediv, gradients_1/Mean_112_grad/truediv, gradients_1/Mean_113_grad/truediv, gradients_1/Mean_114_grad/truediv, gradients_1/Mean_115_grad/truediv, gradients_1/Mean_116_grad/truediv, gradients_1/Mean_117_grad/truediv, gradients_1/Mean_118_grad/truediv, gradients_1/Mean_119_grad/truediv, gradients_1/Mean_120_grad/truediv, gradients_1/Mean_121_grad/truediv, gradients_1/Mean_122_grad/truediv, gradients_1/Mean_123_grad/truediv, gradients_1/Mean_124_grad/truediv, gradients_1/Mean_125_grad/truediv, gradients_1/Mean_126_grad/truediv, gradients_1/Mean_127_grad/truediv, gradients_1/Mean_128_grad/truediv, gradients_1/Mean_129_grad/truediv, gradients_1/Mean_130_grad/truediv, gradients_1/Mean_131_grad/truediv, gradients_1/Mean_132_grad/truediv, gradients_1/Mean_133_grad/truediv, gradients_1/Mean_134_grad/truediv, gradients_1/Mean_135_grad/truediv, gradients_1/Mean_136_grad/truediv, gradients_1/Mean_137_grad/truediv, gradients_1/Mean_138_grad/truediv, gradients_1/Mean_139_grad/truediv, gradients_1/Mean_140_grad/truediv, gradients_1/Mean_141_grad/truediv, gradients_1/Mean_142_grad/truediv, gradients_1/Mean_143_grad/truediv, gradients_1/Mean_144_grad/truediv, gradients_1/Mean_145_grad/truediv, gradients_1/Mean_146_grad/truediv, gradients_1/Mean_147_grad/truediv, gradients_1/Mean_148_grad/truediv, gradients_1/Mean_149_grad/truediv, gradients_1/Mean_150_grad/truediv, gradients_1/Mean_151_grad/truediv, gradients_1/Mean_152_grad/truediv, gradients_1/Mean_153_grad/truediv, gradients_1/Mean_154_grad/truediv, gradients_1/Mean_155_grad/truediv, gradients_1/Mean_156_grad/truediv, gradients_1/Mean_157_grad/truediv, gradients_1/Mean_158_grad/truediv, gradients_1/Mean_159_grad/truediv, gradients_1/Mean_160_grad/truediv, gradients_1/Mean_161_grad/truediv, gradients_1/Mean_162_grad/truediv, gradients_1/Mean_163_grad/truediv, gradients_1/Mean_164_grad/truediv, gradients_1/Mean_165_grad/truediv, gradients_1/Mean_166_grad/truediv, gradients_1/Mean_167_grad/truediv, gradients_1/Mean_168_grad/truediv, gradients_1/Mean_169_grad/truediv, gradients_1/Mean_170_grad/truediv, gradients_1/Mean_171_grad/truediv, gradients_1/Mean_172_grad/truediv, gradients_1/Mean_173_grad/truediv, gradients_1/Mean_174_grad/truediv, gradients_1/Mean_175_grad/truediv, gradients_1/Mean_176_grad/truediv, gradients_1/Mean_177_grad/truediv, gradients_1/Mean_178_grad/truediv, gradients_1/Mean_179_grad/truediv, gradients_1/Mean_180_grad/truediv, gradients_1/Mean_181_grad/truediv, gradients_1/Mean_182_grad/truediv, gradients_1/Mean_183_grad/truediv, gradients_1/Mean_184_grad/truediv, gradients_1/Mean_185_grad/truediv, gradients_1/Mean_186_grad/truediv, gradients_1/Mean_187_grad/truediv, gradients_1/Mean_188_grad/truediv, gradients_1/Mean_189_grad/truediv, gradients_1/Mean_190_grad/truediv, gradients_1/Mean_191_grad/truediv, gradients_1/Mean_192_grad/truediv, gradients_1/Mean_193_grad/truediv, gradients_1/Mean_194_grad/truediv, gradients_1/Mean_195_grad/truediv, gradients_1/Mean_196_grad/truediv, gradients_1/Mean_197_grad/truediv, gradients_1/Mean_198_grad/truediv, gradients_1/Mean_199_grad/truediv, gradients_1/Mean_200_grad/truediv, gradients_1/Mean_201_grad/truediv, gradients_1/Mean_202_grad/truediv, gradients_1/Mean_203_grad/truediv, gradients_1/Mean_204_grad/truediv, gradients_1/Mean_205_grad/truediv, gradients_1/Mean_206_grad/truediv, gradients_1/Mean_207_grad/truediv, gradients_1/Mean_208_grad/truediv, gradients_1/Mean_209_grad/truediv, gradients_1/Mean_210_grad/truediv, gradients_1/Mean_211_grad/truediv, gradients_1/Mean_212_grad/truediv, gradients_1/Mean_213_grad/truediv, gradients_1/Mean_214_grad/truediv, gradients_1/Mean_215_grad/truediv, gradients_1/Mean_216_grad/truediv, gradients_1/Mean_217_grad/truediv, gradients_1/Mean_218_grad/truediv, gradients_1/Mean_219_grad/truediv, gradients_1/Mean_220_grad/truediv, gradients_1/Mean_221_grad/truediv, gradients_1/Mean_222_grad/truediv, gradients_1/Mean_223_grad/truediv, gradients_1/Mean_224_grad/truediv, gradients_1/Mean_225_grad/truediv, gradients_1/Mean_226_grad/truediv, gradients_1/Mean_227_grad/truediv, gradients_1/Mean_228_grad/truediv, gradients_1/Mean_229_grad/truediv, gradients_1/Mean_230_grad/truediv, gradients_1/Mean_231_grad/truediv, gradients_1/Mean_232_grad/truediv, gradients_1/Mean_233_grad/truediv, gradients_1/Mean_234_grad/truediv, gradients_1/Mean_235_grad/truediv, gradients_1/Mean_236_grad/truediv, gradients_1/Mean_237_grad/truediv, gradients_1/Mean_238_grad/truediv, gradients_1/Mean_239_grad/truediv, gradients_1/Mean_240_grad/truediv, gradients_1/Mean_241_grad/truediv, gradients_1/Mean_242_grad/truediv, gradients_1/Mean_243_grad/truediv, gradients_1/Mean_244_grad/truediv, gradients_1/Mean_245_grad/truediv, gradients_1/Mean_246_grad/truediv, gradients_1/Mean_247_grad/truediv, gradients_1/Mean_248_grad/truediv, gradients_1/Mean_249_grad/truediv, gradients_1/Mean_250_grad/truediv, gradients_1/Mean_251_grad/truediv, gradients_1/Mean_252_grad/truediv, gradients_1/Mean_253_grad/truediv, gradients_1/Mean_254_grad/truediv, gradients_1/Mean_255_grad/truediv)]]
```
Note that indices[255].shape == 28 is correct given the partition list.
"
16871,Request For Tagalog Translation,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to help in the Translation for Tagalog version so that this project will be available in our country. Hope you'll grant my request.

### Source code / logs
"
16868,Backpropagation/weight update issue with custom layer,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win 10
- **TensorFlow installed from (source or binary)**: From pip (binary)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:---
- **GCC/Compiler version (if compiling from source)**:----
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:See source code

### Describe the problem
I am attempting to implement a custom layer. The layer uses the Image to Patch function and simple tensorflow operator. The layer is implemented in keras to simplify the model building and training but the backend is in tensorflow.

I am using a simple cnn as a benchmark, whenever I implement my custom layer ( even only as the first layer to 'encode' the data) backpropagation seems to break as no weights get updated in the entirety of the model.

From my understanding the all the operations used (mult, div, add, minus) are differentiable and things such as reshape, transpose and extract_image_patches should not prevent backpropagation and weight updates.

I tried using the basic layer building method and inheriting from the convolution class (_Conv) and both cases prevent the weight update for the whole model, but such a thing shouldn't be the case.

### Source code / logs

Prototype layer: https://github.com/roya0045/cvar2/blob/master/tfvar.py
Model builder: https://github.com/roya0045/cvar2/blob/master/test2.py"
16867,How to redirect tfdbg dumping directory,"By default, `tfdbg` dumps saved tensors to `/tmp`, but in my case, `/tmp` is mount in `/root `, `/root` has only several G's space, running the example debug is not a problem, but when debugging large network, for which in one run will generate tensors that exceeds 10 G's memory, it would prompts space not enough."
16866,Support for Android Gradle Plugin 2.3.3,"Hello folks, 

Trying to use Tensorflow lite (compiled manually) with android gradle plugin 2.3.3,  I get the following error: 

> Error:Error converting bytecode to dex:
> Cause: Dex cannot parse version 52 byte code.
> This is caused by library dependencies that have been compiled using Java 8 or above.
> If you are using the 'java' gradle plugin in a library submodule add 
> targetCompatibility = '1.7'
> sourceCompatibility = '1.7'
> to that submodule's build.gradle file.

I do understand that there's the 3.0.1 version and it work, but it has many breaking changes and it is not trivial to bump the plugin version. It there any workaround or is there support for 2.3.3 on the road map?

Thanks!"
16865,tensorflow 1.6.0 built from sources: No module named 'tensorflow.python' ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- ** OS **: Linux Ubuntu 16.04
- **TensorFlow installed  **: from source
- **TensorFlow version **: 'v1.5.0-2271-gf7f7036', 1.6.0-rc0
- **Python version **: 3.6
- **Bazel version **: 0.10.0
- **GCC/Compiler version **: 5.0.4
- **CUDA/cuDNN version **: 9.1/ 7.0.5
- **GPU model and memory **: NVIDIA Titan V 12 Gb (2X)
- **Exact command to reproduce**: among others 'from tensorflow.python.client import device_lib'
### Describe the problem
Bug: 

I have installed tensorflow from source today (Feb 8, 2018). It all worked with the installation but when I (in ipython) run the command:
```
In[1]: from tensorflow.python.client import device_lib

ImportError: No module named 'tensorflow.python'  
```

It is a piece of code from Keras' training_utils.py which I use to check if tensorflow 'sees' both my gpus. Of course it is giving the error every time the module tensorflow.python should be imported.

### Source code / logs
```
def _get_available_devices():
       from tensorflow.python.client import device_lib
       local_device_protos = device_lib.list_local_devices()
       return [x.name for x in local_device_protos]
```

if I check myself if there is a python there:

```
(tfcuda9.1) hanneke@hyperion:~/anaconda3/envs/tfcuda9.1/lib/python3.6/site-packages/tensorflow$ ls
aux-bin  core      include      libtensorflow_framework.so  python
contrib  examples  __init__.py  __pycache__                 tools

```
here is a link to how I installed tensorflow as well as cuda and cudnn:
https://github.com/hannekevandijk/InstallingHyperion/blob/master/cuda-tf_installfromsources.py"
16864,CPU execution of ops after gradient clipping on windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:CUDA 9.0, cuDNN 7
- **GPU model and memory**: Titan xp, 12gb
- **Exact command to reproduce**: Script attached below

### Describe the problem
Adding gradient clipping as follows places certain ops on the CPU greatly increasing training time on windows.
```python
    grads = tf.gradients(loss, tf.trainable_variables())
    omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)
    train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))
```
I worte a [script](https://github.com/Dhruv-Mohan/G_clip_issue/blob/master/Gptest.py)  to perform a few tests:

| OS        | TF Version           | Gradient clipping  | Average runtime (m:s.ms) |
| :-------------: |:-------------:| :-----:| :--------: |
| Windows10           | 1.6  | True   | 2:13.57    |
| Windows10           | 1.6  |  False | 0:16.24    |
| Windows10           | 1.5  | True   | 2:13.46    |
| Windows10           | 1.5  |  False | 0:16.52    |
| Windows10           | 1.2  |  True  | 0:24.64    |
| Windows10           | 1.2  |  False | 0:23.80    |
| Linux mint 18.1     | 1.5  | True   | 0:23.45    |
|Linux mint 18.1      | 1.5  |  False | 0:21.29    |

Curiously the issue isn't present on TF1.2

Forcing operation placement on the GPU:
```python
    with tf.device('/gpu:0'):
        grads = tf.gradients(loss, tf.trainable_variables())
        omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)
        train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))
```
results in the following error on windows:
 `Cannot assign a device for operation 'gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.`
The error generated is similar to [#2803](https://github.com/tensorflow/models/issues/2803) and [#3118](https://github.com/tensorflow/models/issues/3118)

Setting soft device placement in session configuration results in similar runtime.

### Source code / logs
I have attached the device placement logs of windows 10 running TF 1.6 [with](https://github.com/tensorflow/tensorflow/files/1707039/Windows_gradclip.txt) and [without](https://github.com/tensorflow/tensorflow/files/1707040/Windows_nogradclip.txt) gradient clipping as well as the device placement logs from Linux mint [with](https://github.com/tensorflow/tensorflow/files/1707038/Mint_gradclip.txt) gradient clipping



A few discrepancies with respect to op placement while gradient clipping are highlighted below:

| Windows10        |  Linux mint        |
| :-------------: |:-------------:| 
| gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:CPU:0     | gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:GPU:0 | 
| global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:CPU:0      | global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0      | 
|gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:CPU:0 | gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0      | 
| gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:CPU:0   | gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0   |
| clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:CPU:0 | clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:GPU:0 |




"
16862,No package nasm,"no such package '@nasm//': java.io.IOException: Error downloading [https://mirror.bazel.build/www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2, http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2]"
16861,ImportError: No module named '_pywrap_tensorflow_internal',"Using TensorFlow backend.

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     17         try:
---> 18             return importlib.import_module(mname)
     19         except ImportError:

C:\ProgramData\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _load_unlocked(spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in module_from_spec(spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap_external.py in create_module(self, spec)

C:\ProgramData\Anaconda3\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     20             return importlib.import_module('_pywrap_tensorflow_internal')
---> 21     _pywrap_tensorflow_internal = swig_import_helper()
     22     del swig_import_helper

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     19         except ImportError:
---> 20             return importlib.import_module('_pywrap_tensorflow_internal')
     21     _pywrap_tensorflow_internal = swig_import_helper()

C:\ProgramData\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-3b3c8d0b8343> in <module>()
----> 1 from keras.models import Sequential, Model
      2 from keras.layers import *
      3 from keras.layers.advanced_activations import LeakyReLU
      4 from keras.activations import relu
      5 from keras.initializers import RandomNormal

C:\ProgramData\Anaconda3\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\__init__.py in <module>()
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\conv_utils.py in <module>()
      1 from six.moves import range
      2 import numpy as np
----> 3 from .. import backend as K
      4 
      5 

C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\__init__.py in <module>()
     81 elif _BACKEND == 'tensorflow':
     82     sys.stderr.write('Using TensorFlow backend.\n')
---> 83     from .tensorflow_backend import *
     84 else:
     85     raise ValueError('Unknown backend: ' + str(_BACKEND))

C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
----> 1 import tensorflow as tf
      2 from tensorflow.python.training import moving_averages
      3 from tensorflow.python.ops import tensor_array_ops
      4 from tensorflow.python.ops import control_flow_ops
      5 from tensorflow.python.ops import functional_ops

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

I'm in a anaconda environment.

**CONDA LIST:**

(gan) C:\Users\ZeroCool22\faceswap-GAN>conda list
 packages in environment at C:\ProgramData\Anaconda3\envs\gan:

Name                    Version                   Build  Channel
absl-py                   0.1.10                    <pip>
backports                 1.0              py36h81696a8_1
backports.weakref         1.0rc1                   py36_0
bleach                    1.5.0                    py36_0    conda-forge
boost                     1.64.0              py36_vc14_4  [vc14]  conda-forge
boost-cpp                 1.64.0                   vc14_1  [vc14]  conda-forge
bzip2                     1.0.6                    vc14_1  [vc14]  conda-forge
ca-certificates           2017.08.26           h94faf87_0
certifi                   2018.1.18                py36_0
click                     6.7                       <pip>
cudatoolkit               8.0                           3    anaconda
cudnn                     6.0                           0    anaconda
decorator                 4.0.11                   py36_0    conda-forge
dlib                      19.4              np112py36_201    conda-forge
dlib                      19.9.0                    <pip>
face-recognition          1.2.1                     <pip>
face-recognition-models   0.3.0                     <pip>
ffmpeg                    3.4.1                         1    conda-forge
freetype                  2.8.1                    vc14_0  [vc14]  conda-forge
h5py                      2.7.1                    py36_2    conda-forge
hdf5                      1.10.1                   vc14_1  [vc14]  conda-forge
html5lib                  0.9999999                py36_0    conda-forge
icc_rt                    2017.0.4             h97af966_0
icu                       58.2                     vc14_0  [vc14]  conda-forge
imageio                   2.1.2                    py36_0    conda-forge
intel-openmp              2018.0.0             hd92c6cd_8
jpeg                      9b                       vc14_2  [vc14]  conda-forge
keras                     2.0.9                    py36_0    conda-forge
libgpuarray               0.7.5                    vc14_0  [vc14]  conda-forge
libiconv                  1.14                     vc14_4  [vc14]  conda-forge
libpng                    1.6.34                   vc14_0  [vc14]  conda-forge
libtiff                   4.0.9                    vc14_0  [vc14]  conda-forge
libwebp                   0.5.2                    vc14_7  [vc14]  conda-forge
libxml2                   2.9.3                    vc14_9  [vc14]  conda-forge
mako                      1.0.7                    py36_0    conda-forge
Markdown                  2.6.11                    <pip>
markdown                  2.6.9                    py36_0    conda-forge
markupsafe                1.0                      py36_0    conda-forge
mkl                       2018.0.1             h2108138_4
moviepy                   0.2.3.2                  py36_0    conda-forge
numpy                     1.14.0                    <pip>
numpy                     1.12.1           py36hf30b8aa_1    anaconda
olefile                   0.44                     py36_0    conda-forge
opencv                    3.3.0                  py36_200    conda-forge
openssl                   1.0.2n               h74b6da3_0
pillow                    5.0.0                    py36_0    conda-forge
pip                       9.0.1                    py36_1    conda-forge
protobuf                  3.5.1               py36_vc14_3  [vc14]  conda-forge
protobuf                  3.5.1                     <pip>
pygpu                     0.7.5                    py36_0    conda-forge
python                    3.6.4                         0    conda-forge
pyyaml                    3.12                     py36_1    conda-forge
qt                        5.6.2                    vc14_1  [vc14]  conda-forge
scipy                     1.0.0            py36h1260518_0
setuptools                38.5.1                    <pip>
setuptools                38.4.0                   py36_0    conda-forge
six                       1.11.0                   py36_1    conda-forge
six                       1.11.0                    <pip>
sqlite                    3.20.1                   vc14_2  [vc14]  conda-forge
tensorboard               0.4.0rc3                 py36_2    conda-forge
tensorflow-gpu            1.3.0                     <pip>
tensorflow-tensorboard    0.1.8                     <pip>
theano                    1.0.1                    py36_1    conda-forge
tk                        8.6.7                    vc14_0  [vc14]  conda-forge
tqdm                      4.11.2                   py36_0    conda-forge
vc                        14                            0    conda-forge
vs2015_runtime            14.0.25420                    0    conda-forge
webencodings              0.5                      py36_0    conda-forge
werkzeug                  0.14.1                     py_0    conda-forge
Werkzeug                  0.14.1                    <pip>
wheel                     0.30.0                    <pip>
wheel                     0.30.0                   py36_2    conda-forge
wincertstore              0.2                      py36_0    conda-forge
yaml                      0.1.7                    vc14_0  [vc14]  conda-forge
zlib                      1.2.11                   vc14_0  [vc14]  conda-forge

(gan) C:\Users\ZeroCool22\faceswap-GAN>
------------------------
**tensorflow_self_check.py results:**

(gan) C:\Users\ZeroCool22\Desktop\Nueva carpeta (3)>python tensorflow_self_check.py
ERROR: Failed to import the TensorFlow module.

- Python version is 3.6.

- TensorFlow is installed at: C:\ProgramData\Anaconda3\envs\gan\lib\site-packages\tensorflow

- All required DLLs appear to be present. Please open an issue on the
  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues

**OS: Windows 10
GPU: 1080TI
CPU: 7700K
RAM: 32gb**"
16860,Tensorflow-gpu 1.6 : failed call to cuInit: CUDA_ERROR_NO_DEVICE,"
- CUDA 9.0
- Cudnn 7.0
- Tensorflow-gpu 1.6

nvida-smi is good
![nvidia-smi](https://user-images.githubusercontent.com/2728049/35970141-d827bac6-0cfb-11e8-886f-92b068c62c4e.png)

Error 
2018-02-08 18:14:24.768537: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
**2018-02-08 18:14:25.438352: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE**
2018-02-08 18:14:25.441350: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: Vincent
2018-02-08 18:14:25.441633: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:165] hostname: Vincent
2018-02-08 18:14:25.443152: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\direct_session.cc:297] Device mapping:

Device mapping: no known devices.
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0

Code : 
`import os
import tensorflow as tf

os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   # see issue #152
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""

from tensorflow.python.client import device_lib
#print (device_lib.list_local_devices())

# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))` "
16859,How to improve tensorflow model accuracy ?,"I have created model for chair by using tensorflow. But that model detecting any object as chair, So how can i improve model to detect only chair. We have provided 300 images of chair for training, Total loss of chair model is less than 0.6.  [this is graph](https://i.stack.imgur.com/5yhW4.png) And also give me information about How to improve accuracy of model to detect only chair."
16857,[BUG] seq2seq attention_wrapper use previous alignment?,"When we use attention model, it's recommended to use previous alignment (attention weight).
But I saw the code in the attention_wrapper.py, it ignore the previous alignment.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399

![image](https://user-images.githubusercontent.com/5347113/35961865-202877be-0cea-11e8-8f88-38e638c8e3ad.png)

@ebrevdo "
16856,Loading model from local in RNN prediction is slower than from HDFS due to page fault ,"We have trained a RNN model and use it to predict. We feed some data and calculate QPS in prediction. We find that when CPU usage is above than 30%, the QPS always stayed in 900+. And not increasing linearly by CPU usage. But if we put the model in HDFS, The QPS can reach 2400+.

Our system infomation:
```
    OS:  RedHat 7.2
    CPU:  2 * 16 core * 2 thread
    Memory: 512G in 1 node
```

In local model case, we use performance tool to trace function call time and find nearly 20% time hanged in page fault which lead to spin_lock.  Those page fault occurs less than 1% in hdfs situation. 

Our performance result listed as below:

**model loading from local**:
![local](https://user-images.githubusercontent.com/36218095/35960886-6ab0bbf6-0ce6-11e8-9923-63fc9257112f.png)

**model loading from hdfs**:
![hdfs](https://user-images.githubusercontent.com/36218095/35960906-7e8d9158-0ce6-11e8-896e-a52fced8c02a.png)

We check the source code (both eigen and tensorflow ) again and again and can not find any suspectable code which lead to page fault. we test loading model (wide and deep, cnn), page fault not happened. In RNN model we modify the code use HDFS file sytem instead of posix file system. page fault not happend too. we print log in every function in core/platform/posix/posix_file_system.cc. The log is only displayed in model loading, not occurs in prediction process. 

Is anyone can help us to find out this problem? Thank you!


"
16855,error reported while using TensorForestEstimator. tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'FertileStatsResourceHandleOp' in binary running on...,"Have I written custom code - Yes, but provided code in link also has same problem
OS Platform and Distribution - Windows 7
TensorFlow installed from - PIP install
TensorFlow version - 1.4 and 1.5 both on CPU
Bazel version - No bazel version used
CUDA/cuDNN version - N/A
GPU model and memory - N/A
Exact command to reproduce - provided a link to python script below to reproduce this issue


I am trying to use ""TensorForestEstimator"" defined in ""tensorflow.contrib.tensor_forest.client.random_forest"". I get following error.

Same error is reported when I run this script
https://www.kaggle.com/biscuitlickz/iris-predictions-using-tensorflow/code

Traceback (most recent call last):
  File ""titanic_random_forest.py"", line 201, in <module>
    app.run(main=main)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""titanic_random_forest.py"", line 136, in main
    est.fit(input_fn=train_input_fn, steps=TRAIN_STEPS)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 480, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 985, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1201, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1165, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\client\random_forest.py"", line 168, in _model_fn
    device_assigner=dev_assn)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\python\tensor_forest.py"", line 376, in __init__
    tree_variables_class=tree_variables_class)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\python\tensor_forest.py"", line 350, in __init__
    self.variables.append(tree_variables_class(params, i, training))
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\python\tensor_forest.py"", line 318, in __init__
    params, '', self.get_tree_name('stats', tree_num))
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\python\ops\stats_ops.py"", line 102, in fertile_stats_variable
    container, shared_name=name, name=name)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\tensor_forest\python\ops\gen_stats_ops.py"", line 141, in fertile_stats_resource_handle_op
    shared_name=shared_name, name=name)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3162, in create_op
    compute_device=compute_device)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3208, in _create_op_helper
    set_shapes_for_outputs(op)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2427, in set_shapes_for_outputs
    return _set_shapes_for_outputs(op)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2400, in _set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2330, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 627, in call_cpp_shape_fn
    require_shape_fn)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 686, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""F:\programs-all\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'FertileStatsResourceHandleOp' in binary running on ATWOOD_SRAO. Make sure the Op and Kernel are registered in the binary running in this process."
16851,Output of Inceptionv3 slim 2016 tflite model is problematic,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A, this is a bug report of a tflite model released by Google
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Any one
- **TensorFlow installed from (source or binary)**: both
- **TensorFlow version (use command below)**: after TF Lite released
- **Python version**: both
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see descriptions below

It seems the output of the [inceptionv3 slim 2016 tflite model released by google](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip) is problematic. If you run [label_image for tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.md), you'll get

```bash
> ./label_image -m inceptionv3_slim_2016.tflite
Loaded model inceptionv3_slim_2016.tflite
resolved reporter
invoked 
average time: 1009.48 ms 
8.06111: 653 military uniform
6.19022: 668 mortarboard
5.83456: 401 academic gown
5.26993: 835 suit
4.80701: 855 theater curtain
```

If you convert InceptionV3 yourself, 
```bash
> curl http://download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz \
 | tar -C /tmp -xzf -
> bazel run --config=opt   //tensorflow/contrib/lite/toco:toco --  \
--input_file=/tmp/inception_v3_2016_08_28_frozen.pb  \
--output_file=/tmp/inceptionv3.tflite   --input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,299,299,3 \
--input_array=input   --output_array=InceptionV3/Predictions/Reshape_1
```

and push the /tmp/inceptionv3.tflite to your android devices, then you can see expected results like

```bash
> ./label_image -m inceptionv3.tflite             
Loaded model inceptionv3.tflite
resolved reporter
invoked 
average time: 1020.93 ms 
0.496246: 653 military uniform
0.0764156: 668 mortarboard
0.0535454: 401 academic gown
0.030444: 835 suit
0.0191629: 855 theater curtain
```"
16850,concurrent.futures for Python 2 installed with tensorflow-gpu with pip3 ,"We noticed an issue with the tensorflow-gpu package available in pip for Python 3: it leads to `concurrent.futures` for Python 2 being installed as a dependency.

```
$ jupyter-notebook --no-browser
Traceback (most recent call last):
  File ""/share/software/user/open/py-jupyter/1.0.0_py36/bin/jupyter-notebook"", line 7, in <module>
    from notebook.notebookapp import main
  File ""/share/software/user/open/py-jupyter/1.0.0_py36/lib/python3.6/site-packages/notebook/notebookapp.py"", line 35, in <module>
    from jinja2 import Environment, FileSystemLoader
  File ""/share/software/user/open/py-jupyter/1.0.0_py36/lib/python3.6/site-packages/jinja2/__init__.py"", line 81, in <module>
    _patch_async()
  File ""/share/software/user/open/py-jupyter/1.0.0_py36/lib/python3.6/site-packages/jinja2/__init__.py"", line 77, in _patch_async
    from jinja2.asyncsupport import patch_all
  File ""/share/software/user/open/py-jupyter/1.0.0_py36/lib/python3.6/site-packages/jinja2/asyncsupport.py"", line 13, in <module>
    import asyncio
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/asyncio/__init__.py"", line 21, in <module>
    from .base_events import *
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/asyncio/base_events.py"", line 17, in <module>
    import concurrent.futures
  File ""/share/software/user/open/py-tensorflow/1.5.0_py36/lib/python3.6/site-packages/concurrent/futures/__init__.py"", line 8, in <module>
    from concurrent.futures._base import (FIRST_COMPLETED,
  File ""/share/software/user/open/py-tensorflow/1.5.0_py36/lib/python3.6/site-packages/concurrent/futures/_base.py"", line 381
    raise exception_type, self._exception, self._traceback
                        ^
SyntaxError: invalid syntax
```

The workaround is to remove concurrent.futures as it is already included in Python 3 standard library.
"
16847,Tensorflow website doesn't show older builds to install using pip,"I was at tensorflow 1.4, upgraded tensorflow to 1.5, had to install cudnn 9.0 and now my system is unstable. I went to the tensorflow website in order to get the tensorflow 1.4 build and noticed that there isn't any reference for these build anymore and now I'm stuck at version 1.5.

Where are these references about the .whl files? Seems that the '.../install/install_windows' page is the same for all the versions, which suggest just an '--upgrade'."
16845,ImportError: DLL load failed: Die angegebene Prozedur wurde nicht gefunden.,"### System information
- Have I written custom code: no
- OS Platform and Distribution: Windows 7, 64-bit
- Tensorflow installed from: Anaconda 5.0.1 via pip 9.0.1
(conda create -p C:\temp\tensorflow-gpu; pip install tensorflow-gpu)
- Python 3.5.2
- TensorFlow 1.5.0
- CUDA 9.0
- cuDNN 7.0
- Bazel: N/A
- GPU model and memory: NVIDIA Quadro K620, 2GB


### Describe the problem
Commands to reproduce:
```
conda create -p C:\temp\tensorflow-gpu
activate C:\temp\tensorflow-gpu
pip install tensorflow-gpu
python
> import tensorflow
```

After installing all dependencies, tensorflow fails to import. Exactly the same error occurs with other Tf/CUDA/cuDNN versions (tried TF 1.4.0 + CUDA 7.0 + cuDNN 5.1 and TF 1.4.0 + CUDA 8.0 + cuDNN 6.0).

While searching, I discovered a similar bug, but it contains a different error message: https://github.com/tensorflow/tensorflow/issues/5949


### Source code / logs
```
(C:\temp\tensorflow-gpu) C:\temp>python
Python 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v
.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\temp\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_m
odule
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Die angegebene Prozedur wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\temp\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_m
odule
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\__init__.py"", line 2
4, in <module>
    from tensorflow.python import *
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\__init__.py"",
 line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\temp\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_m
odule
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Die angegebene Prozedur wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\temp\tensorflow-gpu\lib\site-packages\tensorflow\python\pywrap_tensor
flow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\temp\tensorflow-gpu\lib\importlib\__init__.py"", line 126, in import_m
odule
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
```

When doing ""import tensorflow"", these dlls are loaded:
```
23:09:18,4934154	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\python.exe	SUCCESS	Image Base: 0x1ce20000, Image Size: 0xd000
23:09:18,4935443	python.exe	4848	Load Image	C:\Windows\System32\ntdll.dll	SUCCESS	Image Base: 0x77190000, Image Size: 0x1aa000
23:09:18,4940523	python.exe	4848	Load Image	C:\Windows\System32\kernel32.dll	SUCCESS	Image Base: 0x77070000, Image Size: 0x11f000
23:09:18,4942051	python.exe	4848	Load Image	C:\Windows\System32\KernelBase.dll	SUCCESS	Image Base: 0x7fefd010000, Image Size: 0x6a000
23:09:18,4954354	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\python35.dll	SUCCESS	Image Base: 0x60840000, Image Size: 0x3e4000
23:09:18,4962254	python.exe	4848	Load Image	C:\Windows\System32\ws2_32.dll	SUCCESS	Image Base: 0x7fefe860000, Image Size: 0x4d000
23:09:18,4963049	python.exe	4848	Load Image	C:\Windows\System32\msvcrt.dll	SUCCESS	Image Base: 0x7fefec80000, Image Size: 0x9f000
23:09:18,4965005	python.exe	4848	Load Image	C:\Windows\System32\rpcrt4.dll	SUCCESS	Image Base: 0x7fefe560000, Image Size: 0x12d000
23:09:18,4967902	python.exe	4848	Load Image	C:\Windows\System32\nsi.dll	SUCCESS	Image Base: 0x7fefed20000, Image Size: 0x8000
23:09:18,4969281	python.exe	4848	Load Image	C:\Windows\System32\advapi32.dll	SUCCESS	Image Base: 0x7fefed30000, Image Size: 0xdb000
23:09:18,4975579	python.exe	4848	Load Image	C:\Windows\System32\sechost.dll	SUCCESS	Image Base: 0x7fefefc0000, Image Size: 0x1f000
23:09:18,4980934	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\vcruntime140.dll	SUCCESS	Image Base: 0x7fef9f20000, Image Size: 0x17000
23:09:18,4985102	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-runtime-l1-1-0.dll	SUCCESS	Image Base: 0x7fefae30000, Image Size: 0x4000
23:09:18,4990202	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\ucrtbase.dll	SUCCESS	Image Base: 0x7fef1980000, Image Size: 0xf6000
23:09:18,4993831	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-localization-l1-2-0.dll	SUCCESS	Image Base: 0x7fef9f10000, Image Size: 0x3000
23:09:18,4997199	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-processthreads-l1-1-1.dll	SUCCESS	Image Base: 0x7fef9f00000, Image Size: 0x3000
23:09:18,5000873	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-file-l1-2-0.dll	SUCCESS	Image Base: 0x7fef9c80000, Image Size: 0x3000
23:09:18,5004132	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-timezone-l1-1-0.dll	SUCCESS	Image Base: 0x7fef9c50000, Image Size: 0x3000
23:09:18,5007251	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-file-l2-1-0.dll	SUCCESS	Image Base: 0x7fef78f0000, Image Size: 0x3000
23:09:18,5010347	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-core-synch-l1-2-0.dll	SUCCESS	Image Base: 0x7fef78e0000, Image Size: 0x3000
23:09:18,5013723	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-string-l1-1-0.dll	SUCCESS	Image Base: 0x7fef7290000, Image Size: 0x4000
23:09:18,5017053	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-heap-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5b80000, Image Size: 0x3000
23:09:18,5020326	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-stdio-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5b20000, Image Size: 0x4000
23:09:18,5023556	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-convert-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5a00000, Image Size: 0x4000
23:09:18,5027163	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-math-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5680000, Image Size: 0x5000
23:09:18,5030972	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-locale-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5670000, Image Size: 0x3000
23:09:18,5035867	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-time-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5660000, Image Size: 0x3000
23:09:18,5040232	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-environment-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5650000, Image Size: 0x3000
23:09:18,5043476	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-process-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5640000, Image Size: 0x3000
23:09:18,5049748	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-conio-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5630000, Image Size: 0x3000
23:09:18,5053460	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-filesystem-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5620000, Image Size: 0x3000
23:09:18,5100964	python.exe	4848	Load Image	C:\Windows\System32\cryptsp.dll	SUCCESS	Image Base: 0x7fefc6b0000, Image Size: 0x18000
23:09:18,5144867	python.exe	4848	Load Image	C:\Windows\System32\rsaenh.dll	SUCCESS	Image Base: 0x7fefc3b0000, Image Size: 0x47000
23:09:18,5156094	python.exe	4848	Load Image	C:\Windows\System32\cryptbase.dll	SUCCESS	Image Base: 0x7fefccc0000, Image Size: 0xf000
23:09:22,2812159	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\DLLs\_ctypes.pyd	SUCCESS	Image Base: 0x6dae0000, Image Size: 0x23000
23:09:22,2818451	python.exe	4848	Load Image	C:\Windows\System32\ole32.dll	SUCCESS	Image Base: 0x7fefe1b0000, Image Size: 0x1fc000
23:09:22,2819936	python.exe	4848	Load Image	C:\Windows\System32\gdi32.dll	SUCCESS	Image Base: 0x7fefee10000, Image Size: 0x67000
23:09:22,2821573	python.exe	4848	Load Image	C:\Windows\System32\user32.dll	SUCCESS	Image Base: 0x76f70000, Image Size: 0xfa000
23:09:22,2823988	python.exe	4848	Load Image	C:\Windows\System32\lpk.dll	SUCCESS	Image Base: 0x7fefef90000, Image Size: 0xe000
23:09:22,2825875	python.exe	4848	Load Image	C:\Windows\System32\usp10.dll	SUCCESS	Image Base: 0x7fefe3b0000, Image Size: 0xcb000
23:09:22,2828244	python.exe	4848	Load Image	C:\Windows\System32\oleaut32.dll	SUCCESS	Image Base: 0x7fefe480000, Image Size: 0xda000
23:09:22,2852213	python.exe	4848	Load Image	C:\Windows\System32\imm32.dll	SUCCESS	Image Base: 0x7fefe7d0000, Image Size: 0x2e000
23:09:22,2854622	python.exe	4848	Load Image	C:\Windows\System32\msctf.dll	SUCCESS	Image Base: 0x7fefee80000, Image Size: 0x109000
23:09:22,2873239	python.exe	4848	Load Image	C:\Windows\System32\psapi.dll	SUCCESS	Image Base: 0x77350000, Image Size: 0x7000
23:09:22,3305458	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\core\multiarray.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7fee9230000, Image Size: 0x1a6000
23:09:22,3403641	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\.libs\libopenblas.BNVRK7633HSX7YVO2TADGR4A5KEKXJAW.gfortran-win_amd64.dll	SUCCESS	Image Base: 0x65600000, Image Size: 0x2457000
23:09:22,3413041	python.exe	4848	Load Image	C:\Windows\System32\api-ms-win-crt-utility-l1-1-0.dll	SUCCESS	Image Base: 0x7fef5310000, Image Size: 0x3000
23:09:22,3504746	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\core\umath.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7fef1500000, Image Size: 0xcb000
23:09:22,4512966	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\DLLs\_bz2.pyd	SUCCESS	Image Base: 0x6da10000, Image Size: 0x1a000
23:09:22,4549442	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\DLLs\_lzma.pyd	SUCCESS	Image Base: 0x7fef43d0000, Image Size: 0x29000
23:09:22,4646128	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\DLLs\_hashlib.pyd	SUCCESS	Image Base: 0x7fee9c80000, Image Size: 0x167000
23:09:22,4963274	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\linalg\lapack_lite.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7fef4670000, Image Size: 0xb000
23:09:22,4978504	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\linalg\_umath_linalg.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7fef41a0000, Image Size: 0x21000
23:09:22,5099393	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\DLLs\_decimal.pyd	SUCCESS	Image Base: 0x6d930000, Image Size: 0x52000
23:09:22,5235329	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\fft\fftpack_lite.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7fef4620000, Image Size: 0x18000
23:09:22,5429263	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\numpy\random\mtrand.cp35-win_amd64.pyd	SUCCESS	Image Base: 0x7feef130000, Image Size: 0xaa000
23:09:22,5610733	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\msvcp140.dll	SUCCESS	Image Base: 0x7fef1460000, Image Size: 0x9e000
23:09:22,5617687	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\api-ms-win-crt-multibyte-l1-1-0.dll	SUCCESS	Image Base: 0x7fef4660000, Image Size: 0x5000
23:09:22,5628030	python.exe	4848	Load Image	C:\Windows\System32\nvcuda.dll	SUCCESS	Image Base: 0x7fed7080000, Image Size: 0xd4c000
23:09:22,5629974	python.exe	4848	Load Image	C:\Windows\System32\setupapi.dll	SUCCESS	Image Base: 0x7fefdfd0000, Image Size: 0x1d7000
23:09:22,5631437	python.exe	4848	Load Image	C:\Windows\System32\cfgmgr32.dll	SUCCESS	Image Base: 0x7fefcfc0000, Image Size: 0x36000
23:09:22,5633367	python.exe	4848	Load Image	C:\Windows\System32\devobj.dll	SUCCESS	Image Base: 0x7fefd210000, Image Size: 0x1a000
23:09:22,5634824	python.exe	4848	Load Image	C:\Windows\System32\shell32.dll	SUCCESS	Image Base: 0x7fefd240000, Image Size: 0xd8a000
23:09:22,5636238	python.exe	4848	Load Image	C:\Windows\System32\shlwapi.dll	SUCCESS	Image Base: 0x7fefeb80000, Image Size: 0x71000
23:09:22,5670519	python.exe	4848	Load Image	C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin\cudart64_90.dll	SUCCESS	Image Base: 0x7fef3320000, Image Size: 0x61000
23:09:22,5767031	python.exe	4848	Load Image	C:\temp\cudnn\cuda\bin\cudnn64_7.dll	SUCCESS	Image Base: 0x7fec4ba0000, Image Size: 0x111e9000
23:09:22,5836291	python.exe	4848	Load Image	C:\temp\tensorflow-gpu\Lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd	SUCCESS	Image Base: 0x7feb5630000, Image Size: 0xf564000
23:09:22,5848467	python.exe	4848	Load Image	C:\Windows\System32\wsock32.dll	SUCCESS	Image Base: 0x7fef89d0000, Image Size: 0x9000
```"
16840,Point Tensorflow To My Local Protobuf Installation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: sorta
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7 and 3.0
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 8 and 6
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: make
-**PROTOBUF VERSION**: 3.4.1

Folks, I am using tensorflow C++, and I added it's path to my CMakeLists.txt.
Everything was working fine, but I had to change the protobuf installation to a different path because CAFFE needs a different version of protobuf other than 3.4.1
Now, Tensorflow libraries are complaining they cannot find common.h from protobuf:

/usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory

I installed protobuf 3.4.1 here:
/usr/local/include/google

How do I make Tensorflow realize that protobuf is installed in a different path path?"
16839,Feature request: Enable validate_args for all distributions based on a global parameter,"The probability distributions in Tensorflow have a `validate_args` argument that is initially set to `False`. This is a request for a feature that initializes the value of `validate_args` to the value of a global flag (that can be set from the command line or the preferences).

Rationale: Currently, when the user sets up the model incorrectly e.g. wrong parameter values are provided for the distributions, or data values don't match the support for the distribution, the program silently fails and produces nans in the output (or worse, wrong values). The user has no way to debug this easily because `validate_args` is `False` by default, and manually adding `validate_args=True` while initializing each distribution can be tedious for the user.

I would be happy to contribute this feature to tensorflow if this seems reasonable. Do you have any suggestions on how to go forward?"
16836,TypeError when trying to import tensorflow ,"Hello. I have install TF 1.6 from source and getting next error when trying to import it from python.
```
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py"", line 76, in <module>
    from tensorflow.python.framework.ops import Graph
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 871, in <module>
    EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)
TypeError: Unable to create subclass EagerTensor from base class type. Need its size to be <= 32
>>>
```
------------------------

### System information
- **Linux Ubuntu 16.04**
- **Tensorflow 1.6.0-rc0 built from source**
- **Python 3.6**
- **Bazel version 0.10.0**
- **GCC version 5.4.0**
- **CUDA 9.1. cuDNN 7.0**
- **Nvidia 1050 Ti 4GB**
"
16835,Optimized einsum,"In Numpy, it's my understanding that the np.einsum function has been extended with the functionality of
[opt_einsum](https://github.com/dgasmith/opt_einsum), 
which computes an optimal (or near-optimal) way to perform the tensor contractions.
As far as I've heard, the Tensorflow einsum is a lot more basic (but extremely convenient), and cannot be relied upon for performance. Also, even though `opt_einsum` can give allegedly perfect decomposition paths, these optimizations appear to not always work well in Tensorflow - which might be down to memory order, available numerical routines, views vs reshapes, etc. 

As a feature request, I think it would be hugely beneficial to have a high-performing version of `tf.einsum` in Tensorflow, as it makes it easy to handle many complicated linear algebra tasks compactly.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

"
16834,Tensorflow not supporting 3D convolution with None input size,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Partly
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows7
TensorFlow installed from (source or binary):
ppip/python
TensorFlow version (use command below):
1.5.0
Python version:
python3.4
CUDA/cuDNN version:
9.0 / 7.0
GPU model and memory:
GTX 1080 Ti

Hello,

I am trying to build a 3D fully convolutional network with input of variable size. To do that I define x with 3 None dimensions and then apply a conv3d layers:

exact command to reproduce:
```
x = T.placeholder(shape=(1,nbrChannel,None,None,None),name=""input"")
conv=T.layers.conv3d(inputs=x,filters=num_filters,kernel_size=filter_size,strides=strides,padding=""same"",data_format='channels_first')

```
I get the following error. Please note that I tried to do exactly the same thing with 2D image input (with 2 ""None"" dimensions) and 2D convolution and it worked fine. Therefore, I think this is a problem specific to the conv3d layer function.

> 
> c:\programdata\miniconda3\lib\site-packages\tensorflow\python\layers\convolutional.py in conv3d(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)
>     817       _reuse=reuse,
>     818       _scope=name)
> --> 819   return layer.apply(inputs)
>     820 
>     821 
> 
> c:\programdata\miniconda3\lib\site-packages\tensorflow\python\layers\base.py in apply(self, inputs, *args, **kwargs)
>     760       Output tensor(s).
>     761     """"""
> --> 762     return self.__call__(inputs, *args, **kwargs)
>     763 
>     764   def _add_inbound_node(self,
> 
> c:\programdata\miniconda3\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
>     650 
>     651         if not in_deferred_mode:
> --> 652           outputs = self.call(inputs, *args, **kwargs)
>     653           if outputs is None:
>     654             raise ValueError('A layer\'s `call` method should return a Tensor '
> 
> c:\programdata\miniconda3\lib\site-packages\tensorflow\python\layers\convolutional.py in call(self, inputs)
>     182           outputs_4d = array_ops.reshape(outputs,
>     183                                          [outputs_shape[0], outputs_shape[1],
> --> 184                                           outputs_shape[2] * outputs_shape[3],
>     185                                           outputs_shape[4]])
>     186           outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')
> 
> TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'
"
16832, tensorflow/tensorflow/contrib/factorization/python/ops/gmm_ops_test.py  FAILED,"### System information
- **Code** :  tensorflow/tensorflow/contrib/factorization/python/ops/gmm_ops_test.py
- **OS Platform and Distribution** : Linux Ubuntu 16.04)
- **TensorFlow installed from** : Binary
- **TensorFlow version** : '1.4.0'
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8.0 & cuDNN 6.0
- **GPU model and memory**: Nvidia 1080Ti
- **Exact command to reproduce**: python gmm_ops_test.py

### Issue
gmm test isn't working, either gmm_ops_test.py 

### Error message
```
`======================================================================
ERROR: testParams (__main__.GmmOpsTest)
Tests that the params work as intended.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""gmm_ops_test.py"", line 147, in testParams
    sess.run(training_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: Incompatible shapes: [1,1000,2] vs. [0]
	 [[Node: sub_2 = Sub[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims_2, clusters/read)]]

Caused by op u'sub_2', defined at:
  File ""gmm_ops_test.py"", line 200, in <module>
    tf.test.main()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py"", line 73, in main
    return _googletest.main(argv)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py"", line 99, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py"", line 341, in benchmarks_main
    true_main()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py"", line 98, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py"", line 69, in g_main
    return unittest_main(argv=argv)
  File ""/usr/lib/python2.7/unittest/main.py"", line 95, in __init__
    self.runTests()
  File ""/usr/lib/python2.7/unittest/main.py"", line 232, in runTests
    self.result = testRunner.run(self.test)
  File ""/usr/lib/python2.7/unittest/runner.py"", line 151, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/case.py"", line 393, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""gmm_ops_test.py"", line 143, in testParams
    [[3.0, 3.0], [0.0, 0.0]], 'w')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 154, in __init__
    self._define_graph(data)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 274, in _define_graph
    self._define_log_prob_operation(shard_id, shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 338, in _define_log_prob_operation
    self._define_full_covariance_probs(shard_id, shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py"", line 290, in _define_full_covariance_probs
    diff = shard - self._means
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 894, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 4636, in _sub
    ""Sub"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [1,1000,2] vs. [0]
	 [[Node: sub_2 = Sub[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims_2, clusters/read)]]


======================================================================
ERROR: test_simple_cluster (__main__.GmmOpsTest)
Tests that the clusters are correct.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""gmm_ops_test.py"", line 125, in test_simple_cluster
    random_seed=self.seed)
ValueError: too many values to unpack

----------------------------------------------------------------------
Ran 4 tests in 0.992s

FAILED (errors=2`
```"
16831,Feature Request: Dynamic Convolution Kernels,"I was wondering if it is possible to allow the kernel in conv2d and conv3d to have an additional batch dimension, e.g. to allow the filter shape to be:
`[batch, filter_depth, filter_height, filter_width, in_channels, out_channels]` 
Hence, the convolution kernel can depend on the input data.

In doing so, the convolution kernels can be 'dynamic' and use prior information. 
Currently, the kernels are 'static' and therefore always look for the same patterns in the input data.
However, it would be helpful for the kernel to be a function of some input. 
That way a local transformation (learned through back propagation) can be applied to the kernels, in order to look for patterns unique for that image/ event/ data sample.

I implemented a 3d version of this in python, however, all the indexing and slicing make it rather slow. For the conv2d, I guess one could use tf.extract_image_patches to speed things up, but something equivalent does not exist for the 3d case (unless I'm missing something?).
I tried looking into the code of conv3d and conv2d to see how much effort it would be to implement this. Unfortunately, I'm neither a cuda expert, nor familiar with the way ops and kernels are implemented in tensorflow.

A feature like this would be greatly appreciated.


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A"
16829,tf.contrib.estimator.replicate_model_fn fails when a trainable variable doesn't have gradient,"tf.contrib.estimator.replicate_model_fn fails when the gradient of a trainable variable is None. The error messages are:

```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 235, in replicated_model_fn
    local_ps_devices=ps_devices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 558, in _get_loss_towers
    **optional_params)
  File ""model-60m-1280-2gpus-16-32-64-128-bn50000/net.py"", line 38, in model_fn
    train_op = optimizer.minimize(model.total_loss, global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 353, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 317, in apply_gradients
    with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4304, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4017, in control_dependencies
    c = self.as_graph_element(c)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3124, in _as_graph_element_locked
    types_str))
TypeError: Can not convert a NoneType into a Tensor or Operation.
```"
16828,Code size with XLA AOT,"I currently research about xla using AOT compilation and use Cifar10 as benchmark

After using AOT compilation, I got a binary file that size is 5.3MB, but the original tensorflow graph's size is about 4.2MB. Does it means a using AOT compilation can't promise always reduce code size efficiently?
"
16827,how to build and install cplusplus library and header file to /usr/local?,how to build and install cplusplus library and header file to /usr/local?
16826,first session.Run() when inference too slow on android and mac with c++ api,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android-armeabi-v7a, macOS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0 (for building .so file), 1.3.0 (for model train)
- **Python version**: 3.5.2 (Just used for train)
- **Bazel version (if compiling from source)**:0.8.0
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0
- **CUDA/cuDNN version**: V8.0.61 (Just used for train)
- **GPU model and memory**: 11GB (Just used for train)
- **Exact command to reproduce**:

***In a word, after I loaded the model, the cost time of first use session.Run() is longer(4x) than the second use of session.Run() and the third... when coding with C++ API.***

First, I used tensorflow-1.3.0 (python) to define and train a rnn model. Then, I used `freeze_graph ` and `transform_graph ` to make the model files to be one file and shrink the model file size. After setting the `<WORKSPACE>` by adding the following lines, I build the benchmark tool to test the performance.
```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""27.0.1"",
    # Replace with path to Android SDK on your system
    path = ""/Users/XXX/Library/Android/sdk/"",
)
android_ndk_repository(
    name=""androidndk"",
    path=""/Users/XXX/Downloads/android-ndk-r12b/"",
    api_level=14)
```
building benchmark tool like this:

`bazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic   tensorflow/tools/benchmark:benchmark_model`

test model performance like this:

`./bechmark_model --graph=""frozen_model.pb"" --input_layer=""Placeholder_data/inputs_placeholder:0,Placeholder_data/length_placeholder:0"" --input_layer_shape=""1,10:1"" --input_layer_type=""int32,int32"" --input_layer_values=""6,13:2"" --output_layer=""Top_ids/topk:1""`

and the performance on Oneplus3T(android 8.0):

```
native : stat_summarizer.cc:468 Timings (microseconds): count=300 first=19337 curr=24105 min=19337 max=39753 avg=26515.4 std=4461
native : stat_summarizer.cc:468 Memory (bytes): count=300 curr=1382834(all same)
native : stat_summarizer.cc:468 188 nodes observed
native : stat_summarizer.cc:468
```

and then I tried to use C++ API to use this model, some major code as following:

**model load:**
```
Status RNNInference::InitializeSession(
    int num_threads, const std::string& graph,
    std::unique_ptr<Session>* session,
    std::unique_ptr<GraphDef>* graph_def) {
  tensorflow::SessionOptions options;
  tensorflow::ConfigProto& config = options.config;
 // here num_threads = -1
  if (num_threads > 0) {
    config.set_intra_op_parallelism_threads(num_threads); 
  }
  LOG(INFO) << "" Got config, "" << config.device_count_size() << "" devices"";
  session->reset(tensorflow::NewSession(options));
  graph_def->reset(new GraphDef());
  tensorflow::GraphDef tensorflow_graph;

  Status s = ReadBinaryProto(Env::Default(), graph, graph_def->get());
  if (!s.ok()) {
    LOG(ERROR) << ""Could not create TensorFlow Graph: "" << s;
    return s;
  }

  s = (*session)->Create(*(graph_def->get()));
  if (!s.ok()) {
    LOG(ERROR) << ""Could not create TensorFlow Session: "" << s;
    return s;
  }

  return Status::OK();
}
```

**first inference:**
```
int RNNInference::InitializePredict() {
  int ram_size = GetRamKB();
  // 内存限制
  if (ram_size < MIN_MEMORY_SIZE) {
    return -1;
  }

  tensorflow::Status s;
  const int64 start_time = Env::Default()->NowMicros();
  s = session_.get()->Run(model_inputs_, {""Top_ids/topk:1""}, {}, {});
  if (!s.ok()) {
    // return s;
    LOG(ERROR) << "" rnn_inference -- session error : "" << s;
    return -1;
  }
  const int64 end_time = Env::Default()->NowMicros();
  int64 use_time = end_time - start_time;
  LOG(INFO) << "" rnn_inference -- init session use time is "" << use_time;

  // 速度测试 + 限制 1 - 20
  if (use_time < 1000000) {
    return ceil(use_time / 50000);
  }

  // default
  return 0;
}
```

**second inference and so on:**

```
  // Keep output results
  std::vector<tensorflow::Tensor> output_tensors;
  // Assign new data to model input
  int history_count = input_words_index.size();
  AssignVaulesFromWordHistroy(input_words_index, history_count);
  tensorflow::Status s;
  const int64 start_time = Env::Default()->NowMicros();
  s = session->Run(model_inputs_, {""Top_ids/topk:1""}, {}, &output_tensors);
  const int64 end_time = Env::Default()->NowMicros();
  int64 use_time = end_time - start_time;
  LOG(INFO) << "" input inference use "" << use_time;
  if (!s.ok()) {
    return s;
  }
  auto output_matrix = output_tensors[0].matrix<int32>();
  // dim_size is int64, which is long long
  // int first_dim = (int)(output_tensors[0].dim_size(0) - 1);
  int first_dim = history_count - 1;
  int second_dim = (int)(output_tensors[0].dim_size(1));
  for(int n = 0; n < second_dim; ++n){
    // Pass pad and unk
    if (output_matrix(first_dim, n) > 1) { 
      // int is equal to int32
      output_words_index->push_back(output_matrix(first_dim, n));
    }
  }
```

And I build these file or say my app like this and `adb push` it to my android device:

`bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/contrib/xxx/myapp:demo`
> here I used `print_selective_registration_header ` to get `ops_to_register .h`

Last, I run it on android and test it performance about real application.
**Output:(only show time cost)**
```
native : inference.cc:126  rnn_inference -- init session use time is 198629 (first)
native : inference.cc:165  input inference use 53291 (second)
native : inference.cc:165  input inference use 50341 (third)
native : inference.cc:165  input inference use 60115 (fourth)
native : inference.cc:165  input inference use 44707 (fifth)
```

***If you need other information, please let me know!***
"
16825,Bug of tf.contrib.estimator.replicate_model_fn,"I have used tf.contrib.estimator.replicate_model_fn() recently. I encountered a bug of the implementation (on master version). When there are some global trainable variables created in model_fn() and if the variables are never used (or if they are only used inside tf.cond()), it will fail with the following error messages:

```

```"
16824,CMake option for C API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: branch r1.6
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: 1.7.4 (https://mirror.bazel.build/github.com/NVlabs/cub/archive/1.7.4.zip)
- **GCC/Compiler version (if compiling from source)**: Visual Studio 2015(MSVC 14)
- **CUDA/cuDNN version**: Only CPU
- **GPU model and memory**: Only CPU
- **Exact command to reproduce**: cd tensorflow/contrib/cmake && cmake-gui .

Hi,

I'm using CMake to build the library on Windows, everything works well, but I'd like to make a very tiny library with C API only, is that possible to do this from CMake ?

Thanks very much"
16823,"Centos 7, installed bazel version is 0.10 yet ./configure complain about atleast 0.4.2 version","I was trying to build tensorflow (branch r1.0) on centos7.
./configure exited with following error
`Current Bazel version is 0.10.0- (@non-git), expected at least 0.4.2
ERROR: Error evaluating WORKSPACE file
ERROR: error loading package 'external': Package 'external' contains errors
Building: no action`

Steps involved in installing bazel
1. Go to bazel website
2. Download repo file and copy the same to /etc/repo.d
3. Execute `yum install bazel` 

Have I written custom code: NO
OS Platform and Distribution: Centos 7
TensorFlow installed from: source
TensorFlow version: 0.10
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce: ./configure"
16821,Access to C++ APIs for tfprof,"Hi

How can I get access to the C++ APIs for tfprof?
"
16820,why tfdbg dumps so much things out,"I was running a very small network(only 1 million parameters), everything is right with the training process, then I tried `tfdbg`, when I get a `tfdbg -> run`, it dumps so many things out, then root directory is filled, before run, there is `11G` left, with a run, quickly fills all the space and prompts no space left:
```
2018-02-07 10:23:07.433343: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/add_grad/Shape:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/add_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3;
2018-02-07 10:23:07.443635: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.443804: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.445778: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.445894: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.446192: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
```
How to make it work?"
16813,TensorFlow Lite for Inception v3 - Squeeze operator isn't supported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No, I haven't used any custom code for this.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5 (6c5063a3f099c302412fcefa17edb2efa9921f01)
- **Python version**: 
3
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
gcc4.4
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
GTX 1080 8GB
- **Exact command to reproduce**:
```
bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=inception-retrained-graph_freezed.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=inception-retrained-graph.tflite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=InceptionV3/Predictions/Reshape_1 --input_shapes=1,299,299,3
```

### Describe the problem
I tried to use the above command to convert a retrained inception v3 model into the new tflite format. The input graph has already been freezed. The general process is described in [this tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite#freeze-graph). However, I'm not able to finish the conversion as the tool complains that the Squeeze operator isn't supported. [According to this guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) the script should be able to remove the squeeze operation. [This comment](https://github.com/tensorflow/tensorflow/issues/16001#issuecomment-356635651) suggests that it may not always be possible to remove it. [The inception v3 architecture is guaranteed to work though](https://www.tensorflow.org/mobile/tflite/). Am I safe using the `--allow_custom_ops` flag?

Some final note:
I retrained the inception net using the [slim model](https://github.com/tensorflow/models/tree/master/research/inception/inception/slim) rather than just retraining the last layer using the freezed graph version. May this be the cause of the additional squeeze problem?

### Source code / logs
This is the complete log as returned by the toco script:
```
W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-02-07 01:23:50.754337: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1074 operators, 1657 arrays (0 quantized)
2018-02-07 01:23:50.826283: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 126 operators, 317 arrays (0 quantized)
2018-02-07 01:23:50.827683: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 126 operators, 317 arrays (0 quantized)
2018-02-07 01:23:50.828908: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 11139584 bytes, theoretical optimal value: 8297856 bytes.
2018-02-07 01:23:50.829206: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 11.4574 billion (note that a multiply-add is counted as 2 ops).
2018-02-07 01:23:50.829493: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Squeeze.
[1]    16187 abort (core dumped)  bazel-bin/tensorflow/contrib/lite/toco/toco   --output_format=TFLITE
```
"
16809,  tf.extract_image_patches ,"The documentation of the above function is available on the official Tensorflow page.
https://www.tensorflow.org/api_docs/python/tf/extract_image_patches

Can somebody explain me the meaning of the argument 'rates' and also what do these lines mean, preferably with an example with value of 'rates' >1 ?

**_This is the input stride, specifying how far two consecutive patch samples are in the input. Equivalent to extracting patches with patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), followed by subsampling them spatially by a factor of rates. This is equivalent to rate in dilated (a.k.a. Atrous) convolutions._**"
16807,TF 1.3 unable to create Session in the first time,"I have install TF 1.3 GPU using anaconda. It is failed to create session as run TF in script file.

As run TF interactively or using spyder, same error messages were shown and it fail to create session in the first time. However, it able to create session if run ""sess = tf.Session()"" again. The TF will run either by input line by line script or using ""exec(compile(open(filename, ""rb"").read(), filename, 'exec'))"" 

The error message as create session:

$ source activiate tf13py36
(tf13py36)$ python
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-05 17:44:25.343373: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.343398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.540883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-05 17:44:25.541399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.228
pciBusID 0000:03:00.0
Total memory: 5.94GiB
Free memory: 5.83GiB
2018-02-05 17:44:25.600695: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:44:25.600988: E tensorflow/core/common_runtime/direct_session.cc:171] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1486, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 621, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
>>> sess = tf.Session()
2018-02-05 17:45:19.371509: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:45:19.371738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:03:00.0)
2018-02-05 17:45:19.430913: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
>>> x1 = tf.constant([1,2,3,4])
>>> x2 = tf.constant([5,6,7,8])
>>> result = tf.multiply(x1, x2)
>>> print(sess.run(result))
[ 5 12 21 32]

SYSTEM Infomation
ubuntu16.04
cuda V8.061
cudnn 6021
gtx1060
tensorflow1.3.0
python3.6.1
memory 30G ,used 5.5GB

$ nvidia-smi
Tue Feb  6 10:21:38 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 980 Ti  Off  | 00000000:03:00.0 Off |                  N/A |
| 22%   31C    P8    19W / 250W |    110MiB /  6078MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro 600          Off  | 00000000:04:00.0  On |                  N/A |
| 36%   53C    P0    N/A /  N/A |    524MiB /   959MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     19564      C   python                                        98MiB |
+-----------------------------------------------------------------------------+

I am grateful to anyone for helping me
Thank you very much!"
16805,Feature Request: max_norm argument added to tf.nn.nce_loss ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes

### Describe the problem
I would like to request an additional argument to added to the tf.nn.nce_loss function. This function calls the tf.nn.embedding_lookup in order to build the matrix of negative sample embeddings. tf.nn.embedding_lookup has a max_norm argument, and I would like for the value of the max_norm argument supplied to tf.nn.nce_loss to be propagated to the tf.nn.embedding_lookup call.
"
16803,transform_graph generates faulty model after optimization (quantisation).,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
- **Python version**: 
1.4.1
- **Bazel version (if compiling from source)**:
0.8.1
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.6)

### Describe the problem
I am using a pre-trained style transfer model that I have converted  to .pb format. When I inference a single image through the fp32 model then I can see the styled image at the output of the network and everything works fine. Later on I optimized the model for inference using `transform_graph` tool with these parameters : 

```
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float)
  remove_nodes(op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  sort_by_execution_order'
```
the quantised models (.pb) generated successfully but now I have problem with inference when I execute sess.run().

### Source code / logs
```
Traceback (most recent call last):
  File ""Inf_Image_pb.py"", line 89, in <module>
    Session_out = sess.run(l_output, feed_dict={l_input: image})            
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [474,712,3] != values[2].shape = []
	 [[Node: Reshape/shape = Pack[N=3, T=DT_INT32, axis=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_input_image_0_0, _arg_input_image_0_0, Reshape/shape/2)]]

Caused by op u'Reshape/shape', defined at:
  File ""Inf_Image_pb.py"", line 74, in <module>
    producer_op_list=None
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Shapes of all inputs must match: values[0].shape = [474,712,3] != values[2].shape = []
	 [[Node: Reshape/shape = Pack[N=3, T=DT_INT32, axis=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_input_image_0_0, _arg_input_image_0_0, Reshape/shape/2)]]
```
I have tried other tensorflow versions, other bazel versions to rebuild the tool and the legacy quantisation tool 'quantize_graph' with mode --mode=eightbit but still fail to inference. In addition I tried other transform combinations using the `transform_graph` tool, other batch sizes and other input dimensions but still getting error. What is this error refers to ? I can't find any useful information online ..
"
16801,Tensorflow installer assumes that the user uses CUDA 9.0 while CUDA 9.1 is out already.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16800,How to compile and use Opencv in tensorflow c++?,"I want to implement a model inference in tensorflow c++ and have saved the model as .pb file. But I can't use opencv to process the image. I wonder how can I add the opencv lib to the bazel project? Are there any tricks to solve the problem? Thanks!

This is the code of the bazel BUILD file:

    package(
        default_visibility = [""//tensorflow:internal""],
    )

    licenses([""notice""])  # Apache 2.0

    exports_files([""LICENSE""])

    load(""//tensorflow:tensorflow.bzl"", ""tf_cc_binary"")

    tf_cc_binary(
        name = ""mask_rcnn"",
        srcs = [
            ""main.cc"",
        ],
        #prefix = ""flower"",
        linkopts = select({
            ""//tensorflow:android"": [
                ""-pie"",
                ""-landroid"",
                ""-ljnigraphics"",
                ""-llog"",
                ""-lm"",
                ""-z defs"",
                ""-s"",
                ""-Wl,--exclude-libs,ALL"",
            ],
            ""//conditions:default"": [""-lm""],
        }),
        deps = select({
            ""//tensorflow:android"": [
                # cc:cc_ops is used to include image ops (for label_image)
                # Jpg, gif, and png related code won't be included
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:android_tensorflow_lib"",
                # cc:android_tensorflow_image_op is for including jpeg/gif/png
                # decoder to enable real-image evaluation on Android
                ""//tensorflow/core/kernels:android_tensorflow_image_op"",
            ],
            ""//conditions:default"": [
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:core_cpu"",
                ""//tensorflow/core:framework"",
                ""//tensorflow/core:framework_internal"",
                ""//tensorflow/core:lib"",
                ""//tensorflow/core:protos_all_cc"",
                ""//tensorflow/core:tensorflow"",
            ],
        }),
    )

    filegroup(
        name = ""all_files"",
        srcs = glob(
            [""**/*""],
            exclude = [
                ""**/METADATA"",
                ""**/OWNERS"",
                ""bin/**"",
                ""gen/**"",
            ],
        ),
        visibility = [""//tensorflow:__subpackages__""],
    )


"
16799,Debug prompts use_default_colors() returned ERR,"I was running a example code from [tensorpack](http://tensorpack.readthedocs.io/en/latest/index.html) on Pycharm, which runs properly, then I changed the session to 
`sess = tf_debug.LocalCLIDebugWrapperSession(sess) ` in order to debug
as suggested by official example https://www.tensorflow.org/programmers_guide/debugger, but then I got:
```
Traceback (most recent call last):
  File ""/home/user/.pycharm_helpers/pydev/pydev_run_in_console.py"", line 53, in run_file
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/user/prj/shufflenet_v1/shufflenet.py"", line 231, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_tower))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 96, in launch_train_with_config
    config.steps_per_epoch, config.starting_epoch, config.max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 288, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 239, in main_loop
    self.loop.update_global_step()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 59, in update_global_step
    self._global_step = get_global_step_value()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/tfutils/common.py"", line 77, in get_global_step_value
    get_global_step_var())
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/training_util.py"", line 67, in global_step
    return int(sess.run(global_step_tensor))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 455, in run
    is_callable_runner=bool(callable_runner)))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 253, in on_run_start
    self._prep_cli_for_run_start()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 275, in _prep_cli_for_run_start
    self._run_cli = ui_factory.get_ui(self._ui_type)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/ui_factory.py"", line 56, in get_ui
    return curses_ui.CursesUI(on_ui_exit=on_ui_exit)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 285, in __init__
    self._screen_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 400, in _screen_init
    self._screen_color_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 405, in _screen_color_init
    curses.use_default_colors()
error: use_default_colors() returned ERR
```
which I have no clue for the reason, can someone help?"
16798,tf.nn.conv2d on GPU with data_format='NHWC' gives corrupted result for specific shapes,"Basically what the title says.

For an image of size (1096, 2449) EXACTLY, not 1097 or 1095 or 2448 or 2450 (but 2451 for some reason produces the same effect). The bottom part of the convolution result gets corrupted. It only affects convolution done on the GPU with the 'NHWC' data format.

Honestly, it feels more of a cuDNN bug than anything, but not sure where to post it otherwise.

**Important note : I am on Ubuntu 14.04 hence I can not try tensorflow 1.5 which needs CUDA 9 which needs 16.04. So my test is done on 1.4 with cuda8 and cuDNNv6.**

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip install 1.4 tensorflow-gpu
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.5
- **CUDA/cuDNN version**: cuda 8 cudnn 6
- **GPU model and memory**: TITAN X Pascal
- **Exact command to reproduce**:

```python
import tensorflow as tf
from tensorflow.contrib import layers
sess=tf.InteractiveSession()
h, w = 1096, 2449
input_img = tf.placeholder(tf.float32, shape=(None, h, w, 3))
filters = np.random.randn(1,1,3,2)
with tf.device('/cpu:0'):
    out_cpu = tf.nn.conv2d(input_img, filter=filters, strides=(1,1,1,1), padding='VALID')
with tf.device('/gpu:0'):
    out_gpu = tf.nn.conv2d(input_img, filter=filters, strides=(1,1,1,1), padding='VALID')
    # The following actually works if you manually transpose to use the NCHW data_format
    # tmp = tf.transpose(input_img, (0,3,1,2))
    # out_gpu = tf.nn.conv2d(tmp, filter=filters, strides=(1,1,1,1), padding='VALID', data_format='NCHW')
    # out_gpu = tf.transpose(out_gpu, (0,2,3,1))

out1, out2 = sess.run((out_cpu, out_gpu), feed_dict={
    input_img: np.random.randn(1, h, w, 3)
})
plt.imshow(np.linalg.norm(out1-out2 , axis=-1)[0])
plt.colorbar()
```
Plotting the difference between the CPU conv and the GPU conv :

<img width=""359"" alt=""capture d ecran 2018-02-06 a 12 31 03"" src=""https://user-images.githubusercontent.com/7132817/35857501-a74bb66e-0b39-11e8-9edd-98e69eba5c48.png"">

"
16796,Slim batch image prediciton?,"I just finished training a model by following train_image_classifier.py

CUDA_VISIBLE_DEVICE=0,1 python train_image_classifier.py --train_dir=train_logs --dataset_dir=../train --num_samples=15500 --num_classes=4 --labels_to_names_path=../labels.txt --model_name=inception_resnet_v2 --checkpoint_path=../checkpoints/inception_resnet_v2_2016_08_30.ckpt --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --num_clones=2 --num_preprocessing_threads=8 --max_number_of_steps=100000 --batch_size=32 --learning_rate=0.0001 --learning_rate_decay_type=fixed --save_interval_secs=60 --save_summaries_secs=60 --log_every_n_steps=10 --optimizer=rmsprop --weight_decay=0.00004

and evaluate the model by using eval_image_classifier.py. 

CUDA_VISIBLE_DEVICE=0,1 python eval_image_classifier.py --checkpoint_path=train_logs --eval_dir=eval_logs --dataset_dir=../val --num_samples=797 --num_classes=4 --model_name=inception_resnet_v2

Everything seems great. But, how to test image classifier unfortunately is not provided, and I cannot really find an good example of how to use this model to predict testing images. Is there a good example that I can use my trained model to predict multiple images in a batch way? Thank you for helping. 
"
16795,Android C++ API on arm64-v8a,"Hello,

I try to compile tensorflow for android arm64-v8a. I found lots of issues on similar problem but no answer work for me. Here is the full description of my commands:

Informations of my system:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Not used
- **GPU model and memory**: Not used
- **Have I written custom code** : No
- **Exact command to reproduce** : `bazel build --cxxopt=--std=c++11  -c opt //tensorflow:libtensorflow_cc.so    --verbose_failures    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=arm64-v8a
`


```
ERROR: /home/xavier/Bureau/developpement/NeuralNetwork/tensorflow/tensorflow/core/kernels/BUILD:4276:1: C++ compilation of rule '//tensorflow/core/kernels:random_poisson_op' failed (Exit 1): clang failed: error executing command 
  (cd /home/xavier/.cache/bazel/_bazel_xavier/ef54af8645dec4f38c438d8e1c779747/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/ros/kinetic/lib:/usr/local/lib: \
    PATH=/opt/ros/kinetic/bin:/home/xavier/Android/Sdk/ndk-bundle/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/aarch64-linux-android/bin/ld:/home/xavier/bin:/home/xavier/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/xavier/Android/Sdk/platform-tools:/home/xavier/android-ndk-r14b:/home/xavier/Bureau/developpement/androidscreencast/bin:/home/xavier/Bureau/developpement/androidscreencast:/home/xavier/Bureau/developpement/gerrit_tools:/home/xavier/android-studio/bin:/home/xavier/Bureau/developpement:/home/xavier/dev/common.tools/bin:/home/xavier/dev/gcc-arm-none-eabi-6-2017-q2-update/bin:/home/xavier/dev/dump_Cyril:/home/xavier/opt/gnu-mcu-eclipse/openocd/0.10.0-3-20170826-1813-dev/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -ffunction-sections -funwind-tables -fstack-protector-strong -fpic -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -O2 -g -DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '--std=c++11' -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.o' -DEIGEN_MPL2_ONLY '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/arm64-v8a-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/arm64-v8a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm64-v8a-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm64-v8a-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-24/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/random_poisson_op.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/random_poisson_op/tensorflow/core/kernels/random_poisson_op.o)
In file included from tensorflow/core/kernels/random_poisson_op.cc:27:
In file included from ./tensorflow/core/framework/op_kernel.h:23:
In file included from ./tensorflow/core/framework/allocator.h:23:
In file included from ./tensorflow/core/framework/numeric_types.h:21:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:46:
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:108:5: error: static_assert failed ""THIS_TYPE_IS_NOT_SUPPORTED""
    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:32:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'
    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);
                                       ^             ~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1509:47: note: in instantiation of member function 'Eigen::internal::lgamma_impl<double>::run' requested here
  return EIGEN_MATHFUNC_IMPL(lgamma, Scalar)::run(x);
                                              ^
tensorflow/core/kernels/random_poisson_op.cc:234:58: note: in instantiation of function template specialization 'Eigen::numext::lgamma<double>' requested here
            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);
                                                         ^
tensorflow/core/kernels/random_poisson_op.cc:308:5: note: in instantiation of member function 'tensorflow::functor::PoissonFunctor<Eigen::ThreadPoolDevice, float, float>::operator()' requested here
    functor::PoissonFunctor<CPUDevice, T, U>()(
    ^
tensorflow/core/kernels/random_poisson_op.cc:284:12: note: in instantiation of member function 'tensorflow::(anonymous namespace)::RandomPoissonOp<float, float>::Compute' requested here
  explicit RandomPoissonOp(OpKernelConstruction* context) : OpKernel(context) {
           ^
tensorflow/core/kernels/random_poisson_op.cc:328:15: note: in instantiation of member function 'tensorflow::(anonymous namespace)::RandomPoissonOp<float, float>::RandomPoissonOp' requested here
TF_CALL_float(REGISTER);
              ^
1 error generated.
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 1712.148s, Critical Path: 61.59s
FAILED: Build did NOT complete successfully

```
"
16794,Feature Request: Modification of lstm2d.horizontal_lstm implementation,"I noticed something in the documentation of `lstm2d.horizontal_lstm`. It says:

> Run an LSTM bidirectionally over all the rows of each image.

Kinda looks like a bidirectional_lstm to me. I propose to change the implentation such that it will use bidirectional_lstm within the function replacing this:

```
with variable_scope.variable_scope(""lr""):
  hidden_sequence_lr = lstm1d.ndlstm_base(sequence, num_filters_out // 2)
with variable_scope.variable_scope(""rl""):
  hidden_sequence_rl = (lstm1d.ndlstm_base(
      sequence, num_filters_out - num_filters_out // 2, reverse=1))
output_sequence = array_ops.concat([hidden_sequence_lr, hidden_sequence_rl],
                                       2)
```

With this:

```
cell_fw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
cell_bw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
output_sequence = rnn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, time_major=True, 
                                                                                 dtype=sequence.dtype)
```
  "
16793,tf.gradients(colocate_gradients_with_ops=True) set wrong device when using CPU param weight decay,"**System information**

- OS Platform and Distribution: CentOS Linux 7(x86-64)
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.4.1
- Python version: 2.7.6
- CUDA/cuDNN version: 8.0/6.0
- Have I written custom code: YES
- Bazel version: N/A
- GPU model and memory: PS Toy model
- Exact command to reproduce: List at the end

I am trying to define a two-layered dnn for mnist classification, first fc on cpu second on gpu. The devices are set with replica_device_setter. I computed gradients with **colocate_gradients_with_ops=True**. If as expected, the gradient of first layer should be on **/job:worker/replica:0/task:0/device:CPU:0** and gradient of the second layer on **/job:worker/task:0/device:GPU:0**.

However, i was confused that gradient of the first layer is on device **/job:ps/replica:0/task:0/device:CPU:0**! 

I noticed that this error can be avoided by omitting `loss += l2_loss * weight_decay`, BUT WHY? It's there some conflict between CPU param weight_decay and gradients with colocate_gradients_with_ops=True?

**Source Code**

    import tensorflow as tf

    # cluster specification
    parameter_servers = [""10.194.43.100:2222""]
    workers = [""10.194.43.100:%d""%(2230+i) for i in range(2)]

    cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})
    worker_prefix = ""/job:worker/task:%d""%0
    cpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/cpu:0', cluster=cluster)
    gpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/gpu:0', cluster=cluster, ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(1, tf.contrib.training.byte_size_load_fn))

    normal_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=""float32"")
    weight_decay = 1e-2

    with tf.device(cpu_device):
        W0 = tf.get_variable('W0', shape=[784, 100], initializer=normal_initializer, dtype=""float32"")
        b0 = tf.get_variable('b0', shape=[100], initializer=tf.constant_initializer(0), dtype=""float32"")
        x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input_%d""%i)
        y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input_%d""%i)

        cpu_output = tf.add(tf.matmul(x, W0), b0)

    with tf.variable_scope('v', reuse=False), tf.name_scope('tower_0') as name_scope:
        with tf.device(gpu_device):
            W1 = tf.get_variable('W1', shape=[100, 10], initializer=normal_initializer, dtype=""float32"")
            b1 = tf.get_variable('b1', shape=[10], initializer=tf.constant_initializer(0), dtype=""float32"")

            gpu_output = tf.add(tf.matmul(cpu_output, W1), b1)
            loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gpu_output)
            loss = tf.reduce_mean(loss)

            params = tf.trainable_variables()
            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])
            loss += l2_loss * weight_decay
            grads = tf.gradients(loss, params, colocate_gradients_with_ops=True, aggregation_method=tf.AggregationMethod.DEFAULT)

    with tf.device(cpu_device):
        for grad in grads:
            print grad, grad.device
    print 'done'

**Logs**

    Tensor(""v/tower_0/gradients/AddN_3:0"", shape=(784, 100), dtype=float32, device=/job:ps/task:0) /job:ps/task:0
    Tensor(""v/tower_0/gradients/AddN_2:0"", shape=(100,), dtype=float32, device=/job:ps/task:0) /job:ps/task:0
    Tensor(""v/tower_0/gradients/AddN_1:0"", shape=(100, 10), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0
    Tensor(""v/tower_0/gradients/AddN:0"", shape=(10,), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0
    done"
16791,Tensorflow 1.5.0 Import Error on CUDA 9.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I used the stock version.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip binary for windows with GPU support
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 3.6.1 from Anaconda
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Installed CUDA/CUDNN 8.0/6, 9.0/7, 9.1/7
Used pip to install tensorflow_gpu 1.5.0

The installation process finished normally.

However, when import the module by ""import tensorflow as tf"", error messages raises and it says ImportError: Could not find 'cudart64_90.dll'.

I double checked the CUDA_PATH and PATH environmental variables to make sure that CUDA/CUDNN 9.0/7 are being used. Later I removed the 8.0/6 and 9.1/7, and the problem still exists.

### Source code / logs
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit
"
16790,Save a numpy params as tensorflow model!,"Hi, I have trained a model, and i save the params in a numpy file in dict type.
Now i construct the network manually, and set the params as my trained model.
i want to save the network as tensorflow model, but the session is empty, 
so how could i save the model?"
16782,Module missing,"
"
16781,Feature request: Save optimizer variables under separate name scope,"Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0
Have I written custom code N/A
OS Platform and Distribution N/A 
TensorFlow installed from N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A

The variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. `model/q_networks/online/conv2d/bias/Adam_1` (the Adam optimizer was not creater under the name scopes `model`, `q_networks` or `online`).

This creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.

Here's a workaround:
```
        build_model()  # Build model under namescope 'model', without optimizer
        temp = set(tf.global_variables())
        if save or restore:
            saver = tf.train.Saver(
                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))
        sess.run(tf.global_variables_initializer())
        if restore:
            saver.restore(sess, model_path)
        trainer = tf.train.AdamOptimizer(1e-4)
        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables
```
It would be simpler (to figure out) if the optimizer variables had their own scope.

"
16780,Trying to allocate large output tensor in custom op leads to multiple evaluation of Compute method,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU-version of Tensorflow
- **GPU model and memory**:
- **Exact command to reproduce**: make


### Describe the problem
I wrote a custom op. During debugging i've noticed then Compute method from my op fired multiple times during single op.eval() call.
My quest lead me to row:
```c++
OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({output_size}), &values_tensor));
```
If output_size is small (e.g. 1000), my op works well and executes one time.
If output_size is big (e.g. 15000000), my op executes multiple times.

### Source code / logs
There are 3 files in archive:
- custom op source code
- demo eval script
- makefile config to build op and run eval


[issue.zip](https://github.com/tensorflow/tensorflow/files/1696503/issue.zip)
"
16779,Cross-Compiling the TensorFlow wheel for NVIDIA Jetson with CUDA support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Irrelevant
- **Exact command to reproduce**:

* Install gcc cross-compiler: `sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu`
* Install the CUDA `cross-aarch64` packages
* Build Python 3.5.2 for the target
* I wrote a short blog post with the details: https://jany.st/post/2018-02-05-cross-compiling-tensorflow-for-jetson-tx1-with-bazel.html

```
git clone https://github.com/ljanyst/tensorflow.git
cd tensorflow
git checkout v1.5.0-cross-jetson-tx1
cd third_party/toolchains/cpus/aarch64
./configure.py
cd ../../../..
./configure
bazel build  --config=opt --config=cuda \
   --crosstool_top=//third_party/toolchains/cpus/aarch64:toolchain \
    --cpu=arm  --compiler=cuda \
    //tensorflow/tools/pip_package:build_pip_package
```
### Describe the problem
I have succeeded, but I encountered a bunch of issues on the way. Due to my unfamiliarity with Bazel, my solution is rather hacky. It looks to me like properly fixing it is a rather low hanging fruit for a person familiar with Bazel, so I document what I have discovered here.

#### 1. Configuring CUDA for the target

I could not find a way in the to check CUDA configuration script whether the source is supposed to be configured for a cross build, so I ended up changing the hardcoded paths: https://github.com/ljanyst/tensorflow/commit/1a2a75fed9a9576c4e9e8f89ee556263cf22deaf If there was a way to check if we're building for a platform that is different from the one of the build host, this could be easily turned into some sort of an if statement.

#### 2. Specifying target Python installation

I could not find an easy way to patch that through, so I ended up putting it in the [CROSSTOOL](https://github.com/ljanyst/tensorflow/blob/v1.5.0-cross-jetson-tx1/third_party/toolchains/cpus/aarch64/CROSSTOOL.in) file:

```
cxx_flag: ""-isystem""
cxx_flag: ""__TARGET_PYTHON_INCLUDES__""
```

#### 3. Code generators depend on `libtensorflow_framework.so` which, in turn, depends on CUDA

This means that I needed to have the CUDA and cuDNN libraries for the build host and needed to pass the relevant library paths to the compiler [in a wrapper script](https://github.com/ljanyst/tensorflow/blob/v1.5.0-cross-jetson-tx1/third_party/toolchains/cpus/aarch64/crosstool_wrapper_host_tf_framework#L41).

#### 4. Linking of the code generators fails on the build-host side

Bazel builds both the code generators and `libtensorflow_framework.so` for the host, but does not link the binaries against the framework library.

I have worked around the two above issues by writing a compiler wrapper scripts doing the following:

```python
if ofile is not None:
    is_gen = ofile.endswith('py_wrappers_cc') or ofile.endswith('gen_cc')
    if is_cuda == 'yes' and (ofile.endswith('libtensorflow_framework.so') or is_gen):
        cuda_libdirs = [
            '-L', '{}/targets/x86_64-linux/lib'.format(cuda_dir),
            '-L', '{}/targets/x86_64-linux/lib/stubs'.format(cuda_dir),
            '-L', '{}/lib64'.format(cudnn_dir),
        ]

    if is_gen:
        tf_libs += [
            '-L', 'bazel-out/host/bin/tensorflow',
            '-ltensorflow_framework'
]

call([find_executable('gcc')] + cuda_libdirs + args + tf_libs)
```

#### 5. Incomplete RPATH in the target-side `libtensorflow_framwork.so`

The library gets linked using a bunch of RPATH parameters, but one seems to be missing. It causes linking errors down the road. I have added the following to the original `crosstool_wrapper_driver_is_not_gcc` script to fix the problem:

```python
  ofile = GetOptionValue(sys.argv[1:], 'o')
  if ofile and ofile[0].endswith('libtensorflow_framework.so'):
    cpu_compiler_flags += [
        '-Wl,-rpath,'+os.getcwd()+'/bazel-out/arm-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/lib',
    ]
```

#### 6. Platform metadata of the resulting wheel package

Since the build host platform is `linux_x86_64`, this ends up being written in the wheel metadata. The issue can be fixed by manually passing the `--plat-name` parameter to distutils. I have, therefore, added https://github.com/ljanyst/tensorflow/commit/a9b952e3850b0c416a37f4f4b488adf8189638a7 to `build_pip_package.sh`. Please let me know if you'd like a pull request.

It hope it's helpful.
"
16777,Tensorflow object recognition SSD is really slow with tf 1.5 ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16776,Unnamed Op not showing in list_tensors command during debug session,"I have these two unnamed op tensors `logits` and `outputs` under a variable scope, but the `lt` command isn't listing these two tensors under the op 'MatMul' and 'Softmax' during the `tfdbg` session after a test run on a checkpoint. Here is a snapshot of the code:
```
    with tf.variable_scope(scope):
        d_inputs = dropout(inputs, keep_prob=keep_prob, is_train=is_train)
		d_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)
		JX = tf.shape(inputs)[1]

		with tf.variable_scope(""attention""):
			inputs_ = tf.nn.relu(dense(d_inputs, hidden, use_bias=False, scope=""inputs""))
			memory_ = tf.nn.relu(dense(d_memory, hidden, use_bias=False, scope=""memory""))
			outputs = tf.matmul(inputs_, tf.transpose(memory_, [0, 2, 1])) / (hidden ** 0.5)
			mask = tf.tile(tf.expand_dims(mask, axis=1), [1, JX, 1])
            # The tensor down below 
			logits = tf.nn.softmax(softmax_mask(outputs, mask))
            # And the tensor down below here as well
			outputs = tf.matmul(logits, memory)
			res = tf.concat([inputs, outputs], axis=2)
```
What can I do to retrieve these variables for testing purposes on `tfdbg`?  
For alternative purposes, can I retrieve them using the normal tensorflow session by using `tf.add_to_collection(op_name, tensor)` as mentioned in [this answer][1]?


  [1]: https://stackoverflow.com/questions/44639260/retrieving-an-unnamed-variable-in-tensorflow"
16775,tf.layers.Dense work error with unit is a Tensor,"OS Platform and Distribution
TensorFlow installed from
docker hub

TensorFlow version
1.5.0

CUDA/cuDNN version
CUDA9

GPU model and memory
TITAN XP

Exact command to reproduce
N/A

### Describe the problem
`tf.layers.Dense._rnn_output_size()` not work when `self.units` is a Tensor.

"
16774,Missing functionality in Keras TensorBoard callback ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: 1.5.0 (Keras 2.1.2-tf)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0 / 7.0
- **GPU model and memory**: GTX 1070 8GB
- **Exact command to reproduce**: NA

### Describe the problem

The TensorBoard-callback in Keras supports the writing of embeddings to the log, but this is not supported in the TensorFlow version of Keras, even though the doc-string of the TensorFlow version actually lists these parameters.

https://github.com/tensorflow/tensorflow/blob/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L643-L651

But these are missing from the `__init__`:

https://github.com/tensorflow/tensorflow/blob/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L656-L662

This is the original Keras implementation which has e.g. `embeddings_freq`:

https://github.com/keras-team/keras/blob/ad00676b80556a6354180a1bfa3009d4db316d3e/keras/callbacks.py#L642-L650

There seems to be a difference between the original Keras implementation and the version in TensorFlow. I am wondering if these are somehow developed in parallel so it is actually not the original Keras that is included with TensorFlow?

Thanks!"
16773,TensorBoard: Fit domain to data in 1.5 under Windows cuts off max values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using TensorBoard in custom U-Net implementation
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7
- **GPU model and memory**: GeForce GTX 1050 Ti (4 GB), 32 GB RAM
- **Exact command to reproduce**: tensorboard with tf.summary.scalar

### Describe the problem
After updating from 1.4 to 1.5 I have the problems that the y-scale for the scalar graphs in TensorBoard seems to be misscalculated. The maximum values are not included in the shown graph, they are cut off.

![image](https://user-images.githubusercontent.com/31246640/35806246-1efb92ee-0a7f-11e8-8441-17ebe97473b7.png)

"
16772,XLA with frozen protobuf: Tuples do not have a rank error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, model inference script.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3 LTS (Xenial Xerus)
- **TensorFlow installed from (source or binary)**: Source **with XLA support**
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- **CUDA/cuDNN version**: cuda-8.0/cuDNN-6.0.21
- **GPU model and memory**: Tesla K80, 12 Gb
- **Exact command to reproduce**: `CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python name_of_script.py`

### Describe the problem
I am trying to run model (SSD with MobileNetV1 from [tf-models](https://github.com/tensorflow/models/tree/master/research/object_detection)) with XLA optimization. I am trained this model on my own data set and generated **frozen** protobuf file with provide script [export_inference_graph.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py). 
**Note**: Script that run inference works fine **without** XLA. Models from `keras.application` works fine **with** XLA, so I think there are some problems with support of frozen protobufs. 

### Source code / logs
Source code:
All test completed with next session config:
```
gpu_options = tf.GPUOptions(
            allocator_type='BFC',
            allow_growth=True,
            per_process_gpu_memory_fraction=True
        )

config = tf.ConfigProto(
            allow_soft_placement=True,
            gpu_options=gpu_options
       )

config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
```

Log:
```
(tf-models-env) alexkirnas@host:~/scrips/tfDetector$ CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python ssd_model_inference_time_test_random.py              
/tf-models-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `n
p.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-02-05 10:55:01.159354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning
 NUMA node zero
2018-02-05 10:55:01.160137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.11GiB
2018-02-05 10:55:01.160163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability
: 3.7)
2018-02-05 10:55:03.544862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability
: 3.7)
2018-02-05 10:55:06.752946: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x7f6454024500 executing computations on platform CUDA. Devices:
2018-02-05 10:55:06.752997: I tensorflow/compiler/xla/service/service.cc:170]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2018-02-05 10:55:06.757697: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: pipeli
ne start, before CallInliner]: /tmp/hlo_graph_0.GS9qJI.dot
2018-02-05 10:55:06.757963: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: after
CallInliner, before simplification]: /tmp/hlo_graph_1.KFEf5N.dot

....
Lot of such lines as above
.... 

2018-02-05 10:55:45.106105: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after
 cse, before dce]: /tmp/hlo_graph_401.V0lBLH.dot
2018-02-05 10:55:45.112071: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after
 dce, pipeline end]: /tmp/hlo_graph_402.I2Gnpl.dot
2018-02-05 10:55:45.117834: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [fusion: pipeline st
art, before fusion]: /tmp/hlo_graph_403.Jmq33Y.dot
2018-02-05 10:55:45.122108: F tensorflow/compiler/xla/shape_util.cc:118] Check failed: !ShapeUtil::IsTuple(shape) Tuples do not have a rank
Aborted (core dumped)
```"
16771,core dump is occured using reduce_sum,"Core dump is occured when I tried thus code `tf.reduce_sum(tf.constant([1, 2, 3], dtype=tf.uint8))`.
I consider reduction function is not supported tf.uint8, so please `raise TypeError`.
The code I linked looks relevant the problem. (I couldn't send Pull Request and propose good code.)
I snipped a part of the error message.
Please fix.

日本語：
 `tf.reduce_sum(tf.constant([1, 2, 3], dtype=tf.uint8))`を実行した際にコアダンプが発生しました
恐らくreduce系の関数でtf.uint8をサポートしていないからだと思います　そのため`raise TypeError`などしてほしいです
うまいコードも思いつかずプルリクエストも出せなかったので，この問題に関連すると思われるコードをリンクしました．
エラーメッセージの一部を貼り付けました　修正お願いします

## Environment
tf.__version__ -> '1.4.1'
cv2.__version__ -> '3.3.1'
np.__version__ -> '1.14.0'


https://github.com/tensorflow/tensorflow/blob/bd0d204700df1a1a245b0593a11efd8ede139311/tensorflow/python/ops/math_ops.py#L1318

python test.py
2018-02-05 19:23:40.942499: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
*** Error in `/home/dl-box/miniconda2/envs/onyx/bin/python': double free or corruption (!prev): 0x00005636654f79f0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f7371da17e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f7371daa37a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f7371dae53c]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x30c906)[0x7f731ee39906]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x30c974)[0x7f731ee39974]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1ef281)[0x7f731ed1c281]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1c79fb)[0x7f731ecf49fb]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(cuInit+0x4d)[0x7f731ed47abd]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x89b796)[0x7f73268a4796]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN9perftools8gputools4cuda10CUDADriver4InitEv+0x5d)[0x7f73268a495d]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNK9perftools8gputools4cuda12CudaPlatform18VisibleDeviceCountEv+0x12)[0x7f73268b48a8]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow20BaseGPUDeviceFactory17GetValidDeviceIdsERKSsPSt6vectorIiSaIiEE+0xf0)[0x7f73267d86b0]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x1b1)[0x7f73267db3b3]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x159)[0x7f73267f8ddd]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20DirectSessionFactory10NewSessionERKNS_14SessionOptionsE+0x98)[0x7f732a1be5ba]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10NewSessionERKNS_14SessionOptionsEPPNS_7SessionE+0xfb)[0x7f732683666c]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TF_NewDeprecatedSession+0x21)[0x7f732807e03f]
/home/dl-box/miniconda2/envs/onyx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0xfa3267)[0x7f7327d5e267]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyCFunction_FastCallDict+0x91)[0x56366192b7d1]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e17c)[0x5636619bb17c]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x30a)[0x5636619ddbba]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x197354)[0x5636619b4354]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19824f)[0x5636619b524f]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e255)[0x5636619bb255]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x10b8)[0x5636619de968]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x1974f6)[0x5636619b44f6]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyFunction_FastCallDict+0x1bc)[0x5636619b56fc]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_FastCallDict+0x26f)[0x56366192bc5f]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_Call_Prepend+0x63)[0x5636619308c3]
/home/dl-box/miniconda2/envs/onyx/bin/python(PyObject_Call+0x3e)[0x56366192b69e]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x16a80b)[0x56366198780b]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e4b7)[0x5636619bb4b7]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyObject_FastCallDict+0x8b)[0x56366192ba7b]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x19e2ce)[0x5636619bb2ce]
/home/dl-box/miniconda2/envs/onyx/bin/python(_PyEval_EvalFrameDefault+0x30a)[0x5636619ddbba]
/home/dl-box/miniconda2/envs/onyx/bin/python(PyEval_EvalCodeEx+0x329)[0x5636619b5d39]
/home/dl-box/miniconda2/envs/onyx/bin/python(PyEval_EvalCode+0x1c)[0x5636619b6adc]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x214be4)[0x563661a31be4]
/home/dl-box/miniconda2/envs/onyx/bin/python(PyRun_FileExFlags+0xa1)[0x563661a31fe1]
/home/dl-box/miniconda2/envs/onyx/bin/python(PyRun_SimpleFileExFlags+0x1c4)[0x563661a321e4]
/home/dl-box/miniconda2/envs/onyx/bin/python(Py_Main+0x5ff)[0x563661a35cbf]
/home/dl-box/miniconda2/envs/onyx/bin/python(main+0xee)[0x5636618fcdbe]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f7371d4a830]
/home/dl-box/miniconda2/envs/onyx/bin/python(+0x1c75eb)[0x5636619e45eb]
======= Memory map: ========
56366181d000-563661adb000 r-xp 00000000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6
563661cdb000-563661cde000 r--p 002be000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6
563661cde000-563661d41000 rw-p 002c1000 08:01 4994230                    /home/dl-box/miniconda2/envs/onyx/bin/python3.6
563661d41000-563661d72000 rw-p 00000000 00:00 0 
56366386c000-5636659ec000 rw-p 00000000 00:00 0                          [heap]
7f72b4000000-7f72b4021000 rw-p 00000000 00:00 0 
7f72b4021000-7f72b8000000 ---p 00000000 00:00 0 
.

7f7370df8000-7f7370dfb000 r-xp 00000000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so
7f7370dfb000-7f7370ffa000 ---p 00003000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so
7f7370ffa000-7f7370ffb000 r--p 00002000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so
7f7370ffb000-7f7370ffd000 rw-p 00003000 08:01 4988702                    /home/dl-box/miniconda2/envs/onyx/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so
7f7370ffd000-7f737103d000 rw-p 00000000 00:00 0 
7f737103d000-7f7371412000 r--p 00000000 08:01 1837470                    /usr/lib/locale/locale-archive
7f7371412000-7f737151a000 r-xp 00000000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so
7f737151a000-7f7371719000 ---p 00108000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so
7f7371719000-7f737171a000 r--p 00107000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so
7f737171a000-7f737171b000 rw-p 00108000 08:01 131145                     /lib/x86_64-linux-gnu/libm-2.23.so
7f737171b000-7f7371722000 r-xp 00000000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so
7f7371722000-7f7371921000 ---p 00007000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so
7f7371921000-7f7371922000 r--p 00006000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so
7f7371922000-7f7371923000 rw-p 00007000 08:01 131862                     /lib/x86_64-linux-gnu/librt-2.23.so
7f7371923000-7f7371925000 r-xp 00000000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so
7f7371925000-7f7371b24000 ---p 00002000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so
7f7371b24000-7f7371b25000 r--p 00001000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so
7f7371b25000-7f7371b26000 rw-p 00002000 08:01 131483                     /lib/x86_64-linux-gnu/libutil-2.23.so
7f7371b26000-7f7371b29000 r-xp 00000000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so
7f7371b29000-7f7371d28000 ---p 00003000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so
7f7371d28000-7f7371d29000 r--p 00002000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so
7f7371d29000-7f7371d2a000 rw-p 00003000 08:01 131431                     /lib/x86_64-linux-gnu/libdl-2.23.so
7f7371d2a000-7f7371eea000 r-xp 00000000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so
7f7371eea000-7f73720ea000 ---p 001c0000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so
7f73720ea000-7f73720ee000 r--p 001c0000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so
7f73720ee000-7f73720f0000 rw-p 001c4000 08:01 131427                     /lib/x86_64-linux-gnu/libc-2.23.so
7f73720f0000-7f73720f4000 rw-p 00000000 00:00 0 
7f73720f4000-7f737210c000 r-xp 00000000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so
7f737210c000-7f737230b000 ---p 00018000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so
7f737230b000-7f737230c000 r--p 00017000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so
7f737230c000-7f737230d000 rw-p 00018000 08:01 131426                     /lib/x86_64-linux-gnu/libpthread-2.23.so
7f737230d000-7f7372311000 rw-p 00000000 00:00 0 
7f7372311000-7f7372337000 r-xp 00000000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so
7f7372344000-7f7372509000 rw-p 00000000 00:00 0 
7f737252d000-7f737252e000 rw-p 00000000 00:00 0 
7f737252e000-7f737252f000 rwxp 00000000 00:00 0 
7f737252f000-7f7372536000 r--s 00000000 08:01 2230848                    /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache
7f7372536000-7f7372537000 r--p 00025000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so
7f7372537000-7f7372538000 rw-p 00026000 08:01 131335                     /lib/x86_64-linux-gnu/ld-2.23.so
7f7372538000-7f7372539000 rw-p 00000000 00:00 0 
7ffdfa56f000-7ffdfa58f000 rwxp 00000000 00:00 0                          [stack]
7ffdfa58f000-7ffdfa590000 rw-p 00000000 00:00 0 
7ffdfa5c0000-7ffdfa5c3000 r--p 00000000 00:00 0                          [vvar]
7ffdfa5c3000-7ffdfa5c5000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
Aborted (core dumped)"
16770,TensorBoard Projector has been blocked by CORS policy: No 'Access-Control-Allow-Origin',"**At http://projector.tensorflow.org/ website error.**
### System information
- **Have I written custom code**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.13.3, 
Safari: Version 11.0.3 (13604.5.6)
Chrome: Version 63.0.3239.132 (Official Build) (64-bit)  
- **TensorFlow installed from (source or binary)**:
no

I am trying to publish my data over official projector.tensorflow.org website.
1) Prepared: config, data, metadata.
2) Hosted them on ([Gist](https://gist.githubusercontent.com/VolodymyrPavliukevych/fae3d662ca170666d1e82a85bf530043/raw/efa07424c08b2e8bd6d96f59501685ae578a6018/rada_full_packed_projector_config.json)/[google storage](https://storage.googleapis.com/api.octadero.com/rada/rada_full_packed_projector_config.json)/[my own website](http://octadero.com/API/rada_full_packed_projector_config.json)) 
With and without https.
3) Trying to view on projector: [Published config](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/VolodymyrPavliukevych/fae3d662ca170666d1e82a85bf530043/raw/efa07424c08b2e8bd6d96f59501685ae578a6018/rada_full_packed_projector_config.json)

Receive error: Failed to load [cut](https://gist.github.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed): Redirect from '[cut](https://gist.github.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed)' to '[cut](https://gist.githubusercontent.com/VolodymyrPavliukevych/8e28b38560086b17148aa6d30aa3e264/raw/a63528b9d22bfa9d43c9c906642099f9774713fe/rada_full_packed)' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://projector.tensorflow.org' is therefore not allowed access.

![error-screen](https://user-images.githubusercontent.com/1786378/35802959-c419a2c0-0a7a-11e8-9de7-4e71bb0f30e9.png)

There is way to launch chrome with `--disable-web-security` key, but that ruins idea to share data in public. 
"
16768,OOM when allocating tensor with shape,"I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,
however, I keep getting OOM error on GPU, but it does not happen when using cpu for training:

-------------------------------------
---log below
-------------------------------------

Exception happened during training, message: OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
  File ""sync_train.py"", line 380, in main
    train(config)
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 291, in __init__
    grads_and_vars = self.optimizer.compute_gradients(loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py"", line 922, in _MatMulGrad
    grad_b = math_ops.matmul(a, grad, transpose_a=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
[elided 1 identical lines from previous traceback]
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 189, in __init__
    feed_previous=feed_previous)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 427, in seq2seq
    pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/decoder.py"", line 99, in __call__
    attention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/attention.py"", line 427, in __call__
    lstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])
  File ""/kaldi/exp/tacotron/exp_2/zoneout_lstm.py"", line 48, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 611, in call
    lstm_matrix = self._linear1([inputs, m_prev])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1189, in __call__
    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]"
16767,Is there any way to reduce the model fie size of adam optimizer?,"I find that the model file size is three times bigger after I switch optimizer from `tf.train.GradientDescentOptimizer` to `tf.train.AdamOptimizer`. And here is the reason I found on stackoverflow:

>ADAM adds two running means (for gradient and square of gradient) as additional non-trainable parameters for each trainable parameter

Thus, is there any way to reduce the model fie size of adam optimizer?

THANKS!!"
16766,with tf.control_dependencies fails on return,"Hello,
here is an undesired behavior:

fails
>  with tf.control_dependencies([check_num_pnt]):
>         out_feat = seg_sum 
> return out_feat

works
>  with tf.control_dependencies([check_num_pnt]):
>         out_feat = seg_sum * 1.0
> return out_feat

ps: I should have said fails on copied tensor"
