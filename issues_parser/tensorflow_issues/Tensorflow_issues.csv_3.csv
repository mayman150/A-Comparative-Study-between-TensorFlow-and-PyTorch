Issue Number,Issue Title,Issue Body
59684,tflite model has bad accuracy,"Hey, 
I used tflite converter to convert .h5 file, the model is much smaller than the original, but the accuracy dropped drastically.[ Here ](https://colab.research.google.com/drive/14zMCm0XaszVrUoWd9bMvBjSXSdPvSWuw?userstoinvite=tirumaleshk%40google.com&actionButton=1#scrollTo=JWvOm1R5WyPs )is the code. Could you please check if everything is correct?
Thank you in advance!"
59680,Unit test //tensorflow/tsl/framework/convolution:spatial_convolutions_test fails to build,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

RHEL 8.7

### Mobile device

n/a

### Python version

3.9.13

### Bazel version

5.3.0

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
When building for AARCH64 the unit test //tensorflow/tsl/framework/convolution:spatial_convolutions_test fails to build with

./tensorflow/tsl/framework/convolution/eigen_spatial_convolutions-inl.h:1490:27: error: static assertion failed: YOU_MADE_A_PROGRAMMING_MISTAKE
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=mkl_aarch64_threadpool --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/tsl/framework/convolution:spatial_convolutions_test
```


### Relevant log output

```shell
ERROR: /home/andrew/src/tensorflow/tensorflow/tsl/framework/convolution/BUILD:99:12: Compiling tensorflow/tsl/framework/convolution/eigen_spatial_convolutions_test.cc failed: (Exit 1): gcc failed: error executing command 
  (cd /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib:/opt/rh/gcc-toolset-10/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib \
    PATH=/home/andrew/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/andrew/.local/bin:/home/andrew/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/andrew/src/venv38/bin/python3 \
    PYTHON_LIB_PATH=/home/andrew/src/venv38/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /opt/rh/gcc-toolset-10/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/aarch64-opt/bin/tensorflow/tsl/framework/convolution/_objs/spatial_convolutions_test/eigen_spatial_convolutions_test.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/tsl/framework/convolution/_objs/spatial_convolutions_test/eigen_spatial_convolutions_test.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DGEMM_KERNEL_H '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_NEON_GEBP_NR=4' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/com_google_googletest -iquote bazel-out/aarch64-opt/bin/external/com_google_googletest -iquote external/com_google_benchmark -iquote bazel-out/aarch64-opt/bin/external/com_google_benchmark -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -Ibazel-out/aarch64-opt/bin/external/com_google_benchmark/_virtual_includes/benchmark -isystem third_party/eigen3/mkl_include -isystem bazel-out/aarch64-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/com_google_googletest/googlemock -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googlemock -isystem external/com_google_googletest/googlemock/include -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googlemock/include -isystem external/com_google_googletest/googletest -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googletest -isystem external/com_google_googletest/googletest/include -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googletest/include -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS '-mtune=generic' '-march=armv8-a' -O3 '-std=c++17' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/tsl/framework/convolution/eigen_spatial_convolutions_test.cc -o bazel-out/aarch64-opt/bin/tensorflow/tsl/framework/convolution/_objs/spatial_convolutions_test/eigen_spatial_convolutions_test.o)
# Configuration: 67e3477bbfd3aa6df692c90e4aaaf7a6ee0f55b121a5556fe852592ce2c633e2
# Execution platform: @local_execution_config_platform//:platform
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:162,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/tsl/framework/convolution/eigen_spatial_convolutions.h:19,
                 from tensorflow/tsl/framework/convolution/eigen_spatial_convolutions_test.cc:16:
./tensorflow/tsl/framework/convolution/eigen_spatial_convolutions-inl.h: In instantiation of 'struct Eigen::internal::gemm_pack_rhs<Eigen::QInt8, long int, Eigen::internal::TensorContractionSubMapper<Eigen::QInt8, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, Eigen::TensorMap<Eigen::Tensor<Eigen::QInt8, 4, 0, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, std::array<long int, 1>, std::array<long int, 1>, 1, true, false, 0, Eigen::MakePointer>, 1, 0, false, false>':
tensorflow/tsl/framework/convolution/eigen_spatial_convolutions_test.cc:924:15:   required from 'void Eigen::PackRhsHelper(benchmark::State&, int, int, int, int, int, int, int, Eigen::PaddingType, int, int, int, int, Eigen::Index, Eigen::Index) [with T = Eigen::QInt8; Eigen::Index = long int]'
tensorflow/tsl/framework/convolution/eigen_spatial_convolutions_test.cc:1375:1:   required from here
./tensorflow/tsl/framework/convolution/eigen_spatial_convolutions-inl.h:1490:27: error: static assertion failed: YOU_MADE_A_PROGRAMMING_MISTAKE
 1490 |   EIGEN_STATIC_ASSERT((nr == 4), YOU_MADE_A_PROGRAMMING_MISTAKE)
      |                       ~~~~^~~~~
Target //tensorflow/tsl/framework/convolution:spatial_convolutions_test failed to build
INFO: Elapsed time: 9.629s, Critical Path: 9.11s
INFO: 3 processes: 2 internal, 1 local.
FAILED: Build did NOT complete successfully
//tensorflow/tsl/framework/convolution:spatial_convolutions_test FAILED TO BUILD

FAILED: Build did NOT complete successfully
```
</details>"
59679,"pruned pretrained mobilenet_v1_1.0_224, mode.fit() error","### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.11.0

### Custom Code

No

### OS Platform and Distribution

WSL2 Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

python 3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I wanted to prun pretrained mobilenet_v1 with model_weight mobilenet_1_0_224. After I pruned the pretrained model and start to retrain pruned_model, something was wrong.
```


### Standalone code to reproduce the issue

```shell
This is code

from matplotlib import pyplot as plt
import tensorflow_model_optimization as tfmot
import numpy as np
import os
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds
import tensorflow_model_optimization as tfmot
import tempfile

_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_path = os.path.join(PATH, 'train')
validation_path = os.path.join(PATH, 'validation')

IMG_SIZE = (224, 224)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_path, shuffle=True, image_size=IMG_SIZE)
validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_path, shuffle=True, image_size=IMG_SIZE)
 
val_batches = tf.data.experimental.cardinality(validation_dataset) # check data number of one batch
test_dataset = validation_dataset.take(val_batches // 5) 
val_dataset = validation_dataset.skip(val_batches // 5)

num_images = tf.data.experimental.cardinality(test_dataset)
end_step = np.ceil(num_images / 32).astype(np.int32) * 2

# load base model and freeze the base model
model = tf.keras.applications.MobileNet()
model.load_weights(r'/home/user/.keras/models/mobilenet_1_0_224_tf.h5')

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(test_dataset, epochs=2, validation_data=val_dataset)

_, keras_file = tempfile.mkstemp('.h5')
tf.keras.models.save_model(model, keras_file, include_optimizer=False)
print('Saved baseline model to:', keras_file)
model.save(""mobilenet_no_pruning.h5"")
# loss, accuracy = model.evaluate(validation_dataset)              
# # model_for_pruning.summary() # trainable parameters + non-trainable mask parameters
# print(""initial loss: {:.2f}"".format(loss))
# print(""initial accuracy: {:.2f}"".format(accuracy))

# model applied pruning  
pruning_schedule = {
      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(0.5,0.9,begin_step=0, end_step=end_step)
}        
model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)

# create a new model
model_for_pruning.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

logdir = tempfile.mkdtemp()
callbacks = [
  tfmot.sparsity.keras.UpdatePruningStep(), # pruning parameters
  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),
]
print(""begin fit"")
# model_for_pruning.fit(test_dataset, batch_size=32, epochs=2, validation_data=val_dataset, callbacks=callbacks)
model_for_pruning.fit(test_dataset, batch_size=32, epochs=2, validation_data=val_dataset)
loss, accuracy = model_for_pruning.evaluate(validation_dataset)              
# # model_for_pruning.summary() # trainable parameters + non-trainable mask parameters
print(""initial loss: {:.2f}"".format(loss))
print(""initial accuracy: {:.2f}"".format(accuracy))
```
```


### Relevant log output

The pretrained mode traing is as usual. The errors happens retrained pruned model. The errors are 

```shell
Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
Epoch 1/2
/usr/local/lib/python3.8/dist-packages/keras/backend.py:5585: UserWarning: ""`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?
  output, from_logits = _get_logits(
6/6 [==============================] - 26s 4s/step - loss: 4.2002 - accuracy: 0.5365 - val_loss: 28.3055 - val_accuracy: 0.0000e+00
Epoch 2/2
6/6 [==============================] - 19s 3s/step - loss: 0.3738 - accuracy: 0.9479 - val_loss: 21.0645 - val_accuracy: 0.0223
Saved baseline model to: /tmp/tmp68woeyxc.h5
WARNING:tensorflow:From /home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
begin fit
Epoch 1/2
Traceback (most recent call last):
  File ""fine-tune_mobilenet_v1_with_pruning.py"", line 85, in <module>
    model_for_pruning.fit(test_dataset, batch_size=32, epochs=2, validation_data=val_dataset)
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/__autograph_generated_filefab8qmeh.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
  File ""/tmp/__autograph_generated_filerwugbiee.py"", line 71, in tf__call
    update_mask = ag__.converted_call(ag__.ld(utils).smart_cond, (ag__.ld(training), ag__.ld(add_update), ag__.ld(no_op)), None, fscope)
  File ""/tmp/__autograph_generated_filepmfh1utc.py"", line 37, in tf__smart_cond
    ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(pred), ag__.ld(variables).Variable), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
  File ""/tmp/__autograph_generated_filepmfh1utc.py"", line 33, in else_body
    retval_ = ag__.converted_call(ag__.ld(smart_module).smart_cond, (ag__.ld(pred),), dict(true_fn=ag__.ld(true_fn), false_fn=ag__.ld(false_fn), name=ag__.ld(name)), fscope)
  File ""/tmp/__autograph_generated_filerwugbiee.py"", line 48, in add_update
    with ag__.ld(tf).control_dependencies([ag__.ld(self).pruning_obj.conditional_mask_update()]):
  File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 310, in conditional_mask_update
    return tf.distribute.get_replica_context().merge_call(
  File ""/tmp/__autograph_generated_filepd0d14ah.py"", line 63, in tf__mask_update_distributed
    retval_ = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(maybe_update_masks), (), None, fscope), ag__.ld(update_distributed), ag__.ld(no_update)), None, fscope)
  File ""/tmp/__autograph_generated_file1681ktv2.py"", line 37, in tf__maybe_update_masks
    ag__.if_stmt(ag__.ld(self)._sparsity_m_by_n, if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
  File ""/tmp/__autograph_generated_file1681ktv2.py"", line 33, in else_body
    retval_ = ag__.converted_call(ag__.ld(self)._pruning_schedule, (ag__.converted_call(ag__.ld(self)._step_fn, (), None, fscope),), None, fscope)[0]
TypeError: in user code:

    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1249, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1233, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1222, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1023, in train_step
        y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""/tmp/__autograph_generated_filerwugbiee.py"", line 71, in tf__call
        update_mask = ag__.converted_call(ag__.ld(utils).smart_cond, (ag__.ld(training), ag__.ld(add_update), ag__.ld(no_op)), None, fscope)
    File ""/tmp/__autograph_generated_filepmfh1utc.py"", line 37, in tf__smart_cond
        ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(pred), ag__.ld(variables).Variable), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
    File ""/tmp/__autograph_generated_filepmfh1utc.py"", line 33, in else_body
        retval_ = ag__.converted_call(ag__.ld(smart_module).smart_cond, (ag__.ld(pred),), dict(true_fn=ag__.ld(true_fn), false_fn=ag__.ld(false_fn), name=ag__.ld(name)), fscope)
    File ""/tmp/__autograph_generated_filerwugbiee.py"", line 48, in add_update
        with ag__.ld(tf).control_dependencies([ag__.ld(self).pruning_obj.conditional_mask_update()]):
    File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 310, in conditional_mask_update
        return tf.distribute.get_replica_context().merge_call(
    File ""/tmp/__autograph_generated_filepd0d14ah.py"", line 63, in tf__mask_update_distributed
        retval_ = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(maybe_update_masks), (), None, fscope), ag__.ld(update_distributed), ag__.ld(no_update)), None, fscope)
    File ""/tmp/__autograph_generated_file1681ktv2.py"", line 37, in tf__maybe_update_masks
        ag__.if_stmt(ag__.ld(self)._sparsity_m_by_n, if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
    File ""/tmp/__autograph_generated_file1681ktv2.py"", line 33, in else_body
        retval_ = ag__.converted_call(ag__.ld(self)._pruning_schedule, (ag__.converted_call(ag__.ld(self)._step_fn, (), None, fscope),), None, fscope)[0]

    TypeError: Exception encountered when calling layer 'prune_low_magnitude_conv1' (type PruneLowMagnitude).
    
    in user code:
    
        File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 268, in add_update  *
            with tf.control_dependencies(
        File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/keras/utils.py"", line 54, in smart_cond  *
            pred, true_fn=true_fn, false_fn=false_fn, name=name)
        File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 307, in mask_update_distributed  *
            return tf.cond(maybe_update_masks(), update_distributed, no_update)
        File ""/home/qiao_sh_pudong/.local/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 264, in maybe_update_masks  *
            return self._pruning_schedule(self._step_fn())[0]
    
        TypeError: '_DictWrapper' object is not callable
    
    
    Call arguments received by layer 'prune_low_magnitude_conv1' (type PruneLowMagnitude):
      • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)
      • training=True
      • kwargs=<class 'inspect._empty'>
```
"
59678,Stateless dropout broken on mixed_float16 with XLA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230213

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Stateless/stateful (not legacy) dropout fails with `mixed_float16` and XLA, because the stateless-uniform-random operation does not exist for half types.

Note that without Stateless/stateful (not legacy) dropout, a warning is generated saying that seeds are not respected.

The error reproduces consistently on Colab, with TF 2.11 and the latest nightly.

### Standalone code to reproduce the issue

```py
import tensorflow as tf
tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.backend.experimental.enable_tf_random_generator()

dropout = tf.keras.layers.Dropout(rate=0.5)

@tf.function(jit_compile=True)
def run(x):
    return dropout(x, training=True)

print(run(tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)))
```

https://colab.research.google.com/drive/1F9V0ayyNIpbIQcjjtk7IN6w3EQai5Fk9?usp=sharing

### Relevant log output

```shell
nvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_run_448[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_CPU_JIT: StatelessRandomUniformV2 (No registered 'StatelessRandomUniformV2' OpKernel for XLA_CPU_JIT devices compatible with node {{node dropout_3/stateless_dropout/stateless_random_uniform/StatelessRandomUniformV2}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: Tshape=DT_INT32, dtype=DT_HALF){{node dropout_3/stateless_dropout/stateless_random_uniform/StatelessRandomUniformV2}}
The op is created at: 
File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
  return _run_code(code, main_globals, None,
File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
  exec(code, run_globals)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
  app.launch_new_instance()
File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
  app.start()
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
  self.io_loop.start()
File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
  self.asyncio_loop.run_forever()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
  self._run_once()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
  handle._run()
File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
  self._context.run(self._callback, *self._args)
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
  lambda f: self._run_callback(functools.partial(callback, future))
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
  ret = callback()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
  self.run()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
  yield gen.maybe_future(dispatch(*args))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
  yield gen.maybe_future(handler(stream, idents, msg))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
  self.do_execute(
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
  res = shell.run_cell(code, store_history=store_history, silent=silent)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
  return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
  result = self._run_cell(
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
  return runner(coro)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
  coro.send(None)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
  if (await self.run_code(code, result,  async_=asy)):
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
  exec(code_obj, self.user_global_ns, self.user_ns)
File ""<ipython-input-14-7ae9614dfdd0>"", line 11, in <module>
  print(run(tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)))
File ""<ipython-input-11-7ae9614dfdd0>"", line 9, in run
  return dropout(x, training=True)
File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
  return fn(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1132, in __call__
  _name_scope_unnester(self._name_scope_on_declaration)
File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
  return fn(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/keras/layers/regularization/dropout.py"", line 120, in call
  output = control_flow_util.smart_cond(
File ""/usr/local/lib/python3.8/dist-packages/keras/utils/control_flow_util.py"", line 108, in smart_cond
  return tf.__internal__.smart_cond.smart_cond(
File ""/usr/local/lib/python3.8/dist-packages/keras/layers/regularization/dropout.py"", line 116, in dropped_inputs
  return self._random_generator.dropout(
File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 2156, in dropout
  if self._rng_type == self.RNG_STATEFUL: [Op:__inference_run_448]
```
</details>"
59677,Super Slow Performance (tf.function fails) of GradientTape with LSTM and Manual Training Loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

TF 2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0 (V12.0.140)

### GPU model and memory

2* Nvidia RTX A6000 48 GB

### Current Behaviour?

```shell
Hello all, 
For a specific problem, I need a manual training loop that includes a intermediate function. This function gets the model jacobian (output w.r.t to model's weights) along with other variables to generate the gradients and the loss function. I tried to calculate the model's Jacobian with GradientTape inside a function wrapped by tf.function. However, I get the error that tf.function fails. I saw similar issues dating back to 2020, but they were solved back then. I would be grateful to know your thoughts as the performance of the current code is super slow (~13 seconds for each step).
P.S. I cannot wrap the whole train_step function below with tf.function as the intermediate functions use Python dicts and Numpy arrays (the algorithm is a recursive algorithm if it matters).
Many thanks in advance!
```


### Standalone code to reproduce the issue

```shell
#fn to calculate the model jacobian
@tf.function
def jacobian_calc(model, inputs):
    with tf.GradientTape() as tape:
        tape.watch(model.trainable_weights)
        predictions = model(inputs, training=True)
    model_jacobian = tape.jacobian(predictions,model.trainable_weights)
    return model_jacobian, predictions

def train_step(model, inputs, targets, acc):
    st = time.time()
    model_jacobian, predictions = jacobian_calc(model,inputs)
    et = time.time()
    print(f""Elapsed time is {et-st}"")
    dLoss = []
    for jacobian in model_jacobian:
        p_loss, p_dLoss = loss_grads(targets, predictions, acc, jacobian)
        p_dLoss = tf.convert_to_tensor(p_dLoss, dtype=tf.float32)
        print(type(p_dLoss))
        print(type(model.trainable_weights[0]))
        dLoss.append(p_dLoss/K.eval(inputs).size)
    Loss = p_loss/K.eval(inputs).size
    opt.apply_gradients(zip(dLoss, model.trainable_weights))
    logs = {}
    print(f"" Loosssss is = {Loss}"")
    logs[""loss""] = Loss
    return logs

# load the samples
with open(seq_path+'/real_1000_300.pkl','rb') as dumper:
    samples_real = pickle.load(dumper)
with open(seq_path+'/model_1000_300.pkl','rb') as dumper:
    samples_model = pickle.load(dumper)
with open(seq_path+'/acc_1000_300.pkl','rb') as dumper:
    samples_acc = pickle.load(dumper)

# create training and testing sets
train_x = samples_model[:val_index_opt, :, 0]    #Umodel
train_y = samples_real[:val_index_opt, :, 0]     #Ureal
train_acc = samples_acc[:val_index_opt, :, 0]    #acc
val_x = samples_model[val_index_opt:, :, 0]     #Umodel
val_y = samples_real[val_index_opt:, :, 0]      #Ureal
val_acc = samples_acc[val_index_opt:, :, 0]     #acc

# no data scaling as I need to figure this out later. A dirty fix for now! Sorry:)
train_scaled_x = train_x.reshape(train_x.shape[0],train_x.shape[1],-1)
train_scaled_y = train_y.reshape(train_y.shape[0],train_y.shape[1],-1)
train_scaled_acc = train_acc.reshape(train_acc.shape[0],train_acc.shape[1],-1)
val_scaled_x = val_x.reshape(val_x.shape[0],val_x.shape[1],-1)
val_scaled_y = val_y.reshape(val_y.shape[0],val_y.shape[1],-1)
val_scaled_acc = val_acc.reshape(val_acc.shape[0],val_acc.shape[1],-1)

## Creating the RNN
model = Sequential()
                        
model.add(LSTM(32, input_shape=(None, train_scaled_x.shape[2]), mreturn_sequences=True))
                        model.add(Dense(32, input_shape=(None, train_scaled_x.shape[2])))
model.add(Dense(dense_1, activation='relu'))
model.add(Dense(1, activation='linear'))

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=0.001)

#Train model (only presented the training and not the validation here
training_set = tf.data.Dataset.from_tensor_slices((train_scaled_x,train_scaled_acc,train_scaled_y))
training_set = training_set.batch(batch)
history = {""loss"": []}
for ee in range(1,epoch+1):
    for inputs_batch, acc_batch, targets_batch in training_set:
        logs = train_step(model, inputs_batch,targets_batch,acc_batch)
    print(f""Results at the end of epoch {epoch}"")
    for key, value in logs.items(): #have some other metrics that I omitted here
        print(f""...{key}: {value:.4f}"")
    history[""loss""].append(logs[""loss""])
```


### Relevant log output

```shell
2023-02-13 21:51:20.518488: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-13 21:51:21.105332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/spack/v0.17.0/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/slurm-21-08-1-1-ctwolps4xy7mz7h3hooji4t72xs5vsyz/lib:/home/kshamsaei/miniconda3/envs/tf/lib/
2023-02-13 21:51:21.105406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/spack/v0.17.0/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/slurm-21-08-1-1-ctwolps4xy7mz7h3hooji4t72xs5vsyz/lib:/home/kshamsaei/miniconda3/envs/tf/lib/
2023-02-13 21:51:21.105417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
HELLO
/home/kshamsaei/sdof/sdof-300-1000
2023-02-13 21:51:21.662319: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-13 21:51:22.347877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46671 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:21:00.0, compute capability: 8.6
2023-02-13 21:51:22.348397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46685 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6
WARNING:tensorflow:From /home/kshamsaei/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
2023-02-13 21:51:25.417706: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_746 was passed int32 from sequential/lstm/PartitionedCall:20 incompatible with expected variant.
2023-02-13 21:51:25.926875: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_746 was passed int32 from sequential/lstm/PartitionedCall:20 incompatible with expected variant.
2023-02-13 21:51:26.001961: W tensorflow/core/common_runtime/process_function_library_runtime.cc:915] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/while_grad/while_grad/pfor/PartitionedCall/while/enter/_728 was passed float from sequential/lstm/PartitionedCall:11 incompatible with expected int32.
2023-02-13 21:51:31.457799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
```
</details>"
59676,tensorflow js 4.2.0 version with Vue3 about version dependency problem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

4.2.0

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

_No response_

### Python version

2.7 and 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

4G

### Current Behaviour?

```shell
Uncaught (in promise) TypeError: (0 , _environment__WEBPACK_IMPORTED_MODULE_6__.env)(...).platform.isTypedArray is not a function
    at isTypedArray (util.js:194:71)
    at inferShape (tensor_util_env.js:38:58)
    at Module.tensor (tensor.js:193:85)
    at tfex (tfjs.js:27:66)
    at Proxy.get (index.js??clonedRuleSet-40.use[0]!./node_modules/vue-loader/dist/index.js??ruleSet[0].use[0]!./src/components/tf1.vue?vue&type=script&lang=js:33:60)
    at onClick._cache.<computed>._cache.<computed> (index.js??clonedRuleSet-40.use[0]!./node_modules/vue-loader/dist/templateLoader.js??ruleSet[1].rules[3]!./node_modules/vue-loader/dist/index.js??ruleSet[0].use[0]!./src/components/tf1.vue?vue&type=template&id=4498369f:21:59)
    at callWithErrorHandling (runtime-core.esm-bundler.js:290:18)
    at callWithAsyncErrorHandling (runtime-core.esm-bundler.js:298:17)
    at HTMLButtonElement.invoker (runtime-dom.esm-bundler.js:473:82)
```


### Standalone code to reproduce the issue

```shell
In version 4.2.0 it doesn't run when importing to vue3.

after suffering
I found that downgrading the version fixed the problem.
4.1.0 and below no problem.
```


### Relevant log output

_No response_</details>"
59671,CUDA_ERROR_ILLEGAL_ADDRESS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

Official docker image via apptainer

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

Tesla V100-SXM2-32GB

### Current Behaviour?

Using the Official docker image, the provided code results in a CUDA_ERROR_ILLEGAL_ADDRESS. Note, that while the error is fully consistent (happens every time at the first epoch) it is highly sensitive. If a different seed or model-size is used, the error will not reproduce.

I have tried enabling memory growth as suggested elsewhere. However, this only delays the issue to epoch 18.

The error does not reproduce on Google Colab or when using the nightly docker image. However, due to the sensitivity of the issue, it is unclear what exactly this means. The Google Colab issue could be due to different GPU. On my own environment it's a Tesla V100-SXM2-32GB, not a T4.

### Standalone code to reproduce the issue

This uses `transformers == 4.26.0`, and a synthetic dataset file (based on QQP) that is automatically downloaded. Code is available here: https://gist.github.com/AndreasMadsen/2bf669a3cd4c4a8ba964561b9e72279e

Note, I did make an attempt at reproducing it using Colab: https://colab.research.google.com/drive/1zVbNNfz1lZ6xgyoZ7dKWnKzj6poM5XWC?usp=sharing

However, the error does not reproduce on Colab.

### Relevant log output

```shell
2023-02-06 14:07:37.519351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-06 14:07:37.759155: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-06 14:07:38.624178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-02-06 14:07:38.624677: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-02-06 14:07:38.624725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Configuration:
  persistent_dir = /scratch/anmadc/tf_memory_issue
  seed = 0
  batch_size = 16
  model = roberta-base
  jit_compile = True
  precision = mixed_float16

2023-02-06 14:07:40.255950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-06 14:07:41.029530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0
All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.

Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
WARNING:tensorflow:From /localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
2023-02-06 14:08:24.250220: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x2db1cb30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-06 14:08:24.264540: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2023-02-06 14:08:25.275879: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator tf_roberta_for_sequence_classification/roberta/embeddings/assert_less/Assert/Assert
2023-02-06 14:08:25.309985: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-06 14:08:25.321306: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. tf_roberta_for_sequence_classification/roberta/embeddings/dropout_36/dropout/random_uniform/RandomUniform
2023-02-06 14:09:41.534987: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'sort_19'
ptxas warning : Registers are spilled to local memory in function 'sort_17'

2023-02-06 14:09:41.931415: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-06 14:09:42.331286: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1159] failed to enqueue async memcpy from device to host: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; host dst: 0x2b36f334aeeb; GPU src: 0x2b385f628600; size: 1=0x1
2023-02-06 14:09:42.331472: I tensorflow/compiler/xla/stream_executor/stream.cc:2485] INTERNAL: Unknown error
2023-02-06 14:09:42.331557: I tensorflow/compiler/xla/stream_executor/stream.cc:2489] [stream=0x1b700120,impl=0x52a9880] INTERNAL: stream did not block host until done; was already in an error state
Traceback (most recent call last):
  File ""/scratch/anmadc/workspace/economical-roar/reproduce.py"", line 150, in <module>
    model.fit(dataset_train_batched, verbose=1, epochs=1)
  File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/scratch/anmadc/workspace/economical-roar/reproduce.py"", line 150, in <module>
      model.fit(dataset_train_batched, verbose=1, epochs=1)
    File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/localscratch/anmadc.58777029.0/env/lib/python3.10/site-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
Node: 'StatefulPartitionedCall'
Failed to retrieve branch_index value on stream 0x1b700120: stream did not block host until done; was already in an error state.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_42578]
```
</details>"
59669,"ERROR: no such package '@rules_pkg//': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz, https://mirror.bazel.build/github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz] to /private/var/tmp/_bazel_ysong2/b326506f60d8e1a7b80e7181271420e8/external/rules_pkg/temp3975631198066234777/rules_pkg-0.2.5.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tag: v2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Montery 12.6.1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.6

### GCC/Compiler version

clang 14.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
ERROR: no such package '@rules_pkg//': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz, https://mirror.bazel.build/github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz] to /private/var/tmp/_bazel_ysong2/b326506f60d8e1a7b80e7181271420e8/external/rules_pkg/temp3975631198066234777/rules_pkg-0.2.5.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target

tried add certificates into keystore but doesn't work.
```


### Standalone code to reproduce the issue

```shell
ERROR: no such package '@rules_pkg//': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz, https://mirror.bazel.build/github.com/bazelbuild/rules_pkg/releases/download/0.2.5/rules_pkg-0.2.5.tar.gz] to /private/var/tmp/_bazel_ysong2/b326506f60d8e1a7b80e7181271420e8/external/rules_pkg/temp3975631198066234777/rules_pkg-0.2.5.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
```


### Relevant log output

_No response_</details>"
59667,ValueError: Did not get operators or tensors in subgraph 1,"Hey,
I am trying to evaluate a tflite model, but all my attempts have been unsuccessful. I have provided a full code [here](https://colab.research.google.com/drive/14zMCm0XaszVrUoWd9bMvBjSXSdPvSWuw#scrollTo=huwb9Wiufr9e). Thank you very much!"
59666,flatc Errno 8 Exec format error when building for android,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Debian 11 

### Mobile device

Android 

### Python version

3.9.2

### Bazel version

6.0.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

nvidia 

### Current Behaviour?

```shell
I have a c++ code base that can be used on different applications with different UI's. Currently I've run the Build with CMake example on the documentation and it worked on my linux debian machine. However I want to build tflite for android devices too so I tried the ""Specifics of Android cross-compilation"" part with NDK 25.0.8775105 as my path provided. It eventually did finish the cmake run.
```


### Standalone code to reproduce the issue

```shell
After that I've tried running `cmake --build .` which caused me to get an error regarding `OSError: [Errno 8] Exec format error:` I can see that it's trying to find something like `#!/bin/sh` but I cannot add it since flatc is produced and deleted during the cmake run.
```


### Relevant log output

```shell
[ 21%] Built target flathash
[ 21%] Linking CXX executable flatc
Running scripts/generate_code.py...
Traceback (most recent call last):
  File ""/home/tb/Desktop/tensorflow/tflite_android_build/flatbuffers/scripts/generate_code.py"", line 148, in <module>
    flatc(
  File ""/home/tb/Desktop/tensorflow/tflite_android_build/flatbuffers/scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.9/subprocess.py"", line 505, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.9/subprocess.py"", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/usr/lib/python3.9/subprocess.py"", line 1823, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/tb/Desktop/tensorflow/tflite_android_build/_deps/flatbuffers-build/flatc'
gmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:540: _deps/flatbuffers-build/flatc] Error 1
gmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'
gmake[1]: *** [CMakeFiles/Makefile2:4987: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2
gmake: *** [Makefile:149: all] Error 2
```
</details>"
59665,Cannot import tensorflow after latest tensorboard release: TypeError: Descriptors cannot not be created directly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.3.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Since tensorboard-2.12.0 release, we cannot install and then import tensorflow-2.3.0.
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.3.0
python -c ""import tensorflow""
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File "".../venv-py38-tf/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "".../venv-py38-tf/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 560, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>"
59663,zsh: illegal hardware instruction Run on MAC M2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

13.1 (22C65)

### Mobile device

_No response_

### Python version

Python 3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! Help me pleaseee
tensorboard                  2.11.2
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow                   2.11.0
tensorflow-estimator         2.11.0
tensorflow-io-gcs-filesystem 0.29.0
```


### Standalone code to reproduce the issue

```shell
zsh: illegal hardware instruction
```


### Relevant log output

_No response_</details>"
59662,"The structure of the model generated is incorrect, the input tensors of the concatenate operator have changed from 4 different tensors to 3 different tensors plus one duplicated tensor","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windos10
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): version 2.9.1 

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

This is the file weblink https://colab.research.google.com/gist/Sun2018421/73059bdd906fd84313f3dd7714cbcc7e/tflite.ipynb

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:
It seems like there is an error in the structure of the model generated. The original structure of the concatenate operator with 4 different inputs has become 3 different inputs and 1 repeated input.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
Not a RNN
### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59655,Tensorboard shows huge idle time and no blank spaces on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf. 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

Linux Ubuntu 20.04

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3

### GPU model and memory

Nvidia GeForce RTX 3090

### Current Behaviour?

```shell
I'm new to TF profiler and tensorboard. Could you please tell me how GPU idle time is calculated? I see contradictory data in profiling information on tensorboard and no information about it in docs:

 - I have no blank spaces in events trace (GPU compute stream) presented on `trace_viewer` page.
 - Wherein device (GPU) idle time shown on `tensorflow_stats` page is about 50%???

It's what I see when I use small batch size. After increasing batch size, same situation is on `trace_viewer` page, but GPU idle time decreased up to 7%. So, how is idle time calculated?

By the way, another two questions:
1. What is `steps` on `trace_viewer` page?
2. Why does `trace_viewer` page contain only a part of profiling time? I profiled about 100 seconds of execution, but I see only 10 seconds on `trace_viewer` page.


Thanks a lot in advance!
```


### Standalone code to reproduce the issue

```shell
I've used next setup:
tf.profiler.experimental.start
<training>
tf.profiler.experimental.end
```


### Relevant log output

_No response_</details>"
59654,Tf 2.11 Optimizers. Does anyone have any custom optimizer for the new version?,"I know that we can use tf.keras.optimizers.legacy.Optimizer for making the older custom optimizers to work,but I'm wonder how I can update my code.This the original code that I want to make it function for tf 2.11

`class Gravity(tf.keras.optimizers.Optimizer):
    def __init__(self,
                 learning_rate=0.1,
                 alpha=0.01,
                 beta=0.9,
                 name=""Gravity"",
                 **kwargs):
        super(Gravity, self).__init__(name, **kwargs)
        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
        self._set_hyper('decay', self._initial_decay)
        self._set_hyper('alpha', alpha)
        self._set_hyper('beta', beta)
        self.epsilon = 1e-7

    def _create_slots(self, var_list):
        alpha = self._get_hyper(""alpha"")
        stddev = alpha / self.learning_rate
        initializer = tf.keras.initializers.RandomNormal(mean=0.0,
                                                         stddev=stddev,
                                                         seed=None)
        for var in var_list:
            self.add_slot(var, ""velocity"", initializer=initializer)

    @tf.function
    def _resource_apply_dense(self, grad, var):
        # Get Data
        var_dtype = var.dtype.base_dtype
        lr_t = self._decayed_lr(var_dtype)
        beta = self._get_hyper(""beta"", var_dtype)
        t = tf.cast(self.iterations, float)
        beta_hat = (beta * t + 1) / (t + 2)
        velocity = self.get_slot(var, ""velocity"")

        # Calculations
        max_step_grad = 1 / tf.math.reduce_max(tf.math.abs(grad))
        gradient_term = grad / (1 + (grad / max_step_grad)**2)

        # update variables
        updated_velocity = velocity.assign(beta_hat * velocity +
                                           (1 - beta_hat) * gradient_term)
        updated_var = var.assign(var - lr_t * updated_velocity)

        # updates = [updated_var, updated_velocity]
        # return tf.group(*updates)
    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError

    def get_config(self):
        config = super(Gravity, self).get_config()
        config.update({
            'learning_rate':
            self._serialize_hyperparameter('learning_rate'),
            'decay':
            self._serialize_hyperparameter('decay'),
            'alpha':
            self._serialize_hyperparameter('alpha'),
            'beta':
            self._serialize_hyperparameter('beta'),
            'epsilon':
            self.epsilon,
        })
        return config`
"
59652,tape.gradient computes none after model in model training,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
grads = tape.gradient(class_channel, last_conv_layer_output) does produce NONE after performing first a regression on a ""inner"" model (trained and tested with MNIST and CIFAR10), before freezing the ""inner"" model network and then training a ""outer"" model over the ""inner"" model. After even unfreezing the ""inner"" model tape.gradient does not produce a result when asking for the final layer of the ""inner"" model as used in Grad-CAM.
```


### Standalone code to reproduce the issue

```shell
The minimal code is in the link. Everything can be run, till the final cell where the bug is.

https://colab.research.google.com/drive/1HhFORTW3kQnsr0HMrk6Kl9brAguPvf_t?usp=sharing
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
<ipython-input-12-17e9c91a6483> in <module>
      4 print(class_channel.shape)
      5 grads = tape.gradient(class_channel, last_conv_layer_output)
----> 6 print(grads.shape)

AttributeError: 'NoneType' object has no attribute 'shape'
```
</details>"
59651,Feature Request: More descriptive get_weights() method,"### Issue Type

Feature Request

### Tensorflow Version

tf 2.11.0

### OS Platform and Distribution

Windows 11

### Python version

3.10.9


Hello!
Currently, I can get the weights of each layer of a Keras model by calling TensorFlow's `get_weights()` method. Additionally, this method returns the bias of the layer if the `use_bias` term is true. There is no mention of the order in which weights and biases are returned in TensorFlow or Keras documentation. When the method returns two or more NumPy arrays, how can we determine what these layers represent?

It would be awesome if the `get_weights()` method could also indicate whether a particular NumPy array is a weights array, bias array, etc.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def get_model():
    mnist_model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.45),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.45),
    tf.keras.layers.Dense(10)])
    return mnist_model


def get_model_state_dict(model):
    for index, layer in enumerate(model.layers):
        print(model.layers[index].get_weights())

state_dict = (get_model_state_dict(get_model()))
```


"
59647,The gradient of tf.math.digamma is NaN on GPU and 0.0 on CPU when the input is inf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The gradient of tf.math.digamma is NaN when receiving inf as input. This issue only happens on GPU mode, when I run the program on CPU, the gradient is 0.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np


x = tf.Variable(np.inf, dtype=""float32"")

with tf.GradientTape() as g0:
    g0.watch(x)
    res = tf.math.digamma(x)
tf_grad = g0.gradient(res, x)
print(""The gradient of digamma is: "", tf_grad)  # The gradient of digamma is:  tf.Tensor(nan, shape=(), dtype=float32)
```


### Relevant log output

```shell
The gradient of digamma is:  tf.Tensor(nan, shape=(), dtype=float32)
```
</details>"
59643,protobuf 4 causes segmentation fault on Python 3.8 in unit test,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

10.3.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Unit test //tensorflow/dtensor/python/tests:spmd_test_cpu fails when run with Python 3.8 and protobuf 4 is installed.
Installing protobuf 3.20.3 resolves the issue.
```


### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --test_env=TF_ENABLE_ONEDNN_OPTS=1 --cache_test_results=no --test_timeout=500,900,-1,-1 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --build_tests_only --jobs=100 -- //tensorflow/dtensor/python/tests:spmd_test_cpu
```


### Relevant log output

```shell
Fatal Python error: Segmentation fault

Current thread 0x0000ffffb7906370 (most recent call first):
  File ""/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 1108 in config
  File ""/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 568 in ensure_initialized
  File ""/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 1401 in remove_function
  File ""/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 2739 in remove_function
  File ""/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 172 in __del__
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
/home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/dtensor/python/tests/spmd_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_aarch64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2(+0x15ae14c)[0xffff145de14c]
linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0xffffb78b07a0]
/lib64/libpthread.so.0(raise+0xac)[0xffffb71b2af4]
linux-vdso.so.1(__kernel_rt_sigreturn+0x0)[0xffffb78b07a0]
/lib64/libpython3.8.so.1.0(PyModule_GetState+0x4)[0xffffb72f9a3c]
/home/andrew/src/venv38/lib64/python3.8/site-packages/google/_upb/_message.abi3.so(+0xa390)[0xffff1527a390]
/home/andrew/src/venv38/lib64/python3.8/site-packages/google/_upb/_message.abi3.so(+0x13c9c)[0xffff15283c9c]
/lib64/libpython3.8.so.1.0(_PyObject_MakeTpCall+0x1a8)[0xffffb72ed9c0]
/lib64/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0x53f4)[0xffffb73c1114]
/lib64/libpython3.8.so.1.0(_PyEval_EvalCodeWithName+0xc8c)[0xffffb7371fe4]
/lib64/libpython3.8.so.1.0(_PyFunction_Vectorcall+0x474)[0xffffb73734b4]
/lib64/libpython3.8.so.1.0(+0x12662c)[0xffffb734662c]
/lib64/libpython3.8.so.1.0(PyObject_GetAttr+0x27c)[0xffffb7361e4c]
/lib64/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0xa08)[0xffffb73bc728]
/lib64/libpython3.8.so.1.0(_PyFunction_Vectorcall+0x1d0)[0xffffb7373210]
/lib64/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0x884)[0xffffb73bc5a4]
/lib64/libpython3.8.so.1.0(_PyFunction_Vectorcall+0x1d0)[0xffffb7373210]
/lib64/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0x884)[0xffffb73bc5a4]
/lib64/libpython3.8.so.1.0(_PyFunction_Vectorcall+0x1d0)[0xffffb7373210]
/lib64/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0x4de8)[0xffffb73c0b08]
/lib64/libpython3.8.so.1.0(_PyFunction_Vectorcall+0x1d0)[0xffffb7373210]
/lib64/libpython3.8.so.1.0(+0x133640)[0xffffb7353640]
/lib64/libpython3.8.so.1.0(+0x1f7248)[0xffffb7417248]
/lib64/libpython3.8.so.1.0(+0xcc72c)[0xffffb72ec72c]
/lib64/libpython3.8.so.1.0(_PyGC_CollectNoFail+0x38)[0xffffb745f060]
/lib64/libpython3.8.so.1.0(PyImport_Cleanup+0x394)[0xffffb745f40c]
/lib64/libpython3.8.so.1.0(Py_FinalizeEx+0x6c)[0xffffb7462c34]
/lib64/libpython3.8.so.1.0(Py_Exit+0x14)[0xffffb72cb01c]
/lib64/libpython3.8.so.1.0(+0xab060)[0xffffb72cb060]
/lib64/libpython3.8.so.1.0(+0xab0b8)[0xffffb72cb0b8]
/lib64/libpython3.8.so.1.0(PyRun_SimpleFileExFlags+0x3c4)[0xffffb72cbac0]
/lib64/libpython3.8.so.1.0(Py_RunMain+0x2b8)[0xffffb74645d0]
/lib64/libpython3.8.so.1.0(Py_BytesMain+0x3c)[0xffffb7464d1c]
/lib64/libc.so.6(__libc_start_main+0xdc)[0xffffb6f14384]
/home/andrew/src/venv38/bin/python3(+0x928)[0xaaaab41c0928]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
        tsl::CurrentStackTrace[abi:cxx11]()

        __kernel_rt_sigreturn
        raise
        __kernel_rt_sigreturn
        PyModule_GetState


        _PyObject_MakeTpCall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall

        PyObject_GetAttr
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyFunction_Vectorcall



        _PyGC_CollectNoFail
        PyImport_Cleanup
        Py_FinalizeEx
        Py_Exit


        PyRun_SimpleFileExFlags
        Py_RunMain
        Py_BytesMain
        __libc_start_main

*** End stack trace ***
```
</details>"
59642,LookupError: gradient registry has no entry for: ClipByValue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230210

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Error occurred when mixed precision policy and `tf.saturate_cast` used together.
No error without mixed precision policy.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1hWTWBG7S8h6q4DK1qBNl2id0aK7h-3n4?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
StagingError                              Traceback (most recent call last)
<ipython-input-6-1edf848c14d2> in <module>
     12 model = models.Model(inputs=inputs, outputs=outputs)
     13 model.compile(optimizer='adam', loss='mse', run_eagerly=False, jit_compile=True)
---> 14 model.fit(images, labels, epochs=1, batch_size=2)

1 frames
/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1169           except Exception as e:  # pylint:disable=broad-except
   1170             if hasattr(e, ""ag_error_metadata""):
-> 1171               raise e.ag_error_metadata.to_exception(e)
   1172             else:
   1173               raise

StagingError: in user code:

    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1284, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1249, in run_step  *
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1054, in train_step  **
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py"", line 542, in minimize
        grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 1249, in compute_gradients
        grads_and_vars = self._optimizer.compute_gradients(
    File ""/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py"", line 275, in compute_gradients
        grads = tape.gradient(loss, var_list)

    LookupError: gradient registry has no entry for: ClipByValue
```
</details>"
59641,tf.data.Dataset.map() of tf.data.Dataset feat. tf.RaggedTensor causes broken for-loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Windows 10, WSL2 Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using a tf.data.Dataset for tf.RaggedTensors, the .map() functionality seems to break the ability to iterate over the datapoints via for-loop, causing an internal TypeError on the for-loop itself.
The expected behaviour would be for the program not to throw a TypeError as the same function works if used via a simple for-loop over the data instead of the .map() functionality.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
ragged_data=tf.ragged.constant([[[[1,2],[3,4],[5,6]], [[1,2],[3,4],[5,6]], [[1,2],[3,4],[5,6]]], [[[1,2],[3,4],[5,6]], [[1,2],[3,4],[5,6]], [[1,2],[3,4],[5,6]]]])
ds=tf.data.Dataset.from_tensor_slices(ragged_data)
def test_fn(ragged_data):
    for x in ragged_data:
        continue
    return ragged_data
ds.map(test_fn)
```


### Relevant log output

```shell
TypeError: Input 'y' of 'Less' Op has type int64 that does not match type int32 of argument 'x'.
```
</details>"
59637,How to use exported automl tensorflow model for tabular data in Python,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello,

I followed the instructions from Google Cloud to export and load a trainen AutoML model (training was done with data from BigQuery): https://cloud.google.com/vertex-ai/docs/export/export-model-tabular

As we want to integrate this model in our Spark flow I need to load it and make predictions on it in Python directly. I tested the model with the tensorflow serving docker image as described in the documentation. This works perfectly fine. However, when I try to use the model directly I get an erro ""Error: DATA_LOSS: Failed skipping unrequested field"". Input must be a string tensor. I tried several formats (JSON, CSV) but none of them seemed to be correct.

Can anybody give me a hint on how to convert the tabular data into a string tensor in a way that the model accepts the data and returns a prediction?

Thanks in advance!!
Kay
```


### Standalone code to reproduce the issue

```shell
import struct2tensor
import tensorflow as tf
import json
import requests


MODEL_PATH = '/home/kay/tmp/models/ml_cr_2022_12'

predict_sample = {'Animals': '0.0', 'Apparel': '0.0165423884627339', 'Arts': '0.000893240417492657', 'Baby': '0.0', 'Business': '0.0', 'Cameras': '0.000158694043359865', 'Electronics': '0.0', 'Food': '6.4749054130615e-05', 'Furniture': '0.0', 'Hardware': '0.000247099237409669', 'Health': '0.00928790329794604', 'Home': '0.0442892335578687', 'Luggage': '3.65980448833796e-05', 'Mature': '0.0', 'Media': '0.70324946112978', 'Office': '0.000297127488105344', 'Religious': '0.0', 'Software': '0.0', 'Sporting': '0.0', 'Toys': '0.0', 'Vehicles': '7.25524348202571e-06', 'aov_bench': '59.8599307847218', 'aov_rep': '0', 'avgprice': '12.704623853211', 'bouncerate': '81.06', 'brand_search_volume': '0', 'brand_search_volume_share': '0.0', 'category': 'adult', 'competition': '0.0', 'cpc': '0.0', 'cr_bench': '0.00972125074728272', 'date': '2022-08-01', 'directShare': '40.79', 'displayShare': '0.0', 'domain_cat_1': '0', 'domain_cat_2': '0', 'domain_search_volum_share': '0.0', 'domain_search_volume': '0', 'ekps_aov': '26.859908543707', 'ekps_cr': '0.0194748958674429', 'iib_bench': '1.04867824234893', 'int64_field_0': '1', 'lineid': '1', 'mailShare': '0.0', 'mainCountry': 'Germany', 'mainCountryShare': '84.25', 'no_cat': '0.224926250022808', 'organicShare': '57.66', 'pageviews': '1.74154466250536', 'paidsearchShare': '0.0'}

# Request to tensorflow_serving docker container, works perfectly
response = requests.post(
    'http://localhost:8080/predict',
    json={'instances': [predict_sample]}
)
# prints {""predictions"": 0.0056517720222473145}
print(response.text)

model = tf.saved_model.load(MODEL_PATH)
infer = model.signatures[""serving_default""]

# No matter what I tried as input here, I always get error ""Error consuming . Error: DATA_LOSS: Failed skipping unrequested field""
print(infer(tf.constant('How to feed model with one string tensor??')))
```


### Relevant log output

_No response_</details>"
59635,Non deterministic random numbers generated in Eager and Graph mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Executing the identical Keras model in eager and graph mode produce different results when the identical seed is passed to `tf.random.uniform`. I created a small example that just generates 2 random outputs. In one case, I use different values for seed, in the other I use identical seeds.

As you can see in the output, in the case where the seeds differ, I get the same result using `model(data)` and `model.predict(data)`. However, when my seeds are identical, then I get other results in eager mode.

I tried to pinpoint the source for this, and I think I found it. I modified the PhiloxRandom class to do some output whenever a function is called. It shows the pointer of `this`, which function gets called and also the `key[0:2]` and `columns[0:4]` values. `key` gets initialized through the global seed `tf.random.set_seed(...)` and `columns[2:4]` through the `seed` value passed to `tf.random.uniform`.

What we see is, that in the case where the seed differs, TensorFlow creates two instances of `PhiloxRandom(...)`, one with 0x7C (==124) and one with 0x7B (==123) as seed. The same happens in graph mode. I think this is the expected behavior.

In the case where the seeds are identical, we see that in eager mode only one instance of PhiloxRandom is created, and it gets called twice. So it uses an accumulated seed in the second `tf.random.uniform(...)`, which is wrong. My assumption is that TensorFlow in eager mode caches the operator instance and just reuses it when the ""identical"" layer gets called. However, in this particular case this causes undeterministic results, which differ from the graph mode.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# see: https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism
tf.keras.utils.set_random_seed(1)
tf.config.experimental.enable_op_determinism()

def create(seed_a, seed_b):
    inp = tf.keras.Input(shape=(5,))
    x = inp / inp # == 1.0
    a = tf.random.uniform(tf.shape(x), dtype=x.dtype, minval=-5, maxval=5, seed=seed_a) * x
    b = tf.random.uniform(tf.shape(x), dtype=x.dtype, minval=-5, maxval=5, seed=seed_b) * x
    return tf.keras.Model(inputs=[inp], outputs=[a, b])

def compare(seed_a, seed_b):
    print(f""## RUNNING {seed_a} {seed_b} ##"")
    data = np.ones((1, 5), np.float32)

    model = create(seed_a, seed_b)

    eager = model(data)
    graph = model.predict(data)

    for i, (a, b) in enumerate(zip(eager, graph)):
        print(f'#{i}, A: {a.numpy()}, B: {b}')

tf.random.set_seed(314159)
compare(123, 124)

tf.random.set_seed(314159)
compare(123, 123)
```


### Relevant log output

```shell
# EXECUTION RESULTS --------------------------------------------------------- #

## RUNNING 123 124 ##
#0, A: [[ 0.64956665  4.783145   -4.816675    4.7801285   1.6770649 ]], B: [[ 0.64956665  4.783145   -4.816675    4.7801285   1.6770649 ]]
#1, A: [[ 1.321938   4.436612   2.8036633 -2.5721383 -3.595649 ]], B: [[ 1.321938   4.436612   2.8036633 -2.5721383 -3.595649 ]]

## RUNNING 123 123 ##
#0, A: [[ 2.2466993   2.8111506   1.4583216  -0.34343958  2.204914  ]], B: [[ 0.64956665  4.783145   -4.816675    4.7801285   1.6770649 ]]
#1, A: [[ 0.64956665  4.783145   -4.816675    4.7801285   1.6770649 ]], B: [[ 0.64956665  4.783145   -4.816675    4.7801285   1.6770649 ]]


# /EXECUTION RESULTS -------------------------------------------------------- #

# DEBUG --------------------------------------------------------------------- #

## RUNNING 123 124 ##

### EAGER
0x7fffb27e8170 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007C 00000000
0x4101a50 Skip(1280)
0x7fffb27e86f0 Skip(0)
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007C 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 5150EBBA 4EF8C9E4 4D63E30B 919F139E
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007C 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / BC11F9C7 A411A7B1 99682563 B50F8A71
0x7fffb27e8170 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x412a130 Skip(1280)
0x7fffb27e86f0 Skip(0)
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 5EC85080 05FD3969 A08258B8 61FD2F86
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 4DD57768 F92C9439 A139802D 55996F6F

### GRAPH
0x7fffb27e9890 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007C 00000000
0x7fffb27e9890 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x441f8b0 Skip(1280)
0x7f8e1fffda30 Skip(0)
0x7f8e1fffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7f8e1fffda30 OperatorEND(): 8FF812B0 96A522AD / 5EC85080 05FD3969 A08258B8 61FD2F86
0x7f8e1fffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007B 00000000
0x7f8e1fffda30 OperatorEND(): 8FF812B0 96A522AD / 4DD57768 F92C9439 A139802D 55996F6F
0x4454e30 Skip(1280)
0x7f8dfbffda30 Skip(0)
0x7f8dfbffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007C 00000000
0x7f8dfbffda30 OperatorEND(): 8FF812B0 96A522AD / 5150EBBA 4EF8C9E4 4D63E30B 919F139E
0x7f8dfbffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007C 00000000
0x7f8dfbffda30 OperatorEND(): 8FF812B0 96A522AD / BC11F9C7 A411A7B1 99682563 B50F8A71


## RUNNING 123 123 ##
### EAGER
0x7fffb27e8170 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x4167510 Skip(1280)
0x7fffb27e86f0 Skip(0)
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 5EC85080 05FD3969 A08258B8 61FD2F86
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 4DD57768 F92C9439 A139802D 55996F6F
0x4167510 Skip(1280)
0x7fffb27e86f0 Skip(0)
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000500 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / C7DCC1FC 4C63FB94 2352AAA1 D83B9A9E
0x7fffb27e86f0 OperatorBEGIN(): 0004CB2F 00000000 / 00000501 00000000 0000007B 00000000
0x7fffb27e86f0 OperatorEND(): 8FF812B0 96A522AD / 8DDC3910 CEB432E2 6CF2E131 6770FC94

### GRAPH
0x7fffb27e9890 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7fffb27e9890 PhiloxRandom(uint64_t, uint64_t): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x440ed40 Skip(1280)
0x7f8e1fffda30 Skip(0)
0x7f8e1fffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7f8e1fffda30 OperatorEND(): 8FF812B0 96A522AD / 5EC85080 05FD3969 A08258B8 61FD2F86
0x7f8e1fffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007B 00000000
0x7f8e1fffda30 OperatorEND(): 8FF812B0 96A522AD / 4DD57768 F92C9439 A139802D 55996F6F
0x4500200 Skip(1280)
0x7f8dfbffda30 Skip(0)
0x7f8dfbffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000000 00000000 0000007B 00000000
0x7f8dfbffda30 OperatorEND(): 8FF812B0 96A522AD / 5EC85080 05FD3969 A08258B8 61FD2F86
0x7f8dfbffda30 OperatorBEGIN(): 0004CB2F 00000000 / 00000001 00000000 0000007B 00000000
0x7f8dfbffda30 OperatorEND(): 8FF812B0 96A522AD / 4DD57768 F92C9439 A139802D 55996F6F

# /DEBUG -------------------------------------------------------------------- #
```
</details>"
59631,"`tflite-runtime-nightly` on ARM64 has minimum GLIBC requirement 2.33, which prevents installation in official `python` container images","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `python:3.10-slim-bullseye` container image on **ARM64**
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**:  `tflite-runtime-nightly` `2.13.0.dev20230207`
-   **Python version**: 3.10.10
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**:

### Describe the problem

1. Create a new `python:3.10-slim-bullseye` container
2. Install [`tflite-runtime-nightly` `2.13.0.dev20230207`](https://pypi.org/project/tflite-runtime-nightly/2.13.0.dev20230207/)
3. Build and run the container on **ARM64** (such as from an ARM Macbook using Docker Desktop)
4. Run

```
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/venv/lib/python3.10/site-packages/tflite_runtime/interpreter.py"", line 33, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /lib/aarch64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /opt/venv/lib/python3.10/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so)
```

There are not yet any Debian bookworm-based [`python` official images](https://hub.docker.com/_/python) yet, so it's not possible to bump up to a newer Debian version with a newer GLIBC.

### Other background

This worked fine in a `python:3.10-slim-bullseye` **x86** container, so perhaps the GLIBC version requirements are different due to how the ARM64 wheels were built?

Here are the GLIBC versions of various Linux platforms:

```
Debian Buster:   2.28
       Bullseye: 2.31
       Bookworm: 2.36

Ubuntu 20.04:    2.31
       22.04:    2.35

RHEL   8:        2.28
       9:        2.34
```

Related to:

- #59423

CC @samypr100 @terryheo "
59630,Control Flow Ops with external delegate NPU,"### 1. System information

- OS Platform and Distribution : Ubuntu20.04
- TensorFlow installation : conda
- TensorFlow library : 2.8.1

I am converting a pytorch model to Int8 quantize tensorflowlite model, to run it on a NXP board, on which I use delegates to make it run on the NPU. My original model contains an Unsqueeze block, which is somehow transformed into a While Ops (I can see that with netron, between the onnx and tflite model).

Running this model on the board with the delegate (**libvx_delegate.so**) gives me the error:
`ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`

From this post: https://stackoverflow.com/questions/66682315/finding-dynamic-tensors-in-a-tflite-model I understand that the Control Flow Ops are treated as dynamic. My question is how can I run such a model with the delegate?
"
59627,tf.math.digamma outputs NaN when receiving 0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.math.digamma outputs nan when receiving zero. In contrast, spicy, Matlab, and pytorch output -inf. Following the blue curve in this graph (https://ww2.mathworks.cn/help/matlab/ref/psi.html#mw_8a9c6576-f489-4530-bd3f-f6c8d596c644), I guess digamma outputs -inf for zero is more accurate.
```


### Standalone code to reproduce the issue

```shell
tf.math.digamma(0.)  # outputs <tf.Tensor: shape=(), dtype=float32, numpy=nan>
scipy.special.digamma(0.)  # outputs -inf
torch.digamma(0.) # outputs -inf
```


### Relevant log output

_No response_</details>"
59626,[RNN] TFLite fails to convert LSTM to 16x8 quantization,"Hi, the [MLPerf Tiny](https://mlcommons.org/en/news/mlperf-tiny-v05/) working group would like to implement a streaming LSTM benchmark for audio speech enhancement, based on this [paper](https://arxiv.org/abs/2005.11138). Since audio playback typically uses 16 bits per sample, it seems like any ML model synthesizing audio will need to use 16-bit activations. However, TFLite does not seem to support 16x8 quantization for LSTMs. Please see the colab below which shows a streaming LSTM model successfully converting to TFLite in 8x8 mode, but not in 16x8 mode.

Is there any way to work around this problem and get 16-bit activations out of an LSTM? This would be hugely impactful for the embedded audio community.

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): `pip install tensorflow-cpu`
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

https://colab.research.google.com/drive/1mQq7yqqwH6HjgOTSxTU0I_SMA1XIZ2XM?usp=sharing

### 3. Failure after conversion

> RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor arg1
> Empty min/max for tensor arg1
"
59624,tf.math.digamma fails when receiving a bfloat16 input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This issue is similar to my previous one: https://github.com/tensorflow/tensorflow/issues/59611.
Following the documentation: https://www.tensorflow.org/api_docs/python/tf/math/digamma, tf.math.digamma should accept the bfloat16 input but it fails. BTW the sentence in the documentation online ""Computes Psi, the derivative of Lgamma (the log of the absolute value of"" seems incomplete
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.math.digamma(tf.Variable(0., dtype=""bfloat16""))
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Digamma}} = Digamma[T=DT_BFLOAT16]
All kernels registered for op Digamma:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
 [Op:Digamma]
```
</details>"
59623,[tensorflow_datasets 4.8.2]: Connection refused when trying to load huggingface datasets,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

macOS

### Mobile device

_No response_

### Python version

3.8, 3.9, 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Cannot load huggingface datasets

Used to work fine on 02/01/2023
```


### Standalone code to reproduce the issue

```shell
import tensorflow_datasets
data = tensorflow_datasets.load('glue/mrpc')
```


### Relevant log output

```shell
2023-02-08 17:46:43.871073: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fb417f4b9a0 (URI: http://metadata/computeMetadata/v1/instance/service-accounts/default/token) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003921 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)
```
</details>"
59622,[tensorflow_datasets 3.1.0]: ValueError(_NAME_STR_ERR.format(name_str)) when loading huggingface dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

macOS

### Mobile device

_No response_

### Python version

3.8, 3.9, 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ValueError(_NAME_STR_ERR.format(name_str)) when loading huggingface dataset via tensorflow_datasets.load()
```


### Standalone code to reproduce the issue

```shell
import tensorflow_datasets
data = tensorflow_datasets.load('huggingface:glue/mrpc')
```


### Relevant log output

```shell
pip show tensorflow_datasets 
Name: tensorflow-datasets
Version: 3.1.0
Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.
Home-page: https://github.com/tensorflow/datasets
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/local/conda/envs/tf_base/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py"", line 69, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/Users/local/conda/envs/tf_base/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py"", line 356, in load
    name, name_builder_kwargs = _dataset_name_and_kwargs_from_name_str(name)
  File ""/Users/local/conda/envs/tf_base/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py"", line 391, in _dataset_name_and_kwargs_from_name_str
    raise ValueError(_NAME_STR_ERR.format(name_str))
ValueError: Parsing builder name string huggingface:glue/mrpc failed.
The builder name string must be of the following format:
  dataset_name[/config_name][:version][/kwargs]

  Where:

    * dataset_name and config_name are string following python variable naming.
    * version is of the form x.y.z where {x,y,z} can be any digit or *.
    * kwargs is a comma list separated of arguments and values to pass to
      builder.

  Examples:
    my_dataset
    my_dataset:1.2.*
    my_dataset/config1
    my_dataset/config1:1.*.*
    my_dataset/config1/arg1=val1,arg2=val2
    my_dataset/config1:1.2.3/right=True,foo=bar,rate=1.2
```
</details>"
59620,AttributeError: 'list' object has no attribute 'numpy',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Python version

3.10.9

### Current Behaviour?


Hello!
I'm trying to write a script that gets the weights (just the weights) of each layer of a keras model. I am aware of the `get_weights()` method, but I don't want to use it since it returns specific terms such as bias with the weights, so I can't tell which is which. I used `model.get_layer(layer.name).weights` to extract the weights of a particular layer. The layer weights are returned as a TensorFlow variable. Here's an example:

```shell
[<tf.Variable 'dense_2/kernel:0' shape=(256, 10) dtype=float32, numpy=
array([[-0.08837277, -0.03666556, -0.00683133, ...,  0.09055033,
         0.13219357,  0.0625298 ],
       [-0.00596744, -0.10817644, -0.08093027, ..., -0.03245946,
        -0.1296913 ,  0.14863956],
       [-0.10632615, -0.14407103, -0.09643614, ...,  0.1368066 ,
         0.09166756,  0.12455937],
       ...,
       [ 0.1320965 , -0.14804208, -0.11622404, ...,  0.14576206,
         0.02600923,  0.10927373],
       [ 0.05249688, -0.00176807, -0.06608753, ..., -0.09677176,
         0.1066353 , -0.11133642],
       [-0.07169801, -0.05865452,  0.11303747, ...,  0.11609155,
         0.03141506,  0.00285086]], dtype=float32)>]
```

Despite having a `list` type, the above output cannot be indexed: Assuming the above output is referred to as `var`; `var[0]` throws an `IndexError: list index out of range` error. 

Whenever I try to evaluate tf.Variable with the numpy() method, the error shown in the issue title appears.

Any help would be much appreciated.
Thank you!



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def get_model():
    mnist_model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(256, activation='relu', use_bias=False),
    tf.keras.layers.Dropout(0.45),
    tf.keras.layers.Dense(256, activation='relu', use_bias=False),
    tf.keras.layers.Dropout(0.45),
    tf.keras.layers.Dense(10, use_bias=False)])
    return mnist_model


def get_model_weights(model):
    weights = []
    for index, layer in enumerate(model.layers):
        layer_weights = model.get_layer(layer.name).weights    
        layer_weights.numpy()
        weights.append(layer_weights)
    return weights

get_model_weights(get_model())
```"
59612,LayerNormalization layer cannot be imported,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:  No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04.5 LTS
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.11.0-rc2-17-gd5b57ca93e5 2.11.0
-   **Python version**: 3.9.12
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: cudatoolkit=11.2 cudnn=8.1.0
-   **GPU model and memory**: RTX 3090, 24 GB
-   **Exact command to reproduce**:

```text
(nilm) lindo@titan:~/Develop/nilm/ml/transformer_model$ python3
Python 3.9.12 (main, Jun  1 2022, 11:38:51) 
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python.keras.layers import LayerNormalization
2023-02-08 04:49:08.994050: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-08 04:49:09.517674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/nvvm/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt
2023-02-08 04:49:09.517729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/nvvm/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt:/home/lindo/anaconda3/envs/nilm/lib/:/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorrt
2023-02-08 04:49:09.517738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers' (/home/lindo/anaconda3/envs/nilm/lib/python3.9/site-packages/tensorflow/python/keras/layers/__init__.py)
>>>
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) layer does not exist, in fact none of the normalization layers are in ```tensorflow/python/keras/layers``` and so cannot be imported in a Python script. I have installed tensorflow using pip in a conda environment as described in the [tensorflow installation documentation](https://www.tensorflow.org/install/pip). The expected behavior is the LayerNormalization layer should exist in the source tree and be importable as Python module. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Steps to reproduce.
1. Install tensorflow in a conda environment using pip per the documentation. 
2. Import the LayerNormalization layer.
"
59611,tf.math.tan cannot accept integer variables.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Following the documentation: https://www.tensorflow.org/api_docs/python/tf/math/tan, tf.math.tan should accept integer variables but it fails.
```


### Standalone code to reproduce the issue

```shell
tf.math.tan(tf.Variable(0, dtype=""int8"")  # crash
tf.math.tan(tf.Variable(0, dtype=""int16"")  # crash
tf.math.tan(tf.Variable(0, dtype=""int32"")  # crash
tf.math.tan(tf.Variable(0, dtype=""int64"")  # crash
```


### Relevant log output

```shell
The error message is similar to below:


    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Tan}} = Tan[T=DT_INT8]
All kernels registered for op Tan:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
 [Op:Tan]
```
```
</details>"
59608,change in reproducibility behavior of GlorotUniform between tf 2.6 and 2.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


With tf 2.6 , the following code always produces same result, but produces varying results in tf 2.11

```python
import tensorflow as tf
tf.random.set_seed(42)
initializer = tf.keras.initializers.GlorotUniform()
initializer(shape=(2, 2))
```



### Standalone code to reproduce the issue




### Relevant log output

_No response_</details>"
59605,Install tensorboard as an extra,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The `tensorflow` package currently relies on a great deal of sub-dependencies. Among them the biggest one is definitely the tensorboard, as you can see from this tree obtained through `poetry`. I wonder if the `tensorboard` (and possibly other dependencies) are really needed for a minimal installation of tensorflow. E.g. do I really need the tensorboard for running inference on a model? The more dependencies we have
- The more bloated our virtual environments become
- The slower our CI jobs are
- The bigger are our Docker images that run TensorFlow
- The slower is the dependency resolution of Python packages with tools such as poetry, pipenv, pdm, ..

In comparison, `torch` just depends on `typing-extensions`.

It would be great to have a minimal installation of `tensorflow` by default and rely on pip extras to install any satellite functionalities (such as `pip install tensorflow[tensorboard]`).


tensorflow >=2.3.0
    ├── absl-py >=0.4.0
    ├── astunparse >=1.6.0
    │   ├── six >=1.6.1,<2.0
    │   └── wheel >=0.23.0,<1.0
    ├── flatbuffers >=1.12
    ├── gast >=0.2.1
    ├── google-pasta >=0.1.1
    │   └── six * (circular dependency aborted here)
    ├── grpcio >=1.24.3,<2.0
    ├── h5py >=2.9.0
    │   └── numpy >=1.14.5
    ├── keras >=2.8.0rc0,<2.9
    ├── keras-preprocessing >=1.1.1
    │   ├── numpy >=1.9.1 (circular dependency aborted here)
    │   └── six >=1.9.0 (circular dependency aborted here)
    ├── libclang >=9.0.1
    ├── numpy >=1.20 (circular dependency aborted here)
    ├── opt-einsum >=2.3.2
    │   └── numpy >=1.7 (circular dependency aborted here)
    ├── protobuf >=3.9.2
    ├── setuptools *
    ├── six >=1.12.0 (circular dependency aborted here)
    ├── tensorboard >=2.8,<2.9
    │   ├── absl-py >=0.4 (circular dependency aborted here)
    │   ├── google-auth >=1.6.3,<3
    │   │   ├── cachetools >=2.0.0,<6.0
    │   │   ├── pyasn1-modules >=0.2.1
    │   │   │   └── pyasn1 >=0.4.6,<0.5.0
    │   │   ├── rsa >=3.1.4,<5
    │   │   │   └── pyasn1 >=0.1.3 (circular dependency aborted here)
    │   │   └── six >=1.9.0 (circular dependency aborted here)
    │   ├── google-auth-oauthlib >=0.4.1,<0.5
    │   │   ├── google-auth >=1.0.0 (circular dependency aborted here)
    │   │   └── requests-oauthlib >=0.7.0
    │   │       ├── oauthlib >=3.0.0
    │   │       └── requests >=2.0.0
    │   │           ├── certifi >=2017.4.17
    │   │           ├── charset-normalizer >=2,<4
    │   │           ├── idna >=2.5,<4
    │   │           └── urllib3 >=1.21.1,<1.27
    │   ├── grpcio >=1.24.3 (circular dependency aborted here)
    │   ├── markdown >=2.6.8
    │   │   └── importlib-metadata >=4.4
    │   │       └── zipp >=0.5
    │   ├── numpy >=1.12.0 (circular dependency aborted here)
    │   ├── protobuf >=3.6.0 (circular dependency aborted here)
    │   ├── requests >=2.21.0,<3 (circular dependency aborted here)
    │   ├── setuptools >=41.0.0 (circular dependency aborted here)
    │   ├── tensorboard-data-server >=0.6.0,<0.7.0
    │   ├── tensorboard-plugin-wit >=1.6.0
    │   ├── werkzeug >=0.11.15
    │   └── wheel >=0.26 (circular dependency aborted here)
    ├── tensorflow-estimator >=2.8,<2.9
    ├── tensorflow-io-gcs-filesystem >=0.23.1
    ├── termcolor >=1.1.0
    ├── typing-extensions >=3.6.6
    └── wrapt >=1.11.0
```
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow
```


### Relevant log output

_No response_</details>"
59604,Segmentation Fault when inference in Google Coral Dev Board mini,"### 1. System information

- Mendel Linux
- Pycoral package (built in for coral dev board mini)

### 2. Code

Model Training Link:
https://colab.research.google.com/drive/1rk9Pe115dg5Njwh_5g-sh3m7ShVXiNyN

TF Lite Conversion Link:
https://colab.research.google.com/drive/18HPUPVdvBIyPh9z2rd338phJRCQSf7QC#scrollTo=uSYKpoUI0QRU

TF Lite Conversion Code Snippet:
# Load the HDF5 model
model = tf.keras.models.load_model(""lung_segmentation_80epochs_avg_pooling_700kparam_aug.hdf5"",custom_objects={'dice_coef_loss': dice_coef_loss,
                                      'dice_coef':dice_coef})

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model_quant = converter.convert()

# Save the quantized model to disk
with open(""new_model_quant.tflite"", ""wb"") as f:
    f.write(tflite_model_quant)

### 3. Failure after conversion

-When model was used to inference in Google Coral Dev Board mini, Segmentation Fault occur

python3 semantic_segmentation.py\
 --model lung_segmentation_quant.tflite \
 --input test.png \
 --keep_aspect_ratio \
 --output ${HOME}/segmentation_result.jpg
"
59601,Tensorflow ROCm - Faceswap - Anaconda Enviroment,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tensorflow-rocm 2.10.1.540 / tensorflow 2.2.3

### Custom Code

Yes

### OS Platform and Distribution

Linux PikaOS 22.10

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

AMD RX6800XT

### Current Behaviour?

```shell
Installed faceswap in anaconda enviroment and then ran the gui as below.

$ python faceswap.py gui
Setting Faceswap backend to ROCM
02/07/2023 22:26:06 INFO     Log level set to: INFO
02/07/2023 22:26:06 ERROR    There was an error importing Tensorflow. This is most likely because you do not have TensorFlow installed, or you are trying to run tensorflow-gpu on a system without an Nvidia graphics card. Original import error: Traceback (most recent call last):
02/07/2023 22:26:06 ERROR      File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
02/07/2023 22:26:06 ERROR        from tensorflow.python._pywrap_tensorflow_internal import *
02/07/2023 22:26:06 ERROR    ImportError: librccl.so.1: cannot open shared object file: No such file or directory
02/07/2023 22:26:06 ERROR    
02/07/2023 22:26:06 ERROR    
02/07/2023 22:26:06 ERROR    Failed to load the native TensorFlow runtime.
02/07/2023 22:26:06 ERROR    See https://www.tensorflow.org/install/errors for some common causes and solutions.
02/07/2023 22:26:06 ERROR    If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```


### Standalone code to reproduce the issue

```shell
python faceswap.py gui
```


### Relevant log output

```shell
cat <<EOF > /tmp/check_os.py
import platform
print(""""""os: %s
os kernel version: %s
os release version: %s
os platform: %s
linux distribution: %s
echo ""cat ${OUTPUT_FILE}""ate the fields in the github issue template."" of that file.""e"" >> ${OUTPUT_FILE}E}
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_os.py"", line 17, in <module>
    platform.linux_distribution(),
AttributeError: module 'platform' has no attribute 'linux_distribution'
2023-02-07 22:30:47.806461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: librccl.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/michael/anaconda3/envs/Faceswap-ROCm/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: librccl.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Command 'bazel' not found, but can be installed with:
sudo apt install bazel-bootstrap
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt
```
</details>"
59600,Location of the TFLite API source code,"I had a question. I am currently working on a project with the Google Coral TPU. Essentially, we found that the coral TPU doesn't support every operation supported by tflite and we wanted to make the coral support more operations. I see that all the functions supported by tflite are documented here: https://www.tensorflow.org/mlir/tfl_ops. However, I cannot find the source code for these functions inside the tensorflow repository. Does tensorflow lite just take the standard tensorflow functions and add a wrapper around them? If anyone here has any experience working with the tensorflow source in any way, I would appreciate the help!"
59597,Autocomplete fails for any submodules,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0 (v2.11.0-rc2-17-gd5b57ca93e5 ), 2.13.0-dev20230207 (v1.12.1-88998-gf03d41b6f8b)

### Custom Code

No

### OS Platform and Distribution

Linux Manjaro

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Typing any of the following does not offer any auto-completion:

import tensorflow as tf

tf.keras.<no-auto-completion>
tf.data.<no-auto-completion>
tf.estimator.<no-auto-completion>
tf.math.<no-auto-completion>
tf.lite.<no-auto-completion>


I would expect auto-completion to have some kind of knowledge about the submodules.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.keras.<no-auto-completion>
tf.data.<no-auto-completion>
tf.estimator.<no-auto-completion>
tf.math.<no-auto-completion>
tf.lite.<no-auto-completion>
```


### Relevant log output

```shell
Using pyright version 1.1.292.
```
</details>"
59595,Allow selecting enabled targets,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

11.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Many parts of TensorFlow (e.g. TensorFlow XLA execution_engine in tensorflow/compiler/xla/runtime) build support for several targets unconditionally. This leads to a lot of extra build time when that support depends on LLVM support and thus extra backend needing to be enabled in LLVM. I'd like to work on a patch to be able to select targets to be enabled at configure or build time, keeping the current default of enabling most targets. I would also like to enable AArch64 target by default for aot and kernel_gen since ARM target is already enabled unconditionaly at the moment.
```


### Standalone code to reproduce the issue

```shell
Following the steps in tensorflow/compiler/mlir/README.md ""Using local LLVM repo"" with only AArch64 in targets.bzl fails to build.
```


### Relevant log output

_No response_</details>"
59593,Custom trianing with overriding the `fit` method single or multiple devices.,"### Current Behaviour?

In the official blog post, [HERE](https://www.tensorflow.org/tutorials/distribute/custom_training), it is discussed about the process for single device and multiple device training using `Keras` API. But I've found it quite ambigous to understand the right procedue when you want to do both, [overriding the `fit` method](https://keras.io/guides/customizing_what_happens_in_fit/). It's like combination of custom training loop + using high-level API (`fit`). For example, in custom training loop, it's suggested as follows 

```
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)
```

or ephasize to consider during [loss](https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function) calculation. 

or consider the following also while custom training loop?

```
# `run` replicates the provided computation and runs it
# with the distributed input.
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

@tf.function
def distributed_test_step(dataset_inputs):
  return strategy.run(test_step, args=(dataset_inputs,))
```

And it is not clear if we need to do when we combine both (cutom training + fit method).

There are many, for example [this](https://github.com/keras-team/tf-keras/issues/301), that is, if mixed precision is enabled, should we use `LossScaleOptimizer` and `optimizer.get_scaled_loss(loss)`  and `optimizer.get_unscaled_gradients(gradients)` in the `train_step` or `compile` method would do the job? But the [official documentation](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer) talks about normal fit and custom loop training cases. In case of custom loop, it's suggested to wrap the optimizer and scale the loss and gradient but what about the combination of fit and custom loop (overriding train_step)? Does it sill need to wrap the optimizer and scale the loss and gradient or it will be handled by the API?


### Relevant log output"
59592,ios TFLiteBenchmark compile failer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

ios

### Mobile device

iphone

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I use bazel compile ios ,file is tensorflow/tensorflow/lite/tools/benchmark/ios/build_benchmark_framework.sh.
ERROR: Traceback (most recent call last):
	File ""tensorflow/tensorflow/core/platform/default/rules_cc.bzl"", line 6, column 28, in <toplevel>
		_cc_shared_library = native.cc_shared_library
Error: no native function or rule 'cc_shared_library'

before this tensorflow/configure I also run it  select ios.
```


### Standalone code to reproduce the issue

```shell
I want to know the reason. thank you
```


### Relevant log output

_No response_</details>"
59587,TF dataset generator memory leak?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA RTX 2080ti (1 GPU used)

### Current Behaviour?

```shell
Hi everyone,

Hope you are doing well! I have an issue where potentially a memory leak occurs when I create a TensorFlow sampling dataset via tf.data.Dataset.from_generator. I am currently working on some model-based RL ideas, focusing on using exploration inside a Dreamer VAE model for best results in visual continuous control. The model is written in JAX/Haiku, and I decided to try to use a TensorFlow dataset pipeline to load in the data and use it to train the model.

The data is organized in trajectory format in a directory (400 trajectories, stored as .npz files, are in a directory for use). I've written code for the sampling procedure (see line 86 for the visual sequence buffer impl, line 349 for dataset/loader creation at https://github.com/dhruvsreenivas/byol-offline/blob/main/memory/replay_buffer.py for details), which I've done some preliminary tests on in line 170 of this file: https://github.com/dhruvsreenivas/byol-offline/blob/main/testing.py).

The behavior I'm seeing is that the model can effectively train for around 50 epochs with 50 GB of RAM requested, but jobs ultimately die due to memory leakage after that. I want to train the model for 1000 epochs, which makes it seem like an obscene amount of memory is required, which I don't have available to me.

I have a suspicion that a memory leak is occurring somewhere, which is odd given that I believe that memory should be dynamically allocated based off of the size of the batch requested (I am sampling a batch of 50 subtrajectories, each of length 10 for the initial experiments). I'm wondering how to fix this problem (the sampling is required, so the from_generator part should stay the same I think).
```


### Standalone code to reproduce the issue

```shell
The code to reproduce the experiments is here:
- Replay buffer code: https://github.com/dhruvsreenivas/byol-offline/blob/main/memory/replay_buffer.py -- line 86, 349
- Testing file: https://github.com/dhruvsreenivas/byol-offline/blob/main/testing.py -- line 170

For data I'm using to run the experiments, please see https://github.com/conglu1997/v-d4rl for how to collect and organize the data. I'm using cheetah_run 64x64px, medium expert data directory, seen here: https://drive.google.com/drive/folders/1zbAqR0gYBNG2W-F5_ItweCGg6_YHpZ7o.
```


### Relevant log output

```shell
I only really have the job's code, which basically says that the out-of-memory handler was used because the job ran out of memory. Used 2 CPUs and 1 GPU, with 50 GB RAM.
```
</details>"
59583,tf.bitcast throws InternalError,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.bitcast throws InternalError
```


### Standalone code to reproduce the issue

```shell
tf.bitcast(tf.cast([0,32768,65535], tf.quint16), tf.qint16)
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InternalError: Assigned device '/job:localhost/replica:0/task:0/device:GPU:0' does not have registered OpKernel support for _Arg
         [[{{node input}}]] [Op:Bitcast]
```
</details>"
59582,tf.GradientTape throws internal error: RET_CHECK failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.GradientTape throws internal error: RET_CHECK failure
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Create a simple Keras model.
with tf.device('gpu'):
    x = [-1, 0, 1, 2, 3, 4]
    y = [-3, -1, 1, 3, 5, 7]
    model = tf.keras.models.Sequential(
        [tf.keras.layers.Dense(units=1, input_shape=[1])])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=500)
```


### Relevant log output

```shell
Node: 'StatefulPartitionedCall_1'
RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr 
         [[{{node StatefulPartitionedCall_1}}]] [Op:__inference_train_function_437]
```
</details>"
59578,E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inEfficientDet-D2/model/stack_0/block_1/drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to train my dataset with model zoo, which is the EfficientDet D2 model, but I have an error 

E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inEfficientDet-
D2/model/stack_0/block_1/drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
```


### Standalone code to reproduce the issue

```shell
!python model_main_tf2.py --pipeline_config_path=/mydrive/EfficientDet/data/ssd_efficientdet_d2_768x768_coco17_tpu-8.config --model_dir=/mydrive/EfficientDet/training --alsologtostderr
```


### Relevant log output

```shell
WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
W0206 13:32:47.285290 139961433057024 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
W0206 13:33:01.063754 139961433057024 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
W0206 13:33:12.740289 139961433057024 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
W0206 13:33:26.074406 139961433057024 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
2023-02-06 13:33:33.925763: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inEfficientDet-D2/model/stack_0/block_1/drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
```
</details>"
59577," tf.keras.callbacks.ModelCheckpoint: initial_value_threshold not working, definition with a mistake","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

'2.3.0'

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The manual defines initial_value_threshold as 
""Floating point initial ""best"" value of the metric to be monitored. Only applies if save_best_value=True. Only overwrites the model weights already saved if the performance of current model is better than this value. ""

save_best_value is not defined!

If  save_best_only was meant, then it is not working like the manual says.
I have no other idea, how should I get initial_value_threshold work, it is still starting with -inf.
```


### Standalone code to reproduce the issue

```shell
es = EarlyStopping(monitor='val_loss', verbose=1, patience=20)
tmpmodelfile = 'models/' + model_config_name + '/temp1_best_model.h5'

evaluation = model.evaluate(vd[feature_cols], vd[target], 
                            batch_size=batch_size, verbose=2,
                            return_dict=True)
#3231/3231 - 2s - loss: 0.0500 - mean_squared_error: 0.0500
mc = ModelCheckpoint(tmpmodelfile, monitor='val_loss', 
                     verbose=1, save_best_only=True, 
                     initial_value_threshold=evaluation['loss'],
                     save_best_value=True)
history = model.fit(td[feature_cols], td[target], 
                    validation_data=(vd[feature_cols], vd[target]), 
                    batch_size=batch_size, epochs=epochs, 
                    verbose=2, callbacks=[es, mc])
Epoch 1/400
Epoch 00001: val_loss improved from inf to 0.04999, saving model to models
```


### Relevant log output

_No response_</details>"
59576,tf.math.erfinv should outputs NaN instead of inf when receiving an invalid input.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.2, 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9, 3.8

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
The valid input range of `erfinv` is [-1, 1] and inputs outside of this range are invalid inputs. For these invalid inputs, [Matlab](https://www.mathworks.com/help/matlab/ref/erfinv.html), spicy, and pytorch output NaN. The current TF version outputs Inf. Here is the colab link to reproduce: https://colab.research.google.com/drive/10J4sDIi07sQ2WILXrVnmO3eMo7Kld-XY?usp=sharing
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.math.erfinv(tf.Variable(1.1)))
```


### Relevant log output

```shell
TF: tf.Tensor(inf, shape=(), dtype=float32)
torch: tensor(nan)
spicy: nan
```
</details>"
59575,Remove dependency on six,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Tensorflow [relies on six](https://github.com/tensorflow/tensorflow/blob/c746206c1a377fe8f5571511a778993701d10ecc/tensorflow/tools/pip_package/setup.py#L109) as direct dependency. Since Tensorflow [requires Python >= 3.8](https://github.com/tensorflow/tensorflow/blob/c746206c1a377fe8f5571511a778993701d10ecc/tensorflow/tools/pip_package/setup.py#L357) and `six` is a Python 2 and 3 compatibility library, I'm sure it should be possible to remove this unneeded dependency and rely on Python 3 stdlib


### Standalone code to reproduce the issue

Irrelevant


### Relevant log output

_No response_</details>"
59573,tf.distribute.MultiWorkerMirroredStrategy: Program hangs when connecting with gRPC.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-aarch64 2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.10

### Mobile device

_No response_

### Python version

Python 3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The script hangs when defining strategy, and not just for MultiWorkerMirrored, but any of the strategies from tf.distribute.
```


### Standalone code to reproduce the issue

```shell
def train_dense_model(batch_size):
    # limit imports oustide the call to the function, in order to launch quickly
    # when using dask
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    import json
    # model building
    tf.keras.backend.clear_session()  # For easy reset of notebook state.

    slurm_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver(port_base=15000)
    
    tf_config = json.dumps({'cluster' : slurm_resolver.cluster_spec().as_dict()})
    os.environ['TF_CONFIG'] = tf_config
    
    communication_options = tf.distribute.experimental.CommunicationOptions(bytes_per_pack=50 * 1024 * 1024,
                                                              timeout_seconds=120.0,
                                                              implementation=tf.distribute.experimental.CommunicationImplementation.RING
    )
    
    mirrored_strategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=slurm_resolver, 
                                                                  communication_options=communication_options)
```


### Relevant log output

```shell
2023-02-05 19:36:27.162088: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://node1:15000
2023-02-05 19:36:38.280982: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://node2:15000
2023-02-05 19:36:38.691138: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://node4:15000
2023-02-05 19:36:40.504698: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://node3:15000
```
### Accompanying script for ""sbatch script.sh"":

```shell
#!/bin/bash
#SBATCH --job-name=mnist_tf_distributed     # job name
#SBATCH --nodelist=node[01-04]              # number of nodes
#SBATCH --ntasks-per-node=1                 # number of MPI task per node
#SBATCH --cpus-per-task=4                   # since nodes have 4 cpus
#SBATCH --distribution=block:block          # distribution, might be better to have contiguous blocks
#SBATCH --time=00:10:00                     # job length
#SBATCH --exclusive                         # we reserve the entire node for our job
#SBATCH --output=mnist_tf_distr_log_%j.out  # std out
#SBATCH --error=mnist_tf_distr_log_%j.out   # std err

unset http_proxy https_proxy HTTP_PROXY HTTPS_PROXY

set -x
cd ${SLURM_SUBMIT_DIR}

srun --nodelist=node[01-04] python ./mnist_example.py
```
</details>
"
59571,cwise_op_gpu_logical_or.cu.pic.o' was not created,"./tensorflow/stream_executor/stream.h(2165): warning: type qualifier on return type is meaningless

./tensorflow/stream_executor/stream.h(2168): warning: type qualifier on return type is meaningless

./tensorflow/core/kernels/topk_op_gpu.h(440): warning: integer conversion resulted in a change of sign

ERROR: /sources/tensorflow/tensorflow/core/kernels/BUILD:4079:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_logical_or.cu.pic.o' was not created
ERROR: /sources/tensorflow/tensorflow/core/kernels/BUILD:4079:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2271.782s, Critical Path: 420.46s"
59570,tf.bitwise.invert throws SystemError when input is uint64 tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0.dev20230204

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.bitwise.invert throws SystemError when input is uint64 tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.bitwise.invert(tf.constant(-2147483648, dtype=tf.uint64))
```


### Relevant log output

```shell
SystemError: <class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
```
</details>"
59563,Unable to build TensorFlow from source with avx2 configuration.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

master and r2.12

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS (Focal Fossa)

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!

Unable to build TensorFlow with avx2 configuration.
6d71dc3ae89012d8c301b07d8635b41cfe6e6850 is the first bad commit

Eigen build also breaks after this commit.
```


### Standalone code to reproduce the issue

```shell
python configure.py

bazel --bazelrc=.bazelrc build -c opt --copt=-march=haswell tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
root@ebed50890c33:~/tensorflow# git bisect good
Bisecting: 5 revisions left to test after this (roughly 3 steps)
[6d71dc3ae89012d8c301b07d8635b41cfe6e6850] Update Eigen to commit:3460f3558e7b469efb8a225894e21929c8c77629
root@ebed50890c33:~/tensorflow# bazel --bazelrc=.bazelrc build -c opt --copt=-march=haswell tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=176
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/local/bin/python
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'build' from /root/.mkl.bazelrc:
  'build' options: --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --config=mkl --copt=-march=skylake-avx512
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file /root/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (3 packages loaded, 5645 targets configured).
INFO: Found 1 target...
ERROR: /root/tensorflow/tensorflow/tsl/framework/contraction/BUILD:110:11: Compiling tensorflow/tsl/framework/contraction/eigen_contraction_kernel.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 60 arguments skipped)
In file included from ./tensorflow/tsl/framework/fixedpoint/FixedPoint.h:34,
                 from ./tensorflow/tsl/framework/contraction/eigen_contraction_kernel.h:37,
                 from tensorflow/tsl/framework/contraction/eigen_contraction_kernel.cc:16:
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h: In member function 'void Eigen::internal::gemm_pack_lhs<Eigen::QInt16, Index, DataMapper, Pack1, Pack2, Eigen::QInt16, 0, Conjugate, PanelMode>::operator()(Eigen::QInt16*, const DataMapper&, Index, Index, Index, Index)':
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h:176:5: error: there are no arguments to 'assert' that depend on a template parameter, so a declaration of 'assert' must be available [-fpermissive]
  176 |     assert(false &&
      |     ^~~~~~
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h:176:5: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h: In member function 'void Eigen::internal::gemm_pack_rhs<Eigen::QInt16, Index, DataMapper, nr, 0, Conjugate, PanelMode>::operator()(Eigen::QInt16*, const DataMapper&, Index, Index, Index, Index)':
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h:262:5: error: there are no arguments to 'assert' that depend on a template parameter, so a declaration of 'assert' must be available [-fpermissive]
  262 |     assert(false &&
      |     ^~~~~~
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h: In member function 'void Eigen::internal::gebp_kernel<Eigen::QInt16, Eigen::QInt16, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs>::operator()(const DataMapper&, const Eigen::QInt16*, const Eigen::QInt16*, Index, Index, Index, Eigen::QInt32, Index, Index, Index, Index)':
./tensorflow/tsl/framework/fixedpoint/MatMatProductAVX2.h:363:5: error: there are no arguments to 'assert' that depend on a template parameter, so a declaration of 'assert' must be available [-fpermissive]
  363 |     assert(false &&
      |     ^~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.326s, Critical Path: 6.23s
INFO: 179 processes: 115 internal, 64 local.
FAILED: Build did NOT complete successfully
```
</details>"
59560,conv and fused relu fails on GPUv2 delegate on OpenCL when batch size > 1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Android 13

### Mobile device

Google Pixel 5 (and others)

### Python version

3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Loading this model with the GPUv2 delegate, with OpenCL selected, causes the delegate to fail to prepare when batch = 5. When batch = 1, conv and fused relu works fine.

We'd like to understand whether fusioned ops are expected to work with batch > 1. Is this a tflite bug or a known limitation?

And if a limitation, what is the recommended workaround?


### Standalone code to reproduce the issue


This is the code to produce the tflite model that produces a conv + fused relu, which fails on the GPUv2 delegate on OpenCL.

```python
import numpy as np
import tensorflow as tf

input_shapes = [(5, 224, 224, 3)]

def SimpleNet():
    nc = 3
    model = tf.keras.models.Sequential([
        tf.keras.layers.Input(shape=(input_shapes[0][3],) + input_shapes[0][1:3], batch_size=input_shapes[0][0], name='input'),
        tf.keras.layers.Permute((2, 3, 1)),
        tf.keras.layers.Conv2D(nc, kernel_size=3, padding=""valid"", use_bias=True, activation=""relu""),
        tf.keras.layers.Permute((3, 1, 2)),
    ])

    return model

tf_model = SimpleNet()
tf_model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
tf_model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the model.
with open(""conv_relu_batch5.tflite"", ""wb"") as f:
    f.write(tflite_model)
```
```


### Relevant log output

```shell
[tflite] TfLiteGpuDelegate Init: Unrecognized Write selector
[tflite] Created 0 GPU delegate kernels.
[tflite] TfLiteGpuDelegate Prepare: delegate is not initialized
[tflite] Node number 3 (TfLiteGpuDelegateV2) failed to prepare.
[tflite] Restored original execution plan after delegate application failure.
```
</details>"
59559,Tensorflow Ops for RecordRandomReader,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Currently, there is no option for random access reading TFRecord files. This could be extremely useful when dealing with huge (long) datasets that do not fit into memory, e.g. one could create an index of proto byte offsets and then read protos from their offsets. This would enable full shuffling of large datasets. 

I've discovered a RecordRandomReader class in the Git (although there's no documentation of it on the TF web page) which seems to do just that - except it's not a tf ops, so it can not be added to a tf.data pipeline without wrapping it into py_function first, severely limiting its performance. 

So, could we have a RecordRandomReader-like tf ops? Thanks!
"
59537,TFlite minimal example failing on latest tensorflow repo,"When I run latest tensorflow lite example minimal and it is failing on Linux machine with the below error

Followed steps mentioned here  and ran on x86_64 GNU/Linux
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal



,,,
100%] Linking CXX executable minimal
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(register.cc.o): in function `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()':
register.cc:(.text+0x1cd): undefined reference to `tflite::ops::builtin::Register_ABS()'
/usr/bin/ld: register.cc:(.text+0x1ed): undefined reference to `tflite::ops::builtin::Register_HARD_SWISH()'
/usr/bin/ld: register.cc:(.text+0x207): undefined reference to `tflite::ops::builtin::Register_RELU()'
/usr/bin/ld: register.cc:(.text+0x227): undefined reference to `tflite::ops::builtin::Register_RELU_N1_TO_1()'
/usr/bin/ld: register.cc:(.text+0x241): undefined reference to `tflite::ops::builtin::Register_RELU_0_TO_1()'
/usr/bin/ld: register.cc:(.text+0x25b): undefined reference to `tflite::ops::builtin::Register_RELU6()'
/usr/bin/ld: register.cc:(.text+0x27b): undefined reference to `tflite::ops::builtin::Register_TANH()'
/usr/bin/ld: register.cc:(.text+0x29b): undefined reference to `tflite::ops::builtin::Register_LOGISTIC()'
/usr/bin/ld: register.cc:(.text+0x2bb): undefined reference to `tflite::ops::builtin::Register_AVERAGE_POOL_2D()'
/usr/bin/ld: register.cc:(.text+0x2db): undefined reference to `tflite::ops::builtin::Register_MAX_POOL_2D()'
/usr/bin/ld: register.cc:(.text+0x2fb): undefined reference to `tflite::ops::builtin::Register_L2_POOL_2D()'
/usr/bin/ld: register.cc:(.text+0x315): undefined reference to `tflite::ops::builtin::Register_CONV_2D()'
/usr/bin/ld: register.cc:(.text+0x335): undefined reference to `tflite::ops::builtin::Register_DEPTHWISE_CONV_2D()'
/usr/bin/ld: register.cc:(.text+0x355): undefined reference to `tflite::ops::builtin::Register_SVDF()'
/usr/bin/ld: register.cc:(.text+0x375): undefined reference to `tflite::ops::builtin::Register_RNN()'
/usr/bin/ld: register.cc:(.text+0x395): undefined reference to `tflite::ops::builtin::Register_BIDIRECTIONAL_SEQUENCE_RNN()'
/usr/bin/ld: register.cc:(.text+0x3b5): undefined reference to `tflite::ops::builtin::Register_UNIDIRECTIONAL_SEQUENCE_RNN()'
/usr/bin/ld: register.cc:(.text+0x3d5): undefined reference to `tflite::ops::builtin::Register_EMBEDDING_LOOKUP()'
/usr/bin/ld: register.cc:(.text+0x3f5): undefined reference to `tflite::ops::builtin::Register_EMBEDDING_LOOKUP_SPARSE()'
/usr/bin/ld: register.cc:(.text+0x40f): undefined reference to `tflite::ops::builtin::Register_FULLY_CONNECTED()'
/usr/bin/ld: register.cc:(.text+0x42f): undefined reference to `tflite::ops::builtin::Register_LSH_PROJECTION()'
/usr/bin/ld: register.cc:(.text+0x449): undefined reference to `tflite::ops::builtin::Register_HASHTABLE_LOOKUP()'
/usr/bin/ld: register.cc:(.text+0x463): undefined reference to `tflite::ops::builtin::Register_SOFTMAX()'
/usr/bin/ld: register.cc:(.text+0x483): undefined reference to `tflite::ops::builtin::Register_CONCATENATION()'
/usr/bin/ld: register.cc:(.text+0x4a3): undefined reference to `tflite::ops::builtin::Register_ADD()'
/usr/bin/ld: register.cc:(.text+0x4c0): undefined reference to `tflite::ops::builtin::Register_SPACE_TO_BATCH_ND()'
/usr/bin/ld: register.cc:(.text+0x4e0): undefined reference to `tflite::ops::builtin::Register_BATCH_TO_SPACE_ND()'
/usr/bin/ld: register.cc:(.text+0x500): undefined reference to `tflite::ops::builtin::Register_MUL()'
/usr/bin/ld: register.cc:(.text+0x520): undefined reference to `tflite::ops::builtin::Register_L2_NORMALIZATION()'
/usr/bin/ld: register.cc:(.text+0x540): undefined reference to `tflite::ops::builtin::Register_LOCAL_RESPONSE_NORMALIZATION()'
/usr/bin/ld: register.cc:(.text+0x55a): undefined reference to `tflite::ops::builtin::Register_LSTM()'
/usr/bin/ld: register.cc:(.text+0x57a): undefined reference to `tflite::ops::builtin::Register_BIDIRECTIONAL_SEQUENCE_LSTM()'
/usr/bin/ld: register.cc:(.text+0x59a): undefined reference to `tflite::ops::builtin::Register_UNIDIRECTIONAL_SEQUENCE_LSTM()'
/usr/bin/ld: register.cc:(.text+0x5ba): undefined reference to `tflite::ops::builtin::Register_PAD()'
/usr/bin/ld: register.cc:(.text+0x5da): undefined reference to `tflite::ops::builtin::Register_PADV2()'
/usr/bin/ld: register.cc:(.text+0x5fa): undefined reference to `tflite::ops::builtin::Register_RESHAPE()'
/usr/bin/ld: register.cc:(.text+0x614): undefined reference to `tflite::ops::builtin::Register_RESIZE_BILINEAR()'
/usr/bin/ld: register.cc:(.text+0x634): undefined reference to `tflite::ops::builtin::Register_RESIZE_NEAREST_NEIGHBOR()'
/usr/bin/ld: register.cc:(.text+0x654): undefined reference to `tflite::ops::builtin::Register_SKIP_GRAM()'
/usr/bin/ld: register.cc:(.text+0x66e): undefined reference to `tflite::ops::builtin::Register_SPACE_TO_DEPTH()'
/usr/bin/ld: register.cc:(.text+0x68e): undefined reference to `tflite::ops::builtin::Register_DEPTH_TO_SPACE()'
/usr/bin/ld: register.cc:(.text+0x6ae): undefined reference to `tflite::ops::builtin::Register_GATHER()'
/usr/bin/ld: register.cc:(.text+0x6ce): undefined reference to `tflite::ops::builtin::Register_TRANSPOSE()'
/usr/bin/ld: register.cc:(.text+0x6ee): undefined reference to `tflite::ops::builtin::Register_MEAN()'
/usr/bin/ld: register.cc:(.text+0x70e): undefined reference to `tflite::ops::builtin::Register_DIV()'
/usr/bin/ld: register.cc:(.text+0x72e): undefined reference to `tflite::ops::builtin::Register_SUB()'
/usr/bin/ld: register.cc:(.text+0x74e): undefined reference to `tflite::ops::builtin::Register_SPLIT()'
/usr/bin/ld: register.cc:(.text+0x76e): undefined reference to `tflite::ops::builtin::Register_SPLIT_V()'
/usr/bin/ld: register.cc:(.text+0x78e): undefined reference to `tflite::ops::builtin::Register_SQUEEZE()'
/usr/bin/ld: register.cc:(.text+0x7ae): undefined reference to `tflite::ops::builtin::Register_STRIDED_SLICE()'
/usr/bin/ld: register.cc:(.text+0x7ce): undefined reference to `tflite::ops::builtin::Register_EXP()'
/usr/bin/ld: register.cc:(.text+0x7e8): undefined reference to `tflite::ops::builtin::Register_TOPK_V2()'
/usr/bin/ld: register.cc:(.text+0x808): undefined reference to `tflite::ops::builtin::Register_LOG()'
/usr/bin/ld: register.cc:(.text+0x822): undefined reference to `tflite::ops::builtin::Register_LOG_SOFTMAX()'
/usr/bin/ld: register.cc:(.text+0x842): undefined reference to `tflite::ops::builtin::Register_CAST()'
/usr/bin/ld: register.cc:(.text+0x862): undefined reference to `tflite::ops::builtin::Register_DEQUANTIZE()'
/usr/bin/ld: register.cc:(.text+0x882): undefined reference to `tflite::ops::builtin::Register_PRELU()'
/usr/bin/ld: register.cc:(.text+0x89c): undefined reference to `tflite::ops::builtin::Register_MAXIMUM()'
/usr/bin/ld: register.cc:(.text+0x8bc): undefined reference to `tflite::ops::builtin::Register_MINIMUM()'
/usr/bin/ld: register.cc:(.text+0x8dc): undefined reference to `tflite::ops::builtin::Register_ARG_MAX()'
/usr/bin/ld: register.cc:(.text+0x8fc): undefined reference to `tflite::ops::builtin::Register_ARG_MIN()'
/usr/bin/ld: register.cc:(.text+0x91c): undefined reference to `tflite::ops::builtin::Register_GREATER()'
/usr/bin/ld: register.cc:(.text+0x93c): undefined reference to `tflite::ops::builtin::Register_GREATER_EQUAL()'
/usr/bin/ld: register.cc:(.text+0x95c): undefined reference to `tflite::ops::builtin::Register_LESS()'
/usr/bin/ld: register.cc:(.text+0x97c): undefined reference to `tflite::ops::builtin::Register_LESS_EQUAL()'
/usr/bin/ld: register.cc:(.text+0x99c): undefined reference to `tflite::ops::builtin::Register_FLOOR()'
/usr/bin/ld: register.cc:(.text+0x9b6): undefined reference to `tflite::ops::builtin::Register_CEIL()'
/usr/bin/ld: register.cc:(.text+0x9d0): undefined reference to `tflite::ops::builtin::Register_ROUND()'
/usr/bin/ld: register.cc:(.text+0x9ea): undefined reference to `tflite::ops::builtin::Register_NEG()'
/usr/bin/ld: register.cc:(.text+0xa04): undefined reference to `tflite::ops::builtin::Register_SELECT()'
/usr/bin/ld: register.cc:(.text+0xa24): undefined reference to `tflite::ops::builtin::Register_SELECT_V2()'
/usr/bin/ld: register.cc:(.text+0xa3e): undefined reference to `tflite::ops::builtin::Register_SLICE()'
/usr/bin/ld: register.cc:(.text+0xa5e): undefined reference to `tflite::ops::builtin::Register_SIN()'
/usr/bin/ld: register.cc:(.text+0xa78): undefined reference to `tflite::ops::builtin::Register_COS()'
/usr/bin/ld: register.cc:(.text+0xa92): undefined reference to `tflite::ops::builtin::Register_TRANSPOSE_CONV()'
/usr/bin/ld: register.cc:(.text+0xab2): undefined reference to `tflite::ops::builtin::Register_TILE()'
/usr/bin/ld: register.cc:(.text+0xad2): undefined reference to `tflite::ops::builtin::Register_SUM()'
/usr/bin/ld: register.cc:(.text+0xaf2): undefined reference to `tflite::ops::builtin::Register_REDUCE_PROD()'
/usr/bin/ld: register.cc:(.text+0xb12): undefined reference to `tflite::ops::builtin::Register_REDUCE_MAX()'
/usr/bin/ld: register.cc:(.text+0xb32): undefined reference to `tflite::ops::builtin::Register_REDUCE_MIN()'
/usr/bin/ld: register.cc:(.text+0xb52): undefined reference to `tflite::ops::builtin::Register_REDUCE_ANY()'
/usr/bin/ld: register.cc:(.text+0xb6c): undefined reference to `tflite::ops::builtin::Register_REDUCE_ALL()'
/usr/bin/ld: register.cc:(.text+0xb86): undefined reference to `tflite::ops::builtin::Register_EXPAND_DIMS()'
/usr/bin/ld: register.cc:(.text+0xba0): undefined reference to `tflite::ops::builtin::Register_SPARSE_TO_DENSE()'
/usr/bin/ld: register.cc:(.text+0xbc0): undefined reference to `tflite::ops::builtin::Register_EQUAL()'
/usr/bin/ld: register.cc:(.text+0xbe0): undefined reference to `tflite::ops::builtin::Register_NOT_EQUAL()'
/usr/bin/ld: register.cc:(.text+0xc00): undefined reference to `tflite::ops::builtin::Register_SQRT()'
/usr/bin/ld: register.cc:(.text+0xc1a): undefined reference to `tflite::ops::builtin::Register_RSQRT()'
/usr/bin/ld: register.cc:(.text+0xc3a): undefined reference to `tflite::ops::builtin::Register_SHAPE()'
/usr/bin/ld: register.cc:(.text+0xc54): undefined reference to `tflite::ops::builtin::Register_RANK()'
/usr/bin/ld: register.cc:(.text+0xc6e): undefined reference to `tflite::ops::builtin::Register_POW()'
/usr/bin/ld: register.cc:(.text+0xc88): undefined reference to `tflite::ops::builtin::Register_FAKE_QUANT()'
/usr/bin/ld: register.cc:(.text+0xca8): undefined reference to `tflite::ops::builtin::Register_PACK()'
/usr/bin/ld: register.cc:(.text+0xcc8): undefined reference to `tflite::ops::builtin::Register_ONE_HOT()'
/usr/bin/ld: register.cc:(.text+0xce2): undefined reference to `tflite::ops::builtin::Register_LOGICAL_OR()'
/usr/bin/ld: register.cc:(.text+0xcfc): undefined reference to `tflite::ops::builtin::Register_LOGICAL_AND()'
/usr/bin/ld: register.cc:(.text+0xd16): undefined reference to `tflite::ops::builtin::Register_LOGICAL_NOT()'
/usr/bin/ld: register.cc:(.text+0xd30): undefined reference to `tflite::ops::builtin::Register_UNPACK()'
/usr/bin/ld: register.cc:(.text+0xd50): undefined reference to `tflite::ops::builtin::Register_FLOOR_DIV()'
/usr/bin/ld: register.cc:(.text+0xd70): undefined reference to `tflite::ops::builtin::Register_SQUARE()'
/usr/bin/ld: register.cc:(.text+0xd8a): undefined reference to `tflite::ops::builtin::Register_ZEROS_LIKE()'
/usr/bin/ld: register.cc:(.text+0xda4): undefined reference to `tflite::ops::builtin::Register_FLOOR_MOD()'
/usr/bin/ld: register.cc:(.text+0xdbe): undefined reference to `tflite::ops::builtin::Register_RANGE()'
/usr/bin/ld: register.cc:(.text+0xdd8): undefined reference to `tflite::ops::builtin::Register_LEAKY_RELU()'
/usr/bin/ld: register.cc:(.text+0xdf8): undefined reference to `tflite::ops::builtin::Register_SQUARED_DIFFERENCE()'
/usr/bin/ld: register.cc:(.text+0xe18): undefined reference to `tflite::ops::builtin::Register_FILL()'
/usr/bin/ld: register.cc:(.text+0xe38): undefined reference to `tflite::ops::builtin::Register_MIRROR_PAD()'
/usr/bin/ld: register.cc:(.text+0xe58): undefined reference to `tflite::ops::builtin::Register_UNIQUE()'
/usr/bin/ld: register.cc:(.text+0xe72): undefined reference to `tflite::ops::builtin::Register_REVERSE_V2()'
/usr/bin/ld: register.cc:(.text+0xe92): undefined reference to `tflite::ops::builtin::Register_ADD_N()'
/usr/bin/ld: register.cc:(.text+0xeac): undefined reference to `tflite::ops::builtin::Register_GATHER_ND()'
/usr/bin/ld: register.cc:(.text+0xecc): undefined reference to `tflite::ops::builtin::Register_WHERE()'
/usr/bin/ld: register.cc:(.text+0xeec): undefined reference to `tflite::ops::builtin::Register_ELU()'
/usr/bin/ld: register.cc:(.text+0xf06): undefined reference to `tflite::ops::builtin::Register_REVERSE_SEQUENCE()'
/usr/bin/ld: register.cc:(.text+0xf20): undefined reference to `tflite::ops::builtin::Register_MATRIX_DIAG()'
/usr/bin/ld: register.cc:(.text+0xf3a): undefined reference to `tflite::ops::builtin::Register_QUANTIZE()'
/usr/bin/ld: register.cc:(.text+0xf5a): undefined reference to `tflite::ops::builtin::Register_MATRIX_SET_DIAG()'
/usr/bin/ld: register.cc:(.text+0xf74): undefined reference to `tflite::ops::builtin::Register_IF()'
/usr/bin/ld: register.cc:(.text+0xf8e): undefined reference to `tflite::ops::builtin::Register_WHILE()'
/usr/bin/ld: register.cc:(.text+0xfa8): undefined reference to `tflite::ops::builtin::Register_NON_MAX_SUPPRESSION_V4()'
/usr/bin/ld: register.cc:(.text+0xfc2): undefined reference to `tflite::ops::builtin::Register_NON_MAX_SUPPRESSION_V5()'
/usr/bin/ld: register.cc:(.text+0xfdc): undefined reference to `tflite::ops::builtin::Register_SCATTER_ND()'
/usr/bin/ld: register.cc:(.text+0xff6): undefined reference to `tflite::ops::builtin::Register_DENSIFY()'
/usr/bin/ld: register.cc:(.text+0x1010): undefined reference to `tflite::ops::builtin::Register_SEGMENT_SUM()'
/usr/bin/ld: register.cc:(.text+0x102a): undefined reference to `tflite::ops::builtin::Register_BATCH_MATMUL()'
/usr/bin/ld: register.cc:(.text+0x104a): undefined reference to `tflite::ops::builtin::Register_CUMSUM()'
/usr/bin/ld: register.cc:(.text+0x1064): undefined reference to `tflite::ops::builtin::Register_BROADCAST_TO()'
/usr/bin/ld: register.cc:(.text+0x1084): undefined reference to `tflite::ops::builtin::Register_CALL_ONCE()'
/usr/bin/ld: register.cc:(.text+0x109e): undefined reference to `tflite::ops::builtin::Register_RFFT2D()'
/usr/bin/ld: register.cc:(.text+0x10b8): undefined reference to `tflite::ops::builtin::Register_CONV_3D()'
/usr/bin/ld: register.cc:(.text+0x10d2): undefined reference to `tflite::ops::builtin::Register_IMAG()'
/usr/bin/ld: register.cc:(.text+0x10ec): undefined reference to `tflite::ops::builtin::Register_REAL()'
/usr/bin/ld: register.cc:(.text+0x1106): undefined reference to `tflite::ops::builtin::Register_COMPLEX_ABS()'
/usr/bin/ld: register.cc:(.text+0x1120): undefined reference to `tflite::ops::builtin::Register_BROADCAST_ARGS()'
/usr/bin/ld: register.cc:(.text+0x113a): undefined reference to `tflite::ops::builtin::Register_HASHTABLE()'
/usr/bin/ld: register.cc:(.text+0x1154): undefined reference to `tflite::ops::builtin::Register_HASHTABLE_FIND()'
/usr/bin/ld: register.cc:(.text+0x116e): undefined reference to `tflite::ops::builtin::Register_HASHTABLE_IMPORT()'
/usr/bin/ld: register.cc:(.text+0x1188): undefined reference to `tflite::ops::builtin::Register_HASHTABLE_SIZE()'
/usr/bin/ld: register.cc:(.text+0x11a2): undefined reference to `tflite::ops::builtin::Register_CONV_3D_TRANSPOSE()'
/usr/bin/ld: register.cc:(.text+0x11bc): undefined reference to `tflite::ops::builtin::Register_VAR_HANDLE()'
/usr/bin/ld: register.cc:(.text+0x11d6): undefined reference to `tflite::ops::builtin::Register_READ_VARIABLE()'
/usr/bin/ld: register.cc:(.text+0x11f0): undefined reference to `tflite::ops::builtin::Register_ASSIGN_VARIABLE()'
/usr/bin/ld: register.cc:(.text+0x120a): undefined reference to `tflite::ops::builtin::Register_MULTINOMIAL()'
/usr/bin/ld: register.cc:(.text+0x1224): undefined reference to `tflite::ops::builtin::Register_RANDOM_STANDARD_NORMAL()'
/usr/bin/ld: register.cc:(.text+0x123e): undefined reference to `tflite::ops::builtin::Register_BUCKETIZE()'
/usr/bin/ld: register.cc:(.text+0x1258): undefined reference to `tflite::ops::builtin::Register_RANDOM_UNIFORM()'
/usr/bin/ld: register.cc:(.text+0x1272): undefined reference to `tflite::ops::builtin::Register_GELU()'
/usr/bin/ld: register.cc:(.text+0x1292): undefined reference to `tflite::ops::builtin::Register_DYNAMIC_UPDATE_SLICE()'
/usr/bin/ld: register.cc:(.text+0x12ac): undefined reference to `tflite::ops::builtin::Register_UNSORTED_SEGMENT_PROD()'
/usr/bin/ld: register.cc:(.text+0x12c6): undefined reference to `tflite::ops::builtin::Register_UNSORTED_SEGMENT_MAX()'
/usr/bin/ld: register.cc:(.text+0x12e0): undefined reference to `tflite::ops::builtin::Register_UNSORTED_SEGMENT_MIN()'
/usr/bin/ld: register.cc:(.text+0x12fa): undefined reference to `tflite::ops::builtin::Register_UNSORTED_SEGMENT_SUM()'
/usr/bin/ld: register.cc:(.text+0x1314): undefined reference to `tflite::ops::builtin::Register_ATAN2()'
/usr/bin/ld: register.cc:(.text+0x132e): undefined reference to `tflite::ops::builtin::Register_SIGN()'
/usr/bin/ld: register.cc:(.text+0x134e): undefined reference to `tflite::ops::custom::Register_NUMERIC_VERIFY()'
/usr/bin/ld: register.cc:(.text+0x136a): undefined reference to `tflite::ops::custom::Register_MFCC()'
/usr/bin/ld: register.cc:(.text+0x1386): undefined reference to `tflite::ops::custom::Register_AUDIO_SPECTROGRAM()'
/usr/bin/ld: register.cc:(.text+0x13a2): undefined reference to `tflite::ops::custom::Register_DETECTION_POSTPROCESS()'
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(xnnpack_delegate.cc.o): in function `TfLiteXNNPackDelegateCreateWithThreadpool':
xnnpack_delegate.cc:(.text+0xc74f): undefined reference to `tflite::CpuBackendContext::GetFromContext(TfLiteContext*)'
/usr/bin/ld: xnnpack_delegate.cc:(.text+0xc757): undefined reference to `tflite::CpuBackendContext::get_xnnpack_threadpool()'
collect2: error: ld returned 1 exit status
gmake[2]: *** [CMakeFiles/minimal.dir/build.make:179: minimal] Error 1
gmake[1]: *** [CMakeFiles/Makefile2:1251: CMakeFiles/minimal.dir/all] Error 2
gmake: *** [Makefile:136: all] Error 2

,,,"
59535,Fails to convert SplitV with quantization,"### 1. System information

- Ubuntu20.04
- TensorFlow installation : conda
- TensorFlow library 2.8.1

### 2. Code

When converting a model from Tensorflow to TFlite, I run into errors (regarding SplitV, check error below) when setting the pipeline with Int8 quantization, but not when using regular conversion (without quantization).

I basically follow those instructions:

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_quant_model = converter.convert()
```

I get the following error:

`There are unresolved custom ops: [SplitV]Encountered unresolved custom op: SplitV.`

It works fine if I add the SELECT_TF_OPS, but I want to keep the interpreter as small and fast as possible.
So my assumption is that the SplitV operator in TFlite is not available with the Int8 quantization?
How can I know beforehand which operator is available with the Int8 quantization?
"
59531,"Fail to export model created with audio_classifier.create()  on windows 10, Python 3.9.16","Hi there! 
Invoking the kind support of brightest minds! 

I´m trying to export the model created with audio_classifier.create() function, in  an audio classification model using the TensorFlow Lite Model Maker library, taken from the online tutorial. 

The code runs ok, identefoies the audio but if fails to export the model. 

### 1. System information
- OS: windows 10
- TensorFlow 2.9.0 installed via PIP
- Tflite-model-maker 0.4.2 installed from source : 
         git clone https://github.com/tensorflow/examples
        cd examples/tensorflow_examples/lite/model_maker/pip_package
        pip install -e .

### 2. Code

#!/usr/bin/env python
# coding: utf-8
# ##### Copyright 2021 The TensorFlow Authors.
import tensorflow as tf
import tflite_model_maker as mm
from tflite_model_maker import audio_classifier
import os

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import itertools
import glob
import random
from pathlib import Path
from IPython.display import Audio, Image, display
from scipy.io import wavfile

from tensorflow.keras import models

print(f""TensorFlow Version: {tf.__version__}"")
print(f""Model Maker Version: {mm.__version__}"")

data_dir = './dataset/urbanbirds'

bird_code_to_name = {
  'anupreto': 'Anu preto',
  'bemtevi': 'Bemtevi',
  'ticotico': 'Tico tico',  
  'corruira': 'Corruira',
  'sabia': ""Sabia laranjeira"",   
}

birds_images = {
  'anupreto': 'http://myserver/public/img/Anupreto.jpg', 
  'bemtevi': 'http://myserver/public/img/Bemtevi.jpg', 
  'ticotico': 'http://myserver/public/img/Ticotico.jpg', 
  'corruira': 'http://myserver/public/img/Corruira.jpg', 
  'sabia': 'http://myserver/public/img/Sabia_Laranjeira.jpg', 
}

test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))
base_dir = Path().resolve()
print(f'=== BASE PATH: {base_dir}')

def get_random_audio_file():
  test_list = glob.glob(test_files)
  random_audio_path = random.choice(test_list)
  return random_audio_path


def show_bird_data(audio_path):
  print('PATH: ' + audio_path)
  sample_rate, audio_data = wavfile.read(audio_path, 'rb')
  #bird_code = audio_path.split('/')[-2]
  bird_code = Path(audio_path).stem
  print('Birdcode: ' + bird_code)
  print(f'Bird name: {bird_code_to_name[bird_code]}')
  print(f'Bird code: {bird_code}')
  display(Image(birds_images[bird_code]))

  plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'
  plt.title(plttitle)
  plt.plot(audio_data)
  display(Audio(audio_data, rate=sample_rate))

print('functions and data structures created')

spec = audio_classifier.YamNetSpec(
    keep_yamnet_and_custom_heads=True,
    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,
    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)


train_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'train'), cache=True)
train_data, validation_data = train_data.split(0.8)
test_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'test'), cache=True)


batch_size = 4
#epochs = 100
epochs = 50

print('Training the model')
model = audio_classifier.create(
    train_data,
    spec,
    validation_data,
    batch_size=batch_size,
    epochs=epochs)


print('Evaluating the model')
model.evaluate(test_data)


def show_confusion_matrix(confusion, test_labels):
  """"""Compute confusion matrix and normalize.""""""
  confusion_normalized = confusion.astype(""float"") / confusion.sum(axis=1)
  axis_labels = test_labels
  ax = sns.heatmap(
      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,
      cmap='Blues', annot=True, fmt='.2f', square=True)
  plt.title(""Confusion matrix"")
  plt.ylabel(""True label"")
  plt.xlabel(""Predicted label"")

confusion_matrix = model.confusion_matrix(test_data)
show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)


serving_model = model.create_serving_model()

print(f'------->>> Model\'s input shape and type: {serving_model.inputs}')
print(f'------->>> Model\'s output shape and type: {serving_model.outputs}')


#random_audio = get_random_audio_file()
random_audio = 'C:/ONEDRIVE/WEBDEV/Phyton/SOUND MNGT/BirdFinder_TFmodelMaker/dataset/urbanbirds/test/sabia/sabia.wav'

show_bird_data(random_audio)

sample_rate, audio_data = wavfile.read(random_audio, 'rb')

audio_data = np.array(audio_data) / tf.int16.max
input_size = serving_model.input_shape[1]

splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)

print(f'Test audio path: {random_audio}')
print(f'Original size of the audio data: {len(audio_data)}')
print(f'Number of windows for inference: {len(splitted_audio_data)}')

print(random_audio)

results = []
print('Result of the window ith:  your model class -> score,  (spec class -> score)')
for i, data in enumerate(splitted_audio_data):
  yamnet_output, inference = serving_model(data)
  results.append(inference[0].numpy())
  result_index = tf.argmax(inference[0])
  spec_result_index = tf.argmax(yamnet_output[0])
  t = spec._yamnet_labels()[spec_result_index]
  result_str = f'Result of the window {i}: '   f'\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, '   f'\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'
  print(result_str)


results_np = np.array(results)
mean_results = results_np.mean(axis=0)
result_index = mean_results.argmax()
print(f'======= Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')


print(f'======= EXPROTING MODEL ==========')

# ##>>>>>>>>>>>> Exporting the model
models_path = './birds_models'
print(f'>>>>>>>Exporting the TFLite model to {models_path}')
model.export(models_path, tflite_filename='my_birds_model.tflite')

### 3. Failure after conversion
Follows the output messages:
======= EXPROTING MODEL ==========
Exporing the TFLite model to ./birds_models
INFO:tensorflow:Assets written to: C:\Users\cacer\AppData\Local\Temp\tmpgfe99jhn\assets
INFO:tensorflow:Assets written to: C:\Users\cacer\AppData\Local\Temp\tmpgfe99jhn\assets

2023-02-02 15:59:51.915885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-02-02 15:59:51.916566: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-02-02 15:59:52.180655: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-02 16:00:22.148077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2023-02-02 16:00:22.148617: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-02 16:00:22.151299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: BEOWULFII
2023-02-02 16:00:22.151513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: BEOWULFII
2023-02-02 16:00:22.154293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 16:01:34.292489: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 16:01:34.296911: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-02 16:01:35.435869: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 16:01:35.435906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 16:01:36.112261: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 142.195 M  ops, equivalently 71.097 M  MACs

2023-02-02 16:20:46.025719: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 16:20:46.029231: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-02 16:20:48.424157: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 16:20:48.424274: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 16:20:50.006278: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 142.195 M  ops, equivalently 71.097 M  MACs

2023-02-02 17:16:12.217821: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 17:16:12.221225: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session

2023-02-02 15:59:51.915885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-02-02 15:59:51.916566: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-02-02 15:59:52.180655: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-02 16:00:22.148077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2023-02-02 16:00:22.148617: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-02 16:00:22.151299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: BEOWULFII
2023-02-02 16:00:22.151513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: BEOWULFII
2023-02-02 16:00:22.154293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 16:01:34.292489: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 16:01:34.296911: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-02 16:01:35.435869: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 16:01:35.435906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 16:01:36.112261: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 142.195 M  ops, equivalently 71.097 M  MACs

2023-02-02 16:20:46.025719: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 16:20:46.029231: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-02 16:20:48.424157: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 16:20:48.424274: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 16:20:50.006278: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1972] Estimated count of arithmetic ops: 142.195 M  ops, equivalently 71.097 M  MACs

2023-02-02 17:16:12.217821: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-02-02 17:16:12.221225: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2023-02-02 17:16:13.721493: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 17:16:13.721548: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
Traceback (most recent call last):

  Cell In[4], line 1
    runfile('C:/ONEDRIVE/WEBDEV/Phyton/SOUND MNGT/BirdFinder_TFmodelMaker/BirdModelMakerv2.py', wdir='C:/ONEDRIVE/WEBDEV/Phyton/SOUND MNGT/BirdFinder_TFmodelMaker')

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\debugpy\_vendored\pydevd\_pydev_bundle\pydev_umd.py:167 in runfile
    execfile(filename, namespace)

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\debugpy\_vendored\pydevd\_pydev_imps\_pydev_execfile.py:25 in execfile
    exec(compile(contents + ""\n"", file, 'exec'), glob, loc)

  File C:/ONEDRIVE/WEBDEV/Phyton/SOUND MNGT/BirdFinder_TFmodelMaker/BirdModelMakerv2.py:167
    model.export(models_path, tflite_filename='my_birds_model.tflite')

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\custom_model.py:132 in export
    self._export_tflite(tflite_filepath, **export_tflite_kwargs)

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py:82 in _export_tflite
    self.model_spec.export_tflite(self.model, tflite_filepath, with_metadata,

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py:630 in export_tflite
    model_util.export_tflite(

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_util.py:166 in export_tflite
    tflite_model = converter.convert()

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow\lite\python\lite.py:929 in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow\lite\python\lite.py:921 in _convert_and_export_metrics
    return flatbuffer_utils.convert_object_to_bytearray(model_object)

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow\lite\tools\flatbuffer_utils.py:84 in convert_object_to_bytearray
    model_offset = model_object.Pack(builder)

  File C:\DEV\Anaconda3\envs\P39\lib\site-packages\tensorflow\lite\python\schema_py_generated.py:5951 in Pack
    operatorCodes = builder.EndVector(len(self.operatorCodes))

TypeError: EndVector() takes 1 positional argument but 2 were given

--- from here, follows some warnings about cuda (which doestn makes sense because I´m using a laptop)


Thanks in advance for any light on this!

"
59530,Status of int8 dot/conv with XLA+CUDA,"Are tensorcore-accelerated int8 dot/convs with XLA accessible via tensorflow APIs? https://github.com/tensorflow/tensorflow/pull/30771 suggests that int8 convs are supported, and https://github.com/tensorflow/tensorflow/issues/49140 also suggest that this functionality exists. However, the tensorflow convolution operations don't allow int8 arguments. The matmul operator allows int8 x int8 -> int32 via the output_type argument, and I've successfully compiled with XLA using this option. However, when doing this, I'm not getting the throughput that I'd expect with int8 tensorcores. I was expecting something approaching 2x throughput compared to fp16 for large matmuls ([8192, 8192] inputs). Admittedly my benchmarking is not very rigorous, but I'd be curious to know if my expectation of ~2x throughput is reasonable and whether XLA has the functionality to facilitate it.

Here's a minimal implementation of an XLA-compiled int8 matmul:
```python
@tf.function(jit_compile=True)
def int8_matmul(x, w):
    return tf.matmul(x, w, output_type=tf.int32)


x = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
w = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
y = int8_matmul(x, w)

print(int8_matmul.experimental_get_compiler_ir(x, w)(stage=""optimized_hlo""))
# HloModule a_inference_int8_matmul_17__.10, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0})->s32[2048,2048]{1,0}}

# ENTRY %a_inference_int8_matmul_17__.10 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048]) -> s32[2048,2048] {
#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=""XLA_Args""}
#   ROOT %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=""__cublas$gemm"", metadata={op_type=""BatchMatMulV3"" op_name=""MatMul"" source_file=""int8_xla.py"" source_line=22}, backend_config=""{\""alpha_real\"":1,\""alpha_imag\"":0,\""beta\"":0,\""dot_dimension_numbers\"":{\""lhs_contracting_dimensions\"":[\""1\""],\""rhs_contracting_dimensions\"":[\""0\""],\""lhs_batch_dimensions\"":[],\""rhs_batch_dimensions\"":[]},\""precision_config\"":{\""operand_precision\"":[\""DEFAULT\"",\""DEFAULT\""]},\""epilogue\"":\""DEFAULT\""}""
# }
```

And here's my attempt at implementing a 'canonical' quantized linear layer.  I think this is a fairly standard sequence of operations for a quantized layer, and I believe TensorRT would be able to fuse this entire operation.
```python
@tf.function(jit_compile=True)
def int8_linear_layer(x, w, b, s):
    # read in x and w as pre-quantized int8 tensors
    y = tf.matmul(x, w, output_type=tf.int32)

    # add bias and apply activation in fp32
    y = tf.cast(y, tf.float32)
    y = y + b
    y = tf.nn.relu(y)

    # quantize and store output as int8
    y = tf.round(y / s)
    y = tf.clip_by_value(y, -128, 127)
    y = tf.cast(y, tf.int8)
    return y


x = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
w = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
b = tf.random.normal([2048], dtype=tf.float32)  # bias
s = tf.random.normal([], dtype=tf.float32)  # per-tensor quantization scale for output activation
y = int8_linear_layer(x, w, b, s)

print(int8_linear_layer.experimental_get_compiler_ir(x, w, b, s)(stage=""optimized_hlo""))
# HloModule a_inference_int8_linear_layer_65__.32, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0},f32[2048]{0},f32[])->s8[2048,2048]{1,0}}

# %fused_computation (param_0.2: f32[], param_1.4: f32[2048], param_2.7: s32[2048,2048]) -> s8[2048,2048] {
#   %constant_2 = f32[] constant(-128), metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   %broadcast.4 = f32[2048,2048]{1,0} broadcast(f32[] %constant_2), dimensions={}, metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   %param_2.7 = s32[2048,2048]{1,0} parameter(2)
#   %convert.1 = f32[2048,2048]{1,0} convert(s32[2048,2048]{1,0} %param_2.7), metadata={op_type=""Cast"" op_name=""Cast"" source_file=""int8_xla.py"" source_line=46}
#   %param_1.4 = f32[2048]{0} parameter(1)
#   %broadcast.3 = f32[2048,2048]{1,0} broadcast(f32[2048]{0} %param_1.4), dimensions={1}, metadata={op_type=""AddV2"" op_name=""add"" source_file=""int8_xla.py"" source_line=47}
#   %add.0 = f32[2048,2048]{1,0} add(f32[2048,2048]{1,0} %convert.1, f32[2048,2048]{1,0} %broadcast.3), metadata={op_type=""AddV2"" op_name=""add"" source_file=""int8_xla.py"" source_line=47}
#   %constant_1 = f32[] constant(0), metadata={op_type=""Relu"" op_name=""Relu"" source_file=""int8_xla.py"" source_line=48}
#   %broadcast.2 = f32[2048,2048]{1,0} broadcast(f32[] %constant_1), dimensions={}, metadata={op_type=""Relu"" op_name=""Relu""}
#   %maximum.0 = f32[2048,2048]{1,0} maximum(f32[2048,2048]{1,0} %add.0, f32[2048,2048]{1,0} %broadcast.2), metadata={op_type=""Relu"" op_name=""Relu""}
#   %param_0.2 = f32[] parameter(0)
#   %broadcast.1 = f32[2048,2048]{1,0} broadcast(f32[] %param_0.2), dimensions={}, metadata={op_type=""RealDiv"" op_name=""truediv"" source_file=""int8_xla.py"" source_line=51}
#   %divide.0 = f32[2048,2048]{1,0} divide(f32[2048,2048]{1,0} %maximum.0, f32[2048,2048]{1,0} %broadcast.1), metadata={op_type=""RealDiv"" op_name=""truediv"" source_file=""int8_xla.py"" source_line=51}
#   %round-nearest-even.0 = f32[2048,2048]{1,0} round-nearest-even(f32[2048,2048]{1,0} %divide.0), metadata={op_type=""Round"" op_name=""Round"" source_file=""int8_xla.py"" source_line=51}
#   %constant_0 = f32[] constant(127), metadata={op_type=""Minimum"" op_name=""clip_by_value/Minimum"" source_file=""int8_xla.py"" source_line=52}
#   %broadcast.0 = f32[2048,2048]{1,0} broadcast(f32[] %constant_0), dimensions={}, metadata={op_type=""Minimum"" op_name=""clip_by_value/Minimum"" source_file=""int8_xla.py"" source_line=52}
#   %clamp.1 = f32[2048,2048]{1,0} clamp(f32[2048,2048]{1,0} %broadcast.4, f32[2048,2048]{1,0} %round-nearest-even.0, f32[2048,2048]{1,0} %broadcast.0), metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   ROOT %convert.0 = s8[2048,2048]{1,0} convert(f32[2048,2048]{1,0} %clamp.1), metadata={op_type=""Cast"" op_name=""Cast_1"" source_file=""int8_xla.py"" source_line=53}
# }

# ENTRY %a_inference_int8_linear_layer_65__.32 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048], arg2.3: f32[2048], arg3.4: f32[]) -> s8[2048,2048] {
#   %arg3.4 = f32[] parameter(3), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg2.3 = f32[2048]{0} parameter(2), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=""XLA_Args""}
#   %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=""__cublas$gemm"", metadata={op_type=""BatchMatMulV3"" op_name=""MatMul"" source_file=""int8_xla.py"" source_line=43}, backend_config=""{\""alpha_real\"":1,\""alpha_imag\"":0,\""beta\"":0,\""dot_dimension_numbers\"":{\""lhs_contracting_dimensions\"":[\""1\""],\""rhs_contracting_dimensions\"":[\""0\""],\""lhs_batch_dimensions\"":[],\""rhs_batch_dimensions\"":[]},\""precision_config\"":{\""operand_precision\"":[\""DEFAULT\"",\""DEFAULT\""]},\""epilogue\"":\""DEFAULT\""}""
#   ROOT %fusion = s8[2048,2048]{1,0} fusion(f32[] %arg3.4, f32[2048]{0} %arg2.3, s32[2048,2048]{1,0} %cublas-gemm.1), kind=kLoop, calls=%fused_computation, metadata={op_type=""Cast"" op_name=""Cast_1"" source_file=""int8_xla.py"" source_line=53}
# }
```

System info: Ubuntu 20.04.5 LTS, TF 2.11.0 via pip, A100 GPU, CUDA Version 12.0"
59525,"ConvertFusedBatchNorm returns uninitialized value when data_format = ""NDHWC""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v1.12.1-88697-g620bee79ab3 2.12.0-dev20230201

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.10

### Bazel version

5.3.0

### GCC/Compiler version

gcc-11

### CUDA/cuDNN version

CUDA-11.8/cudnn-8.7.0/TensorRT-8.5.3

### GPU model and memory

RTX3090

### Current Behaviour?

```shell
See code snippet:
https://github.com/tensorflow/tensorflow/blob/4aec415b3f06b19c380d1a0ca92cc2de0d74cc21/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L4399-L4436

In the case of NDHWC layout (triggered by the code below) an uninitialized value is returned from ConvertFusedBatchNorm which causes an exception to be raised.

I would expect it to build correctly. Changing ConvertFusedBatchNorm to do the same thing for NDHWC as for NHWC gets rid of the crash, but I don't know if this is correct.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import (
    BatchNormalization,
    Conv3D,
    Dense,
    Flatten,
    Input,
)
from tensorflow.keras.models import Model
from tensorflow.python.compiler.tensorrt import trt_convert as trt

inputs = Input(shape=(24, 24, 64, 1), name=""x"")
x = inputs
x = Conv3D(16, (3, 3, 3), activation=""relu"", padding=""same"")(x)
x = BatchNormalization()(x)
x = Flatten()(x)
x = Dense(128, activation=""relu"")(x)
x = Dense(128)(x)
m = Model(inputs=[inputs], outputs=[x])

m.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model_dir = ""/tmp/model""
tf.keras.models.save_model(m, model_dir)

converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir,
                        precision_mode=trt.TrtPrecisionMode.FP16)

trt_func = converter.convert()

def input_fn():
    a = np.random.rand(1024, 24, 24, 64, 1).astype(np.float32)
    yield [a]

converter.build(input_fn=input_fn)
```


### Relevant log output

```shell
2023-02-02 11:32:14.336729: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1104] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.
```
</details>"
59522,Closing this as spam. Will be reopened once the template is updated.,"Closing this as spam. Will be reopened once the template is updated.

_Originally posted by @pjpratik in https://github.com/tensorflow/tensorflow/issues/59263#issuecomment-1383962053_"
59521,Tensor flow lite wheel file for ARM9 32-bit architecture ,"i am trying to build .whl file for ARM9 32-bit architecture using Bazel, but fortuanetly there is no tool to build for 32-bit. can someone help me in getting my work done."
59519,"On MTK platforms, tflite calls the GPU with clGetPlatformIDs return -1001","**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible): Redmi/rembrandt/rembrandt:13/TP1A.220624.014/23.1.31:user/release-keys

- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 29
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): 

**Standalone code to reproduce the issue**

on MTK platforms
code:
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {
            LOGE(""delegate init failed!"");
            exit(-1);
        }

get:
clGetPlatformIDs returned -1001 or clGetPlatformIDs returned -30


**Any other info / logs**
This MTK platform has a GPU device.

"
59518,"ConvertFusedBatchNorm returns uninitialized value when data_format = ""NDHWC""","https://github.com/tensorflow/tensorflow/blob/4aec415b3f06b19c380d1a0ca92cc2de0d74cc21/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L4399-L4436

I was trying to optimize my model with tensorrt following this guide: https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html but I was getting an error in the log:

`2023-02-02 11:32:14.336729: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1104] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.`

I added some print statements and recompiled tensorflow leading me to ConvertFusedBatchNorm which clearly returns an uninitialized value when invoked with NDHWC input"
59517,TF to TFLite conversion of model gives error op is neither a custom op nor a flex op,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
import tensorflow as tf
from tensorflow import keras
import pickle
import numpy as np
import sys

print(tf.__version__)
root_dir = ""~/""
saved_model_dir = root_dir+""updatetf/softphy/src/qam_modulator""

def representative_dataset():
  for _ in range(100):
      data = np.random.randint(0, 2, (16200,))
      yield [data.astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]
converter.inference_input_type = tf.int8 # or tf.uint8
converter.inference_output_type = tf.int8 # or tf.uint8

tflite_quant_model = converter.convert()

fileName = 'qammodmodel.tflite'
with open(fileName, 'wb') as f:
     f.write(tflite_quant_model)

```

### 3. Failure after conversion

The conversion fails as below, saved model dir is attached a zip file. I'm not sure what needs to be changed to fix this. Eventually, I'd like to convert the TFLite model to run on TPU. Thanks!


### 5. (optional) Any other info / logs
```
2023-02-02 00:04:34.283275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 00:04:35.515897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-02-02 00:04:35.515975: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-02-02 00:04:39.225887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: li
[qam_modulator.zip](https://github.com/tensorflow/tensorflow/files/10563501/qam_modulator.zip)
bnvinfer.so.7: cannot open shared object file: No such file or directory
2023-02-02 00:04:39.226056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-02-02 00:04:39.226075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2.11.0
2023-02-02 00:04:43.533858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-02-02 00:04:43.539556: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-02 00:04:43.539649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntugsosnow): /proc/driver/nvidia/version does not exist
2023-02-02 00:04:43.541318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 00:04:44.421918: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 00:04:44.421999: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 00:04:44.427118: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.430879: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2023-02-02 00:04:44.430967: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.447508: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-02 00:04:44.448994: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2023-02-02 00:04:44.494916: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.535778: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 111428 microseconds.
2023-02-02 00:04:44.619260: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""HashTableV2:"", ""qam_table""]): error: 'tf.HashTableV2' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.FloorDiv' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.FloorMod' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.AsString' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.ReduceJoin' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.AsString' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.ReduceJoin' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
loc(fused[""HashTableV2:"", ""qam_table""]): error: 'tf.HashTableV2' op is neither a custom op nor a flex op
loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): error: 'tf.LookupTableImportV2' op is neither a custom op nor a flex op
2023-02-02 00:04:44.806180: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2035] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:
Resource ops: HashTableV2, LookupTableFindV2, LookupTableImportV2
Details:
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: AsString, FloorDiv, FloorMod, HashTableV2, LookupTableFindV2, LookupTableImportV2, ReduceJoin
Details:
        tf.AsString(tensor<2025x4xi8>) -> (tensor<2025x4x!tf_type.string>) : {device = """", fill = """", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}
        tf.FloorDiv(tensor<2025x1x1xui8>, tensor<8xui8>) -> (tensor<2025x1x8xui8>) : {device = """"}
        tf.FloorMod(tensor<2025x1x8xi8>, tensor<i8>) -> (tensor<2025x1x8xi8>) : {device = """"}
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
        tf.ReduceJoin(tensor<2025x4x!tf_type.string>, tensor<i32>) -> (tensor<2025x!tf_type.string>) : {device = """", keep_dims = false, separator = """"}

Traceback (most recent call last):
  File ""/home/gsosnow/doc/qammodt2tf.py"", line 38, in <module>
    tflite_quant_model = converter.convert()
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 933, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 911, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1216, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1099, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 808, in convert_saved_model
    data = convert(
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 310, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(fused[""HashTableV2:"", ""qam_table""]): 'tf.HashTableV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""HashTableV2:"", ""qam_table""]): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.FloorDiv' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.FloorMod' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.AsString' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.ReduceJoin' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.AsString' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.ReduceJoin' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(fused[""HashTableV2:"", ""qam_table""]): 'tf.HashTableV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""HashTableV2:"", ""qam_table""]): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.LookupTableImportV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: AsString, FloorDiv, FloorMod, HashTableV2, LookupTableFindV2, LookupTableImportV2, ReduceJoin
Details:
        tf.AsString(tensor<2025x4xi8>) -> (tensor<2025x4x!tf_type.string>) : {device = """", fill = """", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}
        tf.FloorDiv(tensor<2025x1x1xui8>, tensor<8xui8>) -> (tensor<2025x1x8xui8>) : {device = """"}
        tf.FloorMod(tensor<2025x1x8xi8>, tensor<i8>) -> (tensor<2025x1x8xi8>) : {device = """"}
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
        tf.ReduceJoin(tensor<2025x4x!tf_type.string>, tensor<i32>) -> (tensor<2025x!tf_type.string>) : {device = """", keep_dims = false, separator = """"}

```
"
59514,Proto file missing in nightly wheels,"### Have you reproduced the bug with TF nightly?

Yes


### Tensorflow Version

tf 2.12

### Current Behaviour?

I work on the TensorFlow-DirectML plugin, and we use the .proto files included in the TF wheel to generate pb.cc/pb.h files needed by the plugin. One of the files needed is the tensorflow/tsl/profiler/protobuf/xplane.proto file, which has not been included in the nightly wheels so far. Would it be possible to make sure that it's included for TF 2.12?"
59513,module 'numpy' has no attribute 'typeDict',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
While importing the tensorflow
```


### Standalone code to reproduce the issue

```shell
The issue is occured when i was trying to import the tensorflow module
```


### Relevant log output

_No response_</details>"
59512,How to create `tf.keras.layers.Input` without batch_size dimension?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

centos7

### Mobile device

_No response_

### Python version

3.8.14

### Bazel version

5.0

### GCC/Compiler version

10.2

### CUDA/cuDNN version

11.4

### GPU model and memory

A30

### Current Behaviour?

One input of my model has nothing to do with batch_size, for example, its shape is [1,2,3], how to avoid automatically adding 1 dimension when creating  `tf.keras.layers.Input` ? If I manually slice it, the slice operator will be introduced, resulting in a decrease in inference performance.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.keras.layers.Input(shape=(32,64))
# x.shape: (None, 32, 64)
x = x[0, :, :]
# x.shape: (32, 64)

```
"
59511,InvalidArgumentError: Dst node should be assigned to an allowed device. [TPU]," ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Kaggle

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

> InvalidArgumentError: Dst node should be assigned to an allowed device. Found an edge from node concat_variable_140022353296720_handle_inputs_0/shape/_4 assigned to /job:localhost/replica:0/task:0/device:COMPOSITE:0 to node TPUReplicate/_compile/_7103042970398181355/_7 assigned to /job:worker/replica:0/task:0/device:CPU:0

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.experimental import numpy as tnp
import numpy as np

def set_tpu(mixed_precision=True):
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() 
        if mixed_precision:
            keras.mixed_precision.set_global_policy(""mixed_bfloat16"") 
        tf.config.set_soft_device_placement(False)
        strategy = tf.distribute.TPUStrategy(tpu)
        physical_devices = tf.config.list_logical_devices('TPU')
        return (strategy, physical_devices)

    
mxp = False
jit = False
strategy, physical_devices = set_tpu(mixed_precision=mxp)
physical_devices, tf.__version__
```
```python
class CustomModel(keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.val_x = tf.Variable((
            tnp.empty((0, 32), dtype=tf.float32)), shape=[None, 32]
        )
        self.val_gt = tf.Variable(
            tnp.empty((0), dtype=tf.float32), shape=[None]
        )
        self.val_pred = tf.Variable(
            tnp.empty((0, 1), dtype=tf.float32), shape=[None, 1]
        )
        
    def test_step(self, data):
        x, y = data
        y_pred = self(x, training=False)
        self.compiled_loss(y, y_pred, regularization_losses=self.losses)
        self.compiled_metrics.update_state(y, y_pred)
        
        # ATTENTION 
        # Main Cause !!!
        self.val_x.assign(
            tf.concat([self.val_x, x], axis=0)
        )
        self.val_gt.assign(
            tf.concat([self.val_gt, y], axis=0)
        )
        self.val_pred.assign(
            tf.concat([self.val_pred, y_pred], axis=0)
        )
        return {m.name: m.result() for m in self.metrics}
```
```python
with strategy.scope():
    inputs = keras.Input(shape=(32,))
    outputs = keras.layers.Dense(1, dtype='float32')(inputs)
    model = CustomModel(inputs, outputs)
    model.compile(
        optimizer=""adam"", loss=""mse"", metrics=[""mae""],
    )

x = np.random.random((1000, 32))
y = np.random.random((1000))
x_test = np.random.random((10, 32))
y_test = np.random.random((10))

model.fit(
    x, 
    y, 
    epochs=5, 
    validation_data=(x_test, y_test),
    verbose=2, 
    batch_size=4,
)
```

### Relevant log output

> 2023-02-01 11:55:11.789825: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 1206, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1675252511.789496486"",""description"":""Error received from peer ipv4:10.0.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 1206, Output num: 0"",""grpc_status"":3}
Epoch 1/5
2023-02-01 11:55:15.394719: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Dst node should be assigned to an allowed device. Found an edge from node concat_variable_140022353296720_handle_inputs_0/shape/_4 assigned to /job:localhost/replica:0/task:0/device:COMPOSITE:0 to node TPUReplicate/_compile/_7103042970398181355/_7 assigned to /job:worker/replica:0/task:0/device:CPU:0

## Others

- Program works fine in GPU / CPU
- Relevant ticket. https://github.com/keras-team/tf-keras/issues/330"
59510,OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed when using a dataset with tuples,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to create my own transformer and train it. For this purpose, I use dataset to handle my data. The data is created by a code snippet from the tensorflow dataset.from_tensor_slices() method [documentation article](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) . Nevertheless, tensorflow is giving me the following error when I call the fit() method:

> ""OperatorNotAllowedInGraphError: Iterating over a symbolic tf.Tensor is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.""

The used code is reduced significantly just for the purpose of reproducing the issue.

I've also tried passing the data as a dictionary instead of a tuple in the dataset and a couple more things but nothing worked. It seems that I am missing something.
Here is a link to [google colab example](https://colab.research.google.com/drive/1mn6iseJLnJwTmwakYa2XuxszKtR6sV9G#scrollTo=Cj9g0bGN1Fo3)
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

batched_features = tf.constant([[[1, 3], [2, 3]],
                                [[2, 1], [1, 2]],
                                [[3, 3], [3, 2]]], shape=(3, 2, 2))
batched_labels = tf.constant([['A', 'A'],
                              ['B', 'B'],
                              ['A', 'B']], shape=(3, 2, 1))
dataset = tf.data.Dataset.from_tensor_slices((batched_features, batched_labels))
dataset = dataset.batch(1)
for element in dataset.as_numpy_iterator():
  print(element)

class MyTransformer(tf.keras.Model):
    def __init__(self):
        super().__init__()
        
    def call(self, inputs, training):
        print(type(inputs))
        feature, lable = inputs
        return feature

model = MyTransformer()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])

model.fit(dataset , batch_size = 1, epochs = 1)
```


### Relevant log output

_No response_</details>"
59509,list index out of range,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!

I am getting the error that the list index is out of range in the provided code.

2023-02-01 12:50:34.742912: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.792828: W t50ensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File
""D:\pyth42on\lib\site-packages\tensorflow\python\ops\script_ops.py"", lin
e 147rn2k2r/ppp1n1bp/4p1p1/4N3/5B2/8/PPP2PPP/RN3RK1, in __call__

 outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.826137: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.853068: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.879556: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.909811: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.936740: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:34.975022: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:35.014221: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  56
60
6k1/ppp2p1p/5q2/5P2/5r2/2PQ1N1P/PP3PK1/R7File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call

    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range


2023-02-01 12:50:35.059909: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: list index out of range
Traceback (most recent call last):

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 269, in __call__
    return func(device, token, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 147, in __call__
    outputs = self._call(device, args)

  File ""D:\python\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 154, in _call
    ret = self._func(*args)

  File ""D:\python\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""C:\Users\cavir\AppData\Local\Temp\__autograph_generated_file9w2jtgyr.py"", line 23, in helper
    col = ag__.ld(fen)[1]

IndexError: list index out of range
```


### Standalone code to reproduce the issue

```shell
https://github.com/smarthmaster/issue
it is the code and the basic in.txt is this
r3r1k1/1p3ppp/p2q4/3b4/3p4/P2P4/1P1Q1PPP/R3R1K1  0  2
2r1r1k1/1p3ppp/p2q4/3b4/3p4/P2P4/1P1Q1PPP/R3R1K1  56  58
2r1r1k1/1p3ppp/p2q4/3b4/3p4/P2P4/1P1Q1PPP/2R1R1K1  27  18
2r1r1k1/1p3ppp/p1bq4/8/3p4/P2P4/1P1Q1PPP/2R1R1K1  55  47
2r1r1k1/1p3ppp/p1bq4/8/3p4/P2P3P/1P1Q1PP1/2R1R1K1  19  27
2r1r1k1/1p3ppp/p1b5/3q4/3p4/P2P3P/1P1Q1PP1/2R1R1K1  53  45
2r1r1k1/1p3ppp/p1b5/3q4/3p4/P2P1P1P/1P1Q2P1/2R1R1K1  27  19
2r1r1k1/1p3ppp/p1bq4/8/3p4/P2P1P1P/1P1Q2P1/2R1R1K1  49  33
2r1r1k1/1p3ppp/p1bq4/8/1P1p4/P2P1P1P/3Q2P1/2R1R1K1  19  22
2r1r1k1/1p3ppp/p1b3q1/8/1P1p4/P2P1P1P/3Q2P1/2R1R1K1  58  34
2r1r1k1/1p3ppp/p1b3q1/8/1PRp4/P2P1P1P/3Q2P1/4R1K1  2  3
3rr1k1/1p3ppp/p1b3q1/8/1PRp4/P2P1P1P/3Q2P1/4R1K1  62  53
3rr1k1/1p3ppp/p1b3q1/8/1PRp4/P2P1P1P/3Q1KP1/4R3  18  25
3rr1k1/1p3ppp/p5q1/1b6/1PRp4/P2P1P1P/3Q1KP1/4R3  34  10
3rr1k1/1pR2ppp/p5q1/1b6/1P1p4/P2P1P1P/3Q1KP1/4R3  25  18
3rr1k1/1pR2ppp/p1b3q1/8/1P1p4/P2P1P1P/3Q1KP1/4R3  60  59
3rr1k1/1pR2ppp/p1b3q1/8/1P1p4/P2P1P1P/3Q1KP1/3R4  22  19
3rr1k1/1pR2ppp/p1bq4/8/1P1p4/P2P1P1P/3Q1KP1/3R4  10  13
3rr1k1/1p3Rpp/p1bq4/8/1P1p4/P2P1P1P/3Q1KP1/3R4  6  0
3rr3/1p3kpp/p1bq4/8/1P1p4/P2P1P1P/3Q1KP1/3R4  51  30
3rr3/1p3kpp/p1bq4/6Q1/1P1p4/P2P1P1P/5KP1/3R4  13  6
3rr1k1/1p4pp/p1bq4/6Q1/1P1p4/P2P1P1P/5KP1/3R4  30  60
rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR  11  27
rnbqkbnr/ppp1pppp/8/3p4/4P3/8/PPPP1PPP/RNBQKBNR  36  27
rnbqkbnr/ppp1pppp/8/3P4/8/8/PPPP1PPP/RNBQKBNR  3  0
rnb1kbnr/ppp1pppp/8/3q4/8/8/PPPP1PPP/RNBQKBNR  51  35
rnb1kbnr/ppp1pppp/8/3q4/3P4/8/PPP2PPP/RNBQKBNR  27  20
rnb1kbnr/ppp1pppp/4q3/8/3P4/8/PPP2PPP/RNBQKBNR  61  52
rnb1kbnr/ppp1pppp/4q3/8/3P4/8/PPP1BPPP/RNBQK1NR  10  18
rnb1kbnr/pp2pppp/2p1q3/8/3P4/8/PPP1BPPP/RNBQK1NR  57  42
rnb1kbnr/pp2pppp/2p1q3/8/3P4/2N5/PPP1BPPP/R1BQK1NR  20  22
rnb1kbnr/pp2pppp/2p3q1/8/3P4/2N5/PPP1BPPP/R1BQK1NR  54  46
rnb1kbnr/pp2pppp/2p3q1/8/3P4/2N3P1/PPP1BP1P/R1BQK1NR  2  29
rn2kbnr/pp2pppp/2p3q1/5b2/3P4/2N3P1/PPP1BP1P/R1BQK1NR  62  45
rn2kbnr/pp2pppp/2p3q1/5b2/3P4/2N2NP1/PPP1BP1P/R1BQK2R  50  0
rn2kbnr/pp2pppp/2p3q1/8/3P4/2N2NP1/PPb1BP1P/R1BQK2R  59  51
rn2kbnr/pp2pppp/2p3q1/8/3P4/2N2NP1/PPbQBP1P/R1B1K2R  50  29
rn2kbnr/pp2pppp/2p3q1/5b2/3P4/2N2NP1/PP1QBP1P/R1B1K2R  60  62
rn2kbnr/pp2pppp/2p3q1/5b2/3P4/2N2NP1/PP1QBP1P/R1B2RK1  29  47
rn2kbnr/pp2pppp/2p3q1/8/3P4/2N2NPb/PP1QBP1P/R1B2RK1  45  39
rn2kbnr/pp2pppp/2p3q1/8/3P3N/2N3Pb/PP1QBP1P/R1B2RK1  22  20
rn2kbnr/pp2pppp/2p1q3/8/3P3N/2N3Pb/PP1QBP1P/R1B2RK1  61  60
rn2kbnr/pp2pppp/2p1q3/8/3P3N/2N3Pb/PP1QBP1P/R1B1R1K1  20  11
rn2kbnr/pp1qpppp/2p5/8/3P3N/2N3Pb/PP1QBP1P/R1B1R1K1  52  45
rn2kbnr/pp1qpppp/2p5/8/3P3N/2N2BPb/PP1Q1P1P/R1B1R1K1  47  38
rn2kbnr/pp1qpppp/2p5/8/3P2bN/2N2BP1/PP1Q1P1P/R1B1R1K1  51  37
rn2kbnr/pp1qpppp/2p5/8/3P1QbN/2N2BP1/PP3P1P/R1B1R1K1  45  0
rn2kbnr/pp1qpppp/2p5/8/3P1Q1N/2N2bP1/PP3P1P/R1B1R1K1  39  45
rn2kbnr/pp1qpppp/2p5/8/3P1Q2/2N2NP1/PP3P1P/R1B1R1K1  12  20
rn2kbnr/pp1q1ppp/2p1p3/8/3P1Q2/2N2NP1/PP3P1P/R1B1R1K1  35  27
rn2kbnr/pp1q1ppp/2p1p3/3P4/5Q2/2N2NP1/PP3P1P/R1B1R1K1  27  0
rn2kbnr/pp1q1ppp/4p3/3p4/5Q2/2N2NP1/PP3P1P/R1B1R1K1  58  44
rn2kbnr/pp1q1ppp/4p3/3p4/5Q2/2N1BNP1/PP3P1P/R3R1K1  5  19
rn2k1nr/pp1q1ppp/3bp3/3p4/5Q2/2N1BNP1/PP3P1P/R3R1K1  45  28
rn2k1nr/pp1q1ppp/3bp3/3pN3/5Q2/2N1B1P1/PP3P1P/R3R1K1  11  12
rn2k1nr/pp2qppp/3bp3/3pN3/5Q2/2N1B1P1/PP3P1P/R3R1K1  42  25
rn2k1nr/pp2qppp/3bp3/1N1pN3/5Q2/4B1P1/PP3P1P/R3R1K1  1  11
r3k1nr/pp1nqppp/3bp3/1N1pN3/5Q2/4B1P1/PP3P1P/R3R1K1  25  10
r3k1nr/ppNnqppp/3bp3/3pN3/5Q2/4B1P1/PP3P1P/R3R1K1  10  0
r3k1nr/ppbnqppp/4p3/3pN3/5Q2/4B1P1/PP3P1P/R3R1K1  44  35
r3k1nr/ppbnqppp/4p3/3pN3/3B1Q2/6P1/PP3P1P/R3R1K1  28  0
r3k1nr/ppb1qppp/4p3/3pn3/3B1Q2/6P1/PP3P1P/R3R1K1  35  28
r3k1nr/ppb1qppp/4p3/3pB3/5Q2/6P1/PP3P1P/R3R1K1  28  0
r3k1nr/pp2qppp/4p3/3pb3/5Q2/6P1/PP3P1P/R3R1K1  37  28
r3k1nr/pp2qppp/4p3/3pQ3/8/6P1/PP3P1P/R3R1K1  13  21
r3k1nr/pp2q1pp/4pp2/3pQ3/8/6P1/PP3P1P/R3R1K1  28  42
r3k1nr/pp2q1pp/4pp2/3p4/8/2Q3P1/PP3P1P/R3R1K1  20  28
r3k1nr/pp2q1pp/5p2/3pp3/8/2Q3P1/PP3P1P/R3R1K1  42  24
r3k1nr/pp2q1pp/5p2/Q2pp3/8/6P1/PP3P1P/R3R1K1  9  17
r3k1nr/p3q1pp/1p3p2/Q2pp3/8/6P1/PP3P1P/R3R1K1  24  59
rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR  12  20
rnbqkbnr/pppp1ppp/4p3/8/4P3/8/PPPP1PPP/RNBQKBNR  57  42
rnbqkbnr/pppp1ppp/4p3/8/4P3/2N5/PPPP1PPP/R1BQKBNR  10  18
rnbqkbnr/pp1p1ppp/2p1p3/8/4P3/2N5/PPPP1PPP/R1BQKBNR  62  45
rnbqkbnr/pp1p1ppp/2p1p3/8/4P3/2N2N2/PPPP1PPP/R1BQKB1R  11  19
rnbqkbnr/pp3ppp/2ppp3/8/4P3/2N2N2/PPPP1PPP/R1BQKB1R  51  35
rnbqkbnr/pp3ppp/2ppp3/8/3PP3/2N2N2/PPP2PPP/R1BQKB1R  6  21
rnbqkb1r/pp3ppp/2pppn2/8/3PP3/2N2N2/PPP2PPP/R1BQKB1R  58  44
rnbqkb1r/pp3ppp/2pppn2/8/3PP3/2N1BN2/PPP2PPP/R2QKB1R  5  12
rnbqk2r/pp2bppp/2pppn2/8/3PP3/2N1BN2/PPP2PPP/R2QKB1R  61  43
rnbqk2r/pp2bppp/2pppn2/8/3PP3/2NBBN2/PPP2PPP/R2QK2R  4  6
rnbq1rk1/pp2bppp/2pppn2/8/3PP3/2NBBN2/PPP2PPP/R2QK2R  59  51
rnbq1rk1/pp2bppp/2pppn2/8/3PP3/2NBBN2/PPPQ1PPP/R3K2R  20  28
rnbq1rk1/pp2bppp/2pp1n2/4p3/3PP3/2NBBN2/PPPQ1PPP/R3K2R  35  28
rnbq1rk1/pp2bppp/2pp1n2/4P3/4P3/2NBBN2/PPPQ1PPP/R3K2R  28  0
rnbq1rk1/pp2bppp/2p2n2/4p3/4P3/2NBBN2/PPPQ1PPP/R3K2R  48  40
rnbq1rk1/pp2bppp/2p2n2/4p3/4P3/P1NBBN2/1PPQ1PPP/R3K2R  3  10
rnb2rk1/ppq1bppp/2p2n2/4p3/4P3/P1NBBN2/1PPQ1PPP/R3K2R  55  47
rnb2rk1/ppq1bppp/2p2n2/4p3/4P3/P1NBBN1P/1PPQ1PP1/R3K2R  2  20
```


### Relevant log output

_No response_</details>"
59507,saved model regression in 2.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


When calling a method from a loaded tensorflow saved model, I get this exception -

```shell

      1 imported = tf.saved_model.load(""saved_model"")
----> 2 imported(inputs={""x"": [1.]}, training=True)

/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)
    730 
    731 def _call_attribute(instance, *args, **kwargs):
--> 732   return instance.__call__(*args, **kwargs)
    733 
    734 

/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py in canonicalize_function_inputs(self, args, kwargs)
    517         missing_args = [self._arg_names[i] for i in sorted(missing_arg_indices)]
    518         if len(missing_args) == 1:
--> 519           raise TypeError(f""{self.signature_summary()} missing 1 required ""
    520                           f""argument: {missing_args[0]}."")
    521         else:

TypeError: f(self) missing 1 required argument: self.
```

This used to work with tenforflow version 2.6.0 and prior.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class CustomModel(tf.keras.Model):

  def __init__(self):
    super().__init__()


  def call(self, inputs, training):
    print('Tracing with', inputs)
    return inputs


model = CustomModel()


model.__call__ = tf.function(model.__call__)

print('Saving model...')

tf.saved_model.save(model, ""saved_model"", signatures=model.__call__.get_concrete_function(
            inputs={""x"": tf.TensorSpec(shape=[1,], dtype=tf.float32)}, training=tf.TensorSpec(shape=None, dtype=tf.bool, name=""training"")
        ))

imported = tf.saved_model.load(""saved_model"")
imported(inputs={""x"": [1.]}, training=True)
```


### Relevant log output

_No response_</details>"
59506,RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 34 (RSQRT) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

```
!pip install git+https://github.com/openai/whisper.git 
!pip install onnx
!pip install onnx_tf
!git clone https://github.com/usefulsensors/openai-whisper.git
!git clone https://github.com/openai/whisper.git 
%%capture
!pip install optimum[onnxruntime] transformers git+https://github.com/openai/whisper.git
# -*- coding: utf-8 -*-
import warnings
warnings.filterwarnings(""ignore"")

from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from transformers import (
    set_seed,
    AutoProcessor
)
from pathlib import Path
import os

SEED = 42

# Export vanilla & optimized onnx model
def export_vanilla_optimized_onnx(model_checkpoint):
    set_seed(SEED)
    processor = AutoProcessor.from_pretrained(model_checkpoint)

    # Vanilla
    model = ORTModelForSpeechSeq2Seq.from_pretrained(model_checkpoint, from_transformers=True, use_cache=True)
    onnx_path = Path(os.path.join(""exported_onnx_models/"", model_checkpoint))
    model.save_pretrained(onnx_path)
    processor.save_pretrained(onnx_path)


export_vanilla_optimized_onnx('openai/whisper-tiny')
import whisper
import torch
import tensorflow as tf
import onnx
import numpy as np
import argparse
import os
import warnings
import tqdm
from onnx_tf.backend import prepare
from whisper.audio import load_audio, log_mel_spectrogram,pad_or_trim,N_FRAMES, SAMPLE_RATE
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

#load openai->whisper(pytorch)->tiny model
tiny_model = whisper.load_model(""tiny"")

#Export to onnx format
torch.onnx.export(tiny_model.encoder, torch.randn(1,80,3000).to(device), ""./whisper-encoder.onnx"")
onnx_model_path = './whisper-encoder.onnx'
tf_model_path = 'model_tf-encoder'

onnx_model = onnx.load(onnx_model_path)
tf_rep = prepare(onnx_model)
tf_rep.export_graph(tf_model_path)
from datasets import load_dataset
ds = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")
from transformers import WhisperProcessor, TFWhisperForConditionalGeneration

processor = WhisperProcessor.from_pretrained(""openai/whisper-tiny"")
saved_model_dir = 'model_tf-encoder'
tflite_model_path = 'whisper-encoder-hybrid.tflite'

def representative_dataset_data():
    for x in range(5):
      inputs = processor(ds[x][""audio""][""array""], return_tensors=""tf"")
      input_features = inputs.input_features
      yield [input_features]

def representative_dataset_random():
    for _ in range(100):
      data = np.random.rand(1, 80, 3000)
      yield [data.astype(np.float32)]

def representative_dataset():
    for _ in range(1):#Change this to 100 and provide 100 different audio files from known dataset 
      mel_from_file = log_mel_spectrogram('/content/whisper/tests/jfk.flac')
      segment = pad_or_trim(mel_from_file, N_FRAMES)
      segment = tf.expand_dims(segment, 0)
      print(segment.shape)
      yield [segment]

# Convert to tflite(int8) model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.representative_dataset = representative_dataset_data
#converter.inference_input_type = tf.int8  # or tf.uint8
#converter.inference_output_type = tf.int8  # or tf.uint8
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()



# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

import tensorflow as tf
import numpy as np
tflite_model_path = '/content/whisper-encoder-hybrid.tflite'

# Load the TFLite model and allocate tensors
interpreter_enc = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter_enc.allocate_tensors()

print(""== Input details =="")
print(""name:"", interpreter_enc.get_input_details()[0]['name'])
print(""shape:"", interpreter_enc.get_input_details()[0]['shape'])
print(""type:"", interpreter_enc.get_input_details()[0]['dtype'])

print(""\nDUMP INPUT"")
print(interpreter_enc.get_input_details()[0])

print(""\n== Output details =="")
print(""name:"", interpreter_enc.get_output_details()[0]['name'])
print(""shape:"", interpreter_enc.get_output_details()[0]['shape'])
print(""type:"", interpreter_enc.get_output_details()[0]['dtype'])

print(""\nDUMP OUTPUT"")
print(interpreter_enc.get_output_details()[0])

# Get input and output tensors
input_details = interpreter_enc.get_input_details()
output_details = interpreter_enc.get_output_details()
output_tensor = interpreter_enc.get_output_details()[0]['index']

# Test the model with random data
input_shape = input_details[0]['shape']
mel_from_file = log_mel_spectrogram('/content/whisper/tests/jfk.flac')
input_tensor = pad_or_trim(mel_from_file, N_FRAMES)
input_tensor = tf.expand_dims(input_tensor, 0)

audio = whisper.load_audio('/content/whisper/tests/jfk.flac')
audio = whisper.pad_or_trim(audio)
mel = whisper.log_mel_spectrogram(audio)
mel = np.expand_dims(mel,0)
#input_tensor = np.array(input_tensor-128, dtype=np.int8)
interpreter_enc.set_tensor(input_details[0]['index'], mel)

interpreter_enc.invoke()
print(""Whisper Encoder Inference executed successfully\n"")
encoder_output_data = interpreter_enc.get_tensor(output_tensor)
print(encoder_output_data.shape)
print(encoder_output_data)
np.savetxt(""encoder_output.txt"", encoder_output_data.reshape((3,-1)), fmt=""%s"", header=str(encoder_output_data.shape))

```

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
Conversion is successful ,however while running the model getting below error
RuntimeError                              Traceback (most recent call last) <ipython-input-32-17a970b6c12f> in <module>       5 # Load the TFLite model and allocate tensors       6 interpreter_enc = tf.lite.Interpreter(model_path=tflite_model_path) ----> 7 interpreter_enc.allocate_tensors()       8        9 print(""== Input details =="")  /usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)     511   def allocate_tensors(self):     512     self._ensure_safe() --> 513     return self._interpreter.AllocateTensors()     514      515   def _safe_to_run(self):  RuntimeError: tensorflow/lite/kernels/elementwise.cc:88 Type INT16 is unsupported by op Rsqrt.Node number 34 (RSQRT) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59503,Slow compilation of certain operation files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

Not applicableapplicable

### Python version

Not applicable

### Bazel version

Not applicable

### GCC/Compiler version

clang14

### CUDA/cuDNN version

Not applicable

### GPU model and memory

Not applicable

### Current Behaviour?

```shell
Compiling certain ops takes too many time.
Here is a table with measurement of 10 slowest files:
The longest 10 tasks being:

* [673513 ms]: tensorflow/core/kernels/gather_op.cc
* [624861 ms]: tensorflow/core/kernels/argmax_op.cc
* [576809 ms]: tensorflow/core/kernels/resource_variable_ops.cc
* [526177 ms]: tensorflow/core/kernels/conv_grad_input_ops.cc
* [404275 ms]: tensorflow/core/kernels/bias_op.cc 
* [400613 ms]: tensorflow/core/kernels/batch_matmul_op_real.cc
* [395665 ms]: tensorflow/core/kernels/cwise_op_select.cc
* [381854 ms]: tensorflow/core/kernels/matmul_op.cc
* [379893 ms]: tensorflow/core/kernels/conv_ops.cc
* [340674 ms]: tensorflow/core/kernels/cwise_op_equal_to_1.cc
```

These measurements were done on `Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz`.
Out internal build system, being a heterogenious cluster, might schedule these compilations on older CPUs, where single compilation can take up to 15 minutes according to out measurements.

It looks like simply splitting macro instantations into multiple files will allow running compilations on parallel.

I will provide `gather_op.cc` split as an example.


### Standalone code to reproduce the issue

```shell
I am omitting -I and -D flags for the sake of brewity.
I believe that internal build system is able to provide the same statistics as well


clang++-c -m64 -O3 -g -ggnu-pubnames -fexceptions -fno-common -fuse-init-array -fcolor-diagnostics -faligned-allocation -fdebug-default-version=4 -ffunction-sections -fdata-sections -fopenmp=libomp -fopenmp-version=52 -msse2 -msse3 -mssse3 -msse4.1 -msse4.2 -mpopcnt -mcx16 -std=c++20 -w -Wno-everything $(file)
```
```


### Relevant log output

_No response_</details>"
59500,TFLite missing files on TFLITE_PROFILER_SRCS causes undefined reference error.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 11 (bullseye)

### Mobile device

Debian GNU/Linux 11 (bullseye)

### Python version

3.9.2

### Bazel version

3.7.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia

### Current Behaviour?

```shell
Following the official documentation about compiling with cmake on https://www.tensorflow.org/lite/guide/build_cmake, after successfully compiling (Without the `-j` flag on the `cmake --build . -j` step), when trying to use tflite with the cmake example provided I'm getting an `Undefined reference error` on multiple instances. The error is not present on all functions/classes, for example `FlatBufferModel::BuildFromFile` builds and runs just file, however when trying to create an interpreter object the make file gives out an ` undefined reference to `tflite::telemetry::TelemetryReportEvent(TfLiteContext*, char const*, TfLiteStatus)'` error. Many of these error are linked to the usage of `Telementry` functions. So following the answer on a question I asked https://stackoverflow.com/questions/75245826/c-tensorflow-lite-undefined-reference-on-some-functions/75291357#75291357 the problem is fixed. So the telementry files should be added to the CMakeLists.txt file. (More information and the exact steps I've followed to build the library can be found in the stackoverflow question link provided.)
```


### Standalone code to reproduce the issue

```shell
cmake_minimum_required(VERSION 3.16)
project(minimal C CXX)

set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -DTFLITE_DISABLE_TELEMETRY=1"")

set(TENSORFLOW_SOURCE_DIR """" CACHE PATH
  ""Directory that contains the TensorFlow project"" )
if(NOT TENSORFLOW_SOURCE_DIR)
  get_filename_component(TENSORFLOW_SOURCE_DIR
    ""/home/user/Desktop/tensorflow_src"" ABSOLUTE)
endif()

add_subdirectory(
  ""${TENSORFLOW_SOURCE_DIR}/tensorflow/lite""
  ""${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite"" EXCLUDE_FROM_ALL)

add_executable(minimal main.cpp)
target_link_libraries(minimal tensorflow-lite)
```


### Relevant log output

```shell
[100%] Linking CXX executable minimal
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(interpreter.cc.o): in function `tflite::impl::Interpreter::ReportTelemetrySettings(char const*)':
interpreter.cc:(.text+0x292f): undefined reference to `tflite::telemetry::TelemetryReportSettings(TfLiteContext*, char const*, TfLiteTelemetryInterpreterSettings const*)'
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(subgraph.cc.o): in function `tflite::Subgraph::Invoke()':
subgraph.cc:(.text+0x41c0): undefined reference to `tflite::telemetry::TelemetryReportEvent(TfLiteContext*, char const*, TfLiteStatus)'
/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(subgraph.cc.o): in function `tflite::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)':
subgraph.cc:(.text+0x6ad0): undefined reference to `tflite::telemetry::TelemetryReportEvent(TfLiteContext*, char const*, TfLiteStatus)'
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/minimal.dir/build.make:184: minimal] Error 1
make[1]: *** [CMakeFiles/Makefile2:1408: CMakeFiles/minimal.dir/all] Error 2
make: *** [Makefile:149: all] Error 2
```
</details>"
59496,tf.data: random operation inside map function leads to unexpected results on multi-CPU machines,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The following minimal example produces a simple 2x1 image, creates an infinite tf dataset by repeating it and applies a random vertical flip operation. After that it takes 100 elements from the dataset and prints how many of them are flipped:

On my local machine (MacOS with Tensorflow 2.9.2) this code prints the following numbers:

```
44
54
56
52
48
```

However when I run this on my VM on Google Cloud (w/ 8 CPUs and Tensorflow 2.11.0) it always prints either:

```
100
100
100
100
100
```
or

```
0
0
0
0
0
```

What is going on here? What is the correct way to augment images when using tf.data APIs (especially on multi CPU machines)?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant([0, 1])
x = tf.reshape(x, [1, 2, 1, 1])

augmenter = tf.keras.layers.RandomFlip('vertical')
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.repeat()
dataset = dataset.map(augmenter)

def print_dataset(d, n=100):
  ctr = 0
  res = 0
  for x in iter(d):
    if x[0, 0, 0].numpy() > x[1, 0, 0].numpy():
      res += 1
    ctr += 1
    if ctr >= n:
      print(res)
      break

print_dataset(dataset)
print_dataset(dataset)
print_dataset(dataset)
print_dataset(dataset)
print_dataset(dataset)
```


### Relevant log output

_No response_</details>"
59494,CVE-2022-41883 for 2.9.3,"Hello, 

Based on [NVD](https://nvd.nist.gov/vuln/detail/CVE-2022-41883), CVE-2022-41883 will be cherry-picked for 2.9.3. However, I don't see it in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.9.3). Will this still be cherry-picked?

Thanks. "
60418,Skipping full serialization of Keras layer because it is not built.,"i trained SSD-MobileNet on custom dataset and now I want to convert the model to TensorRT model for improve performance and for Inference on Nvidia Jetson TX1 Board.

I Have:
-Ubuntu 20.14 LTS
-Tensorflow 2.11.0
-Python 3.8.10

when I run the:
```
import tensorflow as tf
tf.saved_model.save(model_dir , out_dir)

```
I wil get this error:
```
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f45581d7340>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor object at 0x7f4558265760>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor.SSDMobileNetV2FpnKerasFeatureExtractor object at 0x7f4561e31130>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead object at 0x7f45582562b0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.predictors.heads.keras_class_head.WeightSharedConvolutionalClassHead object at 0x7f4558256ca0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f4558256460>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f45582650d0>, because it is not built.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
```
how can I fix that ? 
"
59492,AttributeError: module 'tensorflow' has no attribute 'contrib' ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run the first set of commands, this is the results I get from the code so you can get a idea of the problem I'm facing.

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)
Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.21.6)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.3.0)
Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.14.1)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.2.0)
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.15.0)
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.4.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (15.0.6.1)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (4.4.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.29.0)
Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (23.1.21)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.2.0)
Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.2)
Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.19.6)
Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (21.3)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.6.3)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.3.0)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.1.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (57.4.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.51.1)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.38.4)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.25.1)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.0.1)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)
Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.16.0)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.1)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.11.0) (3.0.9)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)
Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.2.1)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)
Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.10)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.24.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)
Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.11.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)
Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)
Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)
Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)
Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)
Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)
Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)

When I Initialize Tacotron and Waveglow this is the error i'm receiving.

/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.
  warnings.warn(
Downloading...
From: https://drive.google.com/uc?id=1qTdOAdKmMKe-HZA-1SXCAARBAdZXeSAN
To: /content/merged.dict.txt
100% 7.94M/7.94M [00:00<00:00, 50.5MB/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-3dc3859c8d60> in <module>
     48 
     49 # initialize Tacotron2 with the pretrained model
---> 50 hparams = create_hparams()

/content/tacotron2/hparams.py in create_hparams(hparams_string, verbose)
      6     """"""Create model hyperparameters. Parse nondefault from given string.""""""
      7 
----> 8     hparams = tf.contrib.training.HParams(
      9         ################################
     10         # Experiment Parameters        #

AttributeError: module 'tensorflow' has no attribute 'contrib'

Tensorflow 1 is incompatible with google colab, the code was created to run using tensorflow 1 so this is why the code is not working. I have been facing this error for nearly two months i really would appreciate if you can solve this. I have been told to code this myself but I have no knowledge on coding.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ecuUD2sxfuM6IOhjZEW3ls-jvbpJEei4?usp=share_link#scrollTo=Q0BWBVdCS9ty
```


### Relevant log output

_No response_</details>"
59491,AttributeError: module 'tensorflow' has no attribute 'contrib',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run the first set of commands, this is the results I get from the code so you can get a idea of the problem I'm facing.

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)
Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)
Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.21.6)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.3.0)
Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.14.1)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.2.0)
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.15.0)
Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.4.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (15.0.6.1)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (4.4.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.29.0)
Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (23.1.21)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.2.0)
Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.2)
Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)
Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.19.6)
Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (21.3)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.6.3)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.3.0)
Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.1.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (57.4.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.51.1)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.38.4)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.25.1)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.0.1)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)
Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.16.0)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.1)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.11.0) (3.0.9)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)
Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.2.1)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)
Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.0.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.10)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.24.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)
Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.11.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)
Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)
Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)
Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)
Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)
Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)
Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)
Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)

When I Initialize Tacotron and Waveglow this is the error i'm receiving.

/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.
  warnings.warn(
Downloading...
From: https://drive.google.com/uc?id=1qTdOAdKmMKe-HZA-1SXCAARBAdZXeSAN
To: /content/merged.dict.txt
100% 7.94M/7.94M [00:00<00:00, 50.5MB/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-3dc3859c8d60> in <module>
     48 
     49 # initialize Tacotron2 with the pretrained model
---> 50 hparams = create_hparams()

/content/tacotron2/hparams.py in create_hparams(hparams_string, verbose)
      6     """"""Create model hyperparameters. Parse nondefault from given string.""""""
      7 
----> 8     hparams = tf.contrib.training.HParams(
      9         ################################
     10         # Experiment Parameters        #

AttributeError: module 'tensorflow' has no attribute 'contrib'

Tensorflow 1 is incompatible with google colab, the code was created to run using tensorflow 1 so this is why the code is not working. I have been facing this error for nearly two months i really would appreciate if you can solve this. I have been told to code this myself but I have no knowledge on coding.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ecuUD2sxfuM6IOhjZEW3ls-jvbpJEei4?usp=share_link#scrollTo=Q0BWBVdCS9ty
```


### Relevant log output

_No response_</details>"
59489,tf-nightly wheels missing,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After 27 Jan, the wheels automatically published to pypi for tf-nightly appear to be incomplete. There are various platform tags and python versions missing.
```


### Standalone code to reproduce the issue

```shell
Compare https://pypi.org/project/tf-nightly/2.12.0.dev20230127/#files

Vs

https://pypi.org/project/tf-nightly/2.12.0.dev20230128/#files
```


### Relevant log output

_No response_</details>"
59488,Cross Compilation Python 3.8 Rasberry Pi 4 AARCH64 fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.8.4

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

4.2.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The compilation succeeds.
```


### Standalone code to reproduce the issue

```shell
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh AARCH64
```


### Relevant log output

```shell
2023-01-29 18:19:00 (1.55 MB/s) - 'get-pip.py' saved [2569500/2569500]

/install/install_pip_packages_by_version.sh: line 24: /usr/local/bin/python3.8: No such file or directory
ln: failed to create symbolic link '/usr/include/python3.8/numpy': No such file or directory
The command '/bin/sh -c /install/install_pi_python3x_toolchain.sh ""3.8""' returned a non-zero code: 1
ERROR: docker build failed. Dockerfile is at /home/foobar/projects/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python38
```
</details>"
59487,RuntimeError: merge_call called while defining a new graph or a tf.function.,"**System information**.
- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): 2.6, 2.9

---

## Describe the problem

I have code that works fine but gives the following error if I use `with strategy.scope()`.

>  RuntimeError: `merge_call` called while defining a new graph or a tf.function. This can often happen if the function `fn` passed to `strategy.run()` contains a nested `@tf.function`, and the nested `@tf.function` contains a synchronization point, such as aggregating gradients (e.g, optimizer.apply_gradients), or if the function `fn` uses a control flow statement which contains a synchronization point in the body. Such behaviors are not yet supported. Instead, please avoid nested `tf.function`s or control flow statements that may potentially cross a synchronization boundary, for example, wrap the `fn` passed to `strategy.run` or the entire `strategy.run` inside a `tf.function` or move the control flow out of `fn`

## Describe the expected behavior

I think, It should work.

- Do you want to contribute a PR? (yes/no): No.
- If yes, please read [this page](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md) for instructions


## Standalone code to reproduce the issue

The code is for gradient accumulation techniques. Here it is done by overriding the `trian_step` with `fit` method.  This code works fine (as said above) without `with strategy.scope()`. Now, I like to use it for multi-gpu cases, and so I use strategy scope but ened up the the above mentioned error.

[Gist.](https://colab.research.google.com/drive/1yTu-qM4zTxtzNE7Ki6FvSyD9ijhvHdnw?usp=sharing)
## Follow-up Questions

1. In the above gist, for multi-gpu training, do I need to adjust anythong for `BATCH_SIZE = 32 * strategy.num_replicas_in_sync` inside the `train_step` method? Or it will be handled auto?
2. In the above gist, I use mixed precisoin technique, and so I also wrap ([as described](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer)) optimizer with `LossScaleOptimizer` and use `optimizer.get_scaled_loss(loss)` and `optimizer.get_unscaled_gradients(gradients)`. 
But the [official documentation](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer) talks about normal `fit` and custom loop training cases. In case of custom loop, it's suggested to wrap the optimizer and scale the loss and gradient but what about the combination of `fit` and custom loop (overriding `train_step`)? Does it sill need to wrap the optimizer and scale the loss and gradient or it will be handled by the API?

---

Others: https://github.com/keras-team/tf-keras/issues/107 cc @chenmoneygithub @nikitamaia @bhack "
59485,Issue when loading trained model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I have trained a model using keras, and was able to save it and load it uisng jupyter notebook. But when I try to load it using pycharm it doesnt work for me. 
I am using different virtual envs for jupyter notebook and pycharm, but I have all packages installed so I am unsure why am I getting the problem.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
model = tf.keras.models.load_model('/home/danieln7/Desktop/ML/robot_1_full2')
print(model.summary())
```


### Relevant log output

```shell
File ""/home/danieln7/Desktop/RobotCodeDaniel/test.py"", line 7, in <module>
    model = tf.keras.models.load_model('/home/danieln7/Desktop/ML/robot_1_full2', compile=True)
  File ""/home/danieln7/anaconda3/envs/RobotControl/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/danieln7/anaconda3/envs/RobotControl/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 115, in _process_kwargs
    raise TypeError(
TypeError: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.
```
</details>"
59484,graph execution error,"I'm facing an issue that says ""graph execution error"" tried to solve it but couldn't.
"
59483,Issue created for Rollback of PR #59315: [MacOS][Pluggable Device] Fixes for macos pluggable device.,"Merged PR #59315 is rolled back in a8bb5484cada34c6d02c1d02d128d33c0226946f.
Several tests failed on Intel MacOS including //tensorflow/core/framework:tensor_test

Please follow up with the PR Owner and close this issue once its resolved.

"
59481,AttributeError: module 'tensorflow' has no attribute 'contrib',"Can anyone help me with this error? I have been trying to get this solved for a month. I think the code is outdated on the colab notebook and have no idea how to change the code to make it compatible for TF 2. Thanks if anyone can solve this 



/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.
  warnings.warn(
Downloading...
From: https://drive.google.com/uc?id=1qTdOAdKmMKe-HZA-1SXCAARBAdZXeSAN
To: /content/merged.dict.txt
100% 7.94M/7.94M [00:00<00:00, 254MB/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-11-50620fa974be> in <module>
     49 
     50 # initialize Tacotron2 with the pretrained model
---> 51 hparams = create_hparams()

/content/tacotron2/hparams.py in create_hparams(hparams_string, verbose)
      6     """"""Create model hyperparameters. Parse nondefault from given string.""""""
      7 
----> 8     hparams = tf.contrib.training.HParams(
      9         ################################
     10         # Experiment Parameters        #

AttributeError: module 'tensorflow' has no attribute 'contrib'"
59475,How to make the program end without moving tf_func() out of the loop?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Code doesn't work correctly.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np


class test:
    def __init__(self):
        self.counter=None
    
    
    def sum_func(self):
        if np.sum(self.counter)==1875:
            return True
    
    
    @tf.function
    def tf_func(self):
        flag=self.sum_func()
        return flag
    
    
    def counter_func(self):
        while True:
            for i in range(1875):
                if i==0:
                    self.counter=np.array(1)
                else:
                    self.counter=np.append(self.counter,np.array(1))
                flag=self.tf_func()
                if flag==True:
                    return

t=test()
t.counter_func()
```


### Relevant log output

_No response_</details>"
59474,Conditional padding issue with jit_compile=True,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Function smart_cond (tf.cond) fails to pass gradients back when combined with padding/slicing if JIT enabled.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18O-DM1vEYbmOhQ50zt6f5KVy-8qbMqBH?usp=sharing
```


### Relevant log output

```shell
InvalidArgumentError: Graph execution error:

Incompatible shapes: [16,8,12,3] vs. [16,8,10,3]

Stack trace for op definition: 
File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
  return _run_code(code, main_globals, None,
File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
  exec(code, run_globals)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
  app.launch_new_instance()
File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
  app.start()
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
  self.io_loop.start()
File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
  self.asyncio_loop.run_forever()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
  self._run_once()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
  handle._run()
File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
  self._context.run(self._callback, *self._args)
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
  lambda f: self._run_callback(functools.partial(callback, future))
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
  ret = callback()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
  self.run()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 381, in dispatch_queue
  yield self.process_one()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 225, in wrapper
  runner = Runner(result, future, yielded)
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 714, in __init__
  self.run()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
  yield gen.maybe_future(dispatch(*args))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
  yield gen.maybe_future(handler(stream, idents, msg))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
  self.do_execute(
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
  res = shell.run_cell(code, store_history=store_history, silent=silent)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
  return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
  result = self._run_cell(
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
  return runner(coro)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
  coro.send(None)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
  if (await self.run_code(code, result,  async_=asy)):
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
  exec(code_obj, self.user_global_ns, self.user_ns)
File ""<ipython-input-5-0e949eda0b3b>"", line 2, in <module>
  model.fit(
File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
  return fn(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1409, in fit
  tmp_logs = self.train_function(iterator)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1051, in train_function
  return step_function(self, iterator)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1040, in step_function
  outputs = model.distribute_strategy.run(run_step, args=(data,))
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1030, in run_step
  outputs = model.train_step(data)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 893, in train_step
  self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
File ""/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 537, in minimize
  grads_and_vars = self._compute_gradients(
File ""/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 590, in _compute_gradients
  grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)
File ""/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py"", line 471, in _get_gradients
  grads = tape.gradient(loss, var_list, grad_loss)

	 [[{{node gradient_tape/mean_squared_error/BroadcastGradientArgs}}]]
	 [[StatefulPartitionedCall]] [Op:__inference_train_function_1464]
```
</details>"
59473,tf equivalent of numpy.view(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS Ventura 13.0

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is there an equivalent to numpy's `.view()` function in tensorflow?

https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
59472,Extending tf.Tensor class,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS Ventura 13.0

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I want to extend the tf.Tensor class, but neither of the following options work:

1. option: extend tf.Tensor:

```shell
class MyTFTensor(tf.Tensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value

y = MyTFTensor._from_native(value=tf.zeros((3, 224, 224))
```
Fails with:
```
/var/folders/kb/yxxdttyj4qzcm447np5p22kw0000gp/T/ipykernel_94703/515175960.py in _from_native(cls, value)
      4     @classmethod
      5     def _from_native(cls, value: tf.Tensor):
----> 6         value.__class__ = cls
      7         return value

TypeError: __class__ assignment: 'MyTFTensor' object layout differs from 'tensorflow.python.framework.ops.EagerTensor'
```

3. Option: extend EagerTensor
```shell
from tensorflow.python.framework.ops import EagerTensor
class MyTFTensor(EagerTensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value
```
Fails with:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/var/folders/kb/yxxdttyj4qzcm447np5p22kw0000gp/T/ipykernel_94703/3632871733.py in <cell line: 2>()
      1 from tensorflow.python.framework.ops import EagerTensor
----> 2 class MyTFTensor(EagerTensor):
      3 
      4     @classmethod
      5     def _from_native(cls, value: tf.Tensor):

TypeError: type 'tensorflow.python.framework.ops.EagerTensor' is not an acceptable base type
```

Our goal is to extend it though, we **don't** want to store the tf.tensor instance as an attribute of MyTFTensor, but instead extend the `tf.Tensor` class!


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# 1st option
class MyTFTensor(tf.Tensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value

y = MyTFTensor._from_native(value=tf.zeros((3, 224, 224))

# 2nd option
from tensorflow.python.framework.ops import EagerTensor
class MyTFTensor(EagerTensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value
```


### Relevant log output

_No response_</details>"
59471,How to switch from TextLineDataset to Dataset.from_generator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.9.0

### Custom Code

Yes

### OS Platform and Distribution

Mac OS X

### Mobile device

_No response_

### Python version

3.7.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have working code that interleaves TextLineDatasets. However, when I try to do the same with tf.data.Dataset.from_generator (because I want to switch to binary files), I get a TypeError (see below).
```


### Standalone code to reproduce the issue

```shell
def generator(fname):
    with open(fname, 'r') as f:
        for line in f.readlines():
            yield line

def dataset_from_generator(fname):
    return tf.data.Dataset.from_generator(
        lambda: generator(fname),
        output_signature=tf.TensorSpec(shape = (1,), dtype = tf.string)
    )

fnames = [""file1.txt"", ""file2.txt"",""file1_fr.txt"", ""file2_fr.txt""]
dataset = tf.data.Dataset.from_tensor_slices(fnames)

print(""before interleave"")
for element in dataset:
    print(element)

dataset = dataset.interleave(
    #lambda fname: tf.data.TextLineDataset(fname), # this works
    lambda fname: dataset_from_generator(fname), # this doesn't work
    cycle_length=2)

print()
print(""after interleave"")

for element in dataset:
    print(element)
```


### Relevant log output

```shell
2023-01-27 12:02:58.363036: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: TypeError: expected str, bytes or os.PathLike object, not Tensor
Traceback (most recent call last):

  File ""/Users/joris_pelemans/miniconda3/envs/owl3.7/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 270, in __call__
    ret = func(*args)

  File ""/Users/joris_pelemans/miniconda3/envs/owl3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 642, in wrapper
    return func(*args, **kwargs)

  File ""/Users/joris_pelemans/miniconda3/envs/owl3.7/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1030, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""dataset.py"", line 13, in generator
    with open(fname, 'r') as f:

TypeError: expected str, bytes or os.PathLike object, not Tensor
```
</details>"
59470,How to invoke .h5 model instead of .tflite model for pose classification?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
I need to implement pose_classifier.h5 model instead of pose_classifier.tflite.
```


### Relevant log output

_No response_</details>"
59469,AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

'2.6.0'

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental'
```


### Standalone code to reproduce the issue

```shell
AttributeError                            Traceback (most recent call last)
Cell In [34], line 1
----> 1 trained_model = run_experiment()

Cell In [33], line 8, in run_experiment()
      3 filepath = ""C:/Users/HP/video_classifier""
      4 checkpoint = keras.callbacks.ModelCheckpoint(
      5     filepath, save_weights_only=True, save_best_only=True, verbose=1
      6 )
----> 8 model = get_compiled_model()
      9 history = model.fit(
     10     X_train,
     11     y_train,
   (...)
     14     callbacks=[checkpoint],
     15 )
     17 model.load_weights(filepath)

Cell In [32], line 21, in get_compiled_model()
     16     model = keras.Model(inputs, outputs)
     17     lr_schedule = keras.optimizers.schedules.ExponentialDecay(
     18     initial_learning_rate=1e-3,
     19     decay_steps=10000,
     20     decay_rate=0.9)
---> 21     opt = tf.keras.optimizers.experimental.AdamW(
     22     learning_rate=lr_schedule,
     23     beta_1=0.9,
     24     beta_2=0.999,
     25     epsilon=1e-08,
     26     weight_decay = 1e-04
     27 )
     28     model.compile(
     29         optimizer= opt, loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""]
     30     )
     31     return model

AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental'
```


### Relevant log output

_No response_</details>"
59468,## cc,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59467, tf.lite is x40 faster than tflite_runtime.interpreter,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi, I have a tflite qunatized model (int8), The model running with


interpreter = tf.lite.Interpreter(model_path=model_path,
                                experimental_delegates=[])
```

is 40x faster than:

```
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path=model_path,
                                experimental_delegates=[])
```

Any explications? I have to used the tflite_runtime to run the model with USB google coral

Thanks
```


### Standalone code to reproduce the issue

```shell
The first code:

import tensorflow as tf
interpreter = tf.lite.Interpreter(model_path=model_path,
                                 experimental_delegates=delegates)

=> 80 ms as inference time

The second code:
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path=model_path,
                                 experimental_delegates=delegates)

=> 3s as inference time
```


### Relevant log output

_No response_</details>"
59464,'ValueError: Invalid model' at '_calibration_wrapper.AddIntermediateTensors(model_content)',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip install 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11

### 2. Description
When I try to convert diffusion model into TFLite with **Integer-only**, I got the following error.
```
Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    main()
  File ""main.py"", line 29, in main
    tflite_model = converter.convert()
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 933, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 911, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1216, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1100, in _convert_from_saved_model
    return self._optimize_tflite_model(
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 871, in _optimize_tflite_model
    model = self._quantize(model, q_in_type, q_out_type, q_activations_type,
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 608, in _quantize
    result = _calibrator.add_intermediate_tensors(result)
  File ""/home/minkyukim/projects/test/venv/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 36, in add_intermediate_tensors
    return _calibration_wrapper.AddIntermediateTensors(model_content)
ValueError: Invalid model
```
If I modify the 17th line of `main.py` to `model = DiffusionModelV2Test(img_height=height, img_width=width, max_text_length=77)`, it generates TFLite output without error.
`DiffusionModelV2Test` is a model which I modified to make the model smaller.
The original model contains 3 downsamplings and 3 upsamplings, but `DiffusionModelV2Test` only contains 1 for each.
Also, when if I comment out from line 25 ~ 28, which makes the conversion to weight-only quantization, it generates TFLite without error as well.

So, My question is why does this error occurs?
And regarding the workaround with `DiffusionModelV2Test`, is there any limitation for the size of saved model when using calibration?

FYI.
The model I'm testing is diffusion model originally from keras-cv.
To make it convertable into TFLite, I only modified `keras.layers.Input` with `batch_size` specified as an argument.

### 3. Code

[reproducer gist](https://gist.github.com/kkimmk/239ffc431f9bea47513a3964f5f492ca)

"
59463,Unable to build 2.11 for ppc64le due to XNNPACK configurable attribute deps not matched,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux AlmaLinux 8

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.0.0

### GCC/Compiler version

11.2.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
While trying to build TF 2.11.0 for ppc64le I am getting the error XNNPACK configuration error( see the Relevant log output). Looks like xnnpack is missing pcc64 support. Any idea how to avoid/fix this issue?
```


### Standalone code to reproduce the issue

```shell
try building it for ppc64le arch , I guess
```


### Relevant log output

```shell
> bazel --batch --output_user_root ../build build -s --verbose_failures --distinct_host_configuration=false --copt=-mcpu=power8 --copt=-mtune=power8 --config=opt --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 -j 32 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package
...
...
ERROR: /scratch/cmsbuild/jenkins_a/workspace/ib-run-pr-tests/testBuildDir/BUILD/el8_ppc64le_gcc11/external/tensorflow-sources/2.11.0-e429985b372fa0a46df71b07777fb743/build/104de3b4d7434bb96e7159769f24a756/external/XNNPACK/BUILD.bazel:10546:26: configurable attribute ""deps"" in @XNNPACK//:amalgam_microkernels doesn't match this configuration. Would a default condition help?

Conditions checked:
 @XNNPACK//:aarch32
 @XNNPACK//:aarch64
 @XNNPACK//:x86
 @XNNPACK//:emscripten_wasm
 @XNNPACK//:emscripten_wasmsimd
 @XNNPACK//:emscripten_wasmrelaxedsimd
 @XNNPACK//:riscv
```
```
</details>"
59462,question about using tensorArray to do backpropagation,"I am confused about the tensorflow documentation here:

https://www.tensorflow.org/api_docs/python/tf/TensorArray:

Note that although the array can be read multiple times and positions can be overwritten, behavior may be undefined when storing multiple references to the same array and clear_after_read is False. In particular, avoid using methods like concat() to convert an intermediate TensorArray to a Tensor, then further modifying the TensorArray, particularly if you need to backprop through it later.

This is from tensorflow 2.11 which isn't included in tensorflow 2.7. So I want to know if we really need an intermediate Tensorarray and then concatenate it to a tensor and then calculate loss and do backpropogation, which way is suggested?"
59461,Segmentation Fault during training,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0-dev20230125

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

RTX 3080ti

### Current Behaviour?

```shell
Docker image: tensorflow/tensorflow:nightly-gpu 

The same issue occurs whether I'm using tensorflow 2.11 or 2.12.0-dev.

I'm getting a ""segmentation fault (core dumped)"" on the third epoch of my training run. There is no trace, so I have no clue how to debug. I tried adjusting the batch size, but I always get the same issue (it just may occur on a different epoch number).

The other odd thing is that the seg fault always happens at the exact same batch number (e.g. batch 7402 / 7472 in epoch 3).

If the seg fault occurs on epoch 3, I can load the saved model from epoch 2 and continue to train for another 2 epochs (so 4 total), but it will seg fault again on the 3rd epoch. 

How should I proceed?
```


### Standalone code to reproduce the issue

```shell
Too much code to include everything here, I'm not sure which part would be the most useful. But I can add code snippets here as needed.
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
</details>"
59456,TFLite performance breakdown on GAN models seems inaccurate (using TFLite benchmarking tool),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF2.7

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

3.7.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My goal in running these models in TFLite was to understand the performance breakdown for each type of layer within the GAN models. 

From the ESRGAN model the benchmarking tool reports the following: 
node type	count	avg_ms	avg %
CONV_2D	169	12217.5	89.00%
CONCATENATION	132	596.505	4.35%
TRANSPOSE_CONV	2	475.784	3.47%
LEAKY_RELU	136	319.739	2.33%
ADD	47	103.243	0.75%
MUL	11	14.042	0.10%

 There is a very limited number of TRANSPOSE_CONV layers which barely affect the model's performance. From my understanding of most GAN models, transposed convolutional layers are usually the bottleneck. Research papers have shown performance (latency) improvements across the network by simply accelerating the transposed convolutional layers, but how does it become so effective if these layers are only a small amount of the performance breakdown?

How reliable is the performance benchmarking tool for TFLite in profiling GAN models? Are these results accurate, or is there potential for error where inference time breakdown is represented accurately?
```


### Standalone code to reproduce the issue

```shell
I used the ERSGAN model and Style Transfer ""magenta"" model available at:  https://tfhub.dev/captain-pool/lite-model/esrgan-tf2/1  and https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1 respectively.
I used the Benchmark Model binary available at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark compiled from T2.7 source code.

I simply ran the benchmarking tool with the following parameters:
./benchmark_model  --use_gpu=false //
--num_threads=1 //
--enable_op_profiling=true //
--graph=./models/esrgan.tflite //
--num_runs=2 //
--warmup_runs=0 //
--warmup_min_secs=0 //
--use_vm_sim_delegate=false //
--print_postinvoke_state=true //
--profiling_output_csv_file=test.csv
```


### Relevant log output

_No response_</details>"
59454,Val loss is very different from training loss when measured on the same data,"### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0-dev20221213

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Python version

3.7, 3.9

### Current Behaviour?


When fine-tuning a Keras Applications model on MNIST, and using the same data for training and validation (for debugging), the validation loss is much higher than the training loss.


P.S.:
I don't believe overfitting is relevant because the training and validation data are the same. I know certain layers like Dropout and BatchNormalization have different behavior during training and validation, but the difference seems too large for this to account for it.


### Standalone code to reproduce the issue

```shell
import numpy as np
np.random.seed(1337)  # for reproducibility
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
import tensorflow.keras.layers as layers
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
import tensorflow as tf

batch_size = 32
nb_classes = 10
nb_epoch = 10

# load data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
del X_test
del y_test

# subsample for memory
idx = np.random.choice(X_train.shape[0], 1000, replace=0)
X_train = X_train[idx]
y_train = y_train[idx]

# preprocess images
tmp = []
for i in range(X_train.shape[0]):
    sample = X_train[i,]
    sample = tf.expand_dims(sample, axis=-1)
    sample = tf.image.grayscale_to_rgb(sample)
    sample = tf.image.resize(sample, (224, 224))
    sample /= 255
    tmp.append(sample)
X_train = np.stack(tmp)

# preprocess labels
Y_train = tf.one_hot(y_train, nb_classes).numpy()

# define model
conv = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=[224, 224, 3])
for layer in conv.layers:
    layer.trainable = True
inputs = layers.Input((224, 224, 3))
x = conv(inputs)
x = layers.AveragePooling2D((7, 7))(x)
x = layers.Flatten()(x)
outputs = layers.Dense(nb_classes, activation='softmax')(x)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)

# compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam')

# train
history = model.fit(X_train, 
                    Y_train,
                    batch_size=batch_size, 
                    epochs=nb_epoch,
                    verbose=1, 
                    validation_data=(X_train, Y_train))
```


### Relevant log output

```shell
Epoch 1/10
32/32 [==============================] - 50s 1s/step - loss: 0.4807 - val_loss: 4.7196
Epoch 2/10
32/32 [==============================] - 29s 912ms/step - loss: 0.1487 - val_loss: 9.1618
Epoch 3/10
32/32 [==============================] - 29s 915ms/step - loss: 0.1485 - val_loss: 15.0174
Epoch 4/10
32/32 [==============================] - 30s 950ms/step - loss: 0.0986 - val_loss: 8.3269
Epoch 5/10
32/32 [==============================] - 30s 925ms/step - loss: 0.0546 - val_loss: 11.8151
Epoch 6/10
32/32 [==============================] - 30s 926ms/step - loss: 0.0379 - val_loss: 11.5285
Epoch 7/10
32/32 [==============================] - 29s 924ms/step - loss: 0.1218 - val_loss: 9.5500
Epoch 8/10
32/32 [==============================] - 29s 906ms/step - loss: 0.0689 - val_loss: 9.0055
Epoch 9/10
32/32 [==============================] - 29s 908ms/step - loss: 0.0452 - val_loss: 6.9891
Epoch 10/10
32/32 [==============================] - 30s 952ms/step - loss: 0.0319 - val_loss: 10.5948
```
</details>"
59453,Crash when running tf.random.fixed_unigram_candidate_sampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1gj213OAwWsLvcjtvXaoUCH0_atVDHqFk?usp=share_link
```


### Relevant log output

_No response_</details>"
59452,Segfault when running tf.math.unsorted_segment_sqrt_n,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1bRop6yaxdXjnbL8QvqZDg6LVnImpfZI_?usp=share_link
```


### Relevant log output

_No response_</details>"
59451,Crash when running tf.linalg.diag,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/14nQzmoaY_GQwfaZd2vX_cHdKvEcF2Eax?usp=share_link
```


### Relevant log output

_No response_</details>"
59450,Segfault when running tf.random.learned_unigram_candidate_sampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1zBvo14d7SGoCjWXSGtL-lxXjf0SmpKDN?usp=share_link
```


### Relevant log output

_No response_</details>"
59449,Crash when running tf.random.fixed_unigram_candidate_sampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1CAWUAcGHbpmwRCSM9dwRAhEZwMkilyVM?usp=share_link
```


### Relevant log output

_No response_</details>"
59448,Crash when running tf.nn.conv2d_transpose,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1izhSzpTf5QS8Fun1ZM3AAPaLkbQRvCNl?usp=share_link
```


### Relevant log output

_No response_</details>"
59447,Crash when running tf.nn.atrous_conv2d,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1KLukSceUB05E_8Qxf3U-SZOtnj3BpOIb?usp=share_link
```


### Relevant log output

_No response_</details>"
59446,Crash when running tf.math.unsorted_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1jqZ2LurCCAgkS_IhR-X1DgTfLSsrBhDl?usp=share_link
```


### Relevant log output

_No response_</details>"
59445,Segfault when running tf.math.unsorted_segment_sqrt_n,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1T5VPLUmmxfkNzgNEb768xiMTDpKp-s2o?usp=share_link
```


### Relevant log output

_No response_</details>"
59444,Crash when running tf.math.unsorted_segment_prod,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/14XteRiq7ULRMTHFuwDKMLmyf_tx2rHZz?usp=share_link
```


### Relevant log output

_No response_</details>"
59443,Crash when running tf.math.unsorted_segment_min,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/16gN6KPEFPRn7eBDb41W6FlJsUVGYl7pp?usp=share_link
```


### Relevant log output

_No response_</details>"
59442,Crash when running tf.math.unsorted_segment_max,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1ChGoSvXS7Q6vCZfak7bFBy8VKT-dNgXw?usp=share_link
```


### Relevant log output

_No response_</details>"
59441,Segfault when running tf.keras.layers.upsampling2d,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
segfault
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1i8qY29Nt4BP2J256mQ1-J_9nnbofHxdO?usp=share_link
```


### Relevant log output

_No response_</details>"
59440,Crash when running tf.keras.backend.eye,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Crash
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/15yiYmGH1wXnKryDbJ8NHOaI1Je_SQjib?usp=share_link
```


### Relevant log output

_No response_</details>"
59439,Crash when running tf.image.combined_non_max_suppression,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
https://drive.google.com/drive/folders/1_bVEq3GcTqj8XDSmeSsR2cSS8D6_oUcR?usp=share_link
```


### Relevant log output

_No response_</details>"
59438,Tensorflow profiling tool does not work on Mac M1 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

M1 Mac

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I cannot do profiling on M1 macbook, but with the same code I can profile with linux machine (tried on colab).



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from datetime import datetime

cifar = tf.keras.datasets.cifar100
(x_train, y_train), (x_test, y_test) = cifar.load_data()
model = tf.keras.applications.ResNet50(
    include_top=True,
    weights=None,
    input_shape=(32, 32, 3),
    classes=100,
)

logs = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")

tboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=logs, histogram_freq=1, profile_batch=(50, 150)
)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()
legacy_optimizer = tf.keras.optimizers.legacy.Adam()
# switch optimizer to test performance
model.compile(optimizer=optimizer, loss=loss_fn, metrics=[""accuracy""])

model.fit(
    x_train,
    y_train,
    epochs=2,
    batch_size=16,
    steps_per_epoch=200,
    callbacks=[tboard_callback],
)
```
```


### Relevant log output

_No response_</details>"
59433,pass additional parameters to @tf.custom_gradient,"
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8.0

### Current Behaviour?

Currently we can define a custom gradient using `@tf.custom_gradient` following https://www.tensorflow.org/api_docs/python/tf/custom_gradient

What I'm looking for is to be able to pass to this function additional information (in my case, the loss of my network)

For example (it's stupid, but it's to give an idea) to take the incoming gradient and multiply it by loss that has generated that gradient

"
59432,How to infer  pose classification on single image?,"The first layer of the model needs shape of (None, 51). How to send the image as a feature tensor of 51 feature points which is also nothing but the feature taken as the pose points estimated by  MOVNET?"
59431,Can't convert custom model to tflite with model config and weights file being separate,"Hi,
I am trying to convert a custom model to tflite with these steps:
```
#creates a custom model that I have previously defined (Resnet50 as a backbone (pretrained weights) + transformer encoder + MLP head)
model = create_model(classes=5)   
#the best weights file obtained after my custom model training
model.load_weights(model_weights_path)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]  
tfmodel = converter.convert()
with tf.io.gfile.GFile(tf_lite_path +  'tflite_model' + "".tflite"", 'wb') as f:
    f.write(tfmodel)
```
I do not get any output on this, no errors as well. The question is - is it possible to convert my custom model with creating the model (without saving it) and then loading the weights the way I have done? Is there any other way how to make my model smaller after training?

"
59428,Model saving with optimizer throws attribute error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

macOS Monterey

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When saving a model after it has been compiled with an optimizer (e.g., `SGD`),
an attribute error is thrown.
This is alleviated by setting `include_optimizer=False` when saving.
By default this switch is set to `True`.
```


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential(
    [
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(10, activation=""softmax""),
    ]
)

model.save(""test"") #runs fine

optimizer = tf.keras.optimizers.SGD(),
model.compile(optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy())

model.save(""test"") #throws error
```


### Relevant log output

```python
AttributeError                            Traceback (most recent call last)
Cell In[22], line 1
----> 1 model.save(""test"")

File ~/miniforge3/envs/a/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniforge3/envs/a/lib/python3.10/site-packages/keras/saving/legacy/saving_utils.py:211, in model_metadata(model, include_optimizer, require_config)
    198             raise NotImplementedError(
    199                 ""Optimizers loaded from a SavedModel cannot be saved. ""
    200                 ""If you are calling `model.save` or ""
   (...)
    204                 ""delete the optimizer from the model.""
    205             )
    206         else:
    207             optimizer_config = {
    208                 ""class_name"": keras.utils.get_registered_name(
    209                     model.optimizer.__class__
    210                 ),
--> 211                 ""config"": model.optimizer.get_config(),
    212             }
    213         metadata[""training_config""][""optimizer_config""] = optimizer_config
    214 return metadata

AttributeError: 'tuple' object has no attribute 'get_config'
```
</details>"
59419,E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Using Tensorflow with Freqtrade, seems to be occurring on a few installs using some custom coding. I've tracked down the class I think it is coming from.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Restricted Boltzmann Machine

class RBM():
    def __init__(self, nv=28 * 28, nh=512, cd_steps=3):
        tf.random.set_seed(42)
        self.graph = tf.Graph()
        with self.graph.as_default():
            tf.random.set_seed(42)
            self.W = tf.Variable(tf.compat.v1.truncated_normal((nv, nh)) * 0.01)
            self.bv = tf.Variable(tf.zeros((nv, 1)))
            self.bh = tf.Variable(tf.zeros((nh, 1)))

            self.cd_steps = cd_steps
            self.modelW = None

    def bernoulli(self, p):
        return tf.nn.relu(tf.sign(p - tf.compat.v1.random_uniform(p.shape)))

    def energy(self, v):
        b_term = tf.matmul(v, self.bv)
        linear_tranform = tf.matmul(v, self.W) + tf.squeeze(self.bh)
        h_term = tf.reduce_sum(tf.compat.v1.log(tf.exp(linear_tranform) + 1), axis=1)
        return tf.reduce_mean(-h_term - b_term)

    def sample_h(self, v):
        ph_given_v = tf.sigmoid(tf.matmul(v, self.W) + tf.squeeze(self.bh))
        return self.bernoulli(ph_given_v)

    def sample_v(self, h):
        pv_given_h = tf.sigmoid(tf.matmul(h, tf.transpose(self.W)) + tf.squeeze(self.bv))
        return self.bernoulli(pv_given_h)

    def gibbs_step(self, i, k, vk):
        hk = self.sample_h(vk)
        vk = self.sample_v(hk)
        return i + 1, k, vk

    def train(self, X, lr=0.01, batch_size=64, epochs=5):
        with self.graph.as_default():
            tf_v = tf.compat.v1.placeholder(tf.float32, [batch_size, self.bv.shape[0]])
            v = tf.round(tf_v)
            vk = tf.identity(v)

            i = tf.constant(0)
            _, _, vk = tf.while_loop(cond=lambda i, k, *args: i <= k,
                                     body=self.gibbs_step,
                                     loop_vars=[i, tf.constant(self.cd_steps), vk],
                                     parallel_iterations=1,
                                     back_prop=False)

            vk = tf.stop_gradient(vk)
            loss = self.energy(v) - self.energy(vk)
            optimizer = tf.compat.v1.train.AdamOptimizer(lr).minimize(loss)
            init = tf.compat.v1.global_variables_initializer()

        with tf.compat.v1.Session(graph=self.graph) as sess:
            init.run()
            for epoch in range(epochs):
                losses = []
                for i in range(0, len(X) - batch_size, batch_size):
                    x_batch = X[i:i + batch_size]
                    l, _ = sess.run([loss, optimizer], feed_dict={tf_v: x_batch})
                    losses.append(l)
                print('Epoch Cost %d: ' % epoch, np.mean(losses), end='\r')
            self.modelW = self.W.eval()
```


### Relevant log output

```shell
E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']
caused by: ['/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6Status22MaybeAddSourceLocationENS_14SourceLocationE']
  warnings.warn(f""unable to load libtensorflow_io_plugins.so: {e}"")
/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: ['/home/ubuntu/freqtrade/.env/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']
  warnings.warn(f""file system plugins are not loaded: {e}"")
unknown 2.12.0-dev20230120
```
</details>"
59417,Crash when running tensorflow.python.ops.nn_ops.convolution,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 3, 6, 1, 0], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([1, 1, 1, 1, 1], dtype=tf.float32)
  arg_1 = tf.identity(arg_1_tensor)
  padding = ""VALID""
  strides_0 = 1
  strides_1 = 1
  strides_2 = 1
  strides = [strides_0,strides_1,strides_2,]
  dilation_rate_0 = 2
  dilation_rate_1 = 1
  dilation_rate_2 = 1
  dilation_rate = [dilation_rate_0,dilation_rate_1,dilation_rate_2,]
  data_format = ""NDHWC""
  out = nn_ops.convolution(arg_0,arg_1,padding=padding,strides=strides,dilation_rate=dilation_rate,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 18:06:42.366272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 18:06:42.467404: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 18:06:42.891100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 18:06:42.891312: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 18:06:42.891319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 18:06:43.372431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.382231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.382337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.382630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 18:06:43.383069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.383218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.383339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.735408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.735577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.735694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:06:43.735776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4304 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 18:06:44.316966: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2023-01-22 18:06:45.023547: F tensorflow/stream_executor/cuda/cuda_dnn.cc:804] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
</details>"
59416,Segfault when running tensorflow.python.ops.gen_sparse_ops.sparse_concat,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0 = []
  arg_1_0_tensor = tf.convert_to_tensor(np.ones([4], dtype=str))
  arg_1_0 = tf.identity(arg_1_0_tensor)
  arg_1_1_tensor = tf.convert_to_tensor(np.ones([6], dtype=str))
  arg_1_1 = tf.identity(arg_1_1_tensor)
  arg_1 = [arg_1_0,arg_1_1,]
  arg_2_0_tensor = tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int64)
  arg_2_0 = tf.identity(arg_2_0_tensor)
  arg_2_1_tensor = tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int64)
  arg_2_1 = tf.identity(arg_2_1_tensor)
  arg_2 = [arg_2_0,arg_2_1,]
  arg_3 = 1
  out = gen_sparse_ops.sparse_concat(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 18:04:13.670223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 18:04:13.766992: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 18:04:14.194664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 18:04:14.194828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 18:04:14.194835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 18:04:14.686955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:14.692203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:14.692312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:14.692614: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 18:04:14.693090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:14.693208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:14.693333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:15.052698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:15.052898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:15.052994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 18:04:15.053079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4302 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59415,Crash when running tensorflow.python.ops.gen_array_ops.upper_bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = tf.int32
      arg_3 = None
      out = gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.int32
      gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:58:29.262403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:58:29.358214: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:58:29.774374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:58:29.774529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:58:29.774536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:58:30.250579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.257130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.257254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.257554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:58:30.258364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.258530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.258645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.611427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.611587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.611694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:58:30.611786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4297 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
free(): invalid pointer
Aborted

```
```
</details>"
59414,Unknown crash when running tensorflow.python.ops.gen_nn_ops.max_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 6, 8, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  ksize_0 = 1
  ksize_1 = 3
  ksize_2 = -16
  ksize_3 = 1
  ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
  strides_0 = 1
  strides_1 = 1
  strides_2 = 1
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""VALID""
  explicit_paddings = []
  data_format = ""NHWC""
  out = gen_nn_ops.max_pool(arg_0,ksize=ksize,strides=strides,padding=padding,explicit_paddings=explicit_paddings,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:52:34.603243: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:52:34.697556: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:52:35.114243: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:52:35.114398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:52:35.114405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:52:35.589151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.595319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.595425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.595718: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:52:35.596417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.596518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.596607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.944546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.944690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.944781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:52:35.944869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4299 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 17:52:36.501008: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2023-01-22 17:52:36.501367: F tensorflow/stream_executor/cuda/cuda_dnn.cc:886] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
</details>"
59413,tensorflow doesn't work with CUDA 12 on WSL2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

No

### OS Platform and Distribution

WSL2 Ubuntu 22.04

### Mobile device

N/A

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running tensorflow installation verification command right after pip install, both regular release and nightly, tensorflow was looking for CUDA 11 libraries, instead of 12.
```


### Standalone code to reproduce the issue

```shell
1. follow this instruction to enable GPU in WSL2 and tested sample app fine. 
https://ubuntu.com/tutorials/enabling-gpu-acceleration-on-ubuntu-on-wsl2-with-the-nvidia-cuda-platform#3-install-nvidia-cuda-on-ubuntu
2. run nvidia-smi command with correct output
3. pip install tf-nightly or tensorflow ran fine.

Both command 4 and 5 with error no finding CUDA 12 libraries even though the LD_LIBRARY_PATH is set correctly. 

4.python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" 
5.python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
```


### Relevant log output

```shell
nvidia-smi command output:

# nvidia-smi

Sun Jan 22 17:07:57 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 527.92.01    Driver Version: 528.02       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:0B:00.0  On |                  Off |
|  0%   34C    P5    35W / 450W |   1619MiB / 24564MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A        23      G   /Xwayland                       N/A      |
+-----------------------------------------------------------------------------+

NVCC command output.

./nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Mon_Oct_24_19:12:58_PDT_2022
Cuda compilation tools, release 12.0, V12.0.76
Build cuda_12.0.r12.0/compiler.31968024_0

""python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"""" command output.

python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
2023-01-22 17:41:38.117845: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:41:38.193983: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:38.194028: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-01-22 17:41:38.214984: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
2023-01-22 17:41:38.638503: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:38.638571: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:38.638592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:41:39.158693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-01-22 17:41:39.158768: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.158808: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.158844: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.158877: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.167698: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.167763: W tensorflow/tsl/platform/default/dso_loader.cc:67] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-01-22 17:41:39.167772: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1955] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-22 17:41:39.168021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
tf.Tensor(-1520.5212, shape=(), dtype=float32)
```
</details>"
59412,Segfault when running tensorflow.python.ops.gen_sparse_ops.sparse_cross,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices = []
  values_0_tensor = tf.convert_to_tensor(np.ones([3], dtype=str))
  values_0 = tf.identity(values_0_tensor)
  values_1_tensor = tf.convert_to_tensor(np.ones([1], dtype=str))
  values_1 = tf.identity(values_1_tensor)
  values_2_tensor = tf.convert_to_tensor(np.ones([2], dtype=str))
  values_2 = tf.identity(values_2_tensor)
  values = [values_0,values_1,values_2,]
  shapes_0_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_0 = tf.identity(shapes_0_tensor)
  shapes_1_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_1 = tf.identity(shapes_1_tensor)
  shapes_2_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_2 = tf.identity(shapes_2_tensor)
  shapes = [shapes_0,shapes_1,shapes_2,]
  dense_inputs = []
  hashed_output = True
  num_buckets = 1000
  hash_key = 956888297471
  out_type = tf.int64
  internal_type = tf.string
  out = gen_sparse_ops.sparse_cross(indices=indices,values=values,shapes=shapes,dense_inputs=dense_inputs,hashed_output=hashed_output,num_buckets=num_buckets,hash_key=hash_key,out_type=out_type,internal_type=internal_type,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:38:00.648068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:38:00.744375: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:38:01.170511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:38:01.170671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:38:01.170678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:38:01.652854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:01.659137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:01.659243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:01.659534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:38:01.660386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:01.660489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:01.660580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:02.013691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:02.013832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:02.013927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:38:02.014013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4254 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59411,Segfault when running tensorflow.python.ops.gen_nn_ops.fractional_max_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([5, 20, 30, 3], minval=-256, maxval=257, dtype=tf.int64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1
  arg_1_1 = 4.732050807568877
  arg_1_2 = -42.58578643762691
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 123456
  out = gen_nn_ops.fractional_max_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:22:04.596208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:22:04.921633: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:22:05.669342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:22:05.669696: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:22:05.669704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:22:06.548813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:06.573188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:06.573310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:06.574783: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:22:06.575530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:06.575651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:06.575779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:07.225136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:07.225537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:07.225634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:22:07.225718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4298 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59410,Process get killed when running tensorflow.python.ops.linalg_ops.matrix_solve_ls,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import linalg_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([16, 8, 127, 64, 1024], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([16, 8, 127, 1], dtype=tf.float64)
      arg_1 = tf.identity(arg_1_tensor)
      fast = True
      l2_regularizer = 0.0
      out = linalg_ops.matrix_solve_ls(arg_0,arg_1,fast=fast,l2_regularizer=l2_regularizer,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float64)
      linalg_ops.matrix_solve_ls(arg_0,arg_1,fast=fast,l2_regularizer=l2_regularizer,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:20:10.941818: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:20:11.263211: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:20:12.030442: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:20:12.030745: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:20:12.030753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:20:12.949847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:12.977797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:12.977916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:12.979391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:20:12.980124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:12.980262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:12.980370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:13.588750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:13.589162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:13.589259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:20:13.589343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4286 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 17:20:13.608346: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4261412864 exceeds 10% of free system memory.
Error:cannot compute MatrixSolveLs as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:MatrixSolveLs]
Killed

```
```
</details>"
59409,CUDA launch failure when running tensorflow.python.ops.gen_nn_ops.fused_batch_norm_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 1, 6, 2], dtype=tf.float16)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      epsilon = -3.999
      exponential_avg_factor = 1.0
      data_format = ""NHWC""
      is_training = False
      out = gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float16)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.float32)
      gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:18:14.055020: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:18:14.150250: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:18:14.568912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:18:14.569069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:18:14.569076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:18:15.045405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.052330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.052475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.052813: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:18:15.053492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.053640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.053731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.405633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.405784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.405878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:18:15.405964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4328 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 17:18:16.038099: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2023-01-22 17:18:16.040389: E tensorflow/stream_executor/dnn.cc:868] CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(5448): 'cudnnBatchNormalizationForwardInference( cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(), estimated_mean.opaque(), maybe_inv_var, epsilon)'
Error:{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([1,1,6,2]) [Op:FusedBatchNormV3]

```
```
</details>"
59408,CUDA launch failure when running tensorflow.python.ops.gen_nn_ops.fused_batch_norm_grad_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      y_backprop_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      y_backprop = tf.identity(y_backprop_tensor)
      x_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      x = tf.identity(x_tensor)
      scale_tensor = tf.random.uniform([2], dtype=tf.float32)
      scale = tf.identity(scale_tensor)
      reserve_space_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      epsilon = -21.999
      data_format = ""NHWC""
      is_training = True
      reserve_space_3_tensor = tf.random.uniform([], dtype=tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      out = gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      y_backprop = tf.identity(y_backprop_tensor)
      y_backprop = tf.cast(y_backprop, tf.float32)
      x = tf.identity(x_tensor)
      x = tf.cast(x, tf.float32)
      scale = tf.identity(scale_tensor)
      scale = tf.cast(scale, tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_1 = tf.cast(reserve_space_1, tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      reserve_space_2 = tf.cast(reserve_space_2, tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      reserve_space_3 = tf.cast(reserve_space_3, tf.float32)
      gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 17:02:26.555737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:02:26.649876: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-22 17:02:27.067009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:02:27.067164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/tf-2.10/lib/
2023-01-22 17:02:27.067171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 17:02:27.546127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.552775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.552903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.553197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 17:02:27.553646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.553745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.553833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.911000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.911133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.911223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 17:02:27.911307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4200 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 17:02:28.500075: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2023-01-22 17:02:28.501974: E tensorflow/stream_executor/dnn.cc:868] CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(5594): 'cudnnBatchNormalizationBackward( cudnn.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y_backprop.opaque(), x_descriptor.handle(), x_backprop->opaque(), scale_offset_descriptor.handle(), scale.opaque(), scale_backprop->opaque(), offset_backprop->opaque(), epsilon, mean.opaque(), inv_var.opaque())'
Error:{{function_node __wrapped__FusedBatchNormGradV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([4,10,10,2]) [Op:FusedBatchNormGradV3]

```
```
</details>"
59406,Segfault when running tensorflow.python.ops.gen_array_ops.upper_bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 1, 0], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = tf.int32
      arg_3 = None
      out = gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.int32
      gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 11:31:45.460968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:31:46.001164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:31:46.001329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:31:46.001336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 11:31:46.481782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.486235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.486347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.486642: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:31:46.487105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.487222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.487314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.846119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.846261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.846358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:31:46.846441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4292 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59405,Crash when running tensorflow.python.ops.gen_nn_ops.fractional_max_pool_grad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 4, 4, 16], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([1, 3, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([1, 3, 2, 1], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_0 = 0
      arg_3_1 = 1
      arg_3_2 = 3
      arg_3_3 = 4
      arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
      arg_4_0 = 0
      arg_4_1 = 2
      arg_4_2 = 4
      arg_4 = [arg_4_0,arg_4_1,arg_4_2,]
      overlapping = False
      out = gen_nn_ops.fractional_max_pool_grad(arg_0,arg_1,arg_2,arg_3,arg_4,overlapping=overlapping,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
      arg_4 = [arg_4_0,arg_4_1,arg_4_2,]
      gen_nn_ops.fractional_max_pool_grad(arg_0,arg_1,arg_2,arg_3,arg_4,overlapping=overlapping,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 11:29:39.337357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:29:39.859147: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:29:39.859307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:29:39.859314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 11:29:40.338103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.344914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.345065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.345444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:29:40.345940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.346090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.346198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.704416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.704550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.704643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:29:40.704725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4283 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:{{function_node __wrapped__FractionalMaxPoolGrad_device_/job:localhost/replica:0/task:0/device:CPU:0}} tensor_out_dup is not the same as tensor_out [Op:FractionalMaxPoolGrad]
malloc(): unaligned fastbin chunk detected
Aborted

```
```
</details>"
59404,Segmentation fault when running tensorflow.python.ops.nn_ops.fractional_avg_pool_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = 2
  arg_1_2 = 1
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  seed = 341261001
  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 11:26:14.586743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:26:15.113767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:26:15.113980: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:26:15.113987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 11:26:15.590134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.594662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.594771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.595070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:26:15.595501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.595617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.595709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.960226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.960359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.960450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:26:15.960531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4280 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59403,Crash when running ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 6, 8, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  ksize_0 = 1
  ksize_1 = 1
  ksize_2 = 0
  ksize_3 = 1
  ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
  strides_0 = 1
  strides_1 = 1
  strides_2 = 1
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""VALID""
  explicit_paddings = []
  data_format = ""NHWC""
  out = gen_nn_ops.max_pool(arg_0,ksize=ksize,strides=strides,padding=padding,explicit_paddings=explicit_paddings,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 11:22:43.127824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:22:43.643039: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:22:43.643196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:22:43.643203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 11:22:44.118255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.124694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.124806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.125150: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:22:44.125786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.125926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.126017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.484008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.484181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.484305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:22:44.484403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4269 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 11:22:45.031828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2023-01-22 11:22:45.031891: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:958] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
</details>"
59402,Crash when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3, 3, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 11:14:52.119880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:14:52.640176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:14:52.640329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 11:14:52.640336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 11:14:53.114162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.120472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.120581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.120880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 11:14:53.121441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.121559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.121681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.477386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.477531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.477621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 11:14:53.477706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4253 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 11:14:53.491348: F tensorflow/core/kernels/maxpooling_op.cc:1065] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 166, 0, 18
Aborted

```
```
</details>"
59401,JIT compilation error when running tensorflow.python.ops.math_ops.floordiv,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import math_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -16
  out = math_ops.floordiv(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 10:56:25.836550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 10:56:26.361088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 10:56:26.361287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 10:56:26.361293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 10:56:26.842785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:26.847302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:26.847412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:26.847700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 10:56:26.848332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:26.848434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:26.848523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:27.205811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:27.205945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:27.206037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:56:27.206118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4294 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2023-01-22 10:56:27.388856: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: JIT compilation failed.
Error:can't convert negative int to unsigned

```
```
</details>"
59400,Crash when running tensorflow.python.ops.gen_nn_ops.fused_batch_norm_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      epsilon = -7.999
      exponential_avg_factor = 1.0
      data_format = ""NHWC""
      is_training = False
      out = gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.float32)
      gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 10:50:17.946297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 10:50:18.459583: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 10:50:18.459739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 10:50:18.459746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 10:50:18.926143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:18.930560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:18.930665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:18.930955: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 10:50:18.931633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:18.931734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:18.931822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:19.289017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:19.289150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:19.289240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 10:50:19.289322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4275 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 10:50:19.844187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2023-01-22 10:50:19.846342: E tensorflow/compiler/xla/stream_executor/dnn.cc:887] CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5790): 'cudnnBatchNormalizationForwardInference( cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(), estimated_mean.opaque(), maybe_inv_var, epsilon)'
Error:{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([4,10,10,2]) [Op:FusedBatchNormV3]

```
```
</details>"
59399,Crash when running tensorflow.python.ops.gen_nn_ops.fused_batch_norm_grad_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      y_backprop_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      y_backprop = tf.identity(y_backprop_tensor)
      x_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      x = tf.identity(x_tensor)
      scale_tensor = tf.random.uniform([2], dtype=tf.float32)
      scale = tf.identity(scale_tensor)
      reserve_space_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      epsilon = -43.999
      data_format = ""NHWC""
      is_training = True
      reserve_space_3_tensor = tf.random.uniform([], dtype=tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      out = gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      y_backprop = tf.identity(y_backprop_tensor)
      y_backprop = tf.cast(y_backprop, tf.float32)
      x = tf.identity(x_tensor)
      x = tf.cast(x, tf.float32)
      scale = tf.identity(scale_tensor)
      scale = tf.cast(scale, tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_1 = tf.cast(reserve_space_1, tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      reserve_space_2 = tf.cast(reserve_space_2, tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      reserve_space_3 = tf.cast(reserve_space_3, tf.float32)
      gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 05:52:28.151968: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:52:28.716365: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:52:28.716529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:52:28.716537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 05:52:29.223621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.228281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.228421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.228740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:52:29.229170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.229271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.229359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.601354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.601502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.601600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:52:29.601697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3521 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 05:52:30.152358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2023-01-22 05:52:30.154285: E tensorflow/compiler/xla/stream_executor/dnn.cc:887] CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5936): 'cudnnBatchNormalizationBackward( cudnn.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y_backprop.opaque(), x_descriptor.handle(), x_backprop->opaque(), scale_offset_descriptor.handle(), scale.opaque(), scale_backprop->opaque(), offset_backprop->opaque(), epsilon, mean.opaque(), inv_var.opaque())'
Error:{{function_node __wrapped__FusedBatchNormGradV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([4,10,10,2]) [Op:FusedBatchNormGradV3]

```
```
</details>"
59398,"Checkpointing simple from_generator dataset iterator gives ""PyFunc is stateful"" error.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to build a simple checkpointable dataset. As a first step trying to mimic the behavior of `tf.data.Dataset.range` so that it can be made more complex. Building the iteration logic via a generator results in `PyFunc is stateful` error.
```


### Standalone code to reproduce the issue

```shell
def count():
  i = 0
  while i < 10:
    yield i
    i += 1


ds = tf.data.Dataset.from_generator(
    count, output_signature=tf.TensorSpec(shape=(), dtype=np.int32))

ckpt = tf.train.Checkpoint(iterator=it)
manager = tf.train.CheckpointManager(ckpt, '~/tf-ckpt', max_to_keep=2)
manager.save()
```


### Relevant log output

```shell
_NotOkStatusException: FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} PyFunc is stateful. [Op:SerializeIterator]
```
</details>"
59397,Segfault when running tensorflow.python.ops.gen_logging_ops._print,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to nan input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_logging_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.int8)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0_tensor = tf.cast(tf.random.uniform([3], minval=0, maxval=2, dtype=tf.int32), dtype=tf.bool)
  arg_1_0 = tf.identity(arg_1_0_tensor)
  arg_1 = [arg_1_0,]
  arg_2 = ""nan""
  arg_3 = None
  arg_4 = -0.26650310319346027
  arg_5 = ""nan""
  out = gen_logging_ops._print(arg_0,arg_1,arg_2,arg_3,arg_4,arg_5,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 05:45:40.565981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:45:41.084311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:45:41.084470: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:45:41.084478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 05:45:41.563115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.569995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.570117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.570411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:45:41.571019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.571141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.571230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.922406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.922534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.922624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:45:41.922703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4267 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59396,JIT compilation faild when running tensorflow.python.keras.backend.clip,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to empty tensor or negative argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.keras import backend
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = -674
      arg_2 = False
      out = backend.clip(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      backend.clip(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 05:38:22.265102: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:38:22.793808: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:38:22.793970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:38:22.793978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 05:38:23.279089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.283623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.283730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.284021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:38:23.284444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.284547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.284639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.645697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.645831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.645923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:38:23.646007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4244 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:can't convert negative int to unsigned
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2023-01-22 05:38:23.806315: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: JIT compilation failed.
Error:{{function_node __wrapped__Minimum_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Minimum]
```
```
</details>"
59395,Segfault when running tensorflow.python.ops.string_ops.string_format,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault probably due to NaN input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import string_ops
try:
  inputs_0_tensor = tf.cast(tf.random.uniform([2, 2], minval=0, maxval=2, dtype=tf.int32), dtype=tf.bool)
  inputs_0 = tf.identity(inputs_0_tensor)
  inputs = [inputs_0,]
  template = ""nan""
  placeholder = ""nan""
  summarize = -0.6813518706001572
  out = string_ops.string_format(inputs=inputs,template=template,placeholder=placeholder,summarize=summarize,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 05:35:08.288147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:35:08.826735: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:35:08.826898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:35:08.826905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 05:35:09.310278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.314831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.314959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.315262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:35:09.315676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.315779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.315870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.671291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.671434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.671527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:35:09.671607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4230 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59394,Memory corruption when running tensorflow.python.ops.gen_nn_ops.fractional_max_pool_grad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to large tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 4, 4, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.constant(-76000825245129, shape=[1, 1, 2, 1], dtype=tf.float32,)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([1, 3, 2, 1], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_0 = -37
      arg_3_1 = -89
      arg_3_2 = -82
      arg_3_3 = -83
      arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
      arg_4_0 = 0
      arg_4_1 = 2
      arg_4_2 = 4
      arg_4 = [arg_4_0,arg_4_1,arg_4_2,]
      overlapping = False
      out = gen_nn_ops.fractional_max_pool_grad(arg_0,arg_1,arg_2,arg_3,arg_4,overlapping=overlapping,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
      arg_4 = [arg_4_0,arg_4_1,arg_4_2,]
      gen_nn_ops.fractional_max_pool_grad(arg_0,arg_1,arg_2,arg_3,arg_4,overlapping=overlapping,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 05:22:53.047609: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:22:53.565262: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:22:53.565488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 05:22:53.565494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 05:22:54.054377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.061146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.061253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.061547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 05:22:54.062085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.062200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.062290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.424849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.424988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.425080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 05:22:54.425163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4273 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:{{function_node __wrapped__FractionalMaxPoolGrad_device_/job:localhost/replica:0/task:0/device:CPU:0}} tensor_out_dup is not the same as tensor_out [Op:FractionalMaxPoolGrad]
malloc(): memory corruption (fast)
Aborted
```
```
</details>"
59393,Segmentation fault when running tensorflow.python.ops.gen_sparse_ops.sparse_fill_empty_rows,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segfault due to Large or empty tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  try:
    with tf.device('/CPU'):
      indices_tensor = tf.constant(-8647818616220, shape=[6, 0], dtype=tf.int64,)
      indices = tf.identity(indices_tensor)
      values_tensor = tf.random.uniform([6], minval=-256, maxval=257, dtype=tf.int32)
      values = tf.identity(values_tensor)
      dense_shape_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
      dense_shape = tf.identity(dense_shape_tensor)
      default_value_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
      default_value = tf.identity(default_value_tensor)
      out = gen_sparse_ops.sparse_fill_empty_rows(indices=indices,values=values,dense_shape=dense_shape,default_value=default_value,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      indices = tf.identity(indices_tensor)
      indices = tf.cast(indices, tf.int64)
      values = tf.identity(values_tensor)
      values = tf.cast(values, tf.int32)
      dense_shape = tf.identity(dense_shape_tensor)
      dense_shape = tf.cast(dense_shape, tf.int64)
      default_value = tf.identity(default_value_tensor)
      default_value = tf.cast(default_value, tf.int32)
      gen_sparse_ops.sparse_fill_empty_rows(indices=indices,values=values,dense_shape=dense_shape,default_value=default_value,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 04:57:38.410770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 04:57:39.683814: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 04:57:39.684083: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 04:57:39.684094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 04:57:40.720528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:40.744520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:40.744847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:40.745340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 04:57:40.745919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:40.746106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:40.746271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:41.596881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:41.597155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:41.597418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:57:41.597585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4261 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59392,Error when running tensorflow.python.keras.layers.dense_attention._lower_triangular_mask,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.keras.layers import dense_attention
try:
  arg_0_tensor = tf.constant(-78450714517633, shape=[], dtype=tf.int32,)
  arg_0 = tf.identity(arg_0_tensor)
  out = dense_attention._lower_triangular_mask(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-22 04:45:46.895374: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 04:45:48.013379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 04:45:48.013574: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-22 04:45:48.013581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-22 04:45:48.955940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:48.979903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:48.980037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:48.981184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-22 04:45:48.982243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:48.982356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:48.982457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:49.605987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:49.606304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:49.606404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-22 04:45:49.606675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4193 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-22 04:45:49.668173: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4632444412 exceeds 10% of free system memory.
2023-01-22 04:46:00.072978: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.31GiB (rounded to 4632444416)requested by op Cumsum
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2023-01-22 04:46:00.072996: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc
2023-01-22 04:46:00.073002: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073007: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073012: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2023-01-22 04:46:00.073016: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073020: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073024: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073029: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073033: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073037: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073041: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073045: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073049: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073053: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073057: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073061: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073065: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073069: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073073: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073077: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073081: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073085: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): 	Total Chunks: 1, Chunks in use: 0. 4.09GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-22 04:46:00.073090: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 4.31GiB was 256.00MiB, Chunk State: 
2023-01-22 04:46:00.073096: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 4.09GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 1.2KiB | Requested Size: 1.0KiB | in_use: 1 | bin_num: -1
2023-01-22 04:46:00.073100: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4396810240
2023-01-22 04:46:00.073105: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7fad3c000000 of size 1280 next 1
2023-01-22 04:46:00.073108: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7fad3c000500 of size 4396808960 next 18446744073709551615
2023-01-22 04:46:00.073111: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: 
2023-01-22 04:46:00.073116: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB
2023-01-22 04:46:00.073119: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 1.2KiB
2023-01-22 04:46:00.073123: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 4396810240 memory_limit_: 4396810240 available bytes: 0 curr_region_allocation_bytes_: 8793620480
2023-01-22 04:46:00.073128: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: 
Limit:                      4396810240
InUse:                            1280
MaxInUse:                         1280
NumAllocs:                           1
MaxAllocSize:                     1280
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-01-22 04:46:00.073135: W tensorflow/tsl/framework/bfc_allocator.cc:492] *___________________________________________________________________________________________________
Error:Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Cumsum: Dst tensor is not initialized. [Op:Cumsum]

```
```
</details>"
59391,JIT compilation failed when running gen_math_ops.floor_div,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to negative input argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = -745
      out = gen_math_ops.floor_div(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      gen_math_ops.floor_div(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 20:46:52.586111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:46:53.206103: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:46:53.206274: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:46:53.206281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 20:46:53.803659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:53.808031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:53.808150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:53.808460: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:46:53.809011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:53.809150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:53.809272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:54.229679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:54.229812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:54.229908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:46:54.229991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4224 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:can't convert negative int to unsigned
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2023-01-21 20:46:54.397238: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: JIT compilation failed.
Error:can't convert negative int to unsigned

```
```
</details>"
59390,TFLiteConverter adds (de)quantization blocks before and after operations on a weight variable,"Hello

I'm trying to convert a TensorFlow model to TFLite. I want the model to maintain a non-trainable state vector that gets updated at every inference. I've implemented this using `self.add_weight` with `trainable=False`, as shown in the example.


### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux KDE Neon 5.26
- TensorFlow installation (pip package or built from source): pip installation
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

```

import tensorflow as tf
from keras import initializers

# Class Definition
class Net(tf.keras.Model):
    def __init__(self):
        super(Net, self).__init__()
        
        self.my_weight = self.add_weight(""MyWeight"",    
                                         shape=[1, 10, 1],
                                         initializer=initializers.get('Zeros'),
                                         trainable=False)

    @tf.function(input_signature=[tf.TensorSpec(shape=[1, 5, 1], name='MyInput'),])
    def call(self, x):
        x = x + self.my_weight[:, -5:, :]
        self.my_weight.assign(value=tf.concat((self.my_weight[:, -5:, :], x[:, :5, :]), axis=1))

        return x

import numpy as np

X = np.random.random((1, 5, 1)).astype(np.float32)

model = Net()

for i in range(5):
    Y = model(X, training=False)

# Save Model
model_path = ""my_saved_model""
tf.saved_model.save(model, model_path)
model.summary()

def representative_dataset():
  for i in range(50):
    yield {
      ""MyInput"": np.random.random((1, 5, 1)).astype(np.float32)
    }

converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8

# Convert the model 
tf_lite_model = converter.convert()

# Save the converted model 
open('my_tflite_model.tflite', 'wb').write(tf_lite_model)

```

### 3. Conversion Issue
The conversion process works fine, but when I draw the model on Netron, it seems that the converter adds quantize/dequantize operations everytime the variable is read or updated. Is there a way to tell the converter to quantize this variable from the start, without having to quantize and dequantize everytime it accesses it?

![model](https://user-images.githubusercontent.com/37956289/213896735-1390ffa0-2343-47bd-89aa-358b4297f87b.png)

On another note, my aim is to run such a model on an EdgeTPU, but the EdgeTPU compiler complains about dynamic-sized tensors, without indicating where they are. I believe the culprits are CallOnce, ReadVariable and/or AssignVariable, but I'm not entirely sure how to get around them. Any ideas? Thanks.

"
59389,Memory exhaustion when running tensorflow.python.ops.stateless_random_ops.stateless_random_uniform,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.constant(-49570942441335, shape=[1], dtype=tf.int32,)
      arg_0 = tf.identity(arg_0_tensor)
      dtype = tf.int32
      maxval = 2147483647
      seed_0 = 1
      seed_1 = 2
      seed = [seed_0,seed_1,]
      out = stateless_random_ops.stateless_random_uniform(arg_0,dtype=dtype,maxval=maxval,seed=seed,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int32)
      dtype = tf.int32
      seed = [seed_0,seed_1,]
      stateless_random_ops.stateless_random_uniform(arg_0,dtype=dtype,maxval=maxval,seed=seed,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 20:25:00.797918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:25:01.510771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:25:01.510967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:25:01.510976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 20:25:02.160219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.165584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.165725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.166060: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:25:02.166607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.166739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.166857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.595295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.595446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.595556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:25:02.595651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4278 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-21 20:25:02.611392: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 6280356388 exceeds 10% of free system memory.
2023-01-21 20:25:14.371226: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.85GiB (rounded to 6280356608)requested by op StatelessRandomUniformIntV2
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2023-01-21 20:25:14.371246: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc
2023-01-21 20:25:14.371255: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): 	Total Chunks: 2, Chunks in use: 2. 512B allocated for chunks. 512B in use in bin. 24B client-requested in use in bin.
2023-01-21 20:25:14.371264: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371274: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2023-01-21 20:25:14.371282: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371290: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371298: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371306: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371313: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371321: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371329: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371336: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371344: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371352: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371359: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371367: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371375: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371382: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371390: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371398: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371406: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371414: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): 	Total Chunks: 1, Chunks in use: 0. 4.18GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2023-01-21 20:25:14.371423: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 5.85GiB was 256.00MiB, Chunk State: 
2023-01-21 20:25:14.371434: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 4.18GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 16B | in_use: 1 | bin_num: -1
2023-01-21 20:25:14.371441: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 4486135808
2023-01-21 20:25:14.371449: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f9c38000000 of size 1280 next 1
2023-01-21 20:25:14.371456: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f9c38000500 of size 256 next 2
2023-01-21 20:25:14.371463: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f9c38000600 of size 256 next 3
2023-01-21 20:25:14.371470: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f9c38000700 of size 4486134016 next 18446744073709551615
2023-01-21 20:25:14.371477: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: 
2023-01-21 20:25:14.371485: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 256 totalling 512B
2023-01-21 20:25:14.371493: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB
2023-01-21 20:25:14.371500: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 1.8KiB
2023-01-21 20:25:14.371507: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 4486135808 memory_limit_: 4486135808 available bytes: 0 curr_region_allocation_bytes_: 8972271616
2023-01-21 20:25:14.371518: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: 
Limit:                      4486135808
InUse:                            1792
MaxInUse:                         1792
NumAllocs:                           3
MaxAllocSize:                     1280
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-01-21 20:25:14.371531: W tensorflow/tsl/framework/bfc_allocator.cc:492] *___________________________________________________________________________________________________
2023-01-21 20:25:14.371552: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at stateless_random_ops_v2.cc:67 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1570089097] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Error:{{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1570089097] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformIntV2]

```
```
</details>"
59388,Crash when running tensorflow.python.ops.math_ops.floordiv,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Negative arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import math_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([2, 2], minval=0, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -74
  out = math_ops.floordiv(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 20:19:58.818715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:19:59.375495: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:19:59.375659: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 20:19:59.375666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 20:19:59.867565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:19:59.871780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:19:59.871896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:19:59.872192: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 20:19:59.872611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:19:59.872716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:19:59.872819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:20:00.267194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:20:00.267327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:20:00.267423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 20:20:00.267509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4252 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2023-01-21 20:20:00.430496: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: JIT compilation failed.
Error:can't convert negative int to unsigned

```
```
</details>"
59387,Segfault when running tensorflow.python.ops.gen_string_ops.string_format,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault when feeding the following input combinations.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_string_ops
try:
  arg_0_0_tensor = tf.cast(tf.random.uniform([100, 100], minval=0, maxval=2, dtype=tf.int32), dtype=tf.bool)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0 = [arg_0_0,]
  template = ""{}""
  placeholder = """"
  summarize = False
  out = gen_string_ops.string_format(arg_0,template=template,placeholder=placeholder,summarize=summarize,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 19:56:19.403127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 19:56:20.109647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 19:56:20.109849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 19:56:20.109858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 19:56:20.744012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:20.749087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:20.749223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:20.749553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 19:56:20.750231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:20.750354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:20.750461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:21.186117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:21.186259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:21.186365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 19:56:21.186458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4238 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
</details>"
59386,CUDA launch failure when running ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
CUDA launch failure with the following input arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 2, 1, 6], dtype=tf.float16)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      epsilon = -0.4105469566119495
      exponential_avg_factor = 1.0
      data_format = ""NHWC""
      is_training = False
      out = gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float16)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.float32)
      gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:55:56.490432: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:55:57.095606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:55:57.095787: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:55:57.095796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:55:57.639581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:57.643978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:57.644100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:57.644401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:55:57.644818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:57.644932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:57.645048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:58.038876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:58.039025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:58.039125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:55:58.039212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4261 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-21 18:55:58.423108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2023-01-21 18:55:58.423398: E tensorflow/compiler/xla/stream_executor/dnn.cc:887] CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5790): 'cudnnBatchNormalizationForwardInference( cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(), estimated_mean.opaque(), maybe_inv_var, epsilon)'
Error:{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([1,2,1,6]) [Op:FusedBatchNormV3
```
```
</details>"
59385,Error when running tensorflow.python.ops.gen_math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Empty input
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.random.uniform([3, 3, 3], dtype=tf.float64)
      data = tf.identity(data_tensor)
      indices = []
      segment_ids = []
      out = gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float64)
      indices = []
      segment_ids = []
      gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:52:59.679628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:53:00.427623: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:53:00.427812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:53:00.427820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:53:01.052135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.057293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.057426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.057751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:53:01.058518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.058652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.058766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.497108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.497256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.497364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:53:01.497456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4276 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-21 18:53:01.555407: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1159] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x7fe84f600000; GPU src: 0xfffffffffffffffc; size: 4=0x4
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} SparseSegmentSum: failed to copy last_segment_id from device [Op:SparseSegmentSum]

```
```
</details>"
59384,Runtime error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to large tensor or negative argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.constant(-152265577042003, shape=[2, 3, 4], dtype=tf.int64,),dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -67
  arg_2 = 0
  range_given = False
  round_mode = ""HALF_UP""
  axis = None
  out = array_ops.quantize_and_dequantize_v2(arg_0,arg_1,arg_2,range_given=range_given,round_mode=round_mode,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:47:28.285233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:47:28.904062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:47:28.904236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:47:28.904243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:47:29.447639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.452132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.452261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.452556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:47:29.452998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.453105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.453198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.850356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.850491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.850591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:47:29.850679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4252 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:can't convert negative int to unsigned

```
```
</details>"
59383,Process get killed when running tensorflow.python.ops.ctc_ops._state_to_olabel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to large tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import ctc_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 4], minval=-256, maxval=257, dtype=tf.int32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = 125091515651
      arg_2_tensor = tf.random.uniform([3, 2, 10], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      out = ctc_ops._state_to_olabel(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      ctc_ops._state_to_olabel(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:41:56.825881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:41:57.928041: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:41:57.928214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:41:57.928221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:41:58.831767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:58.856337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:58.856463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:58.857603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:41:58.858581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:58.858727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:58.858843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:59.490418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:59.490746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:59.490844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:41:59.491116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4278 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-21 18:41:59.524171: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 17198850112 exceeds 10% of free system memory.
Killed

```
```
</details>"
59382,Process get killed when running tensorflow.python.ops.array_ops.sequence_mask,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to large tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.constant(-46590074299988, shape=[], dtype=tf.int32,)
      arg_1 = tf.identity(arg_1_tensor)
      dtype = tf.bool
      out = array_ops.sequence_mask(arg_0,arg_1,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.int32)
      dtype = tf.bool
      array_ops.sequence_mask(arg_0,arg_1,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:33:35.311833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:33:35.990997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:33:35.991187: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:33:35.991195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:33:36.592028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:36.597190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:36.597329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:36.597658: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:33:36.598153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:36.598280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:36.598390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:37.029708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:37.029858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:37.029969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:33:37.030064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4279 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-01-21 18:33:37.044992: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 6923708080 exceeds 10% of free system memory.
2023-01-21 18:33:39.035633: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5192781060 exceeds 10% of free system memory.
2023-01-21 18:33:41.541425: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 6923708080 exceeds 10% of free system memory.
Killed

```
```
</details>"
59381,Strange behavior when running tensorflow.python.ops.stateful_random_ops._make_1d_state,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run the following test cases, nothing happens and I have to stop the process by pressing CTRL+C. Not sure what is happening.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import stateful_random_ops
try:
  arg_0 = 125091515651
  arg_1 = 12345
  out = stateful_random_ops._make_1d_state(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:25:28.978159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:25:29.683389: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:25:29.683591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:25:29.683600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

```
```
</details>"
59380,Runtime error when running tensorflow.python.ops.array_ops.slice,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Runtime error probably due to large Tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.random.uniform([2, 3], minval=-256, maxval=257, dtype=tf.int32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0_tensor = tf.saturate_cast(tf.constant(-66726129659100, shape=[1], dtype=tf.int64,),dtype=tf.uint64)
  arg_1_0 = tf.identity(arg_1_0_tensor)
  arg_1_1 = -49
  arg_1 = [arg_1_0,arg_1_1,]
  arg_2_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_2_0 = tf.identity(arg_2_0_tensor)
  arg_2_1 = 2
  arg_2 = [arg_2_0,arg_2_1,]
  out = array_ops.slice(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 18:20:36.637134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:20:37.391777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:20:37.392009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-21 18:20:37.392022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 18:20:38.016015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.021077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.021217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.021540: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 18:20:38.022049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.022170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.022276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.445564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.445708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.445813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 18:20:38.445908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4253 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59379,CMake build of stand-alone mlir-hlo MLIRHLOPythonModules target is broken,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.3 

### Mobile device

_No response_

### Python version

3.8

### Bazel version

not using bazel

### GCC/Compiler version

10.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
When building https://github.com/tensorflow/mlir-hlo out of tree, the `MLIRHLOPythonModules` fails with undefined symbol errors. 

Fix in `mlir-hlo`:

diff --git a/thlo/IR/CMakeLists.txt b/thlo/IR/CMakeLists.txt
index a99ee8db..5667e512 100644
--- a/thlo/IR/CMakeLists.txt
+++ b/thlo/IR/CMakeLists.txt
@@ -34,6 +34,7 @@ add_mlir_dialect_library(THLODialect
   MLIRGmlStTilingInterfaceIncGen
 
   LINK_LIBS PUBLIC
+  GmlStTilingInterface
   GmlStDialect
   MLIRDestinationStyleOpInterface
   MLIRIR
```
```


### Standalone code to reproduce the issue

```shell
To reproduce, configure and build LLVM (sha de3f0f7fa0c7b902dde840913db7e773a02c4173) with the following (the mlir-hlo repository is cloned in `$REPO_ROOT`):


cd llvm-project && mkdir build && cd build
cmake -G Ninja ../llvm \
                -DLLVM_ENABLE_PROJECTS=mlir \
                -DLLVM_EXTERNAL_PROJECTS=""mlir-hlo"" \
                -DLLVM_EXTERNAL_MLIR_HLO_SOURCE_DIR=$REPO_ROOT/mlir-hlo \
                -DCMAKE_BUILD_TYPE=RelWithDebInfo \
                -DLLVM_ENABLE_ASSERTIONS=ON \
                -DLLVM_TARGETS_TO_BUILD=""X86"" \
                -DMLIR_ENABLE_BINDINGS_PYTHON=ON \
                -DMHLO_ENABLE_BINDINGS_PYTHON=ON 
cmake --build . -t MLIRHLOPythonModules
```
```


### Relevant log output

```shell
Link errors for `cmake --build . -t MLIRHLOPythonModules` in `llvm-project/build`:

[50/64] Linking CXX shared library tools/mlir-hlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git
FAILED: tools/mlir-hlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git 
: && /usr/bin/g++-10 -fPIC -fPIC -fno-semantic-interposition -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wdelete-non-virtual-dtor -Wsuggest-override -Wno-comment -Wno-misleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -fPIC -fno-semantic-interposition -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wdelete-non-virtual-dtor -Wsuggest-override -Wno-comment -Wno-misleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -O2 -g -DNDEBUG  -Wl,-z,defs -Wl,-z,nodelete -Wl,-z,defs -Wl,-z,nodelete   -Wl,-rpath-link,/compiler/boyana.norris/developer/llvm/build/./lib  -Wl,--gc-sections  -Wl,-z,defs -shared -Wl,-soname,libMLIRHLOCAPI.so.16git -o tools/mlir-hlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIAsync.dir/Async.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIAsync.dir/AsyncPasses.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIGPU.dir/GPU.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIGPU.dir/GPUPasses.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/AffineExpr.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/AffineMap.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/BuiltinAttributes.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/BuiltinTypes.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Diagnostics.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/DialectHandle.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/IntegerSet.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/IR.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Pass.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Support.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPILinalg.dir/Linalg.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPILinalg.dir/LinalgPasses.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPITransformDialect.dir/Transform.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIQuant.dir/Quant.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIPDL.dir/PDL.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPISparseTensor.dir/SparseTensor.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPISparseTensor.dir/SparseTensorPasses.cpp.o tools/mlir/lib/CAPI/Debug/CMakeFiles/obj.MLIRCAPIDebug.dir/Debug.cpp.o tools/mlir/lib/CAPI/Interfaces/CMakeFiles/obj.MLIRCAPIInterfaces.dir/Interfaces.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIFunc.dir/Func.cpp.o tools/mlir/lib/CAPI/ExecutionEngine/CMakeFiles/obj.MLIRCAPIExecutionEngine.dir/ExecutionEngine.cpp.o tools/mlir/lib/CAPI/Conversion/CMakeFiles/obj.MLIRCAPIConversion.dir/Passes.cpp.o tools/mlir/lib/CAPI/Transforms/CMakeFiles/obj.MLIRCAPITransforms.dir/Passes.cpp.o tools/mlir/lib/CAPI/RegisterEverything/CMakeFiles/obj.MLIRCAPIRegisterEverything.dir/RegisterEverything.cpp.o tools/mlir-hlo/bindings/c/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Attributes.cc.o tools/mlir-hlo/bindings/c/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Dialects.cc.o tools/mlir-hlo/bindings/c/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Types.cc.o tools/mlir-hlo/bindings/c/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Passes.cc.o  -Wl,-rpath,""\$ORIGIN:""  lib/libMLIRAsyncDialect.a  lib/libMLIRAsyncTransforms.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRGPUTransforms.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRBytecodeWriter.a  lib/libMLIRIR.a  lib/libMLIRParser.a  lib/libMLIRSupport.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRLinalgDialect.a  lib/libMLIRPass.a  lib/libMLIRLinalgTransforms.a  lib/libLLVMSupport.a  lib/libMLIRTransformDialect.a  lib/libLLVMSupport.a  lib/libMLIRQuantDialect.a  lib/libLLVMSupport.a  lib/libMLIRPDLDialect.a  lib/libLLVMSupport.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libLLVMSupport.a  lib/libMLIRSupport.a  lib/libLLVMSupport.a  lib/libMLIRInferTypeOpInterface.a  lib/libLLVMSupport.a  lib/libMLIRFuncDialect.a  lib/libLLVMSupport.a  lib/libMLIRExecutionEngine.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libLLVMSupport.a  lib/libLLVMX86CodeGen.a  lib/libLLVMX86Desc.a  lib/libLLVMX86Info.a  lib/libLLVMX86CodeGen.a  lib/libLLVMX86AsmParser.a  lib/libLLVMX86Desc.a  lib/libLLVMX86Disassembler.a  lib/libLLVMX86Info.a  lib/libMLIRAffineToStandard.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRArithAttrToLLVMConversion.a  lib/libMLIRArithToLLVM.a  lib/libMLIRArithToSPIRV.a  lib/libMLIRArmNeon2dToIntr.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRBufferizationToMemRef.a  lib/libMLIRComplexToLLVM.a  lib/libMLIRComplexToLibm.a  lib/libMLIRComplexToStandard.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRControlFlowToSPIRV.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRGPUToNVVMTransforms.a  lib/libMLIRGPUToROCDLTransforms.a  lib/libMLIRGPUToSPIRV.a  lib/libMLIRGPUToVulkanTransforms.a  lib/libMLIRIndexToLLVM.a  lib/libMLIRLinalgToLLVM.a  lib/libMLIRLinalgToStandard.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRMathToFuncs.a  lib/libMLIRMathToLibm.a  lib/libMLIRMathToLLVM.a  lib/libMLIRMathToSPIRV.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRNVGPUToNVVM.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIROpenACCToSCF.a  lib/libMLIROpenMPToLLVM.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRReconcileUnrealizedCasts.a  lib/libMLIRSCFToControlFlow.a  lib/libMLIRSCFToGPU.a  lib/libMLIRSCFToOpenMP.a  lib/libMLIRSCFToSPIRV.a  lib/libMLIRShapeToStandard.a  lib/libMLIRSPIRVToLLVM.a  lib/libMLIRTensorToLinalg.a  lib/libMLIRTensorToSPIRV.a  lib/libMLIRTosaToArith.a  lib/libMLIRTosaToLinalg.a  lib/libMLIRTosaToSCF.a  lib/libMLIRTosaToTensor.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRVectorToGPU.a  lib/libMLIRVectorToSCF.a  lib/libMLIRVectorToSPIRV.a  lib/libLLVMSupport.a  lib/libMLIRTransforms.a  lib/libLLVMSupport.a  lib/libMLIRAffineAnalysis.a  lib/libMLIRAffineDialect.a  lib/libMLIRAffineTransforms.a  lib/libMLIRAffineTransformOps.a  lib/libMLIRAffineUtils.a  lib/libMLIRAMDGPUDialect.a  lib/libMLIRArithDialect.a  lib/libMLIRArithTransforms.a  lib/libMLIRArithUtils.a  lib/libMLIRArmNeonDialect.a  lib/libMLIRArmSVEDialect.a  lib/libMLIRArmSVETransforms.a  lib/libMLIRAsyncDialect.a  lib/libMLIRAsyncTransforms.a  lib/libMLIRAMXDialect.a  lib/libMLIRAMXTransforms.a  lib/libMLIRBufferizationDialect.a  lib/libMLIRBufferizationTransformOps.a  lib/libMLIRBufferizationTransforms.a  lib/libMLIRComplexDialect.a  lib/libMLIRControlFlowDialect.a  lib/libMLIRDLTIDialect.a  lib/libMLIREmitCDialect.a  lib/libMLIRFuncDialect.a  lib/libMLIRFuncTransforms.a  lib/libMLIRGPUOps.a  lib/libMLIRGPUTransforms.a  lib/libMLIRGPUTransformOps.a  lib/libMLIRIndexDialect.a  lib/libMLIRLinalgAnalysis.a  lib/libMLIRLinalgDialect.a  lib/libMLIRLinalgTransformOps.a  lib/libMLIRLinalgTransforms.a  lib/libMLIRLinalgUtils.a  lib/libMLIRLLVMIRTransforms.a  lib/libMLIRLLVMDialect.a  lib/libMLIRNVVMDialect.a  lib/libMLIRROCDLDialect.a  lib/libMLIRMathDialect.a  lib/libMLIRMathTransforms.a  lib/libMLIRMemRefDialect.a  lib/libMLIRMemRefTransformOps.a  lib/libMLIRMemRefTransforms.a  lib/libMLIRMemRefUtils.a  lib/libMLIRMLProgramDialect.a  lib/libMLIRNVGPUDialect.a  lib/libMLIRNVGPUUtils.a  lib/libMLIRNVGPUTransforms.a  lib/libMLIROpenACCDialect.a  lib/libMLIROpenMPDialect.a  lib/libMLIRPDLDialect.a  lib/libMLIRPDLInterpDialect.a  lib/libMLIRQuantDialect.a  lib/libMLIRQuantUtils.a  lib/libMLIRSCFDialect.a  lib/libMLIRSCFTransformOps.a  lib/libMLIRSCFTransforms.a  lib/libMLIRSCFUtils.a  lib/libMLIRShapeDialect.a  lib/libMLIRShapeOpsTransforms.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libMLIRSparseTensorPipelines.a  lib/libMLIRSparseTensorUtils.a  lib/libMLIRSPIRVDialect.a  lib/libMLIRSPIRVModuleCombiner.a  lib/libMLIRSPIRVConversion.a  lib/libMLIRSPIRVTransforms.a  lib/libMLIRSPIRVUtils.a  lib/libMLIRTensorDialect.a  lib/libMLIRTensorInferTypeOpInterfaceImpl.a  lib/libMLIRTensorTilingInterfaceImpl.a  lib/libMLIRTensorTransforms.a  lib/libMLIRTensorUtils.a  lib/libMLIRTosaDialect.a  lib/libMLIRTosaTransforms.a  lib/libMLIRTransformDialect.a  lib/libMLIRTransformDialectTransforms.a  lib/libMLIRTransformDialectUtils.a  lib/libMLIRVectorDialect.a  lib/libMLIRVectorTransforms.a  lib/libMLIRVectorTransformOps.a  lib/libMLIRVectorUtils.a  lib/libMLIRX86VectorDialect.a  lib/libMLIRX86VectorTransforms.a  lib/libMLIRTargetCpp.a  lib/libMLIRSPIRVDeserialization.a  lib/libMLIRSPIRVSerialization.a  lib/libMLIRSPIRVBinaryUtils.a  lib/libMLIRSPIRVTranslateRegistration.a  lib/libMLIRArmNeonToLLVMIRTranslation.a  lib/libMLIRArmSVEToLLVMIRTranslation.a  lib/libMLIRAMXToLLVMIRTranslation.a  lib/libMLIRLLVMIRToLLVMTranslation.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libMLIRNVVMToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVMIRTranslation.a  lib/libMLIROpenMPToLLVMIRTranslation.a  lib/libMLIRROCDLToLLVMIRTranslation.a  lib/libMLIRX86VectorToLLVMIRTranslation.a  lib/libMLIRTargetLLVMIRExport.a  lib/libMLIRToLLVMIRTranslationRegistration.a  lib/libMLIRTargetLLVMIRImport.a  lib/libMLIRFromLLVMIRTranslationRegistration.a  lib/libMLIRAffineToStandard.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRArithAttrToLLVMConversion.a  lib/libMLIRArithToLLVM.a  lib/libMLIRArithToSPIRV.a  lib/libMLIRArmNeon2dToIntr.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRBufferizationToMemRef.a  lib/libMLIRComplexToLLVM.a  lib/libMLIRComplexToLibm.a  lib/libMLIRComplexToStandard.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRControlFlowToSPIRV.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRGPUToNVVMTransforms.a  lib/libMLIRGPUToROCDLTransforms.a  lib/libMLIRGPUToSPIRV.a  lib/libMLIRGPUToVulkanTransforms.a  lib/libMLIRIndexToLLVM.a  lib/libMLIRLinalgToLLVM.a  lib/libMLIRLinalgToStandard.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRMathToFuncs.a  lib/libMLIRMathToLibm.a  lib/libMLIRMathToLLVM.a  lib/libMLIRMathToSPIRV.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRNVGPUToNVVM.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIROpenACCToSCF.a  lib/libMLIROpenMPToLLVM.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRReconcileUnrealizedCasts.a  lib/libMLIRSCFToControlFlow.a  lib/libMLIRSCFToGPU.a  lib/libMLIRSCFToOpenMP.a  lib/libMLIRSCFToSPIRV.a  lib/libMLIRShapeToStandard.a  lib/libMLIRSPIRVToLLVM.a  lib/libMLIRTensorToLinalg.a  lib/libMLIRTensorToSPIRV.a  lib/libMLIRTosaToArith.a  lib/libMLIRTosaToLinalg.a  lib/libMLIRTosaToSCF.a  lib/libMLIRTosaToTensor.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRVectorToGPU.a  lib/libMLIRVectorToSCF.a  lib/libMLIRVectorToSPIRV.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libLLVMSupport.a  lib/libMhloDialect.a  lib/libTHLODialect.a  lib/libChloPasses.a  lib/libMhloPasses.a  lib/libMhloToLhloConversion.a  lib/libMhloToArithmeticConversion.a  lib/libMhloToMemrefConversion.a  lib/libMhloToStandard.a  lib/libMhloToThloConversion.a  lib/libMhloToLinalg.a  lib/libMhloToStablehlo.a  lib/libMhloShapeOpsToStandard.a  lib/libStablehloToMhlo.a  lib/libGmlStTilingInterface.a  lib/libGmlStDialect.a  lib/libLLVMSupport.a  lib/libLLVMSupport.a  lib/libLLVMAsmPrinter.a  lib/libLLVMGlobalISel.a  lib/libLLVMSelectionDAG.a  lib/libLLVMCFGuard.a  lib/libLLVMOrcJIT.a  lib/libLLVMExecutionEngine.a  lib/libLLVMRuntimeDyld.a  lib/libLLVMWindowsDriver.a  lib/libLLVMJITLink.a  lib/libLLVMOrcTargetProcess.a  lib/libLLVMOrcShared.a  lib/libLLVMOption.a  lib/libLLVMMCDisassembler.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRAMDGPUDialect.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRIndexDialect.a  lib/libMLIRArithToSPIRV.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRTosaTransforms.a  lib/libMLIRTosaDialect.a  lib/libMLIRQuantUtils.a  lib/libMLIRNVGPUUtils.a  lib/libMLIRAffineTransforms.a  lib/libMLIRGPUTransforms.a  lib/libMLIRAsyncDialect.a  lib/libMLIRExecutionEngineUtils.a  lib/libLLVMPasses.a  lib/libLLVMCoroutines.a  lib/libLLVMipo.a  lib/libLLVMVectorize.a  lib/libLLVMLinker.a  lib/libLLVMCodeGen.a  lib/libLLVMTarget.a  lib/libLLVMInstrumentation.a  lib/libLLVMIRPrinter.a  lib/libLLVMObjCARCOpts.a  lib/libMLIRNVGPUDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libMLIRSparseTensorUtils.a  lib/libMLIRAffineToStandard.a  lib/libMLIRSPIRVConversion.a  lib/libMLIRSPIRVUtils.a  lib/libMLIRTransformDialect.a  lib/libMLIRTransformDialectUtils.a  lib/libMLIREmitCDialect.a  lib/libMLIRSPIRVDeserialization.a  lib/libMLIRSPIRVSerialization.a  lib/libMLIRSPIRVBinaryUtils.a  lib/libMLIRSPIRVDialect.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libMLIRArmNeonToLLVMIRTranslation.a  lib/libMLIRArmSVEToLLVMIRTranslation.a  lib/libMLIRAMXToLLVMIRTranslation.a  lib/libMLIRNVVMToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIROpenACCDialect.a  lib/libMLIROpenMPToLLVMIRTranslation.a  lib/libMLIROpenMPDialect.a  lib/libMLIRROCDLToLLVMIRTranslation.a  lib/libMLIRROCDLDialect.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRArmNeonDialect.a  lib/libMLIRArmSVETransforms.a  lib/libMLIRArmSVEDialect.a  lib/libMLIRAMXTransforms.a  lib/libMLIRAMXDialect.a  lib/libMLIRX86VectorToLLVMIRTranslation.a  lib/libMLIRTargetLLVMIRExport.a  lib/libMLIRLLVMIRTransforms.a  lib/libMLIRNVVMDialect.a  lib/libLLVMFrontendOpenMP.a  lib/libLLVMScalarOpts.a  lib/libLLVMAggressiveInstCombine.a  lib/libLLVMInstCombine.a  lib/libLLVMTransformUtils.a  lib/libMLIRLLVMIRToLLVMTranslation.a  lib/libMLIRTargetLLVMIRImport.a  lib/libMLIRTranslateLib.a  lib/libMhloAnalysis.a  lib/libMhloScatterUtils.a  lib/libMLIRShapeOpsTransforms.a  lib/libTHLODialect.a  lib/libGmlStDialect.a  lib/libMLIRLinalgTransforms.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRArithToLLVM.a  lib/libMLIRArithAttrToLLVMConversion.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRVectorToSCF.a  lib/libMLIRLinalgAnalysis.a  lib/libMLIRMemRefTransforms.a  lib/libMLIRArithTransforms.a  lib/libMLIRSCFTransforms.a  lib/libMLIRSCFUtils.a  lib/libMLIRTensorTilingInterfaceImpl.a  lib/libMLIRTensorTransforms.a  lib/libMLIRVectorTransforms.a  lib/libMLIRGPUOps.a  lib/libMLIRDLTIDialect.a  lib/libMLIRVectorUtils.a  lib/libMLIRX86VectorTransforms.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRVectorDialect.a  lib/libMLIRMaskableOpInterface.a  lib/libMLIRMaskingOpInterface.a  lib/libMLIRVectorInterfaces.a  lib/libMLIRX86VectorDialect.a  lib/libMhloToArithmeticConversion.a  lib/libHloToLinalgUtils.a  lib/libMLIRLinalgUtils.a  lib/libMLIRLinalgDialect.a  lib/libMLIRParser.a  lib/libMLIRBytecodeReader.a  lib/libMLIRAsmParser.a  lib/libMLIRMathDialect.a  lib/libMLIRTilingInterface.a  lib/libMLIRAffineUtils.a  lib/libMLIRAffineAnalysis.a  lib/libMLIRPresburger.a  lib/libMLIRSCFDialect.a  lib/libMLIRTensorUtils.a  lib/libLmhloDialect.a  lib/libLmhloStructuredInterface.a  lib/libMhloTypeConversion.a  lib/libMLIRFuncTransforms.a  lib/libMLIRBufferizationTransforms.a  lib/libMLIRTransforms.a  lib/libMLIRTransformUtils.a  lib/libMLIRRewrite.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRPDLInterpDialect.a  lib/libMLIRPDLDialect.a  lib/libMLIRCopyOpInterface.a  lib/libMLIRRuntimeVerifiableOpInterface.a  lib/libMLIRBufferizationDialect.a  lib/libMhloDialect.a  lib/libMLIRMhloUtils.a  lib/libMLIRPass.a  lib/libMLIRAnalysis.a  lib/libMLIRLLVMDialect.a  lib/libLLVMBitWriter.a  lib/libLLVMAnalysis.a  lib/libLLVMProfileData.a  lib/libLLVMSymbolize.a  lib/libLLVMDebugInfoDWARF.a  lib/libLLVMDebugInfoPDB.a  lib/libLLVMObject.a  lib/libLLVMMCParser.a  lib/libLLVMMC.a  lib/libLLVMIRReader.a  lib/libLLVMAsmParser.a  lib/libLLVMBitReader.a  lib/libLLVMTextAPI.a  lib/libLLVMDebugInfoCodeView.a  lib/libLLVMDebugInfoMSF.a  lib/libChloOps.a  lib/libStablehloBroadcastUtils.a  lib/libHloOpsCommon.a  lib/libStablehloOps.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRDataLayoutInterfaces.a  lib/libStablehloTypeInference.a  lib/libStablehloAssemblyFormat.a  lib/libStablehloBase.a  lib/libMLIRQuantDialect.a  lib/libMLIRShapeDialect.a  lib/libMLIRFuncDialect.a  lib/libMLIRControlFlowDialect.a  lib/libMLIRTensorDialect.a  lib/libMLIRAffineDialect.a  lib/libMLIRMemRefDialect.a  lib/libMLIRLoopLikeInterface.a  lib/libMLIRComplexDialect.a  lib/libLLVMCore.a  lib/libLLVMBinaryFormat.a  lib/libLLVMTargetParser.a  lib/libLLVMRemarks.a  lib/libLLVMBitstreamReader.a  lib/libMLIRDestinationStyleOpInterface.a  lib/libMLIRDialectUtils.a  lib/libMLIRArithUtils.a  lib/libMLIRArithDialect.a  lib/libMLIRInferTypeOpInterface.a  lib/libMLIRInferIntRangeInterface.a  lib/libMLIRViewLikeInterface.a  lib/libMLIRShapedOpInterfaces.a  lib/libMLIRParallelCombiningOpInterface.a  lib/libMLIRControlFlowInterfaces.a  lib/libMLIRDialect.a  lib/libMLIRSideEffectInterfaces.a  lib/libMLIRCallInterfaces.a  lib/libMLIRCastInterfaces.a  lib/libMLIRIR.a  lib/libMLIRSupport.a  lib/libLLVMSupport.a  -lrt  -ldl  -lm  /usr/lib/x86_64-linux-gnu/libz.so  /usr/lib/x86_64-linux-gnu/libtinfo.so  lib/libLLVMDemangle.a  -lpthread && :
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o): in function `mlir::thlo::ScatterOp::getTiledImplementation(mlir::OpBuilder&, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)':
/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:752: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: /compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:764: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: /compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:769: undefined reference to `mlir::gml_st::materializeIdentitySlice(mlir::OpBuilder&, mlir::Location, mlir::Value, bool)'
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o): in function `mlir::thlo::(anonymous namespace)::fuseConcatenateOpThroughTile(mlir::thlo::ConcatenateOp, mlir::OpBuilder&, mlir::Location, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)':
/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:302: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: /compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:323: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o): in function `mlir::thlo::SortOp::getTiledImplementation(mlir::OpBuilder&, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)':
/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:1090: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: /compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:1099: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o): in function `mlir::thlo::ReverseOp::getTiledImplementation(mlir::OpBuilder&, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)':
/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:1205: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: /compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:1207: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o):/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:852: more undefined references to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)' follow
/usr/bin/ld: lib/libTHLODialect.a(thlo_ops.cc.o): in function `mlir::thlo::DynamicBroadcastInDimOp::getTiledImplementation(mlir::OpBuilder&, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)':
/compiler/boyana.norris/developer/mlir-hlo/thlo/IR/thlo_ops.cc:609: undefined reference to `mlir::gml_st::materializeSlice(mlir::OpBuilder&, mlir::Location, mlir::Value, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, llvm::ArrayRef<mlir::OpFoldResult>, bool)'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```
```
</details>"
59378,Runtime error when running tensorflow.python.keras.backend.relu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensor or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.keras import backend
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.constant(-88883639232948, shape=[2], dtype=tf.int64,),dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      alpha = 0.0
      max_value = None
      threshold = -15
      out = backend.relu(arg_0,alpha=alpha,max_value=max_value,threshold=threshold,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      backend.relu(arg_0,alpha=alpha,max_value=max_value,threshold=threshold,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:27:09.821022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:27:09.938202: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:27:10.513567: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:27:10.513765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:27:10.513773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:27:11.135941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:27:11.165163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:27:11.165179: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:27:11.165430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned
Error:can't convert negative int to unsigned

```
```
</details>"
59377,Runtime error when running tensorflow.python.keras.backend._constant_to_tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.keras import backend
try:
  arg_0 = -338
  arg_1 = tf.uint64
  out = backend._constant_to_tensor(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:24:34.214218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:24:35.054799: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:24:36.517313: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:24:36.517547: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:24:36.517555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:24:39.054394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:24:39.089486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:24:39.089504: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:24:39.091172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59376,Process get killed when running tensorflow.python.ops.gen_math_ops.floor_div_eager_fallback,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensors
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.complex(tf.constant(-116627697387965, shape=[1024, 1024, 1024, 1], dtype=tf.float64), tf.constant(-79813870851347, shape=[1024, 1024, 1024, 1], dtype=tf.float64))
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = None
      ctx = None
      out = gen_math_ops.floor_div_eager_fallback(arg_0,arg_1,ctx=ctx,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.complex128)
      ctx = None
      gen_math_ops.floor_div_eager_fallback(arg_0,arg_1,ctx=ctx,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:20:06.661472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:20:06.782900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:20:07.380179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:20:07.380438: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:20:07.380447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:20:08.011905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:20:08.040378: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:20:08.040394: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:20:08.040643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:20:08.052726: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8589934592 exceeds 10% of free system memory.
2023-01-21 14:20:08.813065: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8589934592 exceeds 10% of free system memory.
Killed

```
```
</details>"
59375,IA capable de lire des chiffre,
59374,Crash when running tensorflow.python.ops.clip_ops.clip_by_value,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensor or negative arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import clip_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.constant(-110224227363235, shape=[], dtype=tf.int64,),dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = -1
      arg_2 = -19
      out = clip_ops.clip_by_value(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      clip_ops.clip_by_value(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:17:34.396356: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:17:34.494553: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:17:35.145932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:17:35.146137: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:17:35.146146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:17:35.806850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:17:35.840204: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:17:35.840222: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:17:35.840468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned
Error:can't convert negative int to unsigned

```
```
</details>"
59373,Crash when running tensorflow.python.ops.gen_nn_ops.fractional_max_pool_grad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large tensor or arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 4, 4, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-16268772587557, shape=[2, 1, 3, 1], dtype=tf.float32,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([1, 3, 2, 1], dtype=tf.float32)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_0 = 0
  arg_3_1 = 1
  arg_3_2 = 3
  arg_3_3 = 4
  arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
  arg_4_0 = True
  arg_4_1 = 125091515651
  arg_4_2 = 125091515651
  arg_4 = [arg_4_0,arg_4_1,arg_4_2,]
  overlapping = False
  out = gen_nn_ops.fractional_max_pool_grad(arg_0,arg_1,arg_2,arg_3,arg_4,overlapping=overlapping,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-25 21:33:53.822517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-25 21:33:53.917687: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-25 21:33:54.342722: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-25 21:33:54.342886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nimashiri/anaconda3/envs/cuda11.2/lib/
2023-01-25 21:33:54.342893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-25 21:33:54.828965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:54.833685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:54.833791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:54.834090: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-25 21:33:54.834689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:54.834824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:54.834918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:55.188747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:55.188899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:55.188995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-25 21:33:55.189084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4304 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
free(): invalid size
Aborted


```
```
</details>"
59372,Segmentation fault when running tensorflow.python.ops.gen_sparse_ops.sparse_cross_hashed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Empty arguments
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices = []
  values = []
  shapes = []
  dense_inputs = []
  num_buckets = -461
  salt = []
  strong_hash = False
  out = gen_sparse_ops.sparse_cross_hashed(indices=indices,values=values,shapes=shapes,dense_inputs=dense_inputs,num_buckets=num_buckets,salt=salt,strong_hash=strong_hash,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:11:03.821500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:11:03.920999: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:11:04.355067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:11:04.355261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:11:04.355268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:11:04.847820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:11:04.872766: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:11:04.872786: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:11:04.873068: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault

```
```
</details>"
59371,Segfault when running tensorflow.python.ops.gen_array_ops.lower_bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to dimension mismatch
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([3, 3], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([3], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = tf.int32
      arg_3 = False
      out = gen_array_ops.lower_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.int32
      gen_array_ops.lower_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:08:08.965389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:08:09.268511: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:08:10.158630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:08:10.158876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:08:10.158886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:08:11.205950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:08:11.244128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:08:11.244148: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:08:11.245639: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault

```
```
</details>"
59370,Runtime error when running tensorflow.python.ops.gen_nn_ops.in_top_kv2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.random.uniform([3], minval=0, maxval=2, dtype=tf.int64), dtype=tf.int16)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.saturate_cast(tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = -285
      out = gen_nn_ops.in_top_kv2(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int16)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.uint64)
      gen_nn_ops.in_top_kv2(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:05:29.559915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:05:29.674650: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:05:30.265706: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:05:30.265897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:05:30.265906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:05:30.887927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:05:30.916555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:05:30.916572: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:05:30.916825: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned
Error:can't convert negative int to unsigned

```
```
</details>"
59369,Segfault when running tensorflow.python.ops.nn_ops.fractional_avg_pool_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to negative or large argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = -32
  arg_1_1 = -34
  arg_1_2 = -46
  arg_1_3 = True
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = False
  seed = 341261001
  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 14:02:26.071375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 14:02:26.188308: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 14:02:26.781007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:02:26.781205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:02:26.781213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 14:02:27.424061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 14:02:27.454021: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 14:02:27.454038: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 14:02:27.454299: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault

```
```
</details>"
59368,free(): invalid pointer when running tensorflow.python.ops.gen_ragged_array_ops.ragged_cross,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to empty input arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_ragged_array_ops
try:
  ragged_values_0_tensor = tf.convert_to_tensor(np.ones([3], dtype=str))
  ragged_values_0 = tf.identity(ragged_values_0_tensor)
  ragged_values = [ragged_values_0,]
  ragged_row_splits_0_tensor = tf.random.uniform([4], minval=-256, maxval=257, dtype=tf.int64)
  ragged_row_splits_0 = tf.identity(ragged_row_splits_0_tensor)
  ragged_row_splits = [ragged_row_splits_0,]
  sparse_indices = []
  sparse_values = []
  sparse_shape = []
  dense_inputs = []
  input_order = ""R""
  hashed_output = False
  num_buckets = 0
  hash_key = 956888297470
  out_values_type = 7
  out_row_splits_type = 9
  out = gen_ragged_array_ops.ragged_cross(ragged_values=ragged_values,ragged_row_splits=ragged_row_splits,sparse_indices=sparse_indices,sparse_values=sparse_values,sparse_shape=sparse_shape,dense_inputs=dense_inputs,input_order=input_order,hashed_output=hashed_output,num_buckets=num_buckets,hash_key=hash_key,out_values_type=out_values_type,out_row_splits_type=out_row_splits_type,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:53:18.602619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:53:18.736809: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:53:19.358808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:53:19.359009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:53:19.359018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:53:20.000091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:53:20.036637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:53:20.036654: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:53:20.036907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
free(): invalid pointer
Aborted

```
```
</details>"
59367,Runtime error when running tensorflow.python.ops.gen_math_ops.dense_bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to empty tensor or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      input_tensor = tf.saturate_cast(tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
      input = tf.identity(input_tensor)
      weights = []
      size = -667
      binary_output = False
      out = gen_math_ops.dense_bincount(input=input,weights=weights,size=size,binary_output=binary_output,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      input = tf.identity(input_tensor)
      input = tf.cast(input, tf.uint64)
      weights = []
      gen_math_ops.dense_bincount(input=input,weights=weights,size=size,binary_output=binary_output,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:50:38.486445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:50:38.623931: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:50:39.187573: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:50:39.187768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:50:39.187777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:50:39.795975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:50:39.825626: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:50:39.825645: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:50:39.825919: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59366,Runtime error when running tensorflow.python.ops.gen_math_ops.ragged_bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Empty tensor or negative argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  splits_tensor = tf.saturate_cast(tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  splits = tf.identity(splits_tensor)
  values_tensor = tf.saturate_cast(tf.random.uniform([3, 3], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  values = tf.identity(values_tensor)
  weights = []
  size = -80
  binary_output = False
  out = gen_math_ops.ragged_bincount(splits=splits,values=values,weights=weights,size=size,binary_output=binary_output,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:48:15.238976: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:48:15.525093: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:48:16.279948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:48:16.280110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:48:16.280118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:48:17.172034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:48:17.199016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:48:17.199033: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:48:17.200445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59365,Error when running tensorflow.python.ops.gen_array_ops.pad_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to negative or empty input
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = []
  arg_2 = -992
  out = gen_array_ops.pad_v2(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:38:39.787930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:38:39.902000: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:38:40.466321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:38:40.466519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:38:40.466528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:38:41.067843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:38:41.096228: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:38:41.096244: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:38:41.096482: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59364,Crash when running tensorflow.python.ops.math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large input argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.random.uniform([10, 4], dtype=tf.float32)
      data = tf.identity(data_tensor)
      indices_0 = 125091515651
      indices_1 = 125091515651
      indices_2 = 125091515651
      indices_3 = 125091515651
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids_0 = 1
      segment_ids_1 = 2
      segment_ids_2 = 2
      segment_ids_3 = 2
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      out = math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float32)
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:36:40.648379: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:36:40.774128: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:36:41.380501: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:36:41.380711: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:36:41.380721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:36:42.052208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:36:42.083425: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:36:42.083443: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:36:42.083700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Bad: indices[0] == 125091515651 out of range [0, 10) [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Bad: indices[0] == 125091515651 out of range [0, 10) [Op:SparseSegmentSum]

```
```
</details>"
59363,Crash when running tensorflow.python.ops.array_ops.reverse_sequence,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large Tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([4, 2, 3, 1, 1], dtype=tf.float64)
      arg_0 = tf.identity(arg_0_tensor)
      batch_axis = 2
      seq_axis = 0
      seq_lengths_tensor = tf.constant(-797221241223, shape=[3], dtype=tf.int32,)
      seq_lengths = tf.identity(seq_lengths_tensor)
      out = array_ops.reverse_sequence(arg_0,batch_axis=batch_axis,seq_axis=seq_axis,seq_lengths=seq_lengths,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float64)
      seq_lengths = tf.identity(seq_lengths_tensor)
      seq_lengths = tf.cast(seq_lengths, tf.int32)
      array_ops.reverse_sequence(arg_0,batch_axis=batch_axis,seq_axis=seq_axis,seq_lengths=seq_lengths,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:34:23.859473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:34:24.165563: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:34:25.033346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:34:25.033617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:34:25.033626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:34:26.052396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:34:26.083207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:34:26.083223: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:34:26.084767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:{{function_node __wrapped__ReverseSequence_device_/job:localhost/replica:0/task:0/device:CPU:0}} seq_lens(0) > input.dims(0) [Op:ReverseSequence]
Error:{{function_node __wrapped__ReverseSequence_device_/job:localhost/replica:0/task:0/device:CPU:0}} seq_lens(0) > input.dims(0) [Op:ReverseSequence]

```
```
</details>"
59362,Process get killed when running tensorflow.python.ops.ragged.ragged_util.repeat_ranges,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops.ragged import ragged_util
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.constant(-73951793837462, shape=[], dtype=tf.float32,)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.constant(-88895168297564, shape=[], dtype=tf.int64,)
      arg_2 = tf.identity(arg_2_tensor)
      out = ragged_util.repeat_ranges(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ragged_util.repeat_ranges(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
023-01-21 13:32:07.162645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:32:07.274542: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:32:07.900834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:32:07.901030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:32:07.901038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:32:08.528296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:32:08.561703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:32:08.561720: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:32:08.562921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:32:08.596909: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8259180176 exceeds 10% of free system memory.
2023-01-21 13:32:09.340065: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16518360360 exceeds 10% of free system memory.
Killed

```
```
</details>"
59361,Bad file descriptor when running tensorflow.python.keras.utils.data_utils._hash_file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to Nan or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.keras.utils import data_utils
try:
  arg_0 = True
  arg_1 = ""nan""
  arg_2 = -0.3344762570580887
  out = data_utils._hash_file(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:28:52.803828: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:28:53.097843: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:28:53.923396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:28:53.923579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:28:53.923589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
  File ""/media/nimashiri/SSD/testing_results/orion/tf-2.10.0/orion_bugs/tensorflow.python.keras.utils.data_utils._hash_file.py"", line 9, in <module>
    out = data_utils._hash_file(arg_0,arg_1,arg_2,)
  File ""/home/nimashiri/anaconda3/envs/tf-2.10/lib/python3.9/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 321, in _hash_file
    for chunk in iter(lambda: fpath_file.read(chunk_size), b''):
  File ""/home/nimashiri/anaconda3/envs/tf-2.10/lib/python3.9/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 321, in <lambda>
    for chunk in iter(lambda: fpath_file.read(chunk_size), b''):
TypeError: argument should be integer or None, not 'float'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/media/nimashiri/SSD/testing_results/orion/tf-2.10.0/orion_bugs/tensorflow.python.keras.utils.data_utils._hash_file.py"", line 11, in <module>
    print(""Error:""+str(e))
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
OSError: [Errno 9] Bad file descriptor

```
```
</details>"
59360,Runtime error when running tensorflow.python.ops.math_ops.subtract,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.constant(-55244952720714, shape=[1], dtype=tf.int64,),dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = -71
      out = math_ops.subtract(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      math_ops.subtract(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 13:22:30.962089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 13:22:31.082268: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 13:22:31.662802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:22:31.663000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:22:31.663009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 13:22:32.271961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 13:22:32.301568: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 13:22:32.301587: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 13:22:32.301880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned
Error:can't convert negative int to unsigned

```
```
</details>"
59359,Process get killed tensorflow.python.ops.signal.window_ops.hann_window,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large input argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops.signal import window_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0 = 125091515651
      periodic = False
      dtype = tf.float64
      out = window_ops.hann_window(arg_0,periodic=periodic,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      dtype = tf.float64
      window_ops.hann_window(arg_0,periodic=periodic,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:56:28.727252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:56:28.846144: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:56:29.407955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:56:29.408152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:56:29.408161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:56:30.003853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:56:30.032131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:56:30.032148: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:56:30.032397: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:56:30.053099: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2149856268 exceeds 10% of free system memory.
2023-01-21 12:56:30.731465: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
2023-01-21 12:56:31.418647: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
2023-01-21 12:56:32.072936: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
2023-01-21 12:56:33.361915: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
Killed

```
```
</details>"
59358,Error when running tensorflow.python.ops.numpy_ops.np_arrays.convert_to_tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
negative input argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops.numpy_ops import np_arrays
try:
  arg_0 = -579
  dtype_hint = tf.uint64
  out = np_arrays.convert_to_tensor(arg_0,dtype_hint=dtype_hint,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:53:46.366850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:53:46.476250: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:53:47.036374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:53:47.036556: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:53:47.036564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:53:47.655879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:53:47.684068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:53:47.684084: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:53:47.684328: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59357,Crash when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0, 2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure with the following input arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3, 3, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:49:08.508138: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:49:08.623891: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:49:09.210798: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:49:09.211024: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:49:09.211033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:49:09.832017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:49:09.861818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:49:09.861835: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:49:09.862084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:49:09.880100: F tensorflow/core/kernels/maxpooling_op.cc:1065] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 146, 0, 18
Aborted

```
```
</details>"
59356,Error when runnning tensorflow.python.ops.sparse_ops.sparse_to_dense,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Zero or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import sparse_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = 2
      arg_0_0 = [arg_0_0_0,]
      arg_0_1_0 = 1
      arg_0_1 = [arg_0_1_0,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = 0
      arg_1 = [arg_1_0,]
      arg_2_0 = -1.0
      arg_2_1 = 1.0
      arg_2 = [arg_2_0,arg_2_1,]
      arg_3 = 0.0
      validate_indices = False
      out = sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,arg_3,validate_indices=validate_indices,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,]
      arg_0_1 = [arg_0_1_0,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1 = [arg_1_0,]
      arg_2 = [arg_2_0,arg_2_1,]
      sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,arg_3,validate_indices=validate_indices,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:45:19.079634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:45:19.390830: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:45:20.257044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:20.257249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:20.257257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:45:21.248344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:45:21.280051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:21.280069: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:45:21.281547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /home/nimashiri/anaconda3/envs/tf-2.10/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
Error:{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense]
Error:{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense]

```
```
</details>"
59355,Process get killed when running tensorflow.python.ops.gen_math_ops.unsorted_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1000], dtype=tf.float64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([1000], minval=-256, maxval=257, dtype=tf.int32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.constant(-67794891775896, shape=[], dtype=tf.int32,)
      arg_2 = tf.identity(arg_2_tensor)
      out = gen_math_ops.unsorted_segment_sum(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.int32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int32)
      gen_math_ops.unsorted_segment_sum(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:27:34.481912: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:27:34.601716: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:27:35.213642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:27:35.213870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:27:35.213879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:27:35.880250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:27:35.911154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:27:35.911172: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:27:35.911453: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:27:35.929852: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 9335931712 exceeds 10% of free system memory.
Killed

```
```
</details>"
59354,Error when running tensorflow.python.ops.gen_math_ops.not_equal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Negative argument or large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.constant(-154005508737179, shape=[2, 2], dtype=tf.int64,),dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -91
  out = gen_math_ops.not_equal(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:22:36.911829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:22:37.022753: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:22:37.605554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:22:37.605764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:22:37.605774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:22:38.252181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:22:38.283523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:22:38.283541: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:22:38.283801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned

```
```
</details>"
59353,Error when running tensorflow.python.ops.gen_math_ops._pow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to empty or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -604
  out = gen_math_ops._pow(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:19:32.012097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:19:32.324662: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:19:33.192638: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:19:33.192841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:19:33.192850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:19:34.192328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:19:34.223712: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:19:34.223729: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:19:34.225099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned

```
```
</details>"
59352,Error when running tensorflow.python.ops.array_ops._constant_if_small,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to empty or negative float inputs.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0 = -673
  arg_1 = []
  arg_2 = tf.uint64
  arg_3 = -0.5220044599057413
  out = array_ops._constant_if_small(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:14:09.136217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:14:09.235416: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:14:09.678820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:14:09.678991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:14:09.678999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:14:10.159520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:14:10.183797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:14:10.183812: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:14:10.184037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59351,Process get killed tensorflow.python.ops.gen_stateless_random_ops.stateless_random_uniform_full_int,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to Large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_stateless_random_ops
try:
  arg_0_tensor = tf.constant(-129431154884146, shape=[1], dtype=tf.int32,)
  arg_0 = tf.identity(arg_0_tensor)
  seed_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int32)
  seed = tf.identity(seed_tensor)
  dtype = tf.int64
  out = gen_stateless_random_ops.stateless_random_uniform_full_int(arg_0,seed=seed,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:09:17.165935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:09:17.471905: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:09:18.323563: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:09:18.323758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:09:18.323766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:09:19.340364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:09:19.372486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:09:19.372506: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:09:19.374114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:09:19.402442: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15836384880 exceeds 10% of free system memory.
Killed

```
```
</details>"
59350,Runtime error tensorflow.python.ops.stateless_random_ops.stateless_truncated_normal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
NaN input or Empty input
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  shape = []
  mean = -404
  stddev = ""nan""
  dtype = tf.uint64
  seed = []
  out = stateless_random_ops.stateless_truncated_normal(shape=shape,mean=mean,stddev=stddev,dtype=dtype,seed=seed,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:03:48.192910: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:03:48.497131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:03:49.378226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:03:49.378430: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:03:49.378439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:03:50.368808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:03:50.399759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:03:50.399775: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:03:50.401226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59349,Runtime error tensorflow.python.data.util.structure.type_spec_from_value,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
runtime error probably due to large tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.data.util import structure
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_tensor = tf.saturate_cast(tf.constant(-104730106681921, shape=[], dtype=tf.int64,),dtype=tf.uint64)
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_1 = -96
      arg_0 = [arg_0_0,arg_0_1,]
      out = structure.type_spec_from_value(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_0 = tf.cast(arg_0_0, tf.uint64)
      arg_0 = [arg_0_0,arg_0_1,]
      structure.type_spec_from_value(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 11:58:01.038629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:58:01.185340: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:58:01.938964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:58:01.939239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:58:01.939251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:58:02.753091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:58:02.794160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:58:02.794182: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:58:02.794479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59348,Runtime error when running tensorflow.python.ops.math_ops.floordiv,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Runtime error
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.random.uniform([3], minval=0, maxval=257, dtype=tf.int64), dtype=tf.uint64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = -427
      out = math_ops.floordiv(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.uint64)
      math_ops.floordiv(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
023-01-21 11:34:35.067769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:34:35.189549: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:34:35.765590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:34:35.765782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:34:35.765790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:34:36.367857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:34:36.396406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:34:36.396423: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:34:36.396669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:can't convert negative int to unsigned
Error:can't convert negative int to unsigned

```
```
</details>"
59347,error when running tensorflow.python.ops.stateless_random_ops.stateless_random_normal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Due to empty input or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  try:
    with tf.device('/CPU'):
      seed_tensor = tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int32)
      seed = tf.identity(seed_tensor)
      shape_tensor = tf.saturate_cast(tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.uint64)
      shape = tf.identity(shape_tensor)
      dtype = tf.uint64
      mean = True
      stddev = -119
      out = stateless_random_ops.stateless_random_normal(seed=seed,shape=shape,dtype=dtype,mean=mean,stddev=stddev,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      seed = tf.identity(seed_tensor)
      seed = tf.cast(seed, tf.int32)
      shape = tf.identity(shape_tensor)
      shape = tf.cast(shape, tf.uint64)
      dtype = tf.uint64
      stateless_random_ops.stateless_random_normal(seed=seed,shape=shape,dtype=dtype,mean=mean,stddev=stddev,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```

Also on 2.11:

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  seed_tensor = tf.saturate_cast(tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int8)
  seed = tf.identity(seed_tensor)
  shape_tensor = tf.saturate_cast(tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int16)
  shape = tf.identity(shape_tensor)
  dtype = tf.uint64
  mean = -296
  stddev = -506
  out = stateless_random_ops.stateless_random_normal(seed=seed,shape=shape,dtype=dtype,mean=mean,stddev=stddev,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2023-01-21 11:18:12.036882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:18:12.148615: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:18:12.654778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:18:12.654956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:18:12.654964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:18:13.195573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:18:13.222236: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:18:13.222251: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:18:13.222489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59346,Runtime error when running tensorflow.python.ops.array_ops.quantize_and_dequantize_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([1], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = False
  arg_2 = -197
  range_given = True
  round_mode = ""nan""
  axis_tensor = tf.random.uniform([1], dtype=tf.bfloat16)
  axis = tf.identity(axis_tensor)
  out = array_ops.quantize_and_dequantize_v2(arg_0,arg_1,arg_2,range_given=range_given,round_mode=round_mode,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 11:14:13.163658: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:14:13.447121: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:14:14.174384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:14:14.174551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:14:14.174558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:14:15.040067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:14:15.067751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:14:15.067767: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:14:15.069183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59345,Process get killed tensorflow.python.ops.array_ops.tile_one_dimension,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = 1
      arg_2_tensor = tf.constant(-11621058265507, shape=[], dtype=tf.int32,)
      arg_2 = tf.identity(arg_2_tensor)
      out = array_ops.tile_one_dimension(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int32)
      array_ops.tile_one_dimension(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
Also on 2.11:
```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.constant(-18291472032905, shape=[2], dtype=tf.float64,)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1 = False
      arg_2_tensor = tf.constant(-114764684977965, shape=[], dtype=tf.int32,)
      arg_2 = tf.identity(arg_2_tensor)
      out = array_ops.tile_one_dimension(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int32)
      array_ops.tile_one_dimension(arg_0,arg_1,arg_2,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```

### Relevant log output

```shell
2023-01-21 11:11:00.368726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:11:00.653685: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:11:01.422843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:11:01.423125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:11:01.423136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:11:02.556098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:11:02.605107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:11:02.605127: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:11:02.606695: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:11:02.633334: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 17971799504 exceeds 10% of free system memory.
Killed
```
```
</details>"
59344,Crash when running tensorflow.python.ops.nn_ops.convolution_internal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Very large tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([32, 12, 12, 8], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([3, 3, 8, 4], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      strides = None
      padding = ""VALID""
      data_format = None
      dilations_tensor = tf.constant(-102378662306538, shape=[1], dtype=tf.float32,)
      dilations = tf.identity(dilations_tensor)
      out = nn_ops.convolution_internal(arg_0,arg_1,strides=strides,padding=padding,data_format=data_format,dilations=dilations,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      dilations = tf.identity(dilations_tensor)
      dilations = tf.cast(dilations, tf.float32)
      nn_ops.convolution_internal(arg_0,arg_1,strides=strides,padding=padding,data_format=data_format,dilations=dilations,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 11:02:42.187245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:02:42.297559: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 11:02:42.846775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:02:42.846966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:02:42.846974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 11:02:43.423840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 11:02:43.454583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 11:02:43.454600: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 11:02:43.454839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 11:02:43.476399: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -1)
Aborted

```
```
</details>"
59343,Error when running tensorflow.python.ops.nn_impl.batch_normalization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to empty input tensor or negative argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import nn_impl
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([3, 3, 3], dtype=tf.float64)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.saturate_cast(tf.random.uniform([3, 3], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint8)
      arg_4 = tf.identity(arg_4_tensor)
      arg_5 = -571
      out = nn_impl.batch_normalization(arg_0,arg_1,arg_2,arg_3,arg_4,arg_5,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.uint64)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.float64)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.uint8)
      nn_impl.batch_normalization(arg_0,arg_1,arg_2,arg_3,arg_4,arg_5,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```

Also on tf-2.11:

```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import nn_impl
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.complex(tf.random.uniform([], dtype=tf.float64),tf.random.uniform([], dtype=tf.float64))
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.saturate_cast(tf.random.uniform([2, 2], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint16)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint32)
      arg_4 = tf.identity(arg_4_tensor)
      arg_5 = -607
      out = nn_impl.batch_normalization(arg_0,arg_1,arg_2,arg_3,arg_4,arg_5,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.complex128)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.uint64)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.uint16)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.uint32)
      nn_impl.batch_normalization(arg_0,arg_1,arg_2,arg_3,arg_4,arg_5,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```

### Relevant log output

```shell
2023-01-21 10:56:09.078632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:56:09.370374: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:56:10.227902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:56:10.228106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:56:10.228115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:56:11.192165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:56:11.221955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:56:11.221972: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:56:11.223466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set

```
```
</details>"
59342,Process get killed when running tensorflow.python.ops.linalg_ops_impl.eye,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
2023-01-21 10:51:47.217992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:51:47.511742: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:51:48.329679: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:48.329871: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:48.329879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:51:49.359133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:51:49.391077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:49.391095: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:51:49.392598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:51:49.426335: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16766191416 exceeds 10% of free system memory.
Killed

```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import linalg_ops_impl
try:
  arg_0_tensor = tf.constant(-9748178579302, shape=[], dtype=tf.int32,)
  arg_0 = tf.identity(arg_0_tensor)
  num_columns = None
  batch_shape_0 = 2
  batch_shape_1 = 3
  batch_shape = [batch_shape_0,batch_shape_1,]
  dtype = tf.bfloat16
  out = linalg_ops_impl.eye(arg_0,num_columns=num_columns,batch_shape=batch_shape,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 10:51:47.217992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:51:47.511742: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:51:48.329679: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:48.329871: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:48.329879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:51:49.359133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:51:49.391077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:51:49.391095: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:51:49.392598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:51:49.426335: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16766191416 exceeds 10% of free system memory.
Killed

```
```
</details>"
59341,Process get killed when running tensorflow.python.ops.stateful_random_ops._make_1d_state,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to very large input argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import stateful_random_ops
try:
  arg_0 = 125091515651
  arg_1 = 12345
  out = stateful_random_ops._make_1d_state(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 10:45:54.421270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:45:54.535268: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:45:55.097219: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:45:55.097403: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:45:55.097412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Killed

```
```
</details>"
59340,Error when running tensorflow.python.ops.gen_array_ops.identity,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to very large input value.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.constant(-23511698154024, shape=[130, 1000, 1000], dtype=tf.int64,),dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  out = gen_array_ops.identity(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 10:40:07.326766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:40:07.425381: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:40:07.893350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:40:07.893521: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:40:07.893529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:40:08.391504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:40:08.415810: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:40:08.415826: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:40:08.416054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:40:08.427915: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1040000000 exceeds 10% of free system memory.
2023-01-21 10:40:08.521752: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1040000000 exceeds 10% of free system memory.
2023-01-21 10:40:08.683694: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1040000000 exceeds 10% of free system memory.

```
```
</details>"
59339,Error when running tensorflow.python.ops.nn_impl.weighted_cross_entropy_with_logits_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
2023-01-21 10:32:51.480512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:32:51.592470: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:32:52.135367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.135551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.135560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:32:52.699798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:32:52.730300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.730316: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:32:52.730557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_impl
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([3, 3, 3], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.cast(tf.random.uniform([3, 3, 3], minval=0, maxval=2, dtype=tf.int32), dtype=tf.bool)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2 = -14
  arg_3 = True
  out = nn_impl.weighted_cross_entropy_with_logits_v2(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 10:32:51.480512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 10:32:51.592470: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 10:32:52.135367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.135551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.135560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 10:32:52.699798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 10:32:52.730300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 10:32:52.730316: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 10:32:52.730557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
```
```
</details>"
59337,tf.saved_model.save()   AttributeError: 'Adam' object has no attribute 'get_slot_names',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

NVIDIA GeForce GTX 960M, Compute Capability 5.0  

### Current Behaviour?

```shell
I'm running the Tensor Flow tutorial at URL https://www.tensorflow.org/text/tutorials/transformer using  the latest version of the PyCharm Community Edition IDE on Windows 10 Laptop with 16 GB of memory

It runs up to the very last statement which is: tf.saved_model.save(translator, export_dir='translator')

It bombs out with the following traceback. How can I recover from this?

Traceback (most recent call last):
  File ""D:\Craig\Python\Projects\pythonProject\pythonProject\main.py"", line 839, in <module>
    tf.saved_model.save(translator, export_dir='translator')
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1232, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1268, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1441, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1394, in _build_meta_graph_impl
    saveable_view = _SaveableView(augmented_graph_view, options)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 264, in __init__
    checkpoint_util.objects_ids_and_slot_variables_and_paths(
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\checkpoint\util.py"", line 448, in objects_ids_and_slot_variables_and_paths
    slot_variables = _serialize_slot_variables(
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\checkpoint\util.py"", line 51, in _serialize_slot_variables
    slot_names = trackable.get_slot_names()
AttributeError: 'Adam' object has no attribute 'get_slot_names'

I'm also getting this message describing a path describing a path problem. I have CUDA Version 12.0 installed on my machine. I had to install a copy of CUDA file libdevice.10.bc in my working directory to get the code to run. Is there a way I can prevent recurrence of this

searched for CUDA in the following directories:
  ./cuda_sdk_lib
  C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-01-20 16:40:09.938510: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
```


### Standalone code to reproduce the issue

```shell
#Import the neccessary modules

import logging
import time

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
import tensorflow as tf

import tensorflow_text

print(tf.__version__)
print(""Num CPUs Available: "", len(tf.config.list_physical_devices('CPU')))
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
print(tf.config.list_physical_devices('CPU'))
print(tf.config.list_physical_devices('GPU'))

#Download the test data set

examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',
                               with_info=True,
                               as_supervised=True)

train_examples, val_examples = examples['train'], examples['validation']

for pt_examples, en_examples in train_examples.batch(3).take(1):    # use cache().repeat() to get whole dataset
  print('> Examples in Portuguese:')
  for pt in pt_examples.numpy():
    print(pt.decode('utf-8'))
  print()

  print('> Examples in English:')
  for en in en_examples.numpy():
    print(en.decode('utf-8'))

# Set up the tokenizer

model_name = 'ted_hrlr_translate_pt_en_converter'
tf.keras.utils.get_file(
    f'{model_name}.zip',
    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',
    cache_dir='.', cache_subdir='', extract=True
)

tokenizers = tf.saved_model.load(model_name)

print(""***** Tokenizer setup complete *****"")

# Tokenize the text strings

[item for item in dir(tokenizers.en) if not item.startswith('_')]

print('> This is a batch of strings:')
for en in en_examples.numpy():
  print(en.decode('utf-8'))

print(""***** Tokenizing started *****"")
encoded = tokenizers.en.tokenize(en_examples)

print(""***** Detokenizing loop started *****"")

print('> This is a padded-batch of token IDs:')
for row in encoded.to_list():
  print(row)

  round_trip = tokenizers.en.detokenize(encoded)

  print('> This is human-readable text:')
  for line in round_trip.numpy():
      print(line.decode('utf-8'))

print(""***** String tokenizing complete *****"")
print(""***** Lower level lookup method *****"")

print('> This is the text split into tokens:')
tokens = tokenizers.en.lookup(encoded)
tokens

print(""***** Calculating token length distribution *****"")

lengths = []

for pt_examples, en_examples in train_examples.batch(1024):
  pt_tokens = tokenizers.pt.tokenize(pt_examples)
  lengths.append(pt_tokens.row_lengths())

  en_tokens = tokenizers.en.tokenize(en_examples)
  lengths.append(en_tokens.row_lengths())
  print('.', end='', flush=True)

print(""\n***** Plotting distribution *****"")

all_lengths = np.concatenate(lengths)

plt.hist(all_lengths, np.linspace(0, 500, 101))
plt.ylim(plt.ylim())
max_length = max(all_lengths)
plt.plot([max_length, max_length], plt.ylim())
plt.title(f'Maximum tokens per example: {max_length}');
plt.show()

print(""***** Setting up data pipeline *****"")
print(""***** Create function for creating Input-Label pairs *****"")

MAX_TOKENS=128
def prepare_batch(pt, en):
    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.
    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.
    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor

    en = tokenizers.en.tokenize(en)
    en = en[:, :(MAX_TOKENS+1)]
    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens
    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens

    return (pt, en_inputs), en_labels

print(""***** Creating batches *****"")

BUFFER_SIZE = 20000
BATCH_SIZE = 24          # initial size 64

def make_batches(ds):
  return (
      ds
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .map(prepare_batch, tf.data.AUTOTUNE)
      .prefetch(buffer_size=tf.data.AUTOTUNE))

# Create training and validation set batches.
train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)

print(""***** Checking Input-Lable pairs *****"")

for (pt, en), en_labels in train_batches.take(1):
  break

print(pt.shape)
print(en.shape)
print(en_labels.shape)

print(en[0][:10])
print(en_labels[0][:10])

print(""***** Creating position encoding value function *****"")

def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

  angle_rates = 1 / (10000**depths)         # (1, depth)
  angle_rads = positions * angle_rates      # (pos, depth)

  pos_encoding = np.concatenate(
      [np.sin(angle_rads), np.cos(angle_rads)],
      axis=-1)

  return tf.cast(pos_encoding, dtype=tf.float32)

pos_encoding = positional_encoding(length=2048, depth=512)

# Check the shape.
print(pos_encoding.shape)

# Plot the dimensions.
plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')
plt.ylabel('Depth')
plt.xlabel('Position')
plt.colorbar()
plt.show()

pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)
p = pos_encoding[1000]
dots = tf.einsum('pd,d -> p', pos_encoding, p)
plt.subplot(2,1,1)
plt.plot(dots)
plt.ylim([0,1])
plt.plot([950, 950, float('nan'), 1050, 1050],
         [0,1,float('nan'),0,1], color='k', label='Zoom')
plt.legend()
plt.subplot(2,1,2)
plt.plot(dots)
plt.xlim([950, 1050])
plt.ylim([0,1])
plt.show()

# Use function to create a PositionEmbedding layer that looks-up a token's embedding vector and adds the position vector:

print(""***** Creating positional embedding layer class *****"")

class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)
    self.pos_encoding = positional_encoding(length=2048, depth=d_model)

  def compute_mask(self, *args, **kwargs):
    return self.embedding.compute_mask(*args, **kwargs)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positonal_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x


embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size(), d_model=512)
embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)

pt_emb = embed_pt(pt)
en_emb = embed_en(en)

en_emb._keras_mask

print(""***** Completed positional embedding layers *****"")

print(""***** Creating attention layer base class *****"")

class BaseAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

d = {'color': 'blue', 'age': 22, 'type': 'pickup'}
result = d['color']

print(""***** Creating cross attention layer base class *****"")
class CrossAttention(BaseAttention):
  def call(self, x, context):
    attn_output, attn_scores = self.mha(
        query=x,
        key=context,
        value=context,
        return_attention_scores=True)

    # Cache the attention scores for plotting later.
    self.last_attn_scores = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x

print(""***** Testing cross attention layer base class *****"")

sample_ca = CrossAttention(num_heads=2, key_dim=512)

print(pt_emb.shape)
print(en_emb.shape)
print(sample_ca(en_emb, pt_emb).shape)

print(""***** Creating global self attention class *****"")

class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

print(""***** Testing global self attention class *****"")

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

print(pt_emb.shape)
print(sample_gsa(pt_emb).shape)

print(""***** Creating causal self attention class *****"")

class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

  print(""***** Testing causal self attention class *****"")

sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

print(en_emb.shape)
print(sample_csa(en_emb).shape)

out1 = sample_csa(embed_en(en[:, :3]))
out2 = sample_csa(embed_en(en))[:, :3]

tf.reduce_max(abs(out1 - out2)).numpy()

print(""*****  Creating feed forward layer class *****"")

class FeedForward(tf.keras.layers.Layer):
  def __init__(self, d_model, dff, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),
      tf.keras.layers.Dense(d_model),
      tf.keras.layers.Dropout(dropout_rate)
    ])
    self.add = tf.keras.layers.Add()
    self.layer_norm = tf.keras.layers.LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    x = self.layer_norm(x)
    return x

print(""***** Testing feed forward layer class *****"")

sample_ffn = FeedForward(512, 2048)

print(en_emb.shape)
print(sample_ffn(en_emb).shape)

print(""***** Creating encoder layer class *****"")

class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()

    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x):
    x = self.self_attention(x)
    x = self.ffn(x)
    return x

print(""***** Testing encoder layer class *****"")

sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

print(pt_emb.shape)
print(sample_encoder_layer(pt_emb).shape)

print(""***** Creating Encoder Class *****"")

class Encoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads,
               dff, vocab_size, dropout_rate=0.1):
    super().__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(
        vocab_size=vocab_size, d_model=d_model)

    self.enc_layers = [
        EncoderLayer(d_model=d_model,
                     num_heads=num_heads,
                     dff=dff,
                     dropout_rate=dropout_rate)
        for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(dropout_rate)

  def call(self, x):
    # `x` is token-IDs shape: (batch, seq_len)
    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.

    # Add dropout.
    x = self.dropout(x)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x)

    return x  # Shape `(batch_size, seq_len, d_model)`.

print(""***** Testing Encoder Class *****"")

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=8500)

sample_encoder_output = sample_encoder(pt, training=False)

# Print the shape.
print(pt.shape)
print(""Sample Encoder Shape: "", sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.

print(""***** Creating decoder layer class  *****"")

class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self,
               *,
               d_model,
               num_heads,
               dff,
               dropout_rate=0.1):
    super(DecoderLayer, self).__init__()

    self.causal_self_attention = CausalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.cross_attention = CrossAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x, context):
    x = self.causal_self_attention(x=x)
    x = self.cross_attention(x=x, context=context)

    # Cache the last attention scores for plotting later
    self.last_attn_scores = self.cross_attention.last_attn_scores

    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
    return x

print(""***** Testing dencoder layer class *****"")

sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

sample_decoder_layer_output = sample_decoder_layer(
    x=en_emb, context=pt_emb)

print(en_emb.shape)
print(pt_emb.shape)
print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

print(""***** Creating Decoder Class *****"")

class Decoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                             d_model=d_model)
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads,
                     dff=dff, dropout_rate=dropout_rate)
        for _ in range(num_layers)]

    self.last_attn_scores = None

  def call(self, x, context):
    # `x` is token-IDs shape (batch, target_seq_len)
    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

    x = self.dropout(x)

    for i in range(self.num_layers):
      x  = self.dec_layers[i](x, context)

    self.last_attn_scores = self.dec_layers[-1].last_attn_scores

    # The shape of x is (batch_size, target_seq_len, d_model).
    return x

print(""***** Testing Decoder Class *****"")

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=8000)

output = sample_decoder(
    x=en,
    context=pt_emb)

# Print the shapes.
print(en.shape)
print(pt_emb.shape)
print(output.shape)

print(""Sample Decoder Shape: "", sample_decoder.last_attn_scores.shape)  # (batch, heads, target_seq, input_seq)

print(""***** Creating Transformer Class *****"")

class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits

print(""***** Testing Transformer Class *****"")

num_layers = 4
d_model = 128
dff = 512
num_heads = 8
dropout_rate = 0.1

#Instantiate Transformer

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),
    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),
    dropout_rate=dropout_rate)

output = transformer((pt, en))

print(en.shape)
print(pt.shape)
print(""Transformer Output Shape: "", output.shape)

attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

transformer.summary()

print(""***** Creating Training Optimizer Class *****"")

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super().__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

print(""***** Testing Training Optimizer *****"")

#Instatiate Training Optimizer

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))
plt.ylabel('Learning Rate')
plt.xlabel('Train Step')
plt.show();

print(""***** Create cross entropy loss function *****"")

def masked_loss(label, pred):
  mask = label != 0
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask

  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
  return loss


def masked_accuracy(label, pred):
  pred = tf.argmax(pred, axis=2)
  label = tf.cast(label, pred.dtype)
  match = label == pred

  mask = label != 0

  match = match & mask

  match = tf.cast(match, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)

print(""********** Compiling Model **********"")

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

print(""********** Fitting  Model **********"")

transformer.fit(train_batches,
                epochs=1,
                validation_data=val_batches)

print(""***** Creating Translator *****"")

class Translator(tf.Module):
  def __init__(self, tokenizers, transformer):
    self.tokenizers = tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS):
    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.
    start_end = self.tokenizers.en.tokenize([''])[0]
    start = start_end[0][tf.newaxis]
    end = start_end[1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.
    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.

    tokens = tokenizers.en.lookup(output)[0]

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    return text, tokens, attention_weights

translator = Translator(tokenizers, transformer)

print(""***** Testing Translator *****"")

def print_translation(sentence, tokens, ground_truth):
  print(f'{""Input:"":15s}: {sentence}')
  print(f'{""Prediction"":15s}: {tokens.numpy().decode(""utf-8"")}')
  print(f'{""Ground truth"":15s}: {ground_truth}')

print(""***** Translater Test Example 1 *****"")

sentence = 'este é um problema que temos que resolver.'
ground_truth = 'this is a problem we have to solve .'

translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)

print(""***** Translater Test Example 2 *****"")

sentence = 'os meus vizinhos ouviram sobre esta ideia.'
ground_truth = 'and my neighboring homes heard about this idea .'

translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)

print(""***** Translater Test Example 3 *****"")

sentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'
ground_truth = ""so i'll just share with you some stories very quickly of some magical things that have happened.""

translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)

print(""***** Creating Attention Head Plots *****"")

sentence = 'este é o primeiro livro que eu fiz.'
ground_truth = ""this is the first book i've ever done.""

translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)

def plot_attention_head(in_tokens, translated_tokens, attention):
  # The model didn't generate `<START>` in the output. Skip it.
  translated_tokens = translated_tokens[1:]

  ax = plt.gca()
  ax.matshow(attention)
  ax.set_xticks(range(len(in_tokens)))
  ax.set_yticks(range(len(translated_tokens)))

  labels = [label.decode('utf-8') for label in in_tokens.numpy()]
  ax.set_xticklabels(
      labels, rotation=90)

  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]
  ax.set_yticklabels(labels)

head = 0
# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.
attention_heads = tf.squeeze(attention_weights, 0)
attention = attention_heads[head]
attention.shape

#These are the input Portuguese tokens
in_tokens = tf.convert_to_tensor([sentence])
in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()
in_tokens = tokenizers.pt.lookup(in_tokens)[0]
in_tokens

#These are the translated English tokens
translated_tokens

plot_attention_head(in_tokens, translated_tokens, attention)

print(""***** Creating Attention Weight Plots *****"")

def plot_attention_weights(sentence, translated_tokens, attention_heads):
  in_tokens = tf.convert_to_tensor([sentence])
  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()
  in_tokens = tokenizers.pt.lookup(in_tokens)[0]

  fig = plt.figure(figsize=(16, 8))

  for h, head in enumerate(attention_heads):
    ax = fig.add_subplot(2, 4, h+1)

    plot_attention_head(in_tokens, translated_tokens, head)

    ax.set_xlabel(f'Head {h+1}')

  plt.tight_layout()
  plt.show()

plot_attention_weights(sentence,
                       translated_tokens,
                       attention_weights[0])

print(""****** Translate words that were not in training set *****"")

sentence = 'Eu li sobre triceratops na enciclopédia.'
ground_truth = 'I read about triceratops in the encyclopedia.'

translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)

plot_attention_weights(sentence, translated_tokens, attention_weights[0])

print(""***** Exporting Translator  *****"")

#Create ExportTranslator Class

class ExportTranslator(tf.Module):
  def __init__(self, translator):
    self.translator = translator

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def __call__(self, sentence):
    (result,
     tokens,
     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)

    return result

#Wrap translator in the ExportTranslator

translator = ExportTranslator(translator)

print(""***** Calling tf.saved_model.save() *****"")

#Save translator

tf.saved_model.save(translator, export_dir='translator')

print(""***** Translator Saved *****"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""D:\Craig\Python\Projects\pythonProject\pythonProject\main.py"", line 839, in <module>
    tf.saved_model.save(translator, export_dir='translator')
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1232, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1268, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1441, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1394, in _build_meta_graph_impl
    saveable_view = _SaveableView(augmented_graph_view, options)
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\save.py"", line 264, in __init__
    checkpoint_util.objects_ids_and_slot_variables_and_paths(
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\checkpoint\util.py"", line 448, in objects_ids_and_slot_variables_and_paths
    slot_variables = _serialize_slot_variables(
  File ""C:\Users\cpodc\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\checkpoint\util.py"", line 51, in _serialize_slot_variables
    slot_names = trackable.get_slot_names()
AttributeError: 'Adam' object has no attribute 'get_slot_names'
```
</details>"
59329,"Hey, I created new env with python=3.8.5 but I didn't fix it. Before install detectron2, I installed this packages:","Hey, I created new env with python=3.8.5 but I didn't fix it. Before install detectron2, I installed this packages:
```
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch
conda install -c fvcore -c iopath -c conda-forge fvcore iopath
conda install -c bottler nvidiacub
conda install jupyter
pip install scikit-image matplotlib imageio plotly opencv-python
pip install black usort flake8 flake8-bugbear flake8-comprehensions
conda install pytorch3d -c pytorch3d
```
After that I am running this command ""python -m pip install 'git+https://github.com/facebookresearch/detectron2.git' ""
I get same error.

_Originally posted by @EdibHamzaArslan in https://github.com/facebookresearch/detectron2/issues/4748#issuecomment-1396214415_"
59327,BUG: Unified Memory doesn't work as expected.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Custom
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.8
-   **Python version**: 3.8
-   **CUDA/cuDNN version**: 11.6.2 /  8.4.0.27
-   **GPU model and memory**: TitanX 12GB
-   **Exact command to reproduce**:
Running the above script without any unified memory:
CUDA_VISIBLE_DEVICES=""0"" TF_FORCE_GPU_ALLOW_GROWTH=1  TF_XLA_FLAGS=""--tf_xla_auto_jit=-1"" python oom.py

And with unified memory supposedly working:
CUDA_VISIBLE_DEVICES=""0"" TF_FORCE_UNIFIED_MEMORY=0 XLA_PYTHON_CLIENT_MEM_FRACTION=1.0  TF_FORCE_GPU_ALLOW_GROWTH=1   TF_XLA_FLAGS=""--tf_xla_auto_jit=-1"" python oom.py

### Describe the problem
When I run without unified memory, the problem goes OOM as expected, I selected the hyper-parameters such that slightly fewer layers do not go OOM. 
However, when I turn on unified memory, the algorithm still goes out of memory - I expected that the script would run just fine and I'd see the main RAM start filling up. 

At first, I thought it was XLA related clustering; however, I've turned that off, and the error still occurs.

### Source code / logs
oom.py:
```python
import tensorflow as tf

tf.config.set_soft_device_placement(True)

RANGE=80
SIZE=int(1024*3)

opt = tf.keras.optimizers.Adam(learning_rate=0.01)
model = tf.keras.Sequential([tf.keras.layers.Dense(SIZE) for _ in range(RANGE)])


@tf.function()
def func():
    with tf.device(""/gpu:0""):
        for _ in range(1):
            print(""STEP"", _)
            with tf.GradientTape() as t:
                inp = tf.ones([2, SIZE], tf.float32)
                y = model(inp)
                gradients = t.gradient(y, model.trainable_weights)
                opt.apply_gradients(zip(gradients, model.trainable_weights))
        return gradients

print(func())
```
"
59325,Segfault when running tf.image.combined_non_max_suppression,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segfault
```


### Standalone code to reproduce the issue

```shell
Please use the files in this link to reproduce the bug:


https://drive.google.com/drive/folders/11bMATgk2P5TZlMj1NmrBsLjJHEeU30kc?usp=share_link
```
```


### Relevant log output

```shell
_result = pywrap_tfe.TFE_Py_FastPathExecute(
2023-01-20 07:28:00.340964: W tensorflow/core/kernels/image/non_max_suppression_op.cc:995] Detected a large value for `max_total_size`. This may cause OOM error. (max_total_size: 1051019270)
Segmentation fault

```
```
</details>"
59324,Divide in Dataset.map lambda function fails with inf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

macOS

### Mobile device

_No response_

### Python version

3.8.15

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


The presented Standalone code will produce inf or nan.
If I will change:
```
x_train = x_train.map(lambda x : tf.divide(x,255.0))
```
to:
```
x_train = x_train.map(lambda x : tf.divide(x,tf.Variable(255.0, dtype=tf.float16)))
```
It works as it should.


### Standalone code to reproduce the issue

```shell
(x_train, _), (_, _) = tensorflow.keras.datasets.cifar10.load_data()

x_train = tf.convert_to_tensor(x_train, dtype=tf.float16)
x_train = tf.data.Dataset.from_tensor_slices(x_train)
x_train = x_train.map(lambda x : tf.divide(x,255.0))
list(x_train.take(1))
```


### Relevant log output

```shell
[<tf.Tensor: shape=(32, 32, 3), dtype=float16, numpy=
 array([[[inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf],
         ...,
         [inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf]],
 
        [[inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf],
         ...,
         [inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf]],
 
        [[inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf],
         ...,
         [inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf]],
 
...
         [inf, inf, inf],
         ...,
         [inf, inf, inf],
         [inf, inf, inf],
         [inf, inf, inf]]], dtype=float16)>]
```
</details>"
59323,"""tflite-model-maker"" package is not possible to install on Windows OS","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello. I am trying to install **tflite-model-maker** package in order to use it for training of mobile models. I found only once source for the installation https://pypi.org/project/tflite-model-maker/

I have tried all 3 methods mentioned there, but every time I try to import the package, there is always some issues or errors.
In all cases the issue is connected mostly with **scann** package.
Could not find a version that satisfies the requirement scann==1.2.6 (from tflite-model-maker) (from versions: none)
No matching distribution found for scann==1.2.6 

Do you know how is possible to successfully install tflite-model-maker package for Windows 11? Thanks a lot in advance!
```


### Standalone code to reproduce the issue

```shell
https://pypi.org/project/tflite-model-maker/
```


### Relevant log output

_No response_</details>"
59319,add_loss on pre-activations in RNN custom cell causes InaccessibleTensorError,"NOTE: I am copying a Stackoverflow question as it matches my problem exactly and is explained well by the author. I am getting the same errors on tf-nightly (2.12) and TF 2.11.

https://stackoverflow.com/questions/73821373/in-tensorflow-keras-how-do-you-use-the-add-loss-method-inside-a-custom-rnn-ce

----------------------------------------------------------------

My Goal: Use the add_loss method inside a custom RNN cell (in graph execution mode) to add an input-dependent loss.

General Setup:

Using Python 3.9
Using TensorFlow 2.8 or 2.10
Assuming import tensorflow as tf, I have a subclassed tf.keras.Model that uses a standard tf.keras.layers.RNN layer and a custom RNN cell (subclasses tf.keras.layers.Layer). Inside my custom RNN cell I call self.add_loss(*) in order to add an input-dependent loss.

Expected Result: When I call Model.fit(), the add_loss method is called for every batch and every timestep. The gradient computation step uses the added losses without raising an error.

Actual Result: When I call Model.fit(), an InaccessibleTensorError is raised during the gradient computation step, specifically when self.losses is called inside Model.train_step().

What I've tried:

The error is not raised when initializing the RNN layer with unroll=True (using eager- or graph-execution). Unfortunately this doesn't help me since my sequences can be long. Inspecting self.losses while debugging shows the correct number of elements (i.e., 4, one for each timestep).
The error is not raised when using eager execution and unroll=False. But inspecting self.losses shows the incorrect number of elements in self.losses; there is an extra element (i.e., 5). Further investigation reveals that there is an extra call to add_loss. Not sure why this occurs.
Switching to the latest stable version of TensorFlow (2.10.0) does not fix the issue.
After searching the web, Stack Overflow and issues/code on TensorFlow's GitHub, I'm completely stumped.


### Standalone code to reproduce the issue

```shell
import pytest
import tensorflow as tf


class FooModel(tf.keras.Model):
    """"""A basic model for testing.

    Attributes:
        cell: The RNN cell layer.

    """"""

    def __init__(self, rnn=None, **kwargs):
        """"""Initialize.

        Args:
            rnn: A Keras RNN layer.
            kwargs:  Additional key-word arguments.

        Raises:
            ValueError: If arguments are invalid.

        """"""
        super().__init__(**kwargs)

        # Assign layers.
        self.rnn = rnn

    def call(self, inputs, training=None):
        """"""Call.

        Args:
            inputs: A dictionary of inputs.
            training (optional): Boolean indicating if training mode.

        """"""
        output = self.rnn(inputs, training=training)
        return output


class BarCell(tf.keras.layers.Layer):
    """"""RNN cell for testing.""""""
    def __init__(self, **kwargs):
        """"""Initialize.

        Args:

        """"""
        super(BarCell, self).__init__(**kwargs)

        # Satisfy RNNCell contract.
        self.state_size = [tf.TensorShape([1]),]

    def call(self, inputs, states, training=None):
        """"""Call.""""""
        output = tf.reduce_sum(inputs, axis=1) + tf.constant(1.0)
        self.add_loss(tf.reduce_sum(inputs))

        states_tplus1 = [states[0] + 1]
        return output, states_tplus1


@pytest.mark.parametrize(
    ""is_eager"", [True, False]
)
@pytest.mark.parametrize(
    ""unroll"", [True, False]
)
def test_rnn_fit_with_add_loss(is_eager, unroll):
    """"""Test fit method (triggering backprop).""""""
    tf.config.run_functions_eagerly(is_eager)

    # Some dummy input formatted as a TF Dataset.
    n_example = 5
    x = tf.constant([
        [[1, 2, 3], [2, 0, 0], [3, 0, 0], [4, 3, 4]],
        [[1, 13, 8], [2, 0, 0], [3, 0, 0], [4, 13, 8]],
        [[1, 5, 6], [2, 8, 0], [3, 16, 0], [4, 5, 6]],
        [[1, 5, 12], [2, 14, 15], [3, 17, 18], [4, 5, 6]],
        [[1, 5, 6], [2, 14, 15], [3, 17, 18], [4, 5, 6]],
    ], dtype=tf.float32)
    y = tf.constant(
        [
            [[1], [2], [1], [2]],
            [[10], [2], [1], [7]],
            [[4], [2], [6], [2]],
            [[4], [2], [1], [2]],
            [[4], [2], [1], [2]],
        ], dtype=tf.float32
    )
    ds = tf.data.Dataset.from_tensor_slices((x, y))
    ds = ds.batch(n_example, drop_remainder=False)

    # A minimum model to reproduce the issue.
    cell = BarCell()
    rnn = tf.keras.layers.RNN(cell, return_sequences=True, unroll=unroll)
    model = FooModel(rnn=rnn)
    compile_kwargs = {
        'loss': tf.keras.losses.MeanSquaredError(),
        'optimizer': tf.keras.optimizers.Adam(learning_rate=.001),
    }
    model.compile(**compile_kwargs)

    # Call fit which will trigger gradient computations and raise an error
    # during graph execution.
    model.fit(ds, epochs=1)
```


### Relevant log output

```shell
Exception has occurred: InaccessibleTensorError
<tf.Tensor 'foo_model/rnn/while/bar_cell/Sum_1:0' shape=() dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.
Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.
```
</details>"
59312,How to change specific body line colour eg. Hip-Knee-Ankle line in swift ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Unable to change specific body line colour eg. Hip-Knee-Ankle line in swift
```


### Standalone code to reproduce the issue

```shell
How to change specific body line colour eg. Hip-Knee-Ankle line in swift
```


### Relevant log output

_No response_</details>"
59311,GPU delegate corrupts Tensor,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible): SnST/IE9111/IE9111:8.1.0/root/D20220531_110302:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 2.5.0 (also tried 2.11.0)
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): - 
- GPU: Adreno 615 GPU

**Standalone code to reproduce the issue**
I created a simpel Model outputting two copies of the same tensor:
```python
class TestModel(tf.keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self._name = ""TestModel""
        
        self.kernel_initializer = tf.keras.initializers.HeNormal()
        self.kernel_regularizer = tf.keras.regularizers.l2
        self.l2_reg = 1e-4
        
        self.b_0_0_conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1, 1),
                                 strides=(2, 2),
                                 padding=""same"",
                                 kernel_initializer=self.kernel_initializer,
                                 kernel_regularizer=self.kernel_regularizer(self.l2_reg),
                                 name=""in_conv"")

        self.b_0_0_bn = tf.keras.layers.BatchNormalization(axis=-1, name=""in_bn"")
        self.b_0_0_relu = tf.keras.layers.ReLU(name=""in_relu"")
        self.b_0_0_mp = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=""same"", name=""in_mp"")

    def call(self, x, mask=None):

        x = self.b_0_0_conv(x)
        x = self.b_0_0_bn(x, training=False)
        x = self.b_0_0_relu(x)
        x = self.b_0_0_mp(x)
        org = x
        return [org, x]
```
Here's the TfLite File: 
[test_model.tflite.zip](https://github.com/tensorflow/tensorflow/files/10455424/test_model.tflite.zip)


**Any other info / logs**
Using NNAPI delegate on my Android device both tensor copies are equal, as expected. Here's a snippet from Logcat:
```
2023-01-19 10:58:26.641  5738-6097  TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Await called in updateUserPreferences for acceleration type NNAPI
2023-01-19 10:58:26.656  5738-5767  ObjectDetector          com...hings.examples.tflitedetector  I  ObjectDetector configured with acceleration mode NNAPI
2023-01-19 10:58:26.657  5738-6097  TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Await unblocked for acceleration type NNAPI
2023-01-19 10:58:26.658  5738-6097  TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Return value is Detector was initialized with acceleration type NNAPI. for acceleration type NNAPI
2023-01-19 10:58:26.683  5738-5767  ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[0.0, 1.1767948, 0.0, 0.0]]]]
2023-01-19 10:58:26.684  5738-5767  ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767948, 0.0, 0.0]]]]
2023-01-19 10:58:26.716  5738-5767  ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[0.0, 1.1767948, 0.0, 0.0]]]]
2023-01-19 10:58:26.716  5738-5767  ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767948, 0.0, 0.0]]]]
```
But switching to GPU delegate shows that tensor1 and tensor2 differ:
```
2023-01-19 11:00:20.940 14695-14725 tflite                  com...hings.examples.tflitedetector  I  Created TensorFlow Lite delegate for GPU.
2023-01-19 11:00:21.094 14695-14725 tflite                  com...hings.examples.tflitedetector  I  Initialized OpenCL-based API.
2023-01-19 11:00:21.269 14695-14725 tflite                  com...hings.examples.tflitedetector  I  Created 1 GPU delegate kernels.
2023-01-19 11:00:21.271 14695-14725 ObjectDetector          com...hings.examples.tflitedetector  I  ObjectDetector configured with acceleration mode GPU
2023-01-19 11:00:21.272 14695-15233 TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Await unblocked for acceleration type GPU
2023-01-19 11:00:21.273 14695-15233 TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Return value is Detector was initialized with acceleration type GPU. for acceleration type GPU
2023-01-19 11:00:21.282 14695-14725 ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[1.479635E34, 4.79495E-39, 0.0, 0.0]]]]
2023-01-19 11:00:21.282 14695-14725 ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767578, 0.0, 0.0]]]]
2023-01-19 11:00:21.317 14695-14725 ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[1.479635E34, 4.79495E-39, 0.0, 0.0]]]]
2023-01-19 11:00:21.317 14695-14725 ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767578, 0.0, 0.0]]]]
```
I also tried Tensorflow Version 2.11.0 on my Android device. NNAPI ouputs two equal tensors, as expected, but GPU delegate: 
```
2023-01-19 11:07:23.958 10822-11010 TfLiteDetectorEndPoint  com...hings.examples.tflitedetector  I  Return value is Detector was initialized with acceleration type GPU. for acceleration type GPU
2023-01-19 11:07:23.966 10822-10850 ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[0.0, 0.0, 0.0, 0.0]]]]
2023-01-19 11:07:23.966 10822-10850 ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767578, 0.0, 0.0]]]]
2023-01-19 11:07:23.975 10822-10850 ObjectDetector          com...hings.examples.tflitedetector  D  tensor1 [[[[0.0, 0.0, 0.0, 0.0]]]]
2023-01-19 11:07:23.975 10822-10850 ObjectDetector          com...hings.examples.tflitedetector  D  tensor2 [[[[0.0, 1.1767578, 0.0, 0.0]]]]
```

Here [Comment](https://github.com/tensorflow/tensorflow/issues/28563#issuecomment-493190201) @impjdi mentioned that residual connections could be an issue using GPU delegate. Sadly I couldn't find any explanation why or how to work around it. 
"
59309,A parameter checking issue in the BatchNorm operator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF 2.3

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.1/7.6.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We found that the implementation of tf.keras.layers.BatchNormalization lacks parameter checks. The specific problem comes from its variance parameter. The reproduction of the error is specifically divided into four steps: 
1. Initialize a BN operator (i.e. source_model), randomly input an input (i.e. data), and get an output (i.e. source_result);
2. Set a constant delta, set it to a negative value, such as -1. Subtract delta from the variance of source_model, and add delta to epsilon to get a new BN operator (i.e. follow_model);
3. Input data to follow_model and get follow_result;
4. Calculate the distance between source_result and follow_result. Theoretically, it should be small or even 0, in practice a very large result can be obtained.
We use the same method to implement in pytorch and mindspore. There is no problem with the output of pytorch, and mindspore prompts that the variance of the BN operator should be in the (0, 1] interval.
```


### Standalone code to reproduce the issue

```shell
from tensorflow.keras.layers import BatchNormalization, Input
from tensorflow.keras.models import Model, clone_model

import os
import re
import numpy as np

def SourceModel(shape):
    x = Input(shape=shape[1:])
    y = BatchNormalization(axis=-1)(x)
    return Model(x, y)

def FollowModel_1(source_model):
    follow_model = clone_model(source_model)
    # 读取参数
    weights = source_model.get_weights()
    weights_names = [weight.name for layer in source_model.layers for weight in layer.weights]
    variance_idx = FindWeightsIdx(""variance"", weights_names)

    # 变异模型
    # delta = np.random.uniform(-1e-3, 1e-3, 1)[0]
    follow_model.layers[1].epsilon += delta     # 写BN层的epsilon
    weights[variance_idx] -= delta
    follow_model.set_weights(weights)

    return follow_model

def FindWeightsIdx(name, weights_names):
    # find layer index by name
    for idx, names in enumerate(weights_names):
        if re.search(name, names):
            return idx
    return -1


os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

shape = (10, 32, 32, 3)
data = np.random.uniform(-1, 1, shape)
delta = -1

source_model = SourceModel(shape)
follow_model = FollowModel_1(source_model)

source_result = source_model(data)
follow_result = follow_model(data)
dis = np.sum(abs(source_result-follow_result))

print(""delta:"", delta, ""; dis:"", dis)
```


### Relevant log output

```shell
delta: -1 ; dis: 4510.4536
```
</details>"
59308,No new version on Zenodo,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


On [Zenodo](https://zenodo.org/record/6574269) only versions up to 2.9.1 are stored. The last update came from May 23, 2022. I was told in [#51071](https://github.com/tensorflow/tensorflow/issues/51071) that new versions should have been added automatically, but they were not. Furthermore, I would be happy if they would come in chronological order and not several versions at the same day. Thanks in advance for the help :)



### Standalone code to reproduce the issue


Look at https://zenodo.org/record/6574269 and issue [#51071](https://github.com/tensorflow/tensorflow/issues/51071)



### Relevant log output

_No response_</details>"
59307,A batch normalization precision error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 1.13.1

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.0/7.4.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We found that the implementation of tf.keras.layers.BatchNormalization does not conform to its mathematical model. The cause of the problem may come from its variance parameter.  The occurrence of error is specifically divided into four steps:
(1)	Initialize a BN operator (i.e. source_model), randomly input an input (i.e. data), and get an output (i.e. source_result);
(2)	Randomly generate a perturbation (i.e. delta). Add the variance of source_model to delta, and subtract delta from epsilon to get a new BN operator (i.e. follow_model);
(3)	Input data to follow_model and get a follow_result;
(4)	Calculate the distance between source_result and follow_result. Theoretically, it should be small or even 0, in practice it can get a result greater than 1
```


### Standalone code to reproduce the issue

```shell
from tensorflow._api.v1.keras.layers import BatchNormalization, Input
from tensorflow._api.v1.keras.models import Model, clone_model

import os
import re
import numpy as np

def SourceModel(shape):
    x = Input(shape=shape[1:])
    y = BatchNormalization(axis=-1)(x)
    return Model(x, y)

def FollowModel_1(source_model):
    follow_model = clone_model(source_model)
    # read weight
    weights = source_model.get_weights()
    weights_names = [weight.name for layer in source_model.layers for weight in layer.weights]
    variance_idx = FindWeightsIdx(""variance"", weights_names)

    # mutation model 
    follow_model.layers[1].epsilon -= delta     # write epsilon
    weights[variance_idx] += delta  # write variance
    follow_model.set_weights(weights)

    return follow_model


def FindWeightsIdx(name, weights_names):
    # find layer index by name
    for idx, names in enumerate(weights_names):
        if re.search(name, names):
            return idx
    return -1

os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

shape = (10, 32, 32, 3)
data = np.random.uniform(-1, 1, shape)
delta = -0.0008

source_model = SourceModel(shape)
follow_model = FollowModel_1(source_model)

source_result = source_model.predict(data)
follow_result = follow_model.predict(data)
dis = np.sum(abs(source_result-follow_result))

print(""delta, dis:"", delta, dis)
```


### Relevant log output

```shell
delta, dis: -0.0008 6.142627
```
</details>"
59306,Feature Request tf.data.Dataset.from_tensor_slices with np.memmap,"
I'm interested in being able to do the following
```python
arr = np.memmap('train.bin', dtype=np.uint16)
ds = tf.data.Dataset.from_tensor_slices(arr)
```
without loading the entire array `arr` into memory."
59301,Error in importing tensor flow ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 4.9

### Custom Code

No

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have installed tensorflow using below command.
! pip install tensorflow
! pip install keras

then I tried to import 
and throwing error 
AttributeError: module 'numpy' has no attribute 'typeDict'

current numpy version 
numpy-1.24.1
numpy-1.21
```


### Standalone code to reproduce the issue

```shell
I have installed tensorflow using below command.
! pip install tensorflow
! pip install keras

then I tried to import 
and throwing error 
AttributeError: module 'numpy' has no attribute 'typeDict'

current numpy version 
numpy-1.24.1
numpy-1.21
```


### Relevant log output

_No response_</details>"
59300,Issue created for Rollback of PR #59233: Fix crash in stateless_random_binomial when shape type is int64,"Merged PR #59233 is rolled back in 8c8ab495e2acd02aeaa87b4997e9d32d1c8e761d.
    Please follow up with the reviewer and close this issue once its resolved."
59299,running distributed tensorflow using MPI,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3.1

### GPU model and memory

8 * rtx 2080 ti / 11gb memory each 

### Current Behaviour?

```shell
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
2023-01-18 13:18:35.789808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-01-18 13:18:35.790848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9687 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5
2023-01-18 13:18:35.791743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9687 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5
2023-01-18 13:18:35.792678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9687 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5
2023-01-18 13:18:35.804893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:0 with 9687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-01-18 13:18:35.805620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:1 with 9687 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5
2023-01-18 13:18:35.806333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:2 with 9687 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5
2023-01-18 13:18:35.807029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:3 with 9687 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5
2023-01-18 13:18:35.810512: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> g01:37672}
2023-01-18 13:18:35.810736: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://g01:37672
/usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2023-01-18 13:18:42.547198: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_DOUBLE
      type: DT_DOUBLE
    }
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 15
        }
      }
      shape {
        dim {
          size: 13
        }
      }
    }
  }
}
2023-01-18 13:18:42.740015: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
[g01:44037:0:44313] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))
==== backtrace (tid:  44313) ====
 0 0x000000000002137e ucs_debug_print_backtrace()  /umbc/ebuild-soft/cascade-lake/build/UCX/1.10.0/GCCcore-10.3.0/ucx-1.10.0/src/ucs/debug/debug.c:656
 1 0x000000000382045b tensorflow::NcclCommunicator::Enqueue()  collective_communicator.cc:0
 2 0x0000000005c9f88a tensorflow::NcclReducer::Run()  ???:0
 3 0x00000000009086dc tensorflow::BaseCollectiveExecutor::ExecuteAsync(tensorflow::OpKernelContext*, tensorflow::CollectiveParams const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void (tensorflow::Status const&)>)::{lambda()#3}::operator()()  base_collective_executor.cc:0
 4 0x0000000000b99403 tensorflow::UnboundedWorkQueue::PooledThreadFunc()  ???:0
 5 0x0000000000b9f6b1 tensorflow::(anonymous namespace)::PThread::ThreadFn()  env.cc:0
 6 0x0000000000007ea5 start_thread()  pthread_create.c:0
 7 0x00000000000feb0d __clone()  ???:0
=================================
[g01:44037] *** Process received signal ***
[g01:44037] Signal: Segmentation fault (11)
[g01:44037] Signal code:  (-6)
[g01:44037] Failing at address: 0x2ecf70000ac05
[g01:44037] [ 0] /lib64/libpthread.so.0(+0xf630)[0x2aaaab7e6630]
[g01:44037] [ 1] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x382045b)[0x2aaab68fc45b]
[g01:44037] [ 2] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow11NcclReducer3RunESt8functionIFvRKNS_6StatusEEE+0x1ca)[0x2aaab8d7b88a]
[g01:44037] [ 3] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(+0x9086dc)[0x2aaadc7556dc]
[g01:44037] [ 4] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow18UnboundedWorkQueue16PooledThreadFuncEv+0x1b3)[0x2aaadc9e6403]
[g01:44037] [ 5] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(+0xb9f6b1)[0x2aaadc9ec6b1]
[g01:44037] [ 6] /lib64/libpthread.so.0(+0x7ea5)[0x2aaaab7deea5]
[g01:44037] [ 7] /lib64/libc.so.6(clone+0x6d)[0x2aaaac468b0d]
[g01:44037] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 44037 on node g01 exited on signal 11 (Segmentation fault).
```


### Standalone code to reproduce the issue

```shell
from getOneHot import getOneHot
from mpi4py import MPI
comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Load in the parameter files
from json import load as loadf
with open(""params.json"", 'r') as inFile:
    params = loadf(inFile)

# Get data files and prep them for the generator
from tensorflow import distribute as D
callbacks = []
devices = getDevices()
print(devices)
set_tf_config_mpi()
strat = D.experimental.MultiWorkerMirroredStrategy(
        communication=D.experimental.CollectiveCommunication.NCCL)
# Create network
from sys import argv
resume_training = False
print(argv)
if ""resume_latest"" in argv:
    resume_training = True

with strat.scope():
    # Scheduler
    if isinstance(params[""learning_rate""], str):
        # Get the string for the importable function
        lr = params[""learning_rate""]
        from tensorflow.keras.callbacks import LearningRateScheduler
        # Use a dummy learning rate
        params[""learning_rate""] = 0.1
        # model = create_model(**params)
        # Get the importable function
        lr = lr.split(""."")
        baseImport = __import__(lr[0], globals(), locals(), [lr[1]], 0)
        lr = getattr(baseImport, lr[1])
        # Make a schedule
        lr = LearningRateScheduler(lr)
        callbacks.append(lr)
    # Resume Model?
    model_name = None
    if resume_training:
        initial_epoch, model_name = getInitialEpochsAndModelName(rank)
    if model_name is None:
        initial_epoch=0
        model = create_model(**params)
        resume_training = False
    else:
        from tensorflow.keras.models import load_model
        model = load_model(model_name)
# Load data from disk
import numpy
if ""root"" in params.keys():
    root = params['root']
else:
    root = ""./""
if ""filename"" in params.keys():
    filename = params[""filename""]
else:
    filename = ""dataset_timeseries.csv""

restricted = [
        'euc1', 'e1', 'x1', 'y1', 'z1',
        'euc2', 'e2', 'x2', 'y2', 'z2',
        'euc3', 'e3', 'x3', 'y3', 'z3',
        ]
x, y = getOneHot(""{}/{}"".format(root, filename), restricted=restricted, **params)
# val_x, val_y = getOneHot(""{}/{}"".format(root, val_filename), restricted=restricted)
val_x, val_y = None, None
params[""gbatch_size""] = params['batch_size'] * len(devices)
print(""x.shape ="", x.shape)
print(""y.shape ="", y.shape)
print(""epochs  ="", params['epochs'], type(params['epochs']))
print(""batch   ="", params['batch_size'], type(params['batch_size']))
print(""gbatch  ="", params['gbatch_size'], type(params['gbatch_size']))
# Load data into a distributed dataset
# Dataset object does nothing in place:
# https://stackoverflow.com/questions/55645953/shape-of-tensorflow-dataset-data-in-keras-tensorflow-2-0-is-wrong-after-conver
from tensorflow.data import Dataset
data = Dataset.from_tensor_slices((x, y))

# Create validation set
v = params['validation']
if val_x is not None:
    vrecord = val_x.shape[0]
    val  = Dataset.from_tensor_slices((val_x, val_y))
    validation = val # data.take(vrecord)
else:
    vrecord = int(x.shape[0]*v)
    validation = data.take(vrecord)
validation = validation.batch(params['gbatch_size'])
validation = validation.repeat(params['epochs'])
# Validation -- need to do kfold one day
# This set should NOT be distributed
vsteps = vrecord // params['gbatch_size']
if vrecord % params['gbatch_size'] != 0:
    vsteps += 1
# Shuffle the data during preprocessing or suffer...
# Parallel randomness == nightmare
# data = data.shuffle(x.shape[0])
# Ordering these two things is very important! 
# Consider 3 elements, batch size 2 repeat 2
# [1 2 3] -> [[1 2] [3]] -> [[1 2] [3] [1 2] [3]] (correct) batch -> repeat
# [1 2 3] -> [1 2 3 1 2 3] -> [[1 2] [3 1] [2 3]] (incorrect) repeat -> batch
# data = data.skip(vrecord)
data    = data.batch(params['gbatch_size'])
data    = data.repeat(params['epochs'])
records = x.shape[0] # - vrecord
steps   = records // params['gbatch_size']
if records % params['gbatch_size']:
    steps += 1
print(""steps   ="", steps)
# Note that if we are resuming that the number of _remaining_ epochs has
# changed!
# The number of epochs * steps is the numbers of samples to drop
print(""initial   cardinality = "", data.cardinality())
print(""initial v cardinality = "", data.cardinality())
data       = data.skip(initial_epoch*steps)
validation = validation.skip(initial_epoch*vsteps)
print(""final     cardinality = "", data.cardinality())
print(""final v   cardinality = "", data.cardinality())
# data = strat.experimental_distribute_dataset(data)
# Split into validation and training
callbacks  = createCallbacks(params, callbacks, rank, resume_training)
print(callbacks)

history = model.fit(data, epochs=params['epochs'],
        batch_size=params['gbatch_size'],
        steps_per_epoch=steps,
        verbose=0, 
        initial_epoch=initial_epoch,
        validation_data=validation,
        validation_steps=vsteps,
        callbacks=callbacks)
if rank == 0:
    model.save(""model-final"")
else:
    model.save(""checkpoints/model-tmp"")
```


### Relevant log output

_No response_</details>"
59293,Official TF images published to Dockerhub can be made smaller.,"First of all I was not sure how to classify this issue, so please re-classify if needed.

The issue concerns the GPU Dockerfile(s) used to build TF images that are later published to Dockerhub. E.g., in the:
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile

after line 62 (both files) two more commands are lacking:

```
&& apt-get clean \
&& rm -rf /var/lib/apt/lists/*; }
```
which (if added) would reduce the size of the Docker image. These two line shall be also added after line 89 (both files),

Similarly for these Dockerfiles:
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile

after lines 71 and 126 both."
59291,"Tensorflow 
@Khaokho29th","**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
59289,No quantized winograd convolution in tflite,"Winograd accelerate convolution both in float and int computation, I'm curious why there is no winograd convolution op in TFLite.  Weight transform is needed before weight quantization to keep accuracy as much as possible and then a new winograd_conv op is needed to handle the quantized transformed weight. 
Is there any concern about adding a winograd conv op?
"
59288,Missing fp16 ops when converting from TF to TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (or github SHA if from source): 2.11


**Provide the text output from tflite_convert**

```
[full_error.txt](https://github.com/tensorflow/tensorflow/files/10441958/full_error.txt)

error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: AddV2, Cast, ConcatV2, Conv2D, DepthwiseConv2dNative, Elu, GatherV2, Max, Minimum, Mul, Pad, RealDiv, Relu, Sqrt, Sum, Transpose
```

**Standalone code to reproduce the issue** 
converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)
tflite_model  = converter.convert()


"
59287,Model weight not saved when exporting SavedModel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

WSL2+tensorflow/tensorflow:2.8.0-gpu-jupyter image

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I need to change the signature in SavedModel by loading and re-export it with different configuration. The new exported model lost track of the model weight. Here's what I've tried
- Load the SavedModel (>300MB, with two signatures, exported in TF1.15) with TF2.8
- Select the signature I need
- Export Model with new signature config
- Load Model And Test

The re-exported model can be successfully loaded. But it won't be able to make any prediction due to initialized variable. I checked the exported model and the size is approximately 300kb, which means that most of the model weights were lost.
```
I looked into [this answer](https://stackoverflow.com/questions/59504276/resave-tf1-x-saved-model-pb-into-new-tf2-0-saved-model-pb) that solves the problem on his case. To me the only difference is that he converted variables in the original variable to constants. But for this approach the original `.ckpt` file is a prerequisite, which is not my case

### Standalone code to reproduce the issue

```shell
Script for reproducing the issue

# https://stackoverflow.com/questions/57048064/saved-model-prune-in-tf2-0
# https://stackoverflow.com/questions/59504276/resave-tf1-x-saved-model-pb-into-new-tf2-0-saved-model-pb
import tensorflow as tf
import numpy as np
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = """"
export_path_origin = ""baidu-ske-2019-len128""  # A Model with multiple signatures
assert tf.__version__[0] == '2'
print(""test"")

imported = tf.saved_model.load(export_dir=export_path_origin)
print(imported.signatures)
f_serving_default = imported.signatures[""serving_default""]
f_predict = imported.signatures[""predict""]


inputs = {
    ""input_ids"": tf.constant([[0] * 128]),
    ""segment_ids"": tf.constant([[0] * 128]),
    ""input_mask"":  tf.constant([[0] * 128])
}
serving_default_output = f_serving_default(**inputs)
# print(serving_default_output)
predict_output = f_predict(**inputs)
# print(predict_output)

signature_to_keep = f_predict

fetches = {k: v.name for k, v in signature_to_keep.structured_outputs.items()}
feeds = [input_.name for input_ in signature_to_keep.inputs]
new_fn = imported.prune(feeds, fetches=fetches)
new_fn.graph.finalize()
original_output = signature_to_keep(**inputs)
new_outputs = new_fn(**inputs)
print(new_outputs)

# Assert equal
for k in new_outputs:
    t_origin, t_new = original_output[k], new_outputs[k]
    assert t_origin.dtype == t_new.dtype
    if t_origin.dtype.is_floating:
        np.testing.assert_allclose(t_origin.numpy(), t_new.numpy())
    else:
        assert np.all(t_origin.numpy() == t_new.numpy())



class Exportable(tf.Module):
    def __init__(self, fn):
        super(Exportable, self).__init__()
        self.tf1_model = fn

    tf.function(input_signature=[tf.TensorSpec([None, 128], tf.int32), tf.TensorSpec([None, 128], tf.int32), tf.TensorSpec([None, 128], tf.int32)])
    def forward(self, input_ids, segment_ids, input_mask):
        with tf.init_scope():
            out = self.tf1_model(input_ids=input_ids,
                                segment_ids=segment_ids, input_mask=input_mask)
        return out


new_saved_model = Exportable(new_fn)

print(new_saved_model.forward(**inputs))    # Eager Execution Works
new_model_path = './new_saved_model'

tf.saved_model.save(new_saved_model, new_model_path)
imported = tf.saved_model.load(export_dir=new_model_path)
imported.signatures[""serving_default""](**inputs) # Error: Uninitialized variables
```
Here's the metadata of the original model from by `tensorflow/serving`
```
{
    ""model_spec"":{
     ""name"": ""default"",
     ""signature_name"": """",
     ""version"": ""1""
    }
    ,
    ""metadata"": {""signature_def"": {
     ""signature_def"": {
      ""predict"": {
       ""inputs"": {
        ""segment_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""segment_ids:0""
        },
        ""input_mask"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_mask:0""
        },
        ""input_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_ids:0""
        }
       },
       ""outputs"": {
        ""token_label_predictions"": {
         ""dtype"": ""DT_INT64"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""token_label_loss/ArgMax:0""
        },
        ""predicate_head_probabilities"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""49"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""predicate_head_select_loss/Sigmoid:0""
        }
       },
       ""method_name"": ""tensorflow/serving/predict""
      },
      ""serving_default"": {
       ""inputs"": {
        ""segment_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""segment_ids:0""
        },
        ""input_mask"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_mask:0""
        },
        ""input_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_ids:0""
        }
       },
       ""outputs"": {
        ""predicate_head_probabilities"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""49"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""predicate_head_select_loss/Sigmoid:0""
        },
        ""token_label_logits"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""61"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""token_label_loss/Reshape_1:0""
        }
       },
       ""method_name"": ""tensorflow/serving/predict""
      }
     }
    }
    }
    }
```
```


### Relevant log output

```shell
test
2023-01-18 11:28:10.218951: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-01-18 11:28:10.219019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b0c1756a148b): /proc/driver/nvidia/version does not exist
2023-01-18 11:28:10.219329: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
_SignatureMap({'serving_default': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FFB8402C9A0>, 'predict': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FFB38122730>})
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
{'predicate_head_probabilities': <tf.Tensor: shape=(1, 128, 128, 49), dtype=float32, numpy=
array([[[[1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        ...,

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711487e-06, 2.0477496e-06, 7.3399161e-07, ...,
          3.9122031e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]]]], dtype=float32)>, 'token_label_predictions': <tf.Tensor: shape=(1, 128), dtype=int64, numpy=
array([[60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]])>}
{'predicate_head_probabilities': <tf.Tensor: shape=(1, 128, 128, 49), dtype=float32, numpy=
array([[[[1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        ...,

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711487e-06, 2.0477496e-06, 7.3399161e-07, ...,
          3.9122031e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]]]], dtype=float32)>, 'token_label_predictions': <tf.Tensor: shape=(1, 128), dtype=int64, numpy=
array([[60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]])>}
2023-01-18 11:28:13.781743: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""/home/polonsky/Documents/volcano_poc/Bravo/test.py"", line 69, in <module>
    imported.signatures[""serving_default""](**inputs) # Error: Uninitialized variables
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1601, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1610, in _call_impl
    return self._call_with_structured_signature(args, kwargs,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1691, in _call_with_structured_signature
    return self._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 133, in _call_flat
    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1853, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 499, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:

Detected at node 'bert/encoder/layer_5/output/dense/bias/read' defined at (most recent call last):
    File ""/home/polonsky/Documents/volcano_poc/Bravo/test.py"", line 68, in <module>
      imported = tf.saved_model.load(export_dir=new_model_path)
Node: 'bert/encoder/layer_5/output/dense/bias/read'
Attempting to use uninitialized value StatefulPartitionedCall/bert/encoder/layer_5/output/dense/bias
         [[{{node bert/encoder/layer_5/output/dense/bias/read}}]] [Op:__inference_signature_wrapper_11982]
```
</details>"
59284,Tensorflow data validation-visualize_statistics()-Not working on databricks,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am unable to display the output of tfdv.visualize_statistics() output on databricks where as the same is working on local system.

Is it because that facets are not supported on databricks? Please provide a solution on how I can use modules of tensorflow data validation library on databricks.
```


### Standalone code to reproduce the issue

```shell
tfdv.visualize_statistics()

#link
https://www.tensorflow.org/tfx/data_validation/get_started
```


### Relevant log output

_No response_</details>"
59283,Issue in tfa.metrics.F1Score,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: - 
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.9
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: -
-   **GPU model and memory**: - 
-   **Exact command to reproduce**: -

### Describe the problem

In the documentation of the tensorflow addons F1Score is not mentioned, that the metric is not native working for binary-classification. An example would be realy helpfull to understand that for the binary classification the ""num_classes"" argument must be set to ""1"" and there must be a threshold of ""0.5"". 

### Source code / logs

I think the following code example would clearify the use of the F1Score function for binary classification. 

f1 = tfa.metrics.F1Score(num_classes=1, average=None, threshold=0.5)
n_output = 1
model = Sequential([
    InputLayer(input_shape=(X_train.shape[1], X_train.shape[2]), name=""Input""),
    Normalization(),
    Conv1D(16, kernel_size=(5), padding='same', activation='tanh'),
    BatchNormalization(),
    MaxPooling1D(),
    Conv1D(32, kernel_size=(5), padding='same', activation='tanh'),
    BatchNormalization(),
    MaxPooling1D(),
    Flatten(),
    Dropout(0.65),
    Dense(8, activation='tanh'),
    Dense(4, activation='tanh'),
    Dense(1 ,  activation=""sigmoid"")
])

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy',f1])
model.summary()
"
59281,lstm/gru layers produce non deterministic results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2

### GPU model and memory

V100

### Current Behaviour?

```shell
I have created a custom model for forecasting. I am facing issues with the results reproducibility when lstm/gru layers are used to model temporal information. I experimented with a different number of layers/units, and different batch sizes. Interestingly, some of these experiments result in producing non deterministic results. That means for the same architecture with the same hyper-parameters, I am getting different results for two successive runs. I am using cyclic learning rate with 'Adamax'/'Adam' optimizer. I am enabling determinism by using 

tf.keras.utils.set_random_seed(1)
tf.config.experimental.enable_op_determinism()

at the start of the program. I am not able to post my custom model. However, I was able to recreate the issue using MNIST data with a simple stacked GRU Model. I am determining the result reproducibility by checking the sum of predictions. The results of two successive runs are as follows:

Run1: After training: -18.18314552307129
Run2: After training: -19.713579177856445

Here, the difference might be small but I am seeing a higher divergence with the custom model.  

The code might result in reproducible output for a few runs but produces different results upon restarting notebook Runtime/Kernel.
```


### Standalone code to reproduce the issue

```shell
!pip install -q -U tensorflow_addons

import tensorflow as tf
import numpy as np
import tensorflow_addons as tfa
from tensorflow.keras import backend as K

tf.keras.utils.set_random_seed(1)
tf.config.experimental.enable_op_determinism()

batch_size = 4096
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train/255.0, x_test/255.0
x_validate, y_validate = x_test[:-10], y_test[:-10]
x_test, y_test = x_test[-10:], y_test[-10:]

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
x_validate = np.expand_dims(x_validate, -1)


def get_model():
    kernel_initializer = tf.keras.initializers.he_normal(seed=13)
    layer_initializer = tf.keras.initializers.glorot_uniform(seed=13)
    rc_layer_initializer = tf.keras.initializers.orthogonal(seed=13)
    ip = tf.keras.layers.Input(shape=(28, 28), name=""input"")
    gru1 = tf.keras.layers.GRU(128, return_sequences=True, 
                               kernel_initializer=layer_initializer, 
                               recurrent_initializer=rc_layer_initializer)(ip)
    gru2 = tf.keras.layers.GRU(64, return_sequences=False, 
                               kernel_initializer=layer_initializer, 
                               recurrent_initializer=rc_layer_initializer)(gru1)
    out = tf.keras.layers.Dense(10, kernel_initializer=kernel_initializer)(gru2)
    model = tf.keras.models.Model(inputs = ip, outputs=out, name='ANN-model')
    print(model.summary())
    return model


def train_model(model):
    steps_per_epoch = len(x_train) // batch_size
    clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-4,
        maximal_learning_rate=1e-1,
        scale_fn=lambda x: 1/(2.**(x-1)),
        step_size=2 * steps_per_epoch
    )
    optimizer = tf.keras.optimizers.Adam(clr)
    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  optimizer=optimizer,
                  metrics=['accuracy'])
    
    model_pred = model.predict(x_test)
    print(""Before training: {}"".format(np.sum(model_pred)))
    
    reg = model.fit(x_train, y_train, validation_data=(x_validate, y_validate), batch_size=batch_size, epochs=5, shuffle=False)
    
    model_pred = model.predict(x_test)
    print(""After training: {}"".format(np.sum(model_pred)))

# Run1
K.clear_session()
model = get_model()
train_model(model)

# Run2
K.clear_session()
model2 = get_model()
train_model(model2)
```


### Relevant log output

```shell
Run1 results:

1/1 [==============================] - 2s 2s/step
Before training: -2.2939682006835938
Epoch 1/5
15/15 [==============================] - 4s 86ms/step - loss: 2.1610 - accuracy: 0.2261 - val_loss: 1.6038 - val_accuracy: 0.3941
Epoch 2/5
15/15 [==============================] - 0s 30ms/step - loss: 1.3274 - accuracy: 0.5408 - val_loss: 0.7607 - val_accuracy: 0.7368
Epoch 3/5
15/15 [==============================] - 0s 31ms/step - loss: 0.4288 - accuracy: 0.8677 - val_loss: 0.2311 - val_accuracy: 0.9302
Epoch 4/5
15/15 [==============================] - 0s 31ms/step - loss: 0.1894 - accuracy: 0.9428 - val_loss: 0.1639 - val_accuracy: 0.9490
Epoch 5/5
15/15 [==============================] - 0s 31ms/step - loss: 0.1461 - accuracy: 0.9559 - val_loss: 0.1317 - val_accuracy: 0.9617
1/1 [==============================] - 0s 23ms/step
After training: -18.18314552307129

Run2 Results:
None
1/1 [==============================] - 1s 585ms/step
Before training: -2.2939682006835938
Epoch 1/5
15/15 [==============================] - 4s 84ms/step - loss: 2.1610 - accuracy: 0.2261 - val_loss: 1.6038 - val_accuracy: 0.3941
Epoch 2/5
15/15 [==============================] - 0s 31ms/step - loss: 1.3274 - accuracy: 0.5408 - val_loss: 0.7607 - val_accuracy: 0.7368
Epoch 3/5
15/15 [==============================] - 0s 30ms/step - loss: 0.4289 - accuracy: 0.8677 - val_loss: 0.2320 - val_accuracy: 0.9304
Epoch 4/5
15/15 [==============================] - 0s 30ms/step - loss: 0.1897 - accuracy: 0.9429 - val_loss: 0.1660 - val_accuracy: 0.9501
Epoch 5/5
15/15 [==============================] - 0s 29ms/step - loss: 0.1464 - accuracy: 0.9555 - val_loss: 0.1314 - val_accuracy: 0.9611
1/1 [==============================] - 0s 23ms/step
After training: -19.713579177856445
```
</details>"
59279,Add warning in Dataset.shuffle API for possible data leakage,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9,2.10,2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi Tensorflow Team,

Could you please add a pitfall warning in the Docs of ""tf.data.Dataset"" API, shuffle method about the ""reshuffle_each_iteration"" might lead to validation data leakage? 

It's only very recently that I learned using shuffle method followed by take/skip methods to generate train/test/validation sets could lead to val/test data leaking into training set (because the full dataset is shuffled before split for each epoch).
```


### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
dataset = dataset.shuffle(buffer_size=BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE)

train_dataset = dataset.take(TRAIN_BATCH_SIZE)
val_dataset = dataset.skip(TRAIN_BATCH_SIZE)

model.fit(train_dataset, validation_data=val_dataset, batch_size=BATCH_SIZE, epochs=EPOCHS)
```


### Relevant log output

_No response_</details>"
59276,openCL delegate generates 'inf' values with a sequence of Dense/FullyConnected nodes.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.1

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

tested on Snapdragon 888, 865

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The openCL delegate has an issue with a sequence of Dense/FullyConnected nodes. Our experiments have revealed that if we use a sequence of Dense layers in a special pattern, the corresponding tflite version of this model will generate a bunch of inf/nan values for some random indices in some runs. This issue happens with both FP16 and FP32 tflite versions. This issue can't be reproduced with the XNNPACK delegate
```


### Standalone code to reproduce the issue

```shell
We have implemented a small tool (with a comprehensive ReadMe) to reproduce the mentioned issue. Here is the link to the repository:
https://github.com/Bahar-BM/test_openCL
```


### Relevant log output

_No response_</details>"
59275,keras.models.clone_model not working,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
To clone a model using keras.models.clone_model to keep ema weights
```


### Standalone code to reproduce the issue

```shell
When cloning a model that inherits from (tf.keras.models.Model), can't use keras.models.clone_model.
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
Input In [20], in <cell line: 1>()
----> 1 gan = GAN(generator=generator, discriminator=discriminator)
      5 # Define losses
      6 generator_loss, discriminator_loss = get_loss('nsl')

Input In [19], in AdatGAN.__init__(self, generator, discriminator)
      4 super(AdatGAN, self).__init__()
      5 self.generator = generator
----> 6 self.ema_generator = tf.keras.models.clone_model(self.generator)
      7 self.discriminator = discriminator
      8 self.noise_dim = noise_dim

File ~/anaconda3/envs/tf28/lib/python3.8/site-packages/keras/models/cloning.py:505, in clone_model(model, input_tensors, clone_function)
    501     return _clone_sequential_model(
    502         model, input_tensors=input_tensors, layer_fn=clone_function
    503     )
    504 else:
--> 505     return _clone_functional_model(
    506         model, input_tensors=input_tensors, layer_fn=clone_function
    507     )

File ~/anaconda3/envs/tf28/lib/python3.8/site-packages/keras/models/cloning.py:173, in _clone_functional_model(model, input_tensors, layer_fn)
    167     raise ValueError(
    168         ""Expected `model` argument ""
    169         ""to be a functional `Model` instance, ""
    170         f""got a `Sequential` instance instead: {model}""
    171     )
    172 if not model._is_graph_network:
--> 173     raise ValueError(
    174         ""Expected `model` argument ""
    175         ""to be a functional `Model` instance, ""
    176         f""but got a subclassed model instead: {model}""
    177     )
    179 new_input_layers = {}  # Cache for created layers.
    180 if input_tensors is not None:
    181     # Make sure that all input tensors come from a Keras layer.

ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclassed model instead: <__main__.Generator object at 0x7efff88591c0>
```
</details>"
59274,How to save_weights of QAT model in tensorflow model optimization?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to train a model using quantise awareness training. I am using tensorflow model optimization. I create a model, quantise it, fit it, save the weight.

Then at a later date I redefine and quantise and try to load the weights.

However, the model starts the whole training process from the start again.

This article explains how to save the whole QAT model.
https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide#checkpoint_and_deserialize

However, I wish to save the weights not model. Please don't suggest to follow the save whole model! All help would be much appreciated.
```


### Standalone code to reproduce the issue

```shell
from tensorflow_model_optimization.quantization.keras import quantise_model
model = define_model()
qat_model = quantize_model(model)
qat_model.fit(...)
qat_model.save_weights(""qat_weights.h5"")
... Finish for Now ...


... Pick up at a later date ...
model = define_model()
qat_model = quantize_model(model)
qat_model.load_weights(""qat_weights.h5"")
```


### Relevant log output

Starts the whole training process from the start at 0%

_No response_</details>"
59273,tf.lite from tensorflow runs faster than the tflite_runtime,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu

### Mobile device

Arm64

### Python version

3.9

### Bazel version

a

### GCC/Compiler version

a

### CUDA/cuDNN version

a

### GPU model and memory

a

### Current Behaviour?

```shell
If you run the tf.lite method from tf 2.11.0 its approx 30-50% faster than the tflite_runtime 2.11.0

I have been trying different compile options as presume something in the build if giving the full TF Lite methods better performnce, but not found anything.

All works fine but just thought I would mention tflite_runtime does seem slower and curious to why?
```


### Standalone code to reproduce the issue

```shell
a
```


### Relevant log output

```shell
a
```
</details>"
59272,Model save issues with custom optimizer - DPKerasSGDOptimizer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Problem 
A keras sequential Model was compiled using DPKerasSGD optimizer and saved. But when the saved model is loaded it throws the below error

""Unknown optimizer: 'DPOptimizerClass'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details""

Expected Behaviour
Since the model was already compiled using the optimizer it should run perfectly fine when fitted with the data. 

Whereas the same works when the optimizer is replaced. Please provide a solution on how to save the model with this custom optimizer.
```


### Standalone code to reproduce the issue

```shell
#building the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')])

#compiling the model
  model.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True,reduction=tf.losses.Reduction.NONE),
    optimizer=tensorflow_privacy.DPKerasSGDOptimizer(l2_norm_clip=1,noise_multiplier=2,learning_rate=0.01),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ])

#saving the model
model.save('my_model')

#loading the model
keras.models.load_model('mymodel')

#throws the error
```


### Relevant log output

_No response_</details>"
59271,Rust binding for Tensorflow Lite,"Is there any kind of plan to create Tensorflow Lite bindings for Rust? It would be convenient for projects that only need the model execution layer, without having to import the rest of the project."
59269,CUDNN_STATUS_EXECUTION_FAILED while compiling model on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Rocky linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5750): 'cudnnBatchNormalizationForwardTrainingEx( cudnn.handle(), mode, bn_ops, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), side_input.opaque(), x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(), exponential_average_factor, batch_mean_opaque, batch_var_opaque, epsilon, saved_mean->opaque(), saved_inv_var->opaque(), activation_desc.handle(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
```


### Relevant log output

_No response_</details>"
59268,Generate flatbuffer files during build,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

any

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently the output of the flatbuffer schema files are committed to the sources. This makes PRs updating the flatbuffer version and schema changes very big (see e.g. https://github.com/tensorflow/tensorflow/commit/bf0901b663e87f081c7a317fe1ac705ab1a2df8b) and also requires users to build against the exact same flatbuffer version. This may be an issue when an another version already exists which then fails the build. Even flatbuffers 2.0.6 vs 2.0.7 is so different they are not compatible.

If the flatbuffer C++files/headers etc. were generated during the build process this would be much easier and even avoids trouble when it was forgotten to update the generated files on schema changes.
```


### Standalone code to reproduce the issue

```shell
Build
```


### Relevant log output

_No response_</details>"
59265,How to totally restore a session to avoid sess.run speed declination?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Here's part of my code:

```python
 def fun(self):
        self.lr=1e-2
        self.gen_optimizer()
        self.sess.run(tf.variables_initializer(self._optimizer.variables()))
        self.sess_backup=copy.copy(self.sess)
        #tf.train.Saver().save(self.sess,'tmp.ckpt')
        for u in range(0,self.num_test_users): 
            print('Evaluating user',u,':')
            self.sess=copy.copy(self.sess_backup)
            #tf.train.Saver().restore(self.sess,'tmp.ckpt') 
            #self.sess.run(tf.variables_initializer(self._optimizer.variables()))
            tmp1,tmp2,tmp3=self.fun2(12,u)
            for i in range(0,self.adapt_itrs): #local adaption
                self.adjust(i,tmp1,tmp2,tmp3,u)
```
I want to totally restore the session each time it enters the for loop. Just as what is commented out, I tried to use tf.train.Saver, but it can't totally restore: in the for loop, the model is trained several times using sess.run, and as time goes by, the running speed is slower and slower, which means that there must be redundant graphs stored in self.sess. So I tried to use copy.copy to copy the whole session as a backup to be restored later, but it reports that `Run with options is not supported for this session.` since the following steps use the feed_dict.



### Standalone code to reproduce the issue

Here's a meaning less program but it can clarify my problem:
```python
import copy
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_v2_behavior()
sess=tf.Session()
_input=tf.placeholder(dtype=tf.float32, shape=[100, 20], name=""input"")
_output=tf.placeholder(dtype=tf.float32, shape=[100, 1], name=""output"")
W = tf.get_variable(name=""W"", initializer=tf.truncated_normal(shape=[20, 1],
                                         mean=0, stddev=0.03),dtype=tf.float32)
output=tf.matmul(_input,W)
loss=tf.reduce_sum(tf.square(_output-output))
_optimizer = tf.train.AdamOptimizer(0.01)
optimizer = _optimizer.minimize(loss, global_step=tf.Variable(0, trainable=False))
input_mat=np.random.rand(100,20)
output_mat=input_mat@np.random.rand(20,1)
init=tf.global_variables_initializer()
sess.run(init)
_,l=sess.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat})
for i in range(5):
    '''
    Each time entering the loop, reset the session to the original one.
    Don't use tf.train.Saver, because it cannot totally restore the whole session since the running speed
    is still lower and lower.
    '''
    sess2=copy.copy(sess)
    _,l=sess2.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat}) #Reports ""Run with options is not supported for this session.""
    sess2.close()
sess.close()
```


### Relevant log output

```shell
= RESTART: D:/programming/python/research_backup/AutoRec/Autorec_filmtrust_exp2/try.py
WARNING:tensorflow:From D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\compat\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Traceback (most recent call last):
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1377, in _do_call
    return fn(*args)
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1360, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1453, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.UnimplementedError: Run with options is not supported for this session.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/programming/python/research_backup/AutoRec/Autorec_filmtrust_exp2/try.py"", line 26, in <module>
    _,l=sess2.run([optimizer,loss],feed_dict={_input:input_mat,_output:output_mat}) #Reports ""Run with options is not supported for this session.""
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1370, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""D:\Users\myhac\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\client\session.py"", line 1396, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:

Run with options is not supported for this session.
```
```
</details>"
59264,"issue ""could not load dynamic library""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.2 cuDNN=8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tf doesn't find the dynamic library so i'm doing my stuff on my cpu. (sorry if there already is an answer, i didn't find anything that helped me)
```


### Standalone code to reproduce the issue

```shell
It happens every time
```


### Relevant log output

_No response_</details>"
59263,Issu,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached."
59262,I have some problems I think it is connected with dataset that i make myself,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I got an Error with model.fit
For your information, I will also add that the table used for the dataset 
consists of 2 columns, one of which contains the paths to the images on
 the disk, and the second one contains the labels for each image. 
 And I’m also new to github and therefore I don’t know how to make it so
 that you can run all the code because I don’t really want to share a huge
 folder with files, I can also clarify that the training arrays consist of 800 .png images
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1VsjzbjAhZru1kdY7iElxQWrsIW9rMfGA?usp=sharing
```


### Relevant log output

```shell
UnimplementedError                        Traceback (most recent call last)
<ipython-input-27-c0002c3040bd> in <module>
----> 1 model.fit(x_train, y_train, epochs=10)

1 frames
/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---> 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

UnimplementedError: Graph execution error:

Detected at node 'sparse_categorical_crossentropy/Cast' defined at (most recent call last):
    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-27-c0002c3040bd>"", line 1, in <module>
      model.fit(x_train, y_train, epochs=10)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1051, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1030, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 890, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 948, in compute_loss
      return self.compiled_loss(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py"", line 201, in __call__
      loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 139, in __call__
      losses = call_fn(y_true, y_pred)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 243, in call
      return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 1860, in sparse_categorical_crossentropy
      return backend.sparse_categorical_crossentropy(
    File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 5223, in sparse_categorical_crossentropy
      target = cast(target, 'int64')
    File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 2066, in cast
      return tf.cast(x, dtype)
Node: 'sparse_categorical_crossentropy/Cast'
Cast string to int64 is not supported
	 [[{{node sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_469]
```
</details>"
59261,"tflite Model Maker installation Error metadata-generation-failed, No such file or directory: '..\\VERSION'","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello everyone, I hope you can help me with the following problem

I have checked the requirements several times but always get the same error:

pip install tflite-model-maker results in FileNotFoundError: [Errno 2] No such file or directory: '..\\VERSION' 

Clone source code from GitHub and install results in TypeError: 'WindowsPath' object is not subscriptable
```


### Standalone code to reproduce the issue

```shell
pip install tflite-model-maker
```


### Relevant log output

```shell
Using cached sentencepiece-0.1.83.tar.gz (497 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [8 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""C:\Users\1\AppData\Local\Temp\pip-install-uny2d85u\sentencepiece_d8babf4dcc3a48be9b4874119d6463fb\setup.py"", line 29, in <module>
          with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:
        File ""C:\Users\1\AppData\Local\Programs\Python\Python310\lib\codecs.py"", line 905, in open
          file = builtins.open(filename, mode, buffering)
      FileNotFoundError: [Errno 2] No such file or directory: '..\\VERSION'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

-------------------------------------------------------------------------

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [12 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""C:\Users\1\Desktop\Dev\tflite_model_maker\examples\tensorflow_examples\lite\model_maker\pip_package\setup.py"", line 122, in <module>
          INTERNAL_NAME).run()
        File ""C:\Users\1\Desktop\Dev\tflite_model_maker\examples\tensorflow_examples\lite\model_maker\pip_package\setup_util.py"", line 163, in run
          namespace_packages = find_namespace_packages(where=self.build_dir)
        File ""C:\Users\1\Desktop\Dev\tflite_model_maker\tfliteMM-venv\lib\site-packages\setuptools\__init__.py"", line 64, in find
          convert_path(where),
        File ""C:\Users\1\AppData\Local\Programs\Python\Python310\lib\distutils\util.py"", line 123, in convert_path
          if pathname[0] == '/':
      TypeError: 'WindowsPath' object is not subscriptable
      [end of output]
```
</details>"
59260,tflite_convert: command not found,"I have Ubuntu 20.04 running on a WSL. with conda
NVIDIA Tenserflow 1.15 is installed on it.

As git model I have the TensorFlow Models v.1.13.0 release
https://github.com/tensorflow/models/tree/r1.13.0

now i created a custom model and wanted to convert to tflite.
 but i get the error.....

**------------------------>tflite_convert: command not found<-----------------------------**

what could be the reason? or how can i fix it?

it has to stay at version 1.15, because i have to create models for a Coral EdgeTPU.

pip list

Package Version
------------------------ --------------
absl-py 1.4.0
argon2-cffi 21.3.0
argon2-cffi-bindings 21.2.0
astor 0.8.1
astunparse 1.6.3
async-generator 1.10
attrs 22.2.0
backcall 0.2.0
bleach 4.1.0
cachetools 4.2.4
certifi 2022.12.7
cffi 1.15.1
charset-normalizer 2.0.12
cloudpickle 2.2.0
cycler 0.11.0
cython 0.29.33
dataclasses 0.8
decorator 5.1.1
defusedxml 0.7.1
entrypoints 0.4
guest 0.3.3
google-auth 2.16.0
google-auth-oauthlib 0.4.6
google-pasta 0.2.0
grpcio 1.48.2
h5py 2.10.0
idna 3.4
importlib-metadata 4.8.3
ipykernel 5.5.6
ipython 7.16.3
ipython-genutils 0.2.0
ipywidgets 7.7.2
jedi 0.17.2
jinja2 3.0.3
jsonschema 3.2.0
jupyter 1.0.0
jupyter-client 7.1.2
jupyter-console 6.4.3
jupyter-core 4.9.2
jupyterlab-pygments 0.1.2
jupyterlab-widgets 1.1.1
keras-applications 1.0.8
keras-preprocessing 1.1.2
kiwisolver 1.3.1
lxml 4.9.2
Markdown 3.3.7
MarkupSafe 2.0.1
matplotlib 3.3.4
mistune 0.8.4
nbclient 0.5.9
nbconvert 6.0.7
nbformat 5.1.3
nest-asyncio 1.5.6
notebook 6.4.10
numpy 1.19.5
nvidia-cublas-cu11 11.11.3.6
nvidia-cuda-cupti-cu11 11.8.87
nvidia-cuda-nvcc-cu11 11.8.89
nvidia-cuda-runtime-cu11 11.8.89
nvidia-cudnn-cu11 8.7.0.84
nvidia-cufft-cu11 10.9.0.58
nvidia-curand-cu11 10.3.0.86
nvidia-cusolver-cu11 11.4.1.48
nvidia-cusparse-cu11 11.7.5.86
nvidia-dali-cuda110 1.20.0
nvidia-dali-nvtf-plugin 1.20.0+nv22.12
nvidia-horovod 0.26.1+nv22.12
nvidia-nccl-cu11 2.16.2
nvidia-pyindex 1.0.9
nvidia-tensorflow 1.15.5+nv22.12
oauthlib 3.2.2
object-detection 0.1
opt-einsum 3.3.0
packaging 21.3
pandas 1.1.5
pandocfilters            1.5.0
parso                    0.7.1
pathlib                  1.0.1
pexpect                  4.8.0
pickleshare              0.7.5
Pillow                   8.4.0
pip                      21.2.2
prometheus-client        0.15.0
prompt-toolkit           3.0.36
protobuf                 3.19.6
psutil                   5.9.4
ptyprocess               0.7.0
pyasn1                   0.4.8
pyasn1-modules           0.2.8
pycparser                2.21
Pygments                 2.14.0
pyparsing                3.0.9
pyrsistent               0.18.0
python-dateutil          2.8.2
pytz                     2022.7
PyYAML                   6.0
pyzmq                    25.0.0
qtconsole                5.2.2
QtPy                     2.0.1
requests                 2.27.1
requests-oauthlib        1.3.1
rsa                      4.9
scipy                    1.5.4
Send2Trash               1.8.0
setuptools               59.6.0
six                      1.16.0
sklearn                  0.0.post1
slim                     0.1
tensorboard              2.10.1
tensorboard-data-server  0.6.1
tensorboard-plugin-wit   1.8.1
tensorflow-estimator     1.15.1
tensorrt                 8.5.2.2
termcolor                1.1.0
terminado                0.12.1
testpath                 0.6.0
torch                    1.10.2
tornado                  6.1
traitlets                4.3.3
typing_extensions        4.1.1
urllib3                  1.26.14
utils                    1.0.1
wcwidth                  0.2.5
webencodings             0.5.1
Werkzeug                 2.0.3
wheel                    0.37.1
widgetsnbextension       3.6.1
wrapt                    1.14.1
zipp                     3.6.0"
59259,Conversion of Diffusion model inside Colab generates a non valid .tflite file,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab connected to VM of GCP (high RAM is needed)
- TensorFlow installation (pip package or built from source): Version 2.11.0
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

Link for colab notebook to open: 
`https://colab.research.google.com/drive/1I7m4sGWi6U1ODbG0Fln42GU45CEf7Qsk?authuser=1#scrollTo=uGjOTEFoZrDq`
Link for colab notebook to download:
`https://colab.research.google.com/drive/1I7m4sGWi6U1ODbG0Fln42GU45CEf7Qsk?usp=share_link`

- Include code to invoke the TFLite Converter Python API and the errors.
```
converter = tf.lite.TFLiteConverter.from_keras_model(diffusion_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Save the model.
with open('diffusion_model.tflite', 'wb') as f:
  f.write(tflite_model)
```

- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
- https://huggingface.co/fchollet/stable-diffusion/blob/main/kcv_diffusion_model.h5
- https://drive.google.com/file/d/1coWb7CWBYrU7pDFX3SWpmmWUqus68R7w/view?usp=share_link


### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model cannot be opened by netron.app and android application. The error is:
""Not valid .tflite flatbuffer file""

### 5. (optional) Any other info / logs
When the default quantization is used 
```
converter = tf.lite.TFLiteConverter.from_keras_model(diffusion_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Save the model.
with open('diffusion_model4.tflite', 'wb') as f:
  f.write(tflite_model)
```

the procedure exits with:
```
"" WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, group_normalization_63_layer_call_fn, group_normalization_63_layer_call_and_return_conditional_losses while saving (showing 5 of 1320). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmptbzu7bs5/assets
INFO:tensorflow:Assets written to: /tmp/tmptbzu7bs5/assets
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-9-c1e1de4224af>](https://localhost:8080/#) in <module>()
      6   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
      7 ]
----> 8 tflite_model = converter.convert()
      9 
     10 # Save the model.

16 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/importer.py](https://localhost:8080/#) in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    495   # _ProcessNewOps.
    496   with graph._mutation_lock():  # pylint: disable=protected-access
--> 497     with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:
    498       try:
    499         with graph._c_graph.get() as c_graph:  # pylint: disable=protected-access

ValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 3440961451 ""
```

but per documentation this is supposed to create a 4x times smaller file?? 
ie ~850MB?

`https://www.tensorflow.org/lite/performance/post_training_quantization#optimization_methods`
"
59258,How to get label of detection after (@tensorflow/tfjs-tflite) predict returns result from a custom tflite model in Reactjs web App,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tfjs@^3.7.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

Windows laptop

### Python version

2.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am using 

     import * as tflite from '@tensorflow/tfjs-tflite';


and have successfully loaded a custom tflite model:

     tfliteModel = await tflite.loadTFLiteModel(""model/myModel.tflite"");

as well as gotten a prediction
     var outputTensor = await tfliteModel.predict(Img);

The code works well when I only have 1 label to detect initially. Now I need to detect multiple labels. I created a new custom model and tested that it is able to detect multiple labels in an Android project, eg. I can get labels 'L1', and 'L2'

Can I detect multi labels using the code above after I load in the new model? If not, what changes do I have to make to detect multi labels and know which label I am detecting (eg. obtain the name of the label)
```


### Standalone code to reproduce the issue

```shell
See above. Not a bug issue.
```


### Relevant log output

```shell
Instance of result, outputTensor's value

outputTensor:
{
  ""StatefulPartitionedCall:1"": {
    kept: false,
    isDisposedInternal: false,
    shape: [
      1,
      25,
    ],
    dtype: ""float32"",
    size: 25,
    strides: [
      25,
    ],
    dataId: {
      id: 364,
    },
    id: 364,
    rankType: ""2"",
  },
  ""StatefulPartitionedCall:3"": {
    kept: false,
    isDisposedInternal: false,
    shape: [
      1,
      25,
      4,
    ],
    dtype: ""float32"",
    size: 100,
    strides: [
      100,
      4,
    ],
    dataId: {
      id: 365,
    },
    id: 365,
    rankType: ""3"",
  },
  ""StatefulPartitionedCall:0"": {
    kept: false,
    isDisposedInternal: false,
    shape: [
      1,
    ],
    dtype: ""float32"",
    size: 1,
    strides: [
    ],
    dataId: {
      id: 366,
    },
    id: 366,
    rankType: ""1"",
  },
  ""StatefulPartitionedCall:2"": {
    kept: false,
    isDisposedInternal: false,
    shape: [
      1,
      25,
    ],
    dtype: ""float32"",
    size: 25,
    strides: [
      25,
    ],
    dataId: {
      id: 367,
    },
    id: 367,
    rankType: ""2"",
  },
}
```
</details>"
59257,Tensorflow eager execution via C++ API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I was wondering if there is a way to run Tensorflow in eager mode when the Ops are created using the C++ API. For example consider the following code


#include <vector>
#include <iostream>

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/framework/scope.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>
#include ""tensorflow/cc/framework/gradients.h""

using namespace tensorflow;

int main() {
    Scope root = Scope::NewRootScope();
    
    ClientSession session(root);
    
    auto c1 = ops::Const(root, { {1.0, 2.0} });
    auto m = ops::MatMul(root, c1, { {3.0}, {4.0} });
    auto m2 = ops::Exp(root, m);
    auto m3 = ops::ReduceSum(root, m2, -1);
    
    std::vector<Tensor> outputs;
    TF_CHECK_OK(session.Run({m3}, &outputs));
    
    std::cout << ""Value: "" << outputs[0].DebugString() << std::endl;
    
    return 0;
}
```

In the above example, the tensors `m`, `m2` and `m3` are not computed until the graph is run using the `session.Run(...)` command which makes debugging very hard. Is there a way to enable execution similar to the eager execution in Python so that debugging is easier?
```


### Standalone code to reproduce the issue

```shell
#include <vector>
#include <iostream>

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/framework/scope.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>
#include ""tensorflow/cc/framework/gradients.h""

using namespace tensorflow;

int main() {
    Scope root = Scope::NewRootScope();
    
    ClientSession session(root);
    
    auto c1 = ops::Const(root, { {1.0, 2.0} });
    auto m = ops::MatMul(root, c1, { {3.0}, {4.0} });
    auto m2 = ops::Exp(root, m);
    auto m3 = ops::ReduceSum(root, m2, -1);
    
    std::vector<Tensor> outputs;
    TF_CHECK_OK(session.Run({m3}, &outputs));
    
    std::cout << ""Value: "" << outputs[0].DebugString() << std::endl;
    
    return 0;
}
```


### Relevant log output

_No response_</details>"
59255,CUDA version,"Hi, Which is the CUDA version that is recommended for TF 2.11?

Is CUDA 11.7 compatible?"
59254,custom model with TF2 (2.11.0) for M.2 Edge TPU,"Hello.

I am new to this matter.
Through a lot of reading and various guides, I managed to get TensorFlow 2.11.0 running on Ubuntu 20.04 WSL.
I was also able to create a model and use the tensoboard to see the progress.

But the TF2 models don't seem to work with the Coral TPU.

Or I am doing something fundamentally wrong.

Maybe someone has an idea what could be wrong.

My setup....

from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md , I got SSD MobileNet V2 FPNLite 320x320.

from https://github.com/tensorflow/models a git-clone

with generate_tfrecord.py I created train.record and test.record

with
model_main_tf2.py --model_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline.config --num_train_steps=2000

created the model.

with
model_main_tf2.py --model_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite

started an eval.

with
exporter_main_v2.py --input_type=image_tensor --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --trained_checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --output_directory=/home/dom/tensorflow/workspace/training_demo/exported-models/

and

export_tflite_graph_tf2.py --pipeline_config_path=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite/pipeline. config --trained_checkpoint_dir=/home/dom/tensorflow/workspace/training_demo/models/ssd_mobilenet_v2_fpnlite --output_directory=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/

one freeze and one model export each as TFLite

finally per
tflite_convert --saved_model_dir=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/saved_model --output_file=/home/dom/tensorflow/workspace/training_demo/exported-models/tfliteexport/saved_model/detect. tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess: 2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --allow_custom_ops --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_nudging_weights_to_use_fast_gemm_kernel=true

as last then per
edgetpu_compiler -s detect.tflite to create the coral file.

but with this probelem.....

input model: detect.tflite
input size: 10.97MiB
output model: detect_edgetpu.tflite
Output size: 10.97MiB
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming non-cached model parameters: 0.00B
Number of edge TPU subgraphs: 0
Total number of operations: 157
Operation log: detect_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 157
See the operation log file for individual operation details.

pipeline.config

model {
ssd {
num_classes: 5
image_resizer {
fixed_shape_resizer {
height: 320
width: 320
}
}
feature_extractor {
type: ""ssd_mobilenet_v2_fpn_keras""
depth_multiplier: 1.0
min_depth: 16
conv_hyperparams {
regularizer {
l2_regularizer {
weight: 3.9999998989515007e-05
}
}
initializer {
random_normal_initializer {
mean: 0.0
stddev: 0.009999999776482582
}
}
activation: RELU_6
batch_norm {
decay: 0.996999979019165
scale: true
epsilon: 0.0010000000474974513
}
}
use_depthwise: true
override_base_feature_extractor_hyperparams: true
fpn {
min_level: 3
max_level: 7
additional_layer_depth: 128
}
}
box_coder {
faster_rcnn_box_coder {
y_scale: 10.0
x_scale: 10.0
height_scale: 5.0
width_scale: 5.0
}
}
matcher {
argmax_matcher {
matched_threshold: 0.5
unmatched_threshold: 0.5
ignore_thresholds: false
negatives_lower_than_unmatched: true
force_match_for_each_row: true
use_matmul_gather: true
}
}
similarity_calculator {
iou_similarity {
}
}
box_predictor {
weight_shared_convolutional_box_predictor {
conv_hyperparams {
regularizer {
l2_regularizer {
weight: 3.9999998989515007e-05
}
}
initializer {
random_normal_initializer {
mean: 0.0
stddev: 0.009999999776482582
}
}
activation: RELU_6
batch_norm {
decay: 0.996999979019165
scale: true
epsilon: 0.0010000000474974513
}
}
depth: 128
num_layers_before_predictor: 4
kernel_size: 3
class_prediction_bias_init: -4.599999904632568
share_prediction_tower: true
use_depthwise: true
}
}
anchor_generator {
multiscale_anchor_generator {
min_level: 3
max_level: 7
anchor_scale: 4.0
aspect_ratios: 1.0
aspect_ratios: 2.0
aspect_ratios: 0.5
scales_per_octave: 2
}
}
post_processing {
batch_non_max_suppression {
score_threshold: 9.99999993922529e-09
iou_threshold: 0.6000000238418579
max_detections_per_class: 100
max_total_detections: 100
use_static_shapes: false
}
score_converter: SIGMOID
}
normalize_loss_by_num_matches: true
loss {
localization_loss {
weighted_smooth_l1 {
}
}
classification_loss {
weighted_sigmoid_focal {
gamma: 2.0
alpha: 0.25
}
}
classification_weight: 1.0
localization_weight: 1.0
}
encode_background_as_zeros: true
normalize_loc_loss_by_codesize: true
inplace_batchnorm_update: true
freeze_batchnorm: false
}
}
train_config {
batch_size: 6
data_augmentation_options {
random_horizontal_flip {
}
}
data_augmentation_options {
random_crop_image {
min_object_covered: 0.0
min_aspect_ratio: 0.75
max_aspect_ratio: 3.0
min_area: 0.75
max_area: 1.0
overlap_thresh: 0.0
}
}
sync_replicas: true
optimizer {
momentum_optimizer {
learning_rate {
cosine_decay_learning_rate {
learning_rate_base: 0.07999999821186066
total_steps: 50000
warmup_learning_rate: 0.026666000485420227
warmup_steps: 1000
}
}
momentum_optimizer_value: 0.8999999761581421
}
use_moving_average: false
}
fine_tune_checkpoint: ""/home/dom/tensorflow/workspace/training_demo/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0""
num_steps: 50000
startup_delay_steps: 0.0
replicas_to_aggregate: 8
max_number_of_boxes: 100
unpad_groundtruth_tensors: false
fine_tune_checkpoint_type: ""detection""
fine_tune_checkpoint_version: V2
}
train_input_reader {
label_map_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/label_map.pbtxt""
tf_record_input_reader {
input_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/train.record""
}
}
eval_config {
metrics_set: ""coco_detection_metrics""
use_moving_averages: false
}
eval_input_reader {
label_map_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/label_map.pbtxt""
shuffle: false
num_epochs: 1
tf_record_input_reader {
input_path: ""/home/dom/tensorflow/workspace/training_demo/annotations/test.record""
}
}

graph_rewriter {
quantization {
delay: 48000
weight_bits: 8
activation_bits: 8
}
}

pip list

Package Version

absl-py 1.4.0
anyio 3.6.2
apache-beam 2.43.0
argon2-cffi 21.3.0
argon2-cffi-bindings 21.2.0
arrow 1.2.3
astor 0.8.1
asttokens 2.2.1
astunparse 1.6.3
attrs 22.2.0
avro-python3 1.10.2
backcall 0.2.0
beautifulsoup4 4.11.1
bleach 5.0.1
cachetools 5.2.1
certifi 2022.12.7
cffi 1.15.1
chardet 5.1.0
charset-normalizer 2.1.1
click 8.1.3
cloudpickle 2.2.0
cmake 3.25.0
colorama 0.3.3
comm 0.1.2
contextlib2 21.6.0
contourpy 1.0.6
crcmod 1.7
cycler 0.11.0
cython 0.29.33
debugpy 1.6.5
decorator 5.1.1
defusedxml 0.7.1
dill 0.3.1.1
dm-tree 0.1.8
docopt 0.6.2
entrypoints 0.4
etils 1.0.0
executing 1.2.0
fastavro 1.7.0
fasteners 0.18
fastjsonschema 2.16.2
flatbuffers 23.1.4
fonttools 4.38.0
fqdn 1.5.1
guest 0.4.0
gin-config 0.5.0
git-clone 1.0.6
google-api-core 2.11.0
google-api-python client 2.72.0
google-auth 2.16.0
google-auth-httplib2 0.1.0
google-auth-oauthlib 0.4.6
google-pasta 0.2.0
googleapis-common-protos 1.58.0
grpcio 1.51.1
h5py 3.7.0
hdfs 2.7.0
html5lib 1.1
httplib2 0.20.4
idna 3.4
immutabledict 2.2.3
importlib-metadata 6.0.0
importlib-Ressourcen 5.10.2
ipykernel 6.20.1
ipython 8.8.0
ipython-genutils 0.2.0
ipywidgets 8.0.4
isoduration 20.11.0
jedi 0.18.2
Jinja2 3.1.2
joblib 1.2.0
jsonpointer 2.3
jsonschema 4.17.3
jupyter 1.0.0
jupyter-client 7.4.8
jupyter-Konsole 6.4.4
jupyter-core 5.1.3
jupyter-events 0.6.2
jupyter-server 2.0.6
jupyter-server-terminals 0.4.4
jupyterlab-pygments 0.2.2
jupyterlab-widgets 3.0.5
kaggle 1.5.12
keras 2.11.0
Keras-Applikationen 1.0.8
Keras-Vorverarbeitung 1.1.2
kiwisolver 1.4.4
libclang 15.0.6.1
lvis 0.5.3
lxml 4.9.2
Markdown 3.4.1
MarkupSafe 2.1.1
matplotlib 3.6.3
matplotlib-inline 0.1.6
mistune 2.0.4
nbclassic 0.4.8
nbclient 0.7.2
nbconvert 7.2.7
nbformat 5.7.3
nest-asyncio 1.5.6
notebook 6.5.2
notebook-shim 0.2.2
numpy 1.22.4
oauth2client 4.1.3
oauthlib 3.2.2
objekt-erkennung 0.1
objsize 0.5.2
opencv-python 4.7.0.68
opencv-python-headless 4.7.0.68
opt-einsum 3.3.0
orjson 3.8.5
paketierung 23.0
pandas 1.5.2
pandoc-Filter 1.5.0
parso 0.8.3
pexpect 4.8.0
pickleshare 0.7.5
pillow 9.4.0
pip 20.2.4
platformdirs 2.6.2
portalocker 2.6.0
prometheus-Klient 0.15.0
promise 2.3
prompt-toolkit 3.0.36
proto-plus 1.22.2
protobuf 3.19.6
psutil 5.9.4
ptyprocess 0.7.0
pure-eval 0.2.2
py-cpuinfo 9.0.0
pyarrow 9.0.0
pyasn1 0.4.8
pyasn1-Baustein 0.2.8
pycocotools 2.0.6
pycparser 2.21
pydot 1.4.2
Pygments 2.14.0
pymongo 3.13.0
pyparsing 2.4.7
pyrsistent 0.19.3
python-dateutil 2.8.2
python-json-logger 2.0.4
python-slugify 7.0.0
pytz 2022.7
PyYAML 5.4.1
pyzmq 25.0.0
qtconsole 5.4.0
QtPy 2.3.0
regex 2022.10.31
requests 2.28.1
requests-oauthlib 1.3.1
rfc3339-validator 0.1.4
rfc3986-validator 0.1.1
rsa 4.9
sacrebleu 2.2.0
scikit-learn 1.2.0
scipy 1.10.0
Send2Trash 1.8.0
sentencepiece 0.1.97
seqeval 1.2.2
setuptools 65.6.3
six 1.16.0
sniffio 1.3.0
soupsieve 2.3.2.post1
stack-data 0.6.2
tabulieren 0.9.0
tensorboard 2.11.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit 1.8.1
tensorflow 2.11.0
tensorflow-addons 0.19.0
tensorflow-datensätze 4.8.1
tensorflow-schätzer 2.11.0
tensorflow-hub 0.12.0
tensorflow-io 0.29.0
tensorflow-io-gcs-filesystem 0.29.0
tensorflow-metadaten 1.12.0
tensorflow-model-optimization 0.7.3
tensorflow-text 2.11.0
termcolor 1.1.0
terminado 0.17.1
text-unidecode 1.3
tf-models-official 2.11.2
tf-slim 1.1.0
threadpoolctl 3.1.0
tinycss2 1.2.1
toml 0.10.2
tornado 6.2
tqdm 4.31.1
traitlets 5.8.1
typguard 2.13.3
typing-erweiterungen 4.4.0
unzip 1.0.0
uri-vorlage 1.2.0
uritemplate 4.1.1
urllib3 1.26.14
wcwidth 0.2.5
webcolors 1.12
webencodings 0.5.1
websocket-client 1.4.2
Werkzeug 2.2.2
rad 0.37.1
widgetsnbextension 4.0.5
wrapt 1.14.1
zipp 3.11.0
zstandard 0.19.0"
59252,Test failure due to inconsistencies related to MKL ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux RHEL 7

### Python version

3.10

### Bazel version

5.1.1

### GCC/Compiler version

11.3

### Current Behaviour?

```shell
I see failing tests while running `bazel test`.

The `testGetMemoryInfoCPU` is guarded by a skip `if test_util.IsMklEnabled():`
And the other tests has a similar condition.

So it looks like `IsMklEnabled` doesn't return true when it should. Further investigation leads to several macros in the build files:

- `IsMklEnabled` returns true when `defined(INTEL_MKL) && defined(ENABLE_MKL)`
- There is `if_mkl(["":mkl_cpu_allocator""])` which is why `testGetMemoryInfoCPU` fails when true
- `if_mkl` is true on `linux_x86_64` unconditionally
- `if_mkl([""-DINTEL_MKL""])`
- `if_enable_mkl([""-DENABLE_MKL""])`

I'd suggest to simplify this so either there is MKL or there is not. Having multiple macros with different defaults doesn't make sense to me. Or maybe the default for ""ENABLE_MKL"" should be the same as for ""INTEL_MKL"", e.g. True on x86
```


### Standalone code to reproduce the issue

```shell
bazel test
```


### Relevant log output

```shell
FAIL: testGetMemoryInfoCPU (__main__.ContextTest)
ContextTest.testGetMemoryInfoCPU
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/dev/shm/jfg508/TensorFlow/2.9.1/foss-2022a/TensorFlow/bazel-root/bac5198b45911f6921886c0013c301e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/context_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2229, in decorated
    return func(self, *args, **kwargs)
  File ""/dev/shm/jfg508/TensorFlow/2.9.1/foss-2022a/TensorFlow/bazel-root/bac5198b45911f6921886c0013c301e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/context_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context_test.py"", line 141, in testGetMemoryInfoCPU
    with self.assertRaisesRegex(ValueError, 'Allocator stats not available'):
AssertionError: ValueError not raised

FAIL: test_simple (__main__.NodeFileWriterTest)
NodeFileWriterTest.test_simple
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/dev/shm/jfg508/TensorFlow/2.9.1/foss-2022a/TensorFlow/bazel-root/bac5198b45911f6921886c0013c301e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/framework/node_file_writer_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2157, in decorated
    return func(self, *args, **kwargs)
  File ""/dev/shm/jfg508/TensorFlow/2.9.1/foss-2022a/TensorFlow/bazel-root/bac5198b45911f6921886c0013c301e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/framework/node_file_writer_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/node_file_writer_test.py"", line 142, in test_simple
    self.assertEqual(node_def1.op, 'MatMul')
AssertionError: 
- _MklMatMul
? ----
+ MatMul
```
</details>"
59251,Update curl to 7.87.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Security Vulnerabilities fixed in curl 7.87.0
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
59250,accessing layers in a list through index results in near zero gradients,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

RTX 3080TI 

### Current Behaviour?

```shell
if i put layers in a list and then access them through index, the gradients are nearly zero (say 1e-7) and the model isn't learning. if i use the layers directly the gradients look normal.
```


### Standalone code to reproduce the issue

```shell
#======================================
# accesssing through list like below will result in nearly zero gradient
class Mymodel(keras.models.Model):
   def __init__(self):
       self.layers = [layers.LSTM(hid_size) for i in range(3)]
   def call(self,x):
       for i in range(3):
          x = self.layers[i](x)
       return x
```


### Relevant log output

_No response_</details>"
59245,Problem with loading example dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.9.2

### Custom Code

No

### OS Platform and Distribution

Google collab 

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Good afternoon!
I learm the colab notebook, example «Transfer Learning for the Audio Domain with TensorFlow Lite Model Maker» (https://www.tensorflow.org/lite/models/modify/model_maker/audio_classification)
In this example, The Birds dataset cannot be loaded. This link (https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset .zip') is invalid.
How can I get this dataset for this example?
```


### Standalone code to reproduce the issue

```shell
Example text from: https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#scrollTo=upNRfilkNSmr

birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',
                                                'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',
                                                cache_dir='./',
                                                cache_subdir='dataset',
                                                extract=True)
                                                

This link (https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset .zip') is invalid.
```


### Relevant log output

```shell
Downloading data from https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)
    276       try:
--> 277         urlretrieve(origin, fpath, dl_progress)
    278       except urllib.error.HTTPError as e:

8 frames
HTTPError: HTTP Error 404: Not Found

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)
    277         urlretrieve(origin, fpath, dl_progress)
    278       except urllib.error.HTTPError as e:
--> 279         raise Exception(error_msg.format(origin, e.code, e.msg))
    280       except urllib.error.URLError as e:
    281         raise Exception(error_msg.format(origin, e.errno, e.reason))

Exception: URL fetch failure on https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip: 404 -- Not Found
```
</details>"
59242,Segfault in python/eager/context.py:remove_function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

 CUDA Version: 11.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Fatal Python error: Segmentation fault

  File ""/home/clime/.virtualenvs/keras/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 1363 in remove_function
  File ""/home/clime/.virtualenvs/keras/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 2709 in remove_function
  File ""/home/clime/.virtualenvs/keras/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 184 in __del__
```

Happens during a garbage collection call in a child process. I have used python `multiprocessing.Process` class to spawn the new process. The segfault happens specifically on this line:

```
pywrap_tfe.TFE_ContextRemoveFunction(self._handle, name)
```

The function that was attempted to be removed was:

```
b'__inference_predict_function_76451'
```
The segfault in dmesg looks like this:

```
[Jan12 15:33] python3[230830]: segfault at 7ff0f2e24910 ip 00007ff2e071f59b sp 00007fff57b38ff0 error 4 in libc.so.6[7ff2e06b1000+195000]
[  +0,000012] Code: 0f 1e fa 41 57 41 56 41 55 41 54 55 53 48 83 ec 48 64 48 8b 04 25 28 00 00 00 48 89 44 24 38 31 c0 48 85 ff 0f 84 55 01 00 00 <8b> 87 d0 02 00 00 48 89 fb 85 c0 0f 88 44 01 00 00 48 39 bf 28 06
```

I didn't try to reproduce with tf-nightly as I have hit some other problem after installing it.
```


### Standalone code to reproduce the issue

```shell
I don't have a reproducer available, sorry, but my guess is that it's some weird interaction of python garbage collector with tensorflow objects in a python child process that inherited tensorflow objects from the main/parent process.
```


### Relevant log output

_No response_</details>"
59241,Is there any way to disable remapping optimizer in Tensorflow1.15.0?,"## System information
-   **Centos7**
-   **Python 2.7**
-   **tensorflow1.15.0**
-   **CUDA10.0**
-   **CUDNN7**
-   **GPU: P100**


## Describe the problem
I used `tensorflow.python.client.timeline` tool in tf1.x to profile computation graph execution procedure. Then I find remapping optimizer fuse MatMul, BiasAdd and Elu into _FusedMatMul before graph execution. I also used `tf.config.optimizer.set_experimental_options()` to set remapping false but it didn't work. I don't known if I used it correctly. 

## Source code / logs
Here are my codes:
### Function to set Options
```bash
@contextlib.contextmanager
def graph_optimizer_options(options):
    old_opts = tf.config.optimizer.get_experimental_options()
    tf.config.optimizer.set_experimental_options(options)
    try:
        yield
    finally:
        tf.config.optimizer.set_experimental_options(old_opts)
```
### Options are here:
```bash
    options = {'constant_folding': False,
               'layout_optimizer': False,
               'shape_optimization': False,
               'remapping': False,
               'arithmetic_optimization': False,
               'dependency_optimization': False,
               'loop_optimization': False,
               'function_optimization': False,
               'debug_stripper': False,
               'disable_model_pruning': False,
               'scoped_allocator_optimization': False,
               'pin_to_host_optimization': False,
               'implementation_selector': False,
               'auto_mixed_precision': False,
               'disable_meta_optimizer': False,
               'min_graph_nodes': False}
```
### execute Computational Graph with tf.Session().
```bash
    with tf.Session(graph=graph, config=config) as sess:
        run_options = tf.compat.v1.RunOptions(trace_level = tf.compat.v1.RunOptions.FULL_TRACE)
        run_metadata = tf.compat.v1.RunMetadata()
        tf.graph_util.import_graph_def(graph_def, name="""")
        
        @tf.function
        def _run():
            sess.run(outputs_name, feed_dict = feed_dict, options = run_options, run_metadata = run_metadata)

        start_time = time.time()
        with graph_optimizer_options(options):
            print(tf.config.optimizer.get_experimental_options())
            _run()
        end_time = time.time()
        tl = timeline.Timeline(run_metadata.step_stats)
        # TODO better record meassurement
        ctf = tl.generate_chrome_trace_format()
        with open(tl_saved_path, 'w') as f:
            f.write(ctf)
    
    print(""finish running on {}, run time: {}"".format(device, end_time - start_time))
```
the result of ""tf.config.optimizer.get_experimental_options()""
```bash
{'debug_stripper': False, 'function_optimization': False, 'disable_model_pruning': False, 'constant_folding': False, 'implementation_selector': False, 'pin_to_host_optimization': False, 'auto_mixed_precision': False, 'disable_meta_optimizer': False, 'loop_optimization': False, 'scoped_allocator_optimization': False, 'dependency_optimization': False, 'shape_optimization': False, 'remapping': False, 'layout_optimizer': False, 'arithmetic_optimization': False}
```
Remapping seem to be False but _FusedMatMul still exist. Is there any way to disable remapping optimizer in Tensorflow1.15.0?
"
59240,Support Visual Studio 2022 compiler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows 11 x64

### Mobile device

N/A

### Python version

3.11.0

### Bazel version

6.0

### GCC/Compiler version

Visual Studio 2022

### CUDA/cuDNN version

11.8/8.7.0

### GPU model and memory

RTX3090

### Current Behaviour?

```shell
only visual studio 2019 compiler supported
```


### Standalone code to reproduce the issue

```shell
build
```


### Relevant log output

_No response_</details>"
59458,How to train with mobilenet models using model_spec.get() ??,"Hello. I am using this tutorial to train my dataset for the object detection and efficientdet_lite (0-4) models are working fine.
https://www.tensorflow.org/lite/models/modify/model_maker/object_detection

I found a documentation for **model_spec.get** and it seems you can train only with **efficientdet_lite** models.
https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/model_spec

Is there any way to train the model with **ssd-mobilenet-v2** or **ssd-mobilenet-v1** models and what I should write inside of **model_spec.get ('........')** ?? Look forward to hearing from you!"
59239,Error while evaluating tflite model with bazel run_eval ,"Hi, i managed to train a SSD_mobilenet_v1 using model_main_tf2.py. After that i managed to export with export_tflite_graph_tf2.py changing the max_detections paramenter to 33, and then i successfully exported a tflite model. I tried then to run some inference with the new tflite model and it worked as expected.
I decided then to try to evaluate  the tflite model using this [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection),  but when i run this command: 
`bazel run -c opt -- //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval --model_file=/home/gab/PycharmProjects/tensorflow_prova2/new_model_quant_F16Q.tflite --ground_truth_images_path=/home/gab/PycharmProjects/tensorflow_prova2/images  --model_output_labels=/home/gab/PycharmProjects/tensorflow_prova2/label_map.txt --output_file_path=/home/gab/PycharmProjects/tensorflow_prova2/coco_output.txt --debug_mode=true`
output shows odd results like this:
Object [0]
  Score: 33
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.999825
    Normalized Bottom: 0.999262
    Normalized Left: 0.999445
    Normalized Right: 0.999066
Object [1]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.998896
    Normalized Bottom: 0.997808
    Normalized Left: 0.998822
    Normalized Right: 0.996969
Object [2]
  Score: 0.780337
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.996294
    Normalized Bottom: 0.988497
    Normalized Left: 0.994716
    Normalized Right: 0.985629
Object [3]
  Score: 0.479028
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.972919
    Normalized Bottom: 0.0584207
    Normalized Left: 0.0640493
    Normalized Right: 0.0434026
Object [4]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.0374295
    Normalized Bottom: 0.0290796
    Normalized Left: 0.0313877
    Normalized Right: 0.0227272
Object [5]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.0221782
    Normalized Bottom: 0.0208216
    Normalized Left: 0.0213712
    Normalized Right: 0.0207996
Object [6]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.0199532
    Normalized Bottom: 0.0192215
    Normalized Left: 0.0193954
    Normalized Right: 0.0187497
Object [7]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.0186509
    Normalized Bottom: 0.0180727
    Normalized Left: 0.0181409
    Normalized Right: 0.0176707
Object [8]
  Score: 0
  Class-ID: 1
  Bounding Box:
    Normalized Top: 0.0176005
    Normalized Bottom: 2.02493
    Normalized Left: 0.0637459
    Normalized Right: 0.325256

This is for the first image of the folder i used, one thing i noted is that the first score of the first object is always 33 that is the parameter i changed when i exported the trained model with export_tflite_graph_tf2.py

Is there a way to fix this so i can evaluate my tflite model?
Thanks
"
59235,tensorboardX ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
59234,serialized_pb,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
59232,I cannot run a transformer model with token-level output on accelerated hardware in TF Lite,"# Description of the issue
Okay so to preface this, I have been working on this for over a month. My goal can be explained in one sentence: I want to run my small bert-like model with token-level output on the GPU of my Android phone using the Interpreter API and the GPU delegate. The Task API does unfortunately not offer that.

Please don't hesitate to reach out to me if you are missing any information to understand or reproduce this issue. I really want to get this working!

# System Information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.10.1

# The model and the TF Lite conversion
I want to use a simple BERT-based model to classify something on token-level. The following code should get you an idea of the model:

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig


 encoder_config = {
  ""attention_probs_dropout_prob"": 0.1,
  ""model_type"": ""bert"",
  ""emb_size"": 512,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 1024,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""max_position_embeddings"": 128,
  ""num_attention_heads"": 8,
  ""num_hidden_layers"": 4,
  ""type_vocab_size"": 2,
  ""vocab_size"": 25000
}
configuration = BertConfig.from_dict(encoder_config)  # default parameters and configuration for BERT

def create_model(max_len = 128):
    ## BERT encoder
    encoder = TFBertModel(configuration)

    ## Classification Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int64)
    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int64)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int64)

    embedding = encoder(
        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask
    )[0]

    ## Token-level classification
    relu_layer = layers.Dense(80, activation=""relu"", name=""relu_1"")(embedding)
    classification_head = layers.Dense(len(config.get(""labels"")))(relu_layer)

    model = keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[classification_head],
    )

    return model

model = create_model()
```
After I created the model, I converted it to the TF Lite format with the following code:
```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS
]
tflite_model = converter.convert()

# Save the model.
with open('../dev/model.tflite', 'wb') as f:
  f.write(tflite_model)
```

The conversion output the following text:
```
WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 216). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: C:\Users\USERNAME\AppData\Local\Temp\tmpb4tsljq3\assets
INFO:tensorflow:Assets written to: C:\Users\USERNAME\AppData\Local\Temp\tmpb4tsljq3\assets
```
But this did not bother me too much as these don't seem to be errors.

# Running the model on Android with hardware acceleration
To not introduce any errors from my side, I decided to use the [pre-built benchmarking app](https://www.tensorflow.org/lite/performance/measurement#android_benchmark_app) that TensorFlow provides. All the following adb commands are taken from that page.

First, I installed the app and pushed my model to the phone, which is in my case, a Pixel 3a.
```
adb install -r -d -g android_aarch64_benchmark_model.apk
adb push model.tflite /data/local/tmp
```

## Running the model on CPU
Then, I ran the model on the CPU to see if everything works:
```
adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '""--graph=/data/local/tmp/model.tflite --num_threads=1""'
```
and get the following output, where everything worked as expected:
```
01-11 23:28:57.758  7651  7651 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/model.tflite
01-11 23:28:59.026  7651  7651 I tflite  : Log parameter values verbosely: [0]
01-11 23:28:59.026  7651  7651 I tflite  : Graph: [/data/local/tmp/model.tflite]
01-11 23:28:59.028  7651  7651 I tflite  : Loaded model /data/local/tmp/model.tflite
01-11 23:28:59.028  7651  7651 I tflite  : Initialized TensorFlow Lite runtime.
01-11 23:28:59.034  7651  7651 I tflite  : Created TensorFlow Lite XNNPACK delegate for CPU.
01-11 23:28:59.036  7651  7651 I tflite  : The input model file size (MB): 137.514
01-11 23:28:59.036  7651  7651 I tflite  : Initialized session in 9.34ms.
01-11 23:28:59.036  7651  7651 I tflite  : Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
01-11 23:29:03.662  7651  7651 I tflite  : count=1 curr=1381510
01-11 23:29:03.662  7651  7651 I tflite  : Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
01-11 23:29:24.958  7651  7651 I tflite  : count=50 first=505840 curr=424069 min=421685 max=505840 avg=425911 std=11770
01-11 23:29:24.958  7651  7651 I tflite  : Inference timings in us: Init: 9340, First inference: 1381510, Warmup (avg): 1.38151e+06, Inference (avg): 425911
01-11 23:29:24.958  7651  7651 I tflite  : Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
01-11 23:29:24.958  7651  7651 I tflite  : Memory footprint delta from the start of the tool (MB): init=1.84766 overall=117.375
```

## Running the model on GPU
After the successful run on CPU, I want to make use of hardware acceleration to see whether this would speedup my inference and therefore I ran this command:
```
adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '""--graph=/data/local/tmp/model.tflite --use_gpu=true""'
```
which output this:
```
01-11 23:31:59.519  8399  8399 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/model.tflite --use_gpu=true
01-11 23:31:59.706  8399  8399 I tflite  : Log parameter values verbosely: [0]
01-11 23:31:59.706  8399  8399 I tflite  : Graph: [/data/local/tmp/model.tflite]
01-11 23:31:59.707  8399  8399 I tflite  : Use gpu: [1]
01-11 23:31:59.708  8399  8399 I tflite  : Loaded model /data/local/tmp/model.tflite
01-11 23:31:59.709  8399  8399 I tflite  : Initialized TensorFlow Lite runtime.
01-11 23:31:59.713  8399  8399 I tflite  : Created TensorFlow Lite delegate for GPU.
01-11 23:31:59.713  8399  8399 I tflite  : GPU delegate created.
01-11 23:31:59.715  8399  8399 E tflite  : Failed to apply GPU delegate.
```

There is no more information extractable from the benchmarking app, which is why I wrote an app where I can print the stacktrace of the error. And **when I try to create an Interpreter Object with the Interpreter API with a GPU delegate as options**, I get the following error:
```
I/tflite: Created TensorFlow Lite delegate for GPU.
W/System.err: java.lang.IllegalArgumentException: Internal error: Error applying delegate:
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:106)
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
W/System.err:     at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:197)
W/System.err:     at com.example.mobileinferencedemo.inference.PunctuationInference.<init>(PunctuationInference.kt:115)
W/System.err:     at com.example.mobileinferencedemo.MainActivity.onCreate(MainActivity.kt:28)
W/System.err:     at android.app.Activity.performCreate(Activity.java:8054)
W/System.err:     at android.app.Activity.performCreate(Activity.java:8034)
W/System.err:     at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1341)
W/System.err:     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3688)
W/System.err:     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3864)
W/System.err:     at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:103)
W/System.err:     at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
W/System.err:     at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
W/System.err:     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2253)
W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:106)
W/System.err:     at android.os.Looper.loopOnce(Looper.java:201)
W/System.err:     at android.os.Looper.loop(Looper.java:288)
W/System.err:     at android.app.ActivityThread.main(ActivityThread.java:7870)
W/System.err:     at java.lang.reflect.Method.invoke(Native Method)
W/System.err:     at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548)
W/System.err:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1003)
```

It is important to note where this application fails. It fails when I try to create an Interpreter Object with the Interpreter API with a GPU delegate as options. I did not run any inference, I just wanted to create the Interpreter. If you're curious, here is the code for that (which I took from [the docs](https://www.tensorflow.org/lite/android/delegates/gpu#use_gpu_with_interpreter_api)):
```kotlin
val compatList = CompatibilityList()

val options = Interpreter.Options().apply{
    if(compatList.isDelegateSupportedOnThisDevice){
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        this.numThreads = 4
    }
}

try {
    val myInterpreter: Interpreter
    if(options != null){
        myInterpreter = Interpreter(File(path.toString()), options)
    } else {
        myInterpreter = Interpreter(File(path.toString()))
    }

    myInterpreter.close()
} catch (e: Exception) {
    Log.e(""tf_load_model"",""""+e.message)
    e.printStackTrace()
}
```

This try block already fails when trying to create the Interpreter (which I manually confirmed in the Android Studio debugger).


# Steps to reproduce this issue
1. Create the model with the code provided at the start of this issue
2. Convert it to TF lite with the code above
3. Try to run the converted TF lite model on the phone using the pre-built TensorFlow lite benchmarking app and use the `--use_gpu=true` flag
4. Observe the error ""Failed to apply GPU delegate""

# Additional information
I really am curious if this is an error from my side or just generally a bug. The steps for reproducing the error should be rather quick, so I hope someone can look into this. There are just almost no resources on this, which makes it really hard to know if someone else succeeded running a token-level transformer model on the GPU using the Interpreter API. If this matter resolves, I would be happy to contribute a resource or guide to this.

## The question of operator support
There is a [list of supported ML operations for GPU](https://www.tensorflow.org/lite/performance/gpu#supported_ops) inference with the GPU delegate.

When printing the operators of my converted TF lite model in python
```python
interpreter = tf.lite.Interpreter(model_path=""..\dev\model.tflite"")
all_ops = list(map(lambda x: x[""op_name""],interpreter._get_ops_details()))
print(np.unique(all_ops))
```
I get this list: 
```
['ADD' 'BATCH_MATMUL' 'CAST' 'CONCATENATION' 'FULLY_CONNECTED' 'GATHER'
 'GELU' 'MEAN' 'MUL' 'PACK' 'REDUCE_PROD' 'RESHAPE' 'RSQRT' 'SHAPE'
 'SOFTMAX' 'SQUARED_DIFFERENCE' 'STRIDED_SLICE' 'SUB' 'TRANSPOSE']
```

I can see that some operators (e.g. GELU) are not in the list of supported operators, but my problem is that there should be an appropriate warning if the graph contains unsupported operators ([as described in the documentation](https://www.tensorflow.org/lite/performance/gpu#troubleshooting_gpu_support)). Furthermore, I did not even come to invoking the model, the creation of the Interpreter object with the GPU delegate option already crashes my application. If I could at least create an Interpreter and then run the model and it fails with these warnings, well then I know that supported ops could be a problem, but I do not even get to that point."
59231,Movinet Colab 'TypeError',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, I'm reporting an issue I'm getting when simply trying to run the extended moving tutorial in google Colab. I'm not entirely sure of the other information, but without changing anything and just trying to run the tutorial, I cannot import from official.projects.movinet.modeling import movinet. The 'TypeError' reads: 'ABCMeta' object not subscriptable and can't find any solution to fixing it.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb
```


### Relevant log output

_No response_</details>"
59229,"WARNING : [ W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found] Windows 10, TF2.10.1, CUDA 11.2, cuDNN 8.1.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.2; cuDNN = 8.1

### GPU model and memory

NVIDIA GTX 1070 8GB VRAM

### Current Behaviour?

```shell
I did everything like its instructed in https://www.tensorflow.org/install/pip but the warning still come up. I need GPU Support for my project. But when I just import tensorflow inside my python, the warning come up

I am sure 100% in my conda and my environment Path, there is cudart64_110.dll 
I tried to reinstall my Nvidia Drivers, CUDA, and my cuDNN but it still doesnt work.
Is there any solution for this?
```


### Standalone code to reproduce the issue

```shell
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
python3 -m pip install ""tensorflow<2.11""
python3 -c ""import tensorflow as tf""
```


### Relevant log output

```shell
W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
```
</details>"
59225,Is there way to remove BDBA critical Vulnerability ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
No Critical vulnerabilities in BDBA for TensorFlow.
```


### Standalone code to reproduce the issue

```shell
BDBA report Critical Vulnerabilities CVE-2022-46908 (Sqlite3 3.39.4 component) and CVE-2022-43551, CVE-2022-35260, CVE-2022-32221, CVE-2022-42915 and CVE-2022-42916
(curl 7.85.0 component)
https://nvd.nist.gov/vuln/search
```


### Relevant log output

_No response_</details>"
59223,"Swift | iOS - Creating segmenter object throws exception ""EXC_BAD_ACCESS "" in ReplaceNodeSubsetsWithDelegateKernels method in real device only","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TFLite Model Maker 0.3.2

### Custom Code

Yes

### OS Platform and Distribution

Mac

### Mobile device

iPhone 8 Plus

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
let segmenter = try ImageSegmenter.segmenter(options: options)

This line should not crash.
```


### Standalone code to reproduce the issue

```shell
I am calling this method

// Specify the options for the `ImageSegmenter`.
      let options = ImageSegmenterOptions(modelPath: modelPath)
    
      //  options.classificationOptions.scoreThreshold = 5
       // options.classificationOptions.maxResults = 5
        options.baseOptions.computeSettings.cpuSettings.numThreads = Int(Int32(5))

      do {
        let segmenter = try ImageSegmenter.segmenter(options: options)

        // Create an ImageSegmentationHelper instance and return.
        let segmentationHelper = ImageSegmentationHelper(
          tfLiteQueue: tfLiteQueue,
          segmenter: segmenter
        )
        DispatchQueue.main.async {
          completionHandler(.success(segmentationHelper))
        }
      } catch let error {
        print(""Failed to create the interpreter with error: \(error.localizedDescription)"")
        DispatchQueue.main.async {
          completionHandler(.failure(InitializationError.internalError(error)))
        }
        return
      }
```


### Relevant log output

```shell
flite::Subgraph::ReplaceNodeSubsetsWithDelegateKernels(TfLiteRegistration, TfLiteIntArray const*, TfLiteDelegate*)

throwing exception EXC_BAD_ACCESS (code=2, address=0x2817b9f00)
```
</details>"
59221,protobuf 4 is not supported,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Fedora Linux 37 (Xfce)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I try to use tensorflow with protobuf newer than version 3 I get this:


$ poetry add 'tensorflow==2.11.0' 'protobuf>=4'

Updating dependencies
Resolving dependencies... (0.2s)

Because tensorflow-issue depends on tensorflow (2.11.0) which depends on protobuf (>=3.9.2,<3.20), protobuf is required.
So, because tensorflow-issue depends on protobuf (>=4), version solving failed.


The reason for this is:

https://github.com/tensorflow/tensorflow/blame/d0dc98c5c8f5deea2447a3405af20f5fb245a561/tensorflow/tools/pip_package/setup.py#L100-L107

My expectation was that tensorflow supports more recent versions of protobuf.
```


### Standalone code to reproduce the issue

```shell
mkdir -vp /var/tmp/tensorflow-issue
poetry -C /var/tmp/tensorflow-issue init --no-interaction
poetry -C /var/tmp/tensorflow-issue add 'tensorflow==2.11.0' 'protobuf>=4'
```


### Relevant log output

_No response_</details>"
59218,How to design tf.keras callback to save model predictions for each batch and each epoch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to create a tf.keras callback to save model predictions for each batch and each epoch during the training using the training data sets to a numpy array

i have tried the following callback, however it gives error like

AttributeError: 'PredictionCallback' object has no attribute 'X_train'
```


### Standalone code to reproduce the issue

```shell
class PredictionCallback(tf.keras.callbacks.Callback):    

  def on_epoch_end(self, epoch, logs={}):

    y_pred = self.model.predict(self.X_train)

    print('prediction: {} at epoch: {}'.format(y_pred, epoch))

    pd.DataFrame(y_pred).assign(epoch=epoch).to_csv('{}_{}.csv'.format(filename, epoch))

    cnn_model.fit(X_train, y_train,validation_data=[X_valid,y_valid],epochs=epochs,batch_size=batch_size,
               callbacks=[model_checkpoint,reduce_lr,csv_logger, early_stopping,PredictionCallback()],
               verbose=1)
```


### Relevant log output

```shell
AttributeError: 'PredictionCallback' object has no attribute 'X_train'


i also tried tensorflow - Create keras callback to save model predictions and targets for each batch during training - Stack Overflow but not get success yet.Hope experts will help me.Thanks.
```
</details>"
59217,Request: Some way to initialise a dynamically-sized state variable in TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **All**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (or github SHA if from source): **2.9.1**

**Provide the text output from tflite_convert**
```
tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'zeros:0' shape=(None, None) dtype=float32> because it depends transitively on placeholder <tf.Operation 'arr' type=Placeholder> via at least one path, e.g.: zeros (Fill) <- Shape (Shape) <- arr (Placeholder)
```

**Standalone code to reproduce the issue** 

I want to define a stateful graph where the shape of the state variable is defined when model is LOADED, not when it's saved.

As a simple example - lets say I just want to compute a temporal difference - ie. a graph that returns the difference between the input in two consecutive calls. The following should pass:

```
func = load_tflite_model_func(tflite_model_file_path)
runtime_shape = 60, 80
rng = np.random.RandomState(1234)
ims = [rng.randn(*runtime_shape).astype(np.float32) for _ in range(3)]
assert np.allclose(func(ims[0]), ims[0])
assert np.allclose(func(ims[1]), ims[1]-ims[0])
assert np.allclose(func(ims[2]), ims[2]-ims[1])
```

However as far as I know there is not way I can save a model, or define some function `load_tflite_model_func` for loading it, that would make this work for any `runtime_shape`, because variables in the graph can only be saved with a pre-determined size.   

A full stand-alone notebook demonstrating the issue is here:
https://colab.research.google.com/drive/19CwkF1MSGlfMXKxlrndCNqTv7V4fx55Q

**Request is for TFLite to be able to initialise state-variables, where the shape of the initialised variable can depend on the input-shape**

See also https://stackoverflow.com/questions/75052366/how-do-i-to-save-a-stateful-tflite-model-where-shape-of-state-depends-on-input "
59215,tf.debugging.enable_check_numerics() doesn't work with XLA JIT compilation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.7 CUDNN=8.5.0.96-1

### GPU model and memory

Tesla T4

### Current Behaviour?

```shell
Training fails on following error when `tf.debugging.enable_check_numerics()` is enabled together with JIT compilation of model.

I was able to reproduce this with TF 2.9.2, 2.11.0 and current nightly build, on Tesla T4 and GeForce GTX 1080 Ti.


InvalidArgumentError: Graph execution error:

Detected unsupported operations when trying to compile graph __inference_run_step_1284[] on XLA_GPU_JIT: DebugNumericSummaryV2 (No registered 'DebugNumericSummaryV2' OpKernel for XLA_GPU_JIT devices compatible with node {{node DebugNumericSummaryV2}}){{node DebugNumericSummaryV2}}
```
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

tf.debugging.enable_check_numerics()

# https://keras.io/examples/vision/mnist_convnet/

num_classes = 10
input_shape = (28, 28, 1)
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

batch_size = 128
epochs = 15
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""], jit_compile=True)
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
```


### Relevant log output

```shell
Epoch 1/15
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-1059bcc7b5d0> in <module>
     26 
     27 model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""], jit_compile=True)
---> 28 model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

1 frames
/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---> 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected unsupported operations when trying to compile graph __inference_run_step_1284[] on XLA_GPU_JIT: DebugNumericSummaryV2 (No registered 'DebugNumericSummaryV2' OpKernel for XLA_GPU_JIT devices compatible with node {{node DebugNumericSummaryV2}}){{node DebugNumericSummaryV2}}
The op is created at: 
File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
  return _run_code(code, main_globals, None,
File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
  exec(code, run_globals)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
  app.launch_new_instance()
File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
  app.start()
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
  self.io_loop.start()
File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
  self.asyncio_loop.run_forever()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
  self._run_once()
File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
  handle._run()
File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
  self._context.run(self._callback, *self._args)
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
  lambda f: self._run_callback(functools.partial(callback, future))
File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
  ret = callback()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
  self.run()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 381, in dispatch_queue
  yield self.process_one()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 225, in wrapper
  runner = Runner(result, future, yielded)
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 714, in __init__
  self.run()
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
  yielded = self.gen.send(value)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
  yield gen.maybe_future(dispatch(*args))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
  yield gen.maybe_future(handler(stream, idents, msg))
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
  self.do_execute(
File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
  yielded = next(result)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
  res = shell.run_cell(code, store_history=store_history, silent=silent)
File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
  return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
  result = self._run_cell(
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
  return runner(coro)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
  coro.send(None)
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
  if (await self.run_code(code, result,  async_=asy)):
File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
  exec(code_obj, self.user_global_ns, self.user_ns)
File ""<ipython-input-5-1059bcc7b5d0>"", line 28, in <module>
  model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
  return fn(*args, **kwargs)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1409, in fit
  tmp_logs = self.train_function(iterator)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1051, in train_function
  return step_function(self, iterator)
File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1040, in step_function
  outputs = model.distribute_strategy.run(run_step, args=(data,))
	 [[StatefulPartitionedCall]] [Op:__inference_train_function_1343]
```
</details>"
59214,Is there any way to modify the quantized parameters from the generated tflite model?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installation (pip package or built from source): 2.11.0
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_model_quant = converter.convert()

I used the above code to generate a fully quantized tflite model. However, all the quantization parameters are set by default so the final accuracy of the tflite mode is a little bit low. So I wonder is there any way to rewrite the quantization parameters by myself. Since the integer numbers are stored in the tflite model, I can not modify them easily."
59212,Failure to compile on PPC due to Eigen incompatibility,"### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9.1

### Current Behaviour?

Compiling TensorFlow from source yields: 

```
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AltiVec/MatrixVectorProduct.h: In instantiation of 'void EigenForTFLite::internal::gemv_row(Index, Index, const LhsMapper&, const RhsMapper&, ResScalar*, Index, ResScalar) [with Index = long int; LhsScalar = float; LhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 1, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 16, EigenForTFLite::MakePointer>; RhsScalar = float; RhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; ResScalar = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AltiVec/MatrixVectorProduct.h:2019:1:   required from 'static void EigenForTFLite::internal::general_matrix_vector_product<Index, float, LhsMapper, 0, ConjugateLhs, float, RhsMapper, ConjugateRhs, Version>::run(Index, Index, const LhsMapper&, const RhsMapper&, EigenForTFLite::internal::general_matrix_vector_product<Index, float, LhsMapper, 0, ConjugateLhs, float, RhsMapper, ConjugateRhs, Version>::ResScalar*, Index, EigenForTFLite::internal::general_matrix_vector_product<Index, float, LhsMapper, 0, ConjugateLhs, float, RhsMapper, ConjugateRhs, Version>::ResScalar) [with Index = long int; LhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 1, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 16, EigenForTFLite::MakePointer>; bool ConjugateLhs = false; RhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; bool ConjugateRhs = false; int Version = 0; EigenForTFLite::internal::general_matrix_vector_product<Index, float, LhsMapper, 0, ConjugateLhs, float, RhsMapper, ConjugateRhs, Version>::ResScalar = float]'

<snip>

external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AltiVec/MatrixVectorProduct.h:1976:5: error: 'class EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>' has no member named 'load'
```

See my issue in Eigen: https://gitlab.com/libeigen/eigen/-/merge_requests/764#note_1231907378

### Standalone code to reproduce the issue

Compile TensorFlow on PPC/POWER9


The issue is that the specialized `TensorContractionInputMapper` in `eigen_spatial_convolutions-inl.h` (at master at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tsl/framework/convolution/eigen_spatial_convolutions-inl.h) is missing the new `load` functions as required by Eigen"
59211,[tflite] Converting a JAX model with `lax.switch` to TFLite fails (related to `StatelessCase` / `tf.Case`),"### 1. System information

- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installation: pip
- TensorFlow library: 2.9.2 

### 2. Code & Minimal Reproduction Colab Notebook

I have the following minimal code (see [this Colab notebook](https://colab.research.google.com/gist/josephrocca/76ab8cb26dc83a0b5fd58945962033a0)) that uses `jax.lax.switch` and tries to convert the model to TFLite:
```py
def test_jax(n, operand):

  def fn1(a):
    return a+2
  
  def fn2(a):
    return a*2

  result = jax.lax.switch(
    n,
    [fn1, fn2],
    operand,
  )

  return result
```

```py
jax.jit(test_jax)(1, 2)  # returns `4`
```

```py
my_model = tf.Module()
my_model.f = tf.function(jax2tf.convert(test_jax, enable_xla=False), jit_compile=True, autograph=False, input_signature=[
    tf.TensorSpec([], tf.uint32, name=""n""),
    tf.TensorSpec([], tf.uint32, name=""operand""),
])
tf.saved_model.save(my_model, './test')
```

```py
converter = tf.lite.TFLiteConverter.from_saved_model('./test')

converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

tflite_model = converter.convert()

with open('test.tflite', 'wb') as f:
  f.write(tflite_model)
```

That TFLite conversion code outputs:
```
ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[""StatelessCase:"", ""jax2tf_test_jax_/switch_case/indexed_case@__inference_converted_fun_tf_58""] at fused[""PartitionedCall:"", ""PartitionedCall@__inference_signature_wrapper_66""]) at fused[""PartitionedCall:"", ""PartitionedCall""])): 'tf.Case' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""StatelessCase:"", ""jax2tf_test_jax_/switch_case/indexed_case@__inference_converted_fun_tf_58""] at fused[""PartitionedCall:"", ""PartitionedCall@__inference_signature_wrapper_66""]) at fused[""PartitionedCall:"", ""PartitionedCall""])): Error code: ERROR_NEEDS_CUSTOM_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom 
Custom ops: StatelessCase
Details:
	tf.Case(tensor<i32>, tensor<ui32>) -> (tensor<*xui32>) : {Tin = [ui32], Tout = [ui32], _read_only_resource_inputs = [], _xla_propagate_compile_time_consts = true, branches = [@jax2tf_test_jax__switch_case_indexed_case_branch0_220, @jax2tf_test_jax__switch_case_indexed_case_branch1_230, @jax2tf_test_jax__switch_case_indexed_case_branch2_240, @jax2tf_test_jax__switch_case_indexed_case_branch3_250], device = """", is_stateless = true}
```

And I tried using `experimental_from_jax` ([Colab notebook](https://colab.research.google.com/gist/josephrocca/4317a1db075a6e42ed600dfe23e47a75)), but for some reason it is crashing my notebook.

Thanks!"
59209,[macos-arm64] //tensorflow/core/kernels:quantized_instance_norm_test fails in macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
==================== Test output for //tensorflow/core/kernels:quantized_instance_norm_test:
[==========] Running 5 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 5 tests from QuantizedInstanceNormTest
[ RUN      ] QuantizedInstanceNormTest.TestBasic
2022-12-28 08:47:32.513609: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-12-28 08:47:32.515963: I tensorflow/core/kernels/quantized_instance_norm_test.cc:121] max diff 0.0240083
[       OK ] QuantizedInstanceNormTest.TestBasic (7 ms)
[ RUN      ] QuantizedInstanceNormTest.TestZeroInput
2022-12-28 08:47:32.517460: I tensorflow/core/kernels/quantized_instance_norm_test.cc:121] max diff 0
[       OK ] QuantizedInstanceNormTest.TestZeroInput (1 ms)
[ RUN      ] QuantizedInstanceNormTest.TestMaxInput
2022-12-28 08:47:32.519027: I tensorflow/core/kernels/quantized_instance_norm_test.cc:121] max diff 0
[       OK ] QuantizedInstanceNormTest.TestMaxInput (1 ms)
[ RUN      ] QuantizedInstanceNormTest.TestOutputRangeGiven
tensorflow/core/kernels/quantized_instance_norm_test.cc:120: Failure
Expected: (max_diff()) <= (0.1), actual: 0.462106 vs 0.1
2022-12-28 08:47:32.520742: I tensorflow/core/kernels/quantized_instance_norm_test.cc:121] max diff 0.462106
[  FAILED  ] QuantizedInstanceNormTest.TestOutputRangeGiven (1 ms)
[ RUN      ] QuantizedInstanceNormTest.TestClamp
2022-12-28 08:47:32.522150: I tensorflow/core/kernels/quantized_instance_norm_test.cc:121] max diff 0.0261555
[       OK ] QuantizedInstanceNormTest.TestClamp (1 ms)
[----------] 5 tests from QuantizedInstanceNormTest (13 ms total)

[----------] Global test environment tear-down
[==========] 5 tests from 1 test suite ran. (13 ms total)
[  PASSED  ] 4 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] QuantizedInstanceNormTest.TestOutputRangeGiven

 1 FAILED TEST
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/core/kernels:quantized_instance_norm_test` from the TensorFlow root directory. 

Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59208,[macos-arm64] //tensorflow/core/kernels:conv_ops_test_cpu  fail in macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
INFO: From Testing //tensorflow/core/kernels:conv_ops_test_cpu:
==================== Test output for //tensorflow/core/kernels:conv_ops_test_cpu:
[==========] Running 67 tests from 7 test suites.
[----------] Global test environment set-up.
[----------] 25 tests from FusedResizePadConvOpTest
[ RUN      ] FusedResizePadConvOpTest.HandwrittenConvHalf
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (105 not close to 84)
Expected: true
i = 0 Tx[i] = 105 Ty[i] = 84
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (150 not close to 128)
Expected: true
i = 1 Tx[i] = 150 Ty[i] = 128
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (183 not close to 160)
Expected: true
i = 2 Tx[i] = 183 Ty[i] = 160
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (95 not close to 86)
Expected: true
i = 3 Tx[i] = 95 Ty[i] = 86
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (235 not close to 187)
Expected: true
i = 4 Tx[i] = 235 Ty[i] = 187
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (312 not close to 280)
Expected: true
i = 5 Tx[i] = 312 Ty[i] = 280
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (357 not close to 321)
Expected: true
i = 6 Tx[i] = 357 Ty[i] = 321
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (178 not close to 165)
Expected: true
i = 7 Tx[i] = 178 Ty[i] = 165
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (187 not close to 142)
Expected: true
i = 8 Tx[i] = 187 Ty[i] = 142
tensorflow/core/framework/tensor_testutil.cc:184: Failure
Value of: IsClose(Tx[i], Ty[i], typed_atol, typed_rtol)
  Actual: false (234 not close to 228)
Expected: true
i = 9 Tx[i] = 234 Ty[i] = 228
tensorflow/core/framework/tensor_testutil.cc:187: Failure
Expected: (num_failures) < (max_failures), actual: 10 vs 10
Too many mismatches (atol = 1.0000000000000001e-05 rtol = 0), giving up.
[  FAILED  ] FusedResizePadConvOpTest.HandwrittenConvHalf (5 ms)
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/core/kernels:conv_ops_test_cpu` from the TensorFlow root directory. 

Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59207,[macos-arm64] Tests in //tensorflow/python/kernel_tests/... fail on macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Logs are too big to paste here. See https://gist.githubusercontent.com/nitins17/c72bc19271080fd053ef2bbd722342da/raw/f640ed6133a57d9981edf405d53738b76972abd7/nonpip_test_failures_tf_python.log for complete logs.
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/python/kernel_tests/nn_ops:conv_ops_test_cpu` from the TensorFlow root directory. 

Other tests that fail:
1. //tensorflow/python/kernel_tests/math_ops:batch_matmul_op_test_cpu
2. //tensorflow/python/kernel_tests/math_ops:tensordot_op_test_cpu
3. //tensorflow/python/kernel_tests/math_ops:matmul_op_test_cpu
4. //tensorflow/python/kernel_tests/nn_ops:conv_ops_3d_test_cpu


Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59206,[macos-arm64] //tensorflow/python:nn_batchnorm_test_cpu fails in macos Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
ERROR: testBasic (__main__.MomentsTest)
MomentsTest.testBasic
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/python/nn_batchnorm_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1624, in decorated
    return f(self, *args, **kwargs)
  File ""/private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/python/nn_batchnorm_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_batchnorm_test.py"", line 522, in testBasic
    self.RunMomentTest(
  File ""/private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/python/nn_batchnorm_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/nn_batchnorm_test.py"", line 499, in RunMomentTest
    x_numpy = x_numpy.astype(np.float128)
  File ""/Users/admin/tf_macos_ci_job/.tf-venv/lib/python3.9/site-packages/numpy/__init__.py"", line 313, in __getattr__
    raise AttributeError(""module {!r} has no attribute ""
AttributeError: module 'numpy' has no attribute 'float128'

----------------------------------------------------------------------
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/python:nn_batchnorm_test_cpu` from the TensorFlow root directory. 

Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59205,[macos-arm64] //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails in macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters
[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test
2022-12-28 07:13:32.077216: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
tensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:86: Failure
Value of: _status_or_value13.status().ok()
  Actual: false
Expected: true
INTERNAL: TargetRegistry::lookupTarget failed: No available targets are compatible with triple ""i686-none-android""
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (16 ms)
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (16 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (16 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test

 1 FAILED TEST
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test` from the TensorFlow root directory. 

Other tests that fail similarly:
1. //tensorflow/compiler/xla/service/cpu/tests:cpu_eigen_dot_operation_test
2. //tensorflow/core/platform:__tensorflow_tsl_platform_profile_utils_cpu_utils_test
3. //tensorflow/compiler/xla/tests:execution_profile_test_with_xla_hlo_profile_cpu


Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59204,[macos-arm64] //tensorflow/compiler/xla/service:hlo_execution_profile_test fails in macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4, Arm64

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
INFO: From Testing //tensorflow/compiler/xla/service:hlo_execution_profile_test:
==================== Test output for //tensorflow/compiler/xla/service:hlo_execution_profile_test:
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from HloExecutionProfileTest
[ RUN      ] HloExecutionProfileTest.Basic
2022-12-28 06:42:38.460138: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-12-28 06:42:38.462696: F ./tensorflow/compiler/xla/service/human_readable_profile_builder.h:40] Check failed: clock_rate_ghz >= 1e-9 (1e-09 vs. -1e-09)
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/compiler/xla/service:hlo_execution_profile_test` from the TensorFlow root directory. 

Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979

`//tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu` fails with a similar error.
```


### Relevant log output

_No response_</details>"
59203,[macos-arm64] //tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test fails on macOS Arm64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf nightly

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.4

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

5.3.0

### GCC/Compiler version

Clang from Xcode 13.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
==================== Test output for //tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test:
-- Testing: 1 tests, 1 workers --
FAIL: MLIR tests :: const-fold.mlir (1 of 1)
******************** TEST 'MLIR tests :: const-fold.mlir' FAILED ********************
Script:
--
: 'RUN: at line 1';   /private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/const-fold.mlir -canonicalize | FILECHECK_OPTS="""" /private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/llvm-project/llvm/FileCheck /private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/const-fold.mlir
--
Exit Code: 1

Command Output (stderr):
--
/private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/const-fold.mlir:165:16: error: CHECK-DAG: expected string not found in input
 // CHECK-DAG: [[cst1:%.*]] = arith.constant dense<0.841470957> : tensor<f32>
               ^
<stdin>:54:34: note: scanning from here
 func.func @elementwise_unary_ops() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
                                 ^
<stdin>:60:3: note: possible intended match here
 %cst_4 = arith.constant dense<8.414710e-01> : tensor<f32>
  ^

Input file: <stdin>
Check file: /private/var/tmp/_bazel_admin/4a764cdae64932883d338b1294a81bf1/execroot/org_tensorflow/bazel-out/darwin_arm64-opt/bin/tensorflow/compiler/mlir/lite/tests/const-fold.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/const-fold.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
           .
           .
           .
          49:  %cst_0 = arith.constant dense<5.250000e+00> : tensor<4xf16> 
          50:  %cst_1 = arith.constant dense<-2.250000e+00> : tensor<4xf16> 
          51:  %cst_2 = arith.constant dense<6.750000e+00> : tensor<f16> 
          52:  return %cst_2, %cst_1, %cst_0, %cst : tensor<f16>, tensor<4xf16>, tensor<4xf16>, tensor<4xf16> 
          53:  } 
          54:  func.func @elementwise_unary_ops() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) { 
dag:165'0                                      X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found
          55:  %cst = arith.constant dense<4.000000e+00> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          56:  %cst_0 = arith.constant dense<5.000000e-01> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          57:  %cst_1 = arith.constant dense<2.000000e+00> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          58:  %cst_2 = arith.constant dense<0.000000e+00> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          59:  %cst_3 = arith.constant dense<0.540302336> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          60:  %cst_4 = arith.constant dense<8.414710e-01> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dag:165'1       ?                                                         possible intended match
          61:  %cst_5 = arith.constant dense<1.000000e+00> : tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          62:  return %cst_5, %cst_4, %cst_3, %cst_2, %cst_1, %cst_0, %cst : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32> 
dag:165'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          63:  } 
dag:165'0     ~~~
          64:  func.func @mul_int() -> (tensor<i32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) { 
dag:165'0     ~~~~~~~~~~~~~~~~~~~
          65:  %cst = arith.constant dense<-8> : tensor<4xi32> 
           .
           .
           .
>>>>>>

--

********************
********************
Failed Tests (1):
  MLIR tests :: const-fold.mlir


Testing Time: 0.36s
  Failed: 1
```
```


### Standalone code to reproduce the issue

```shell
Run `bazel --bazelrc=""macos.bazelrc"" test //tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test` from the TensorFlow root directory. 

Bazel configs I used are in https://gist.github.com/nitins17/73e0818b3a6bec240775c2619540a979
```


### Relevant log output

_No response_</details>"
59201,Crash when running tensorflow.python.ops.math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Abort when running .python.ops.math_ops.sparse_segment_sum
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.random.uniform([1], dtype=tf.float64)
      data = tf.identity(data_tensor)
      indices = []
      segment_ids = []
      out = math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float64)
      indices = []
      segment_ids = []
      math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-09 17:01:07.909340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1159] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x7f6e6ec00000; GPU src: 0xfffffffffffffffc; size: 4=0x4
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} SparseSegmentSum: failed to copy last_segment_id from device [Op:SparseSegmentSum]
2023-01-09 17:01:07.910082: F tensorflow/core/common_runtime/gpu/gpu_util.cc:386] CPU->GPU Memcpy failed
Aborted

```
```
</details>"
59200,Check failure when running tensorflow.python.ops.gen_math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure due to dimension mismatch.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.random.uniform([1], dtype=tf.float16)
      data = tf.identity(data_tensor)
      indices = []
      segment_ids = []
      out = gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float16)
      indices = []
      segment_ids = []
      gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-09 16:51:20.530035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1159] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x7f9efea00000; GPU src: 0xfffffffffffffffc; size: 4=0x4
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} SparseSegmentSum: failed to copy last_segment_id from device [Op:SparseSegmentSum]
2023-01-09 16:51:20.531363: F tensorflow/core/common_runtime/gpu/gpu_util.cc:386] CPU->GPU Memcpy failed
Aborted

```
```
</details>"
59199,Check failure when running tensorflow.python.ops.gen_sparse_ops.sparse_to_dense,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0_tensor = tf.random.uniform([10], minval=-256, maxval=257, dtype=tf.int32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_2 = tf.identity(arg_2_tensor)
  default_value = -1
  validate_indices = False
  out = gen_sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,default_value=default_value,validate_indices=validate_indices,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-09 16:19:19.624081: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted

```
```
</details>"
59198,Double free when running tensorflow.python.ops.gen_nn_ops.fractional_avg_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

Binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Double free or corruption.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = 1
  arg_1_2 = 2
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 341261001
  out = gen_nn_ops.fractional_avg_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:Value for attr 'T' of int16 is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:Value for attr 'T' of int16 is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
double free or corruption (!prev)
Aborted

```
```
</details>"
59197,Check failure when running tensorflow.python.ops.gen_nn_ops.max_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 10, 14, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  ksize_0 = 1
  ksize_1 = 0
  ksize_2 = 3
  ksize_3 = 1
  ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""VALID""
  explicit_paddings = []
  data_format = ""NHWC""
  out = gen_nn_ops.max_pool(arg_0,ksize=ksize,strides=strides,padding=padding,explicit_paddings=explicit_paddings,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 6, 8, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  ksize_0 = 1
  ksize_1 = -52.0
  ksize_2 = 2
  ksize_3 = 1
  ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
  strides_0 = 1
  strides_1 = 1
  strides_2 = 1
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""VALID""
  explicit_paddings = []
  data_format = ""NHWC""
  out = gen_nn_ops.max_pool(arg_0,ksize=ksize,strides=strides,padding=padding,explicit_paddings=explicit_paddings,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2023-01-09 15:14:48.597471: F tensorflow/stream_executor/cuda/cuda_dnn.cc:886] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
</details>"
59196,Segfault when running tensorflow.python.ops.gen_linalg_ops.matrix_solve_ls,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
segfault
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_linalg_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.complex(tf.random.uniform([16, 8, 127, 127], dtype=tf.float64),tf.random.uniform([16, 8, 127, 127], dtype=tf.float64))
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.complex(tf.random.uniform([16, 8, 127, 1], dtype=tf.float64),tf.random.uniform([16, 8, 127, 1], dtype=tf.float64))
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = -42.0
      fast = False
      out = gen_linalg_ops.matrix_solve_ls(arg_0,arg_1,arg_2,fast=fast,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.complex128)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.complex128)
      gen_linalg_ops.matrix_solve_ls(arg_0,arg_1,arg_2,fast=fast,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Value for attr 'T' of uint8 is not in the list of allowed values: double, float, half, complex64, complex128
	; NodeDef: {{node MatrixSolveLs}}; Op<name=MatrixSolveLs; signature=matrix:T, rhs:T, l2_regularizer:double -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]; attr=fast:bool,default=true> [Op:MatrixSolveLs]
Error:Value for attr 'T' of uint8 is not in the list of allowed values: double, float, half, complex64, complex128
	; NodeDef: {{node MatrixSolveLs}}; Op<name=MatrixSolveLs; signature=matrix:T, rhs:T, l2_regularizer:double -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]; attr=fast:bool,default=true> [Op:MatrixSolveLs]
Error:Value for attr 'T' of uint8 is not in the list of allowed values: double, float, half, complex64, complex128
	; NodeDef: {{node MatrixSolveLs}}; Op<name=MatrixSolveLs; signature=matrix:T, rhs:T, l2_regularizer:double -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]; attr=fast:bool,default=true> [Op:MatrixSolveLs]
Error:{{function_node __wrapped__MatrixSolveLs_device_/job:localhost/replica:0/task:0/device:CPU:0}} l2_regularizer must be >= 0. [Op:MatrixSolveLs]
malloc(): unaligned tcache chunk detected
Segmentation fault

```
```
</details>"
59195,segfault in tensorflow.python.ops.gen_nn_ops.fractional_max_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
segfault
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1
  arg_1_1 = 0
  arg_1_2 = 0
  arg_1_3 = 0
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = False
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 123456
  out = gen_nn_ops.fractional_max_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1
  arg_1_1 = -3.0
  arg_1_2 = 2
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 123456
  out = gen_nn_ops.fractional_max_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```
```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 10, 10, 1], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = True
  arg_1_1 = -0.6640134831719401
  arg_1_2 = -0.47262459901529796
  arg_1_3 = -62
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 123456
  out = gen_nn_ops.fractional_max_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```

### Relevant log output

_No response_</details>"
59192,Double free when running ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Double free.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = 3
  arg_1_2 = 1
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  seed = 341261001
  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Value for attr 'T' of bool is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:Value for attr 'T' of bool is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:Value for attr 'T' of bool is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
double free or corruption (!prev)
Aborted

```
```
</details>"
59191,Crash when running gen_nn_ops.max_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure with the following input combination.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 10, 14, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  ksize_0 = 1
  ksize_1 = -1
  ksize_2 = 2
  ksize_3 = 1
  ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""VALID""
  explicit_paddings = []
  data_format = ""NHWC""
  out = gen_nn_ops.max_pool(arg_0,ksize=ksize,strides=strides,padding=padding,explicit_paddings=explicit_paddings,data_format=data_format,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-09 11:10:47.659984: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:958] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
</details>"
59190,Illegal memory access when running gen_math_ops.dense_bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Illegal memory access.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      input_tensor = tf.random.uniform([4096], minval=-256, maxval=257, dtype=tf.int32)
      input = tf.identity(input_tensor)
      weights = []
      size = 0
      binary_output = True
      out = gen_math_ops.dense_bincount(input=input,weights=weights,size=size,binary_output=binary_output,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      input = tf.identity(input_tensor)
      input = tf.cast(input, tf.int32)
      weights = []
      gen_math_ops.dense_bincount(input=input,weights=weights,size=size,binary_output=binary_output,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__DenseBincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:DenseBincount]
Error:{{function_node __wrapped__DenseBincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:DenseBincount]
2023-01-09 11:04:45.833405: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-09 11:04:45.833428: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59189,Illegal memory access when running tensorflow.python.ops.bincount_ops.bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Illegal memory access.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import bincount_ops
try:
  arg_0 = []
  minlength = 5
  out = bincount_ops.bincount(arg_0,minlength=minlength,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-09 10:49:51.634579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1179] failed to enqueue async memcpy from host to device: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; GPU dst: 0x7f7aa4000800; host src: 0x7f7ba6800000; size: 4=0x4
2023-01-09 10:49:51.634596: E tensorflow/compiler/xla/stream_executor/stream.cc:321] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2023-01-09 10:49:51.634606: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-09 10:49:51.634882: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59188,Crash when running gen_nn_ops.fused_batch_norm_grad_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Strange crash
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  y_backprop_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
  y_backprop = tf.identity(y_backprop_tensor)
  x_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
  x = tf.identity(x_tensor)
  scale_tensor = tf.saturate_cast(tf.random.uniform([2], minval=0, maxval=257, dtype=tf.int64), dtype=tf.uint32)
  scale = tf.identity(scale_tensor)
  reserve_space_1_tensor = tf.random.uniform([2], dtype=tf.float32)
  reserve_space_1 = tf.identity(reserve_space_1_tensor)
  reserve_space_2_tensor = tf.random.uniform([2], dtype=tf.float32)
  reserve_space_2 = tf.identity(reserve_space_2_tensor)
  epsilon = 0.001
  data_format = ""NHWC""
  is_training = False
  reserve_space_3_tensor = tf.random.uniform([], dtype=tf.float32)
  reserve_space_3 = tf.identity(reserve_space_3_tensor)
  out = gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:cannot compute FusedBatchNormGradV3 as input #4(zero-based) was expected to be a float tensor but is a int32 tensor [Op:FusedBatchNormGradV3]
2023-01-09 10:45:29.415087: E tensorflow/compiler/xla/stream_executor/dnn.cc:887] CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5936): 'cudnnBatchNormalizationBackward( cudnn.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y_backprop.opaque(), x_descriptor.handle(), x_backprop->opaque(), scale_offset_descriptor.handle(), scale.opaque(), scale_backprop->opaque(), offset_backprop->opaque(), epsilon, mean.opaque(), inv_var.opaque())'
Error:{{function_node __wrapped__FusedBatchNormGradV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([4,10,10,2]) [Op:FusedBatchNormGradV3]
Error:cannot compute FusedBatchNormGradV3 as input #4(zero-based) was expected to be a float tensor but is a bool tensor [Op:FusedBatchNormGradV3]
Error:Value for attr 'U' of half is not in the list of allowed values: float
	; NodeDef: {{node FusedBatchNormGradV3}}; Op<name=FusedBatchNormGradV3; signature=y_backprop:T, x:T, scale:float, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U -> x_backprop:T, scale_backprop:U, offset_backprop:U, reserve_space_4:U, reserve_space_5:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW"", ""NDHWC"", ""NCDHW""]; attr=is_training:bool,default=true> [Op:FusedBatchNormGradV3]
Error:Value for attr 'data_format' of """" is not in the list of allowed values: ""NHWC"", ""NCHW"", ""NDHWC"", ""NCDHW""
	; NodeDef: {{node FusedBatchNormGradV3}}; Op<name=FusedBatchNormGradV3; signature=y_backprop:T, x:T, scale:float, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U -> x_backprop:T, scale_backprop:U, offset_backprop:U, reserve_space_4:U, reserve_space_5:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW"", ""NDHWC"", ""NCDHW""]; attr=is_training:bool,default=true> [Op:FusedBatchNormGradV3]
2023-01-09 10:45:29.432336: F tensorflow/core/common_runtime/gpu/gpu_util.cc:386] CPU->GPU Memcpy failed
Aborted

```
```
</details>"
59186,Unexpected behaviour for save load dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0-dev20230109

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Unexpected behaviour of the save and load function for datasets.
After saving for the first time a dataset I can load it from the disk without problems, modify the dataset as I please and then expect to be able to overwrite the previous dataset on the disk.
Currently, I'm able to save the second time the modified dataset but when I retrieve it from memory the object retrieved contains only the variation that happened before.
This is not what I expect from a 'save' and 'load' function, I should be able to retrieve from the disk an exact copy of what I'm saving.
```


### Standalone code to reproduce the issue

```shell
Colab link: https://colab.research.google.com/drive/1Mw2w6hmHgmORoTipboGCvBMkOv5JCMAu?usp=sharing

In short:
import tensorflow as tf
path=""test-save""
tmp_dst_1 = tf.data.Dataset.from_tensor_slices(tf.ones([4, 10, 3, 1]))
tmp_dst_0 = tf.data.Dataset.from_tensor_slices(tf.zeros([4, 20, 3, 1]))
print([elem.shape for elem in tmp_dst_1])
print([elem.shape for elem in tmp_dst_0])
>>> [TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1])]
>>> [TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1])]

tmp_dst_1.save(path+""/dst_folder"")
from_disk_dst_1 = tf.data.Dataset.load(path+""/dst_folder"")
print([elem.shape for elem in from_disk_dst_1])
>>> [TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1])]

from_disk_dst_1 = from_disk_dst_1.concatenate(tmp_dst_0)
print([elem.shape for elem in from_disk_dst_1])
>>> [TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([10, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1])]
from_disk_dst_1.save(path+""/dst_folder"")

from_disk_new_dst = tf.data.Dataset.load(path+""/dst_folder"")
print([elem.shape for elem in from_disk_new_dst])
>>> [TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1]), TensorShape([20, 3, 1])]
```


### Relevant log output

_No response_</details>"
59184,Text Predictions Tutorials Has  Incorrect Data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! On the Tutorial page, the predictions for generating text is as follows.

To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.

Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.
Try it for the first example in the batch:


sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
This gives us, at each timestep, a prediction of the next character index:


sampled_indices

array([49, 56, 39, 28,  8, 35, 34,  0, 11, 12, 34, 36,  4, 34, 50, 12, 12,
       54, 28, 46, 24, 41, 29, 16, 11, 59,  5, 62, 15, 33,  8, 61,  3, 30,
       10, 37, 57, 48, 41, 61, 11, 55, 43, 45, 57, 51,  6, 36,  8, 18, 32,
       30, 49,  2, 35, 47, 25, 51, 43, 49, 11, 50, 64, 37, 34, 29, 43,  4,
       51, 19,  9, 12, 18, 48, 31,  7, 35, 30, 38,  1, 35, 49, 22, 65, 48,
       13, 26, 12, 56, 25, 30, 41, 28, 27,  1, 62, 41, 34,  4, 29])
Decode these to see the text predicted by this untrained model:


print(""Input:\n"", text_from_ids(input_example_batch[0]).numpy())
print()
print(""Next Char Predictions:\n"", text_from_ids(sampled_indices).numpy())

Input:
 b""! I come, I come!\nWho knocks so hard? whence come you? what's your will?\n\nNurse:\n\nFRIAR LAURENCE:\nWe""
Next Char Predictions:
 b""jqZO-VU[UNK]:;UW\\(Uk;;oOgKbPC:t&wBT-v!Q3Xribv:pdfrl'W-ESQj VhLldj:kyXUPd\\)lF.;EiR,VQY\nVjIzi?M;qLQbON\nwbU$P""


This is within training, and even me who is new, knows this is wrong.
```


### Standalone code to reproduce the issue

```shell
Try the model
Now run the model to see that it behaves as expected.

First check the shape of the output:


for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, ""# (batch_size, sequence_length, vocab_size)"")

(64, 100, 66) # (batch_size, sequence_length, vocab_size)
In the above example the sequence length of the input is 100 but the model can be run on inputs of any length:


model.summary()

Model: ""my_model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       multiple                  16896     
                                                                 
 gru (GRU)                   multiple                  3938304   
                                                                 
 dense (Dense)               multiple                  67650     
                                                                 
=================================================================
Total params: 4,022,850
Trainable params: 4,022,850
Non-trainable params: 0
_________________________________________________________________
To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.

Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.
Try it for the first example in the batch:


sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
This gives us, at each timestep, a prediction of the next character index:


sampled_indices

array([49, 56, 39, 28,  8, 35, 34,  0, 11, 12, 34, 36,  4, 34, 50, 12, 12,
       54, 28, 46, 24, 41, 29, 16, 11, 59,  5, 62, 15, 33,  8, 61,  3, 30,
       10, 37, 57, 48, 41, 61, 11, 55, 43, 45, 57, 51,  6, 36,  8, 18, 32,
       30, 49,  2, 35, 47, 25, 51, 43, 49, 11, 50, 64, 37, 34, 29, 43,  4,
       51, 19,  9, 12, 18, 48, 31,  7, 35, 30, 38,  1, 35, 49, 22, 65, 48,
       13, 26, 12, 56, 25, 30, 41, 28, 27,  1, 62, 41, 34,  4, 29])
Decode these to see the text predicted by this untrained model:


print(""Input:\n"", text_from_ids(input_example_batch[0]).numpy())
print()
print(""Next Char Predictions:\n"", text_from_ids(sampled_indices).numpy())

Input:
 b""! I come, I come!\nWho knocks so hard? whence come you? what's your will?\n\nNurse:\n\nFRIAR LAURENCE:\nWe""

Next Char Predictions:
 b""jqZO-VU[UNK]:;UW\\(Uk;;oOgKbPC:t&wBT-v!Q3Xribv:pdfrl'W-ESQj VhLldj:kyXUPd\\)lF.;EiR,VQY\nVjIzi?M;qLQbON\nwbU$P""
```


### Relevant log output

_No response_</details>"
59183,Stop exposing `tensorboard` console script in standard Tensorflow package?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

OpenSUSE Leap 15.3

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The *tensorflow_cpu* Python package exposes the `tensorboard` command, leading to a duplicate declaration of the corresponding entry point. For environments where such duplicate commands are considered bad behavior, an exception has to be added.

There is some documentation on this inside the corresponding code: https://github.com/tensorflow/tensorflow/blob/3b43530ab8d9513f78368652e22558b4a1286ee4/tensorflow/tools/pip_package/setup.py#L183-L188 Doing a `git blame`, this seems to have been introduced in https://github.com/tensorflow/tensorflow/commit/05c491d30888088873fedfbe81bca378c8c3fc87, which does not seem to provide any additional details on it as well.

For me this sounds like an issue which should rather be fixed within *pip* instead of working around this in *tensorflow* for multiple years. Having a quick look at the issue tracker there, I could not find an issue which describes such a behavior. Has anything been done to possibly fix this/get this fixed for *pip*?
```


### Standalone code to reproduce the issue

```shell
`grep -r ""tensorboard.main:run_main"" .`, where `.` is the directory of the virtual environment the TensorFlow and TensorBoard packages have been installed inside. This will produce a result inside `entry_points.txt` for `tensorboard` and `tensorflow_cpu`.
```


### Relevant log output

_No response_</details>"
59182,"GPU only works in training and tuning, but not inference, gets NON-Okay status","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.5.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.8.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.1

### GPU model and memory

GA100 [A100 PCIe 40GB]

### Current Behaviour?

```shell
I am running an auto encoder, and when I do inference, I get the following error:

Non-OK-status: GpuLaunchKernel( concat_variable_kernel<T, IntType, true>, config.block_count, config.thread_per_block, smem_usage, gpu_device.stream(), input_ptrs, output_scan, static_cast<IntType>(output->dimension(0)), static_cast<IntType>(output->dimension(1)), output->data()) status: Internal: invalid configuration argument
Aborted (core dumped)


Earlier, this code worked, but then out of nowhere, this issue started to happen, and the code did not, to my knowledge change that much. Further, Why would the GPU work when doing training and tuning, but not inference, this does not make sense to me.
```


### Standalone code to reproduce the issue

```shell
The code is a lot larger, but the gist is here: Dataset train and test is of dimension 
(4574215, 529) (16384, 529)

with tf.device(""/CPU:0""):
            print(""Background started"")
            self.pred_back = self.AE_model.predict(X_val, batch_size=self.b_size)
            print(""Background predicted"")
            self.recon_err_back = self.reconstructionError(self.pred_back, X_val)
            print(f""Background done, lenght: {len(self.recon_err_back)}"")

            if len(test_set) > 0:
                print(""Signal started"")
                self.pred_sig = self.AE_model.predict(test_set, batch_size=self.b_size)
                self.recon_sig = self.reconstructionError(self.pred_sig, test_set)
                print(f""Signal done, lenght: {len(self.recon_sig)}"")

            print(""ATLAS data started"")
            self.pred_data = self.AE_model.predict(self.data, batch_size=self.b_size)
            self.recon_data = self.reconstructionError(self.pred_data, self.data)
            print(""ATLAS data done"")
```


### Relevant log output

```shell
2023-01-09 10:37:42.912909: F tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc:161] Non-OK-status: GpuLaunchKernel( concat_variable_kernel<T, IntType, true>, config.block_count, config.thread_per_block, smem_usage, gpu_device.stream(), input_ptrs, output_scan, static_cast<IntType>(output->dimension(0)), static_cast<IntType>(output->dimension(1)), output->data()) status: Internal: invalid configuration argument
Aborted (core dumped)
```
</details>"
59181,[Documentation] `raw_ops.RealDiv`: Input Tensors cannot be of integer dtype,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux WSL2 Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation for `tf.raw_ops.RealDiv` states that the input argument `x` must be a Tensor of types `bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.` Upon usage, however, this op throws an exception when run with an input of any integer dtype when run on a CPU. The op only works for `float` and `complex` dtypes.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

dtype = ""int64""
x = np.array([[1,2,4],[2,3,5]], dtype=dtype)
y = np.array([[1,2,4],[2,3,5]], dtype=dtype)
x = tf.constant(x, dtype=dtype)
y = tf.constant(y, dtype=dtype)
tf.raw_ops.RealDiv(
    x=x, y=y, name=None
)
```


### Relevant log output

```shell
---------------------------------------------------------------------------

NotFoundError                             Traceback (most recent call last)

<ipython-input-3-553c77700511> in <module>
      4 x = tf.constant(x, dtype=dtype)
      5 y = tf.constant(y, dtype=dtype)
----> 6 tf.raw_ops.RealDiv(
      7     x=x, y=y, name=None
      8 )

/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7165 
   7166 

NotFoundError: Could not find device for node: {{node RealDiv}} = RealDiv[T=DT_INT64]
All kernels registered for op RealDiv:
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
 [Op:RealDiv]
```
</details>"
59179,Process killed when running generic_utils.make_batches,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably due to very large input argument.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.keras.utils import generic_utils
try:
  arg_0 = 125091515651
  arg_1 = 512
  out = generic_utils.make_batches(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

_No response_</details>"
59178,Segfault when running tensorflow.python.ops.math_ops.sobol_sample,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault on very large input arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import math_ops
try:
  arg_0 = 5
  arg_1 = 125091515651
  dtype = None
  out = math_ops.sobol_sample(arg_0,arg_1,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

_No response_</details>"
59177,Segfault when running tensorflow.python.ops.gen_sparse_ops.sparse_concat,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segfault probably due to dimension mismatch on input tensor parameters.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0 = []
  arg_1_0_tensor = tf.random.uniform([], dtype=tf.float32)
  arg_1_0 = tf.identity(arg_1_0_tensor)
  arg_1_1_tensor = tf.random.uniform([], dtype=tf.float32)
  arg_1_1 = tf.identity(arg_1_1_tensor)
  arg_1 = [arg_1_0,arg_1_1,]
  arg_2_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64)
  arg_2_0 = tf.identity(arg_2_0_tensor)
  arg_2_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64)
  arg_2_1 = tf.identity(arg_2_1_tensor)
  arg_2 = [arg_2_0,arg_2_1,]
  arg_3 = -2
  out = gen_sparse_ops.sparse_concat(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Segmentation fault
```
</details>"
59175,Process killed when running tensorflow.python.ops.gen_math_ops._histogram_fixed_width,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Process get killed.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0 = -1.0
      arg_0_1 = 0.0
      arg_0_2 = 1.5
      arg_0_3 = 2.0
      arg_0_4 = 5.0
      arg_0_5 = 15
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,arg_0_3,arg_0_4,arg_0_5,]
      arg_1_0 = 0.0
      arg_1_1 = 5.0
      arg_1 = [arg_1_0,arg_1_1,]
      arg_2 = 125091515651
      dtype = tf.int64
      out = gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,arg_0_3,arg_0_4,arg_0_5,]
      arg_1 = [arg_1_0,arg_1_1,]
      dtype = tf.int64
      gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-08 20:36:03.709478: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
2023-01-08 20:36:04.808586: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
2023-01-08 20:36:05.715969: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4299712536 exceeds 10% of free system memory.
Killed

```
```
</details>"
59174,Segfault when running gen_math_ops.sobol_sample with large input argument,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault with very large input parameter
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  arg_0 = 5
  arg_1 = 125091515651
  arg_2 = 0
  dtype = None
  out = gen_math_ops.sobol_sample(arg_0,arg_1,arg_2,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-08 20:32:55.314315: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 10749281340 exceeds 10% of free system memory.
Segmentation fault

```
```
</details>"
59173,Segmentation fault when running gen_data_flow_ops.dynamic_stitch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault probably due to dimension mismatch.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_1_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int32)
      arg_0_1 = tf.identity(arg_0_1_tensor)
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
      arg_1_0 = tf.identity(arg_1_0_tensor)
      arg_1_1_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int32)
      arg_1_1 = tf.identity(arg_1_1_tensor)
      arg_1 = [arg_1_0,arg_1_1,]
      out = gen_data_flow_ops.dynamic_stitch(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_0 = tf.cast(arg_0_0, tf.int32)
      arg_0_1 = tf.identity(arg_0_1_tensor)
      arg_0_1 = tf.cast(arg_0_1, tf.int32)
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = tf.identity(arg_1_0_tensor)
      arg_1_0 = tf.cast(arg_1_0, tf.int32)
      arg_1_1 = tf.identity(arg_1_1_tensor)
      arg_1_1 = tf.cast(arg_1_1, tf.int32)
      arg_1 = [arg_1_0,arg_1_1,]
      gen_data_flow_ops.dynamic_stitch(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-08 19:41:09.950654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:09.955250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:09.955367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:09.955670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-08 19:41:09.956097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:09.956203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:09.956296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:10.309982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:10.310131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:10.310228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-08 19:41:10.310311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4220 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:{{function_node __wrapped__DynamicStitch_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] is out of range [Op:DynamicStitch]
Segmentation fault
```
```
</details>"
59172,Segmentation fault when running gen_sparse_ops.sparse_cross,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segfault when running with the following input arguments.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices = []
  values_0_tensor = tf.convert_to_tensor(np.ones([3], dtype=str))
  values_0 = tf.identity(values_0_tensor)
  values_1_tensor = tf.convert_to_tensor(np.ones([1], dtype=str))
  values_1 = tf.identity(values_1_tensor)
  values_2_tensor = tf.convert_to_tensor(np.ones([2], dtype=str))
  values_2 = tf.identity(values_2_tensor)
  values = [values_0,values_1,values_2,]
  shapes_0_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_0 = tf.identity(shapes_0_tensor)
  shapes_1_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_1 = tf.identity(shapes_1_tensor)
  shapes_2_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  shapes_2 = tf.identity(shapes_2_tensor)
  shapes = [shapes_0,shapes_1,shapes_2,]
  dense_inputs = []
  hashed_output = True
  num_buckets = 1000
  hash_key = 956888297470
  out_type = tf.int64
  internal_type = tf.string
  out = gen_sparse_ops.sparse_cross(indices=indices,values=values,shapes=shapes,dense_inputs=dense_inputs,hashed_output=hashed_output,num_buckets=num_buckets,hash_key=hash_key,out_type=out_type,internal_type=internal_type,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices = []
  values_0_tensor = tf.convert_to_tensor(np.ones([3], dtype=str))
  values_0 = tf.identity(values_0_tensor)
  values_1_tensor = tf.convert_to_tensor(np.ones([1], dtype=str))
  values_1 = tf.identity(values_1_tensor)
  values_2_tensor = tf.convert_to_tensor(np.ones([2], dtype=str))
  values_2 = tf.identity(values_2_tensor)
  values = [values_0,values_1,values_2,]
  shapes_0_tensor = tf.saturate_cast(tf.constant(-108945022180484, shape=[], dtype=tf.int64,),dtype=tf.uint16)
  shapes_0 = tf.identity(shapes_0_tensor)
  shapes_1_tensor = tf.complex(tf.constant(-47816739039262, shape=[16, 0], dtype=tf.float64), tf.constant(-102167786476375, shape=[16, 0], dtype=tf.float64))
  shapes_1 = tf.identity(shapes_1_tensor)
  shapes_2_tensor = tf.constant(-83109722927677, shape=[], dtype=tf.int32,)
  shapes_2 = tf.identity(shapes_2_tensor)
  shapes = [shapes_0,shapes_1,shapes_2,]
  dense_inputs = []
  hashed_output = True
  num_buckets = 1000
  hash_key = 956888297470
  out_type = tf.int64
  internal_type = tf.string
  out = gen_sparse_ops.sparse_cross(indices=indices,values=values,shapes=shapes,dense_inputs=dense_inputs,hashed_output=hashed_output,num_buckets=num_buckets,hash_key=hash_key,out_type=out_type,internal_type=internal_type,)
except Exception as e:
  print(""Error:""+str(e))
```



### Relevant log output

```shell
Segmentation fault
```
```
</details>"
59171,Crash when running linalg_ops.matrix_solve_ls and gen_linalg_ops.matrix_solve_ls,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Crash when running .matrix_solve_ls
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import linalg_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([16, 8, 127], dtype=tf.float64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([16, 8, 127], dtype=tf.float64)
      arg_1 = tf.identity(arg_1_tensor)
      fast = False
      l2_regularizer = -0.1830422440453151
      out = linalg_ops.matrix_solve_ls(arg_0,arg_1,fast=fast,l2_regularizer=l2_regularizer,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float64)
      linalg_ops.matrix_solve_ls(arg_0,arg_1,fast=fast,l2_regularizer=l2_regularizer,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```

And:

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_linalg_ops
try:
  arg_0_tensor = tf.complex(tf.random.uniform([16, 8, 64], dtype=tf.float64),tf.random.uniform([16, 8, 64], dtype=tf.float64))
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.complex(tf.random.uniform([16, 8, 64], dtype=tf.float64),tf.random.uniform([16, 8, 64], dtype=tf.float64))
  arg_1 = tf.identity(arg_1_tensor)
  arg_2 = -0.24630198634734735
  fast = False
  out = gen_linalg_ops.matrix_solve_ls(arg_0,arg_1,arg_2,fast=fast,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
free(): unaligned chunk detected in tcache 2
Aborted
```
```
</details>"
59170,Process kills when running array_ops.concat ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Process killed when running the api with tensors with different ranks.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_0_tensor = tf.saturate_cast(tf.random.uniform([2147483654], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0_1_tensor = tf.saturate_cast(tf.random.uniform([1024], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_1 = tf.identity(arg_0_1_tensor)
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = 0
  out = array_ops.concat(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_0_tensor = tf.saturate_cast(tf.random.uniform([2147483654], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0_1_tensor = tf.saturate_cast(tf.random.uniform([1024], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_1 = tf.identity(arg_0_1_tensor)
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = nan
  out = array_ops.concat(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```

Also on tf-2.11:

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_0_tensor = tf.saturate_cast(tf.random.uniform([2147483654], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0_1_tensor = tf.saturate_cast(tf.random.uniform([1024], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_1 = tf.identity(arg_0_1_tensor)
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = nan
  out = array_ops.concat(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2023-01-08 19:27:23.166350: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 17179869232 exceeds 10% of free system memory.
Killed

```
```
</details>"
59169,Check failure when running array_ops.quantize_and_dequantize_v2 probably due to dimension mismatch.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running .quantize_and_dequantize_v2 probably due to dimension mismatch.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([2, 3, 4], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -1
  arg_2 = -1
  range_given = True
  round_mode = ""nan""
  axis = None
  out = array_ops.quantize_and_dequantize_v2(arg_0,arg_1,arg_2,range_given=range_given,round_mode=round_mode,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Running on Crash oracle!
2023-01-08 19:24:06.219135: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted

```
```
</details>"
59168,Process killed when running array_ops.ones,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Processes killed.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0 = 2147483653
      arg_0 = [arg_0_0,]
      dtype = tf.int32
      out = array_ops.ones(arg_0,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = [arg_0_0,]
      dtype = tf.int32
      array_ops.ones(arg_0,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-08 19:05:30.504391: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8589934612 exceeds 10% of free system memory.
Running on Crash oracle!
2023-01-08 19:05:31.540972: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8589934612 exceeds 10% of free system memory.
2023-01-08 19:05:32.247688: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 8589934612 exceeds 10% of free system memory.
Killed

```
```
</details>"
59167, traied model with tensorflow on transformer pipeline pop out error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

i’m using this github text summarization and I have a problem. I have been struggling for two week and I could not figure that out.
im using a notebook from this github repository:
https://github.com/flogothetis/Abstractive-Summarization-T5-Keras

notebook link:
https://github.com/flogothetis/Abstractive-Summarization-T5-Keras/blob/main/AbstractiveSummarizationT5.ipynb


after train model i wanna use huggingface transformer pipe line to generate summerization
**from transformers import pipeline
summarizer = pipeline(“summarization”, model=model, tokenizer=“t5-small”, framework=“tf”)
summarizer(“some text”)**

but it pop out an error:

**AttributeError: ‘Functional’ object has no attribute 'config’**

Anyone has any idea how can i solve it?

full error:
AttributeError Traceback (most recent call last)
/tmp/ipykernel_20/1872405895.py in
----> 1 summarizer = pipeline(“summarization”, model=model, tokenizer=“t5-small”, framework=“tf”)
2
3 summarizer(“The US has passed the peak on new coronavirus cases, President Donald Trump said and predicted that some states would reopen”)

/opt/conda/lib/python3.7/site-packages/transformers/pipelines/init.py in pipeline(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
432 break
433
→ 434 return task_class(model=model, tokenizer=tokenizer, modelcard=modelcard, framework=framework, task=task, **kwargs)

/opt/conda/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py in init(self, *args, **kwargs)
37
38 def init(self, *args, **kwargs):
—> 39 super().init(*args, **kwargs)
40
41 self.check_model_type(

/opt/conda/lib/python3.7/site-packages/transformers/pipelines/base.py in init(self, model, tokenizer, modelcard, framework, task, args_parser, device, binary_output)
548
549 # Update config with task specific parameters
→ 550 task_specific_params = self.model.config.task_specific_params
551 if task_specific_params is not None and task in task_specific_params:
552 self.model.config.update(task_specific_params.get(task))

AttributeError: ‘Functional’ object has no attribute 'config’
```


### Standalone code to reproduce the issue

```shell
summarizer = pipeline(“summarization”, model=model, tokenizer=“t5-small”, framework=“tf”)
summarizer(“some text”)

but it pop out an error:

AttributeError: ‘Functional’ object has no attribute 'config’
```


### Relevant log output

_No response_</details>"
59166,plot_model() got an unexpected keyword argument 'show_layer_activations',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.6.4

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
plot_model not working according to the documentation mentioned. In Jupyer Notebook of Kaggle.
```


### Standalone code to reproduce the issue

```shell
n=3
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,Input
from tensorflow.keras.callbacks import EarlyStopping

e=EarlyStopping(patience=9,restore_best_weights=True,verbose=1)

model=Sequential()

#Input Layer
model.add(Input(shape=(X_train.shape[1],)))

#Hidden Layer
for counter in range(1,n+1):
    model.add(Dense(n*X_train.shape[1],activation='relu'))
#     if(counter%4==0):
#         model.add(Dropout(0.75))

#Output Layer
model.add(Dense(1))

model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics = ['mean_absolute_error',tf.keras.metrics.RootMeanSquaredError()])

model. Summary()

# from tensorflow.keras.utils import plot_model
tf.keras.utils.plot_model(model, to_file='model.png',show_shapes=True,show_dtype=True,show_layer_activations=True,show_layer_names=True,rankdir='LR')
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_23/976071416.py in <module>
      1 # from tensorflow.keras.utils import plot_model
----> 2 tf.keras.utils.plot_model(model, to_file='model.png',show_shapes=True,show_dtype=True,show_layer_activations=True,show_layer_names=True,rankdir='LR')

TypeError: plot_model() got an unexpected keyword argument 'show_layer_activations'
```
</details>"
59165,BUILD:tensorflow/compiler/mlir/quantization/tensorflow/debugging/mlir_dump.cc:93:10: error: could not convert 'dump_file' from 'std::unique_ptr<llvm::raw_fd_ostream>' to 'absl::lts_20220623::StatusOr<std::unique_ptr<llvm::raw_fd_ostream> >',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
configure:
# ./configure
You have bazel 5.3.0 installed.
Please specify the location of python. [Default is /usr/local/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3.6/dist-packages
  /usr/local/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.6/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```


### Standalone code to reproduce the issue

```shell
Build success.
```


### Relevant log output

```shell
# bazel build  //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=237
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.6/dist-packages --python_path=/usr/local/bin/python3 --action_env PYTHONPATH=/usr/lib/python3.6/dist-packages
INFO: Reading rc options for 'build' from /home/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/5a3ff2087ab590e6ac9c839c9dc43e520891b7de.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/e10e936315410abd222eb58911b1e20fbfa80baf.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/ruy/archive/3286a34cc8de6149ac6844107dfdffac91531e72.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/e2aa7fe97cd09f44d864079c4e8be98064e5b425.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/a50369c0fdd15f0f35b1a91c964644327a88d480.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://golang.org/dl/?mode=json&include=all failed: class java.io.IOException connect timed out
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/cython/cython/archive/3.0.0a11.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (579 packages loaded, 32070 targets configured).
INFO: Found 1 target...
ERROR: /home/tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/debugging/BUILD:11:11: Compiling tensorflow/compiler/mlir/quantization/tensorflow/debugging/mlir_dump.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 159 arguments skipped)
tensorflow/compiler/mlir/quantization/tensorflow/debugging/mlir_dump.cc: In function 'absl::lts_20220623::StatusOr<std::unique_ptr<llvm::raw_fd_ostream> > tensorflow::quantization::{anonymous}::CreateMlirDumpFile(absl::lts_20220623::string_view)':
tensorflow/compiler/mlir/quantization/tensorflow/debugging/mlir_dump.cc:93:10: error: could not convert 'dump_file' from 'std::unique_ptr<llvm::raw_fd_ostream>' to 'absl::lts_20220623::StatusOr<std::unique_ptr<llvm::raw_fd_ostream> >'
   return dump_file;
          ^~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 298.379s, Critical Path: 51.14s
INFO: 8290 processes: 1584 internal, 6706 local.
FAILED: Build did NOT complete successfully
```
</details>"
59163,Segmentation fault when running gen_nn_ops.fractional_avg_pool,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
segfault happens with negative list elements.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([5, 20, 30, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = -5.267949192431123
  arg_1_2 = -52.58578643762691
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 341261001
  out = gen_nn_ops.fractional_avg_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 10, 10, 1], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = True
  arg_1_1 = -0.35668935305391647
  arg_1_2 = -0.7209753581353426
  arg_1_3 = -87
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  deterministic = True
  seed = 87654321
  seed2 = 341261001
  out = gen_nn_ops.fractional_avg_pool(arg_0,arg_1,arg_2,arg_3,deterministic=deterministic,seed=seed,seed2=seed2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-07 13:44:10.489552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.493914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.494017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.494307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-07 13:44:10.494924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.495025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.495113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.840688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.840834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.840928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:44:10.841010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4263 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Fractional average pooling is not yet supported on the batch nor channel dimension. [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Both seed and seed2 should be 0 if deterministic is false. [Op:FractionalAvgPool]
Error:Expected bool for argument 'pseudo_random' not -69.0.
Error:Value for attr 'T' of uint32 is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Segmentation fault

```
```
</details>"
59162,Check failure when running tensorflow.python.ops.gen_experimental_dataset_ops.thread_pool_handle,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure with the following input combination.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_experimental_dataset_ops
try:
  num_threads = 0
  max_intra_op_parallelism = 1
  display_name = """"
  shared_name = ""same""
  out = gen_experimental_dataset_ops.thread_pool_handle(num_threads=num_threads,max_intra_op_parallelism=max_intra_op_parallelism,display_name=display_name,shared_name=shared_name,)
except Exception as e:
  print(""Error:""+str(e))
```

Also on tf-2.10:

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_experimental_dataset_ops
try:
  num_threads = False
  max_intra_op_parallelism = 1
  display_name = ""private_thread_pool_2""
  shared_name = ""privatethreadpool0""
  out = gen_experimental_dataset_ops.thread_pool_handle(num_threads=num_threads,max_intra_op_parallelism=max_intra_op_parallelism,display_name=display_name,shared_name=shared_name,)
except Exception as e:
  print(""Error:""+str(e))
```



### Relevant log output

```
2023-01-08 21:26:31.509907: F tensorflow/tsl/platform/threadpool.cc:100] Check failed: num_threads >= 1 (1 vs. 0)
Aborted

```

</details>"
59161,Illegal memory access when running math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Illegal memory access when running with the following input combination.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import math_ops
try:
  data_tensor = tf.random.uniform([10, 4], dtype=tf.float32)
  data = tf.identity(data_tensor)
  indices_0 = 8
  indices_1 = 3
  indices_2 = 0
  indices_3 = 9
  indices = [indices_0,indices_1,indices_2,indices_3,]
  segment_ids_0 = 1
  segment_ids_1 = 2
  segment_ids_2 = 2
  segment_ids_3 = 2
  segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
  out = math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-07 13:35:27.718173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:27.722459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:27.722561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:27.722861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-07 13:35:27.723830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:27.723935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:27.724027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:28.065156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:28.065293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:28.065386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 13:35:28.065467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4268 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Bad: indices[0] == -1 out of range [0, 10) [Op:SparseSegmentSum]
Error:Value for attr 'Tsegmentids' of float is not in the list of allowed values: int32, int64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:Value for attr 'T' of complex64 is not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} segment ids must be >= 0 [Op:SparseSegmentSum]
Error:Value for attr 'Tsegmentids' of float is not in the list of allowed values: int32, int64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:Can't convert Python sequence with mixed types to Tensor.
Error:Value for attr 'Tsegmentids' of float is not in the list of allowed values: int32, int64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:Value for attr 'T' of complex128 is not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:Value for attr 'Tidx' of float is not in the list of allowed values: int32, int64
	; NodeDef: {{node SparseSegmentSum}}; Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} segment ids must be >= 0 [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} segment ids must be >= 0 [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Bad: indices[2] == -2 out of range [0, 10) [Op:SparseSegmentSum]
Error:{{function_node __wrapped__SparseSegmentSum_device_/job:localhost/replica:0/task:0/device:GPU:0}} segment ids must be >= 0 [Op:SparseSegmentSum]
2023-01-07 13:35:28.213370: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-07 13:35:28.213399: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59160,Check failure when running gen_stateless_random_ops.stateless_random_binomial,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I get check failure with the following input combination.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_stateless_random_ops
try:
  shape_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int64)
  shape = tf.identity(shape_tensor)
  seed_0 = 12
  seed_1 = 34
  seed = [seed_0,seed_1,]
  counts_tensor = tf.random.uniform([], dtype=tf.float32)
  counts = tf.identity(counts_tensor)
  probs_tensor = tf.random.uniform([], dtype=tf.float32)
  probs = tf.identity(probs_tensor)
  dtype = tf.float16
  out = gen_stateless_random_ops.stateless_random_binomial(shape=shape,seed=seed,counts=counts,probs=probs,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  shape_0 = 125091515651
  shape = [shape_0,]
  seed_0 = 12
  seed_1 = 34
  seed = [seed_0,seed_1,]
  counts = 10.0
  probs = 0.4
  output_dtype = tf.float16
  out = stateless_random_ops.stateless_random_binomial(shape=shape,seed=seed,counts=counts,probs=probs,output_dtype=output_dtype,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Error:{{function_node __wrapped__StatelessRandomBinomial_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dimension -144 must be >= 0 [Op:StatelessRandomBinomial]
2023-01-07 13:26:54.833378: F tensorflow/core/framework/tensor.cc:719] Check failed: dtype() == expected_dtype (9 vs. 3) int32 expected, got int64
Aborted

```
```
</details>"
59159, cannot import name 'build_info' from 'tensorflow.python.platform',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

windows 11 22h2

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

no

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

mx150 2gb vram

### Current Behaviour?

```shell
cannot import name 'build_info' from 'tensorflow.python.platform'
```


### Standalone code to reproduce the issue

```shell
cannot import name 'build_info' from 'tensorflow.python.platform'
import tensorflow as tf
from object_detection.utils import config_util
from object_detection.protos import pipeline_pb2
from google.protobuf import text_format
```


### Relevant log output

_No response_</details>"
59157,BUILD：ERROR: Config value 'download_clang' is not defined in any .rc file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

bazel 5.3.0

### GCC/Compiler version

6.0.0-1ubuntu2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
the same as #38491 (https://github.com/tensorflow/tensorflow/issues/38491).
```


### Standalone code to reproduce the issue

```shell
# bazel build //tensorflow/tools/pip_package:build_pip_package
ERROR: Config value 'download_clang' is not defined in any .rc file
```


### Relevant log output

```shell
A BUILD bug happened!

# ./configure
You have bazel 5.3.0 installed.
Please specify the location of python. [Default is /root/anaconda3/bin/python3]: 


Found possible Python library paths:
  /root/anaconda3/lib/python3.9/site-packages
  /usr/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/root/anaconda3/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: y
Clang will be downloaded and used to compile tensorflow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

# bazel build //tensorflow/tools/pip_package:build_pip_package
ERROR: Config value 'download_clang' is not defined in any .rc file
```
</details>"
59156,[Documentation] raw_ops.Conv3D: strides cannot be a list of ints that has length >= 5,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS WSL2

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation for raw_ops.Conv3D states that strides must be a list of ints that has length >= 5, which is immediately followed by another statement stating that strides must be a 1-D tensor of length 5. Both of these statements are partly contradictory to each other since strides cannot be a list that has length > 5 but instead a list that has length == 5. The latter statement is the correct one while the former one should either be removed from the documentation due to redundancy/false information or corrected if kept.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x_in = np.random.randn(1,3,3,3,1)
kernel_in = np.random.randn(3,3,3,1,1)
x = tf.constant(x_in, dtype=tf.float32)
kernel = tf.constant(kernel_in, dtype=tf.float32)

tf.raw_ops.Conv3D(input=x, filter=kernel, strides=[1,1,1,1,1,1], padding=""SAME"", data_format=""NDHWC"", dilations=[1,1,1,1,1])
```


### Relevant log output

```shell
InvalidArgumentError: {{function_node __wrapped__Conv3D_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window strides field must specify 5 dimensions [Op:Conv3D]
```
</details>"
59136,Elaborate on the support of custom_objects in Saved Model for TFX Serving," ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a tensorflow keras model with lots of custom objects, they are all available in `tf.keras.utils.get_custom_objects()`. I am saving and loading the model without any problem using `model.save(path, save_traces=False)` and `tf.keras.models.load_model(path)`, where `model` is a child class of `tf.keras.Model` and it works fine. 
But when I use `tf.saved_model.save(model, path)`, and then load using `tf.saved_model.load(path)`, the custom class is not loaded.

I somewhat understand this might not be supported, but it would be great to have this explicitly stated in the docs.

I tried to dig into differences between the different ways of saving and loading the model, but I'm confused from the docs, the `tf.keras.models.save_model` https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model state that it's using SavedModel format, and when I explicitly state that the `save_format=""tf""` should be used, the loading using `tf.keras.models.load_model` properly loads my custom class. But not when using the `saved_model_load`

In the https://github.com/tensorflow/tensorflow/issues/37439#issuecomment-600290097 I found a mention that keras types can be saved, to the SavedModel format, but I have no clue about the compatibility of these things with TFX Serving etc, which would be great to elaborate more. Does the TFX serving require traced model? Is there a way how to use custom objects in the TFX serving?

### Standalone code to reproduce the issue

any code using custom C++ and python wrappers in tf.keras layers which we add as custom objects, e.g.
```python
>>> model =  <some custom child of tf.keras.Model>
>>> tf.saved_model.save(raw_inference_model, 'out_path/gcs_model')
>>> raw_inference_model2 = tf.saved_model.load('out_path/gcs_model')
>>> raw_inference_model2
<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x7fcdbb657310>

>>> raw_inference_model.save('out_path/gcs_model_k', save_traces=False, save_format='tf')
>>> raw_inference_model3 = keras.models.load_model('out_path/gcs_model_k')
>>> raw_inference_model3
<wannamodel.model.WannaModel at 0x7fcdb82b80d0>
```

### Relevant log output

_No response_"
59135,Segmentation fault when running ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segmentation fault
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = 3
  arg_1_2 = 2
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  seed = 341261001
  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -978 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -39.2679482 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -7 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -1 [Op:FractionalAvgPool]
Error:Value for attr 'T' of bool is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -1 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -23.2679501 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -1 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -21.5857868 [Op:FractionalAvgPool]
Error:Expected bool for argument 'overlapping' not 'zeros'.
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:Value for attr 'T' of complex128 is not in the list of allowed values: float, double, int32, int64
	; NodeDef: {{node FractionalAvgPool}}; Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]> [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -26.5857868 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -58.5857849 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -63 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -1 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: 0 [Op:FractionalAvgPool]
Error:{{function_node __wrapped__FractionalAvgPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} pooling_ratio cannot be smaller than 1, got: -1 [Op:FractionalAvgPool]
Error:Expected bool for argument 'overlapping' not ''.
Error:Expected float for argument 'pooling_ratio' not ''.
Segmentation fault

```
```
</details>"
59134,Check failure when running tensorflow.python.ops.gen_array_ops.matrix_diag_part_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  input_tensor = tf.saturate_cast(tf.random.uniform([20, 20], minval=0, maxval=257, dtype=tf.int64), dtype=tf.uint64)
  input = tf.identity(input_tensor)
  k = -2
  padding_value = -2
  align = ""RIGHT_LEFT""
  out = gen_array_ops.matrix_diag_part_v3(input=input,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagPartV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} input must be at least 2-dim, received shape: [0] [Op:MatrixDiagPartV3]
Error:Cannot convert 44.0 to EagerTensor of dtype int32
Error:can't convert negative int to unsigned
Error:Value for attr 'align' of ""sum"" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Value for attr 'align' of """" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Value for attr 'align' of ""zeros"" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Value for attr 'align' of """" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Value for attr 'align' of ""valid"" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Cannot convert '' to EagerTensor of dtype int32
Error:{{function_node __wrapped__MatrixDiagPartV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} lower_diag_index is out of bound: 1024. It must be between -20 and 20 [Op:MatrixDiagPartV3]
Error:{{function_node __wrapped__MatrixDiagPartV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} input must be at least 2-dim, received shape: [20] [Op:MatrixDiagPartV3]
Error:Expected string for argument 'align' not 1.
Error:Value for attr 'align' of """" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Cannot convert 0 to EagerTensor of dtype bool
Error:{{function_node __wrapped__MatrixDiagPartV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} lower_diag_index is out of bound: -1024. It must be between -20 and 20 [Op:MatrixDiagPartV3]
Error:Cannot convert '' to EagerTensor of dtype int32
Error:Value for attr 'align' of ""valid"" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
Error:Expected string for argument 'align' not 1.
Error:{{function_node __wrapped__MatrixDiagPartV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} lower_diag_index is out of bound: 1. It must be between -20 and 1 [Op:MatrixDiagPartV3]
Error:Value for attr 'align' of ""valid"" is not in the list of allowed values: ""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""
	; NodeDef: {{node MatrixDiagPartV3}}; Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default=""RIGHT_LEFT"",allowed=[""LEFT_RIGHT"", ""RIGHT_LEFT"", ""LEFT_LEFT"", ""RIGHT_RIGHT""]> [Op:MatrixDiagPartV3]
2023-01-06 09:23:01.094003: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted

```
```
</details>"
59133,Process kills when running tensorflow.python.ops.array_ops.concat,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Process get killed.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_0_tensor = tf.saturate_cast(tf.random.uniform([2147483654], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0_1_tensor = tf.saturate_cast(tf.random.uniform([1024], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0_1 = tf.identity(arg_0_1_tensor)
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = 0
  out = array_ops.concat(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1024 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2]
Error:cannot compute ConcatV2 as input #1(zero-based) was expected to be a uint16 tensor but is a double tensor [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got -4 [Op:ConcatV2]
Error:cannot compute ConcatV2 as input #1(zero-based) was expected to be a uint8 tensor but is a uint32 tensor [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 16 [Op:ConcatV2] name: concat
Error:OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: ""ConcatV2"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QINT32 } } } host_memory_arg: ""axis""' [Op:ConcatV2] name: concat
Error:Cannot convert 44.0 to EagerTensor of dtype int32
Error:Tensor conversion requested dtype int8 for Tensor with dtype uint16: <tf.Tensor: shape=(0,), dtype=uint16, numpy=array([], dtype=uint16)>
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2]
Error:Tensor conversion requested dtype float32 for Tensor with dtype bool: <tf.Tensor: shape=(0,), dtype=bool, numpy=array([], dtype=bool)>
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 2 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-3, 3), but got -4 [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [1] vs. shape[1] = [3,0] [Op:ConcatV2]
Error:cannot compute ConcatV2 as input #1(zero-based) was expected to be a double tensor but is a int64 tensor [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2]
Error:cannot compute ConcatV2 as input #1(zero-based) was expected to be a double tensor but is a bfloat16 tensor [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got -2 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [6,2] vs. shape[1] = [6,3] [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got -2 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 16 [Op:ConcatV2]
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-4, 4), but got -16 [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [2,0] vs. shape[1] = [2,1] [Op:ConcatV2] name: concat
Error:cannot compute ConcatV2 as input #1(zero-based) was expected to be a half tensor but is a uint16 tensor [Op:ConcatV2] name: concat
Error:{{function_node __wrapped__ConcatV2_N_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat
Killed

```
```
</details>"
59131,Check failure when running tensorflow.python.ops.gen_array_ops.quantize_and_dequantize_v4,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running .python.ops.gen_array_ops.quantize_and_dequantize_v4
```


### Standalone code to reproduce the issue

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([2, 3, 4], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  arg_0 = tf.identity(arg_0_tensor)
  input_min = -1
  input_max = -1
  signed_input = False
  num_bits = 7
  range_given = True
  round_mode = ""nan""
  narrow_range = True
  axis = -2
  out = gen_array_ops.quantize_and_dequantize_v4(arg_0,input_min=input_min,input_max=input_max,signed_input=signed_input,num_bits=num_bits,range_given=range_given,round_mode=round_mode,narrow_range=narrow_range,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```

### Relevant log output

```shell
023-01-06 09:14:31.245795: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted

```
```
</details>"
59130,Segfault on tensorflow.python.ops.bincount_ops.bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Segfault
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import bincount_ops

try:
    arg_0_tensor = tf.random.uniform([3, 1], minval=-256, maxval=257, dtype=tf.int32)
    arg_0 = tf.identity(arg_0_tensor)
    weights = None
    minlength = 2
    maxlength = 0
    dtype = ""float32""
    axis = -1
    binary_output = True
    out = bincount_ops.bincount(
        arg_0,
        weights=weights,
        minlength=minlength,
        maxlength=maxlength,
        dtype=dtype,
        axis=axis,
        binary_output=binary_output,
    )
except Exception as e:
    print(""Error:"" + str(e))

```
```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import bincount_ops
try:
  arg_0_tensor = tf.random.uniform([3, 1], minval=-256, maxval=257, dtype=tf.int32)
  arg_0 = tf.identity(arg_0_tensor)
  weights = None
  minlength = 4
  maxlength = 0
  dtype = ""float32""
  axis = -1
  binary_output = True
  out = bincount_ops.bincount(arg_0,weights=weights,minlength=minlength,maxlength=maxlength,dtype=dtype,axis=axis,binary_output=binary_output,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Error:maxlength: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.5696368>
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor: shape=(10000,), dtype=int64, numpy=array([-123,  137,  225, ..., -163,  -81,   75])>
Error:Unsupported value for argument axis=-3. Only 0 and -1 are currently supported.
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:{{function_node __wrapped__Bincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:Bincount]
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int8: <tf.Tensor: shape=(10000,), dtype=int8, numpy=array([105, -62, 101, ..., 105, -43, -41], dtype=int8)>
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor: shape=(10000,), dtype=int64, numpy=array([-159, -131,  160, ...,  169,   71,  -14])>
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype bool: <tf.Tensor: shape=(), dtype=bool, numpy=False>
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor: shape=(10000,), dtype=int64, numpy=array([ 162,  215, -202, ...,  -21,  -92,  236])>
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:{{function_node __wrapped__Bincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:Bincount]
Error:{{function_node __wrapped__Bincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:Bincount]
Error:{{function_node __wrapped__Bincount_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input arr must be non-negative! [Op:Bincount]
Error:Value for attr 'T' of uint32 is not in the list of allowed values: int32, int64, float, double
	; NodeDef: {{node Bincount}}; Op<name=Bincount; signature=arr:int32, size:int32, weights:T -> bins:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]> [Op:Bincount]
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor: shape=(10000,), dtype=int64, numpy=array([-98, 135, 191, ..., 129, 218, 106])>
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:arr: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor: shape=(1000,), dtype=int64, numpy=
array([ -60,  157,  215,   79,  195,   18, -112,  -82, -116,   31,  220,
        218,  175,   -7, -152,  -37, -190, -253,   58,   63,  128,  117,
       -189,   -4,  -97,    9,  237,  -33, -235, -211,    2, -244,  104,
          5, -158,  144,  165,  -19,  -42,  234,  -65, -207,  -15,   81,
        206,  135, -119,   97,  173,  254,  254,  122, -175,  229, -256,
         61,  204,  -32, -213, -238,  -23, -221,  -60, -229,  243,   -9,
         50, -243,  -34,  103, -233,  183,  171,  228, -122,  234,   94,
        161, -255,  184,  -84,  215,  -62,   69,  202,   64, -159, -256,
       -133,  127, -133,  135, -105,  129,  114,   49, -217, -209,  133,
        252,   25,   66,   24,  208, -222, -206,  -84,   58,   55, -130,
         42,  -23,   94,   68,  119,  -53, -191, -146,  129,   98,  -74,
        172,  143,  -93,  121,  175,  196, -245,  -79,   26,   46,  132,
        -37,  141,  118, -160,  -74,  108,  -62, -241,  130, -173,  241,
         71,  242,   34,  237,   -4, -230, -122, -221, -185,  246,   39,
        -58,   42,    2, -252,  108, -246, -135,  211,   66,   18,  196,
       -146,  172,  -87,  119,  -91, -137, -120,  146, -139,  116,  -25,
       -244,   19,   52, -241,  229,  177, -187,  -89, -189,   41, -175,
         31,  183,  191,   64,  113,   68,  114, -186,  -73, -115,  -38,
       -248, -228,   51,   -1,   60,  150,  124,  169,  -21,  202,  -71,
        112,  192,  182,   34,  -95,  152,   58,  179,   23,  133,   38,
        218, -154,   39,  -70,  -57, -230, -164,   14,  199, -182,  101,
       -241,  -91, -133,  -13,  240,   76,  142,  -16,    3,   22,  249,
       -197, -101,   -1,  155, -184,  178, -146,   92,  -70, -154, -145,
       -112, -135, -233,  -95,   -7, -204,  -99,  149, -201,  -94,  -15,
         89,   31,  127,  -14, -145, -191,  -11,   23,  -73,  238,  -11,
        -93,  199,  145,  119,   18,  114,  240,  118, -101,   -6,   63,
        -52,  -50,   69,  -77,  -16,  -91, -245,  -98,  -72,  -27,   46,
        241,   11,  239,  212,  252, -124, -213,  -19,  104, -203,   70,
        147,   94,  -28,   99, -129,   20,  -66,  175,  -88,  244,   35,
        117, -136,  -14,   38, -216,   15,   69,  -50, -206,  -95, -203,
          9,  -15,  -40,  183,  176,  210,  100,   73,  -13,  -76,  -52,
       -249, -131,  255,  118,  175, -166, -211,   20,  211,  148,  164,
        249,  252,  -67,  164, -240, -174,  184,  -64, -223,   35,   41,
       -151,   57, -140,   86,   93, -206, -188, -126,  117,  -50, -246,
        -18, -251, -145,   79, -193, -120,  -50,  -32,  118, -215,  -55,
         -9,   62,  190,  -61,   -3,  231,  133, -226,   11, -205, -155,
       -115,  125,  239, -186,   50,  179,  217,   48,  214,  171,  246,
       -243,  201,   50,  180,  -35,   52,    1, -215,  183, -247, -138,
        160,  249,  101,  150,  -99,   43, -210, -180,  -44,  -12,   59,
        124,   27, -247,   27,  -19,  -12, -172, -165,  190,  231,   52,
        206,  170,  236, -132,  -54,  163, -188,  235,  -64,  101,  227,
       -188,   54, -119,  251, -199,  249,   26, -237, -170,   29,   87,
        183, -139, -160, -119,  130,  195, -148,  -85,  217,   53, -186,
        243, -239,  132,  242,  -56,  118,  102, -173,  209,  -79,  -80,
        224,  -92,  143,  127, -117,   72,  -81, -159,  110,  -45,    5,
        -97,  154,  123,  -36, -144,  198, -119, -187,   37,  178,   16,
        197,  240,   31,   26,  -85,   60, -226, -145,  -31,  221, -181,
        193,  -10, -112, -197, -203,  166, -111,  -93, -189, -131, -226,
        155, -124, -136,   92,   97,  124,   52,  209,   21,  -81,   12,
        -71, -221,   54, -183,   96,   87, -202,  135,  -83,  233,   56,
        256,  -27,  -69,  -42,  242, -151,  -61,  209,  194, -137, -211,
        197,   33,   24,   95,    1,   49, -168,  220, -188,  205,  206,
        214,   59,  -55,  131, -182,  108,  211,   15,   -4,   82,  -86,
       -135,   67,  -44,  141,  -52, -205,   90,  -32, -128, -112,  213,
       -181, -134,   13,  195,  248,  -81,   59, -148,  -83,  132,   -7,
        158,   20,  202,  154, -147, -155,    8,  -65,   59,  229,  -16,
       -221, -227, -130, -104,  -15, -194,   53,   80,   33,  128,  165,
        153, -151,  -83, -188,   72, -104,   15,  -62,  -31,  213,   -4,
        156, -170,   12,  -70,  131,  154, -126, -230,   16,  195, -178,
       -252,    0,   20,  -19,  192, -181,   85,  168,  195, -164,  156,
       -176,  -81, -126, -123,   96,   30,   79,  247,  -27,   54,  120,
        100,  126,  108, -117,  122,   78, -108,  226,   77,   10,   53,
       -208,  -35, -239, -149,  -84,   14,   98, -179,   75,  172,  -43,
       -209,  -36,  239, -118, -232, -200, -230, -167,   -9, -224,  195,
        -48,  224,  152,  -20,   -8,  112,  -89,  159,   47,  -27, -212,
         51,  169,  121,  192, -236, -254, -252, -240,   95,  217,  -17,
        -88,   70,  191,   59, -185,  234,   71,   83,  -61, -147,  176,
        -61,   28,   44, -225,    8, -186,   77,  103, -186, -243,   72,
         54,    0,  210,   88,   52, -182,  247, -181,   31,  164,   25,
       -144,   54,  -57,  203,   15,   42,  207,  -89,   12, -109,   28,
       -242,  110,  170,  -25,  182,   20,   73, -135, -104,  221, -146,
       -199, -186, -231,  -28,  -68,    9,  158,   57,   78, -116, -167,
         -1,  117, -250, -118,   19,  171,   15,  -73, -102,  137,  178,
        166,  255,  -59,  208,  -77, -217,  239,  -67,   84, -167,  194,
        161,  222,  -97, -193,  248, -145,   25, -139,  -35,  226,   21,
         91,  -98,  189,  -96,  -14, -214,  218,  236,  -32, -156,   49,
       -108,   81,   -7, -160,   74,   17, -221, -160,  130,   39,   97,
       -108,   55,  -23, -104,  191, -216, -175,  134,  109,   43, -102,
       -166,   73, -142,  240,   78, -112,  145,   67,   74, -207,  180,
        100,  163, -148, -202, -122,   55,   70, -142, -197,  -87, -254,
        -62, -129,  -65, -106, -204,  140,   30,  160,  -13, -167, -103,
        250,  -57, -123,  169,  -64, -183,   19,  -76, -207, -153, -128,
        111,  -85,  -58, -148, -104,   91,  183,  212,  126,  122,   60,
       -207,  -62, -201,  175,  221,  195, -189,  -31, -193, -216,  165,
         36,  240, -202,  156, -237,  -94, -138,  -82, -130,  -73, -145,
         24,   21,  237,   57, -213,  -31,  187,   99, -173, -139,  181,
       -171,  -62,  171,  178, -245,  -75, -235,   20,   -7,  -15,   91,
        199,  -37, -245, -237,  164, -181,  -42,  119,   -1,  114, -200,
        209,  -42,   12,  113,  -31,   31, -162,  191,   83,  217,   70,
         -2, -155,   57,   11,  232,  -89,  163, -243, -113,  194,   95,
       -156,  251,  -76, -154, -225, -126,  234,  148,  -96, -105])>
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Error:Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.
Segmentation fault

```
```
</details>"
59128,Blas SGEMM launch failed issue ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0/10.1

### GPU model and memory

Quadro RTX 6000 24GB

### Current Behaviour?

```shell
Constantly gettin Blas SGEMM launch failed error on a multi-gpu server. 

Tried this 
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""
gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only use the first GPU
  try:
    tf.config.set_visible_devices(gpus[0], 'GPU')
    tf.config.set_memory_growth(physical_devices[0], True)
    logical_gpus = tf.config.list_logical_devices('GPU')
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)
```


### Standalone code to reproduce the issue

```shell
Traceback (most recent call last):
  File ""trainAllPPOs_server.py"", line 209, in <module>
    train()
  File ""trainAllPPOs_server.py"", line 77, in train
    embd = tcn.get_embedding(state, normalize=True)
  File ""/local/home/balbaba/GIRL/TCN.py"", line 50, in get_embedding
    return self.model(inputs/127.5 - 1)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 424, in call
    return self._run_internal_graph(
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 424, in call
    return self._run_internal_graph(
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 248, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1013, in convolution_v2
    return convolution_internal(
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1143, in convolution_internal
    return op(
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 2597, in _conv2d_expanded_batch
    return gen_nn_ops.conv2d(
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 932, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File ""/local/home/balbaba/.virtualenvs/venvgirl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=9216, n=64, k=64 [Op:Conv2D]
```


### Relevant log output

_No response_</details>"
59127,Build tflite_runtime with custom ops,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.7.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi,
Has anybody been able to build(cross-compile) tflite_runtime for aarch64 with custom ops?
I tried running for TF 2.7.2. Ref (https://github.com/tensorflow/tensorflow/tree/r2.7/tensorflow/lite/tools/pip_package)
```


### Standalone code to reproduce the issue

```shell
CI_DOCKER_EXTRA_PARAMS=""-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true""   tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37   tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64
```


### Relevant log output

```shell
2023-01-06 09:48:04 (74.8 MB/s) - 'get-pip.py' saved [2569500/2569500]

/install/install_pip_packages_by_version.sh: line 24: /usr/local/bin/python3.7: No such file or directory
ln: failed to create symbolic link '/usr/include/python3.7/numpy': No such file or directory
The command '/bin/sh -c /install/install_pi_python3x_toolchain.sh ""3.7""' returned a non-zero code: 1
ERROR: docker build failed. Dockerfile is at /home/ubuntu/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python37
```
</details>"
59126,Illegal memory access when running tensorflow.python.ops.gen_sparse_ops.sparse_to_dense,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Illegal memory access with the following input combinations.
```


### Standalone code to reproduce the issue

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0_tensor = tf.random.uniform([0], minval=-256, maxval=257, dtype=tf.int64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int64)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_2 = tf.identity(arg_2_tensor)
  default_value_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int32)
  default_value = tf.identity(default_value_tensor)
  validate_indices = False
  out = gen_sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,default_value=default_value,validate_indices=validate_indices,)
except Exception as e:
  print(""Error:""+str(e))
```

### Relevant log output

```shell
Error:{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [5] [Op:SparseToDense]
2023-01-06 00:56:07.687258: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-06 00:56:07.687278: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59125,Hang when running tensorflow.python.ops.collective_ops.all_gather_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Program hangs when running with very large negative input tensor
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import collective_ops
try:
  arg_0_tensor = tf.constant(-72686848303090, shape=[4], dtype=tf.float32,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_3 = tf.identity(arg_3_tensor)
  communication_hint = ""RING""
  out = collective_ops.all_gather_v2(arg_0,arg_1,arg_2,arg_3,communication_hint=communication_hint,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Hangs
```
</details>"
59124,Check failure when running tensorflow.python.ops.stateless_random_ops.stateless_random_normal from terminal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Probably because of feeding empty tensors.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import stateless_random_ops
try:
  try:
    with tf.device('/CPU'):
      seed_tensor = tf.complex(tf.random.uniform([1], dtype=tf.float32),tf.random.uniform([1], dtype=tf.float32))
      seed = tf.identity(seed_tensor)
      shape_tensor = tf.random.uniform([], dtype=tf.float64)
      shape = tf.identity(shape_tensor)
      dtype = tf.uint64
      mean = -40
      stddev = -39
      out = stateless_random_ops.stateless_random_normal(seed=seed,shape=shape,dtype=dtype,mean=mean,stddev=stddev,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      seed = tf.identity(seed_tensor)
      seed = tf.cast(seed, tf.complex64)
      shape = tf.identity(shape_tensor)
      shape = tf.cast(shape, tf.float64)
      dtype = tf.uint64
      results[""res_gpu""] = stateless_random_ops.stateless_random_normal(seed=seed,shape=shape,dtype=dtype,mean=mean,stddev=stddev,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
2023-01-06 00:39:33.309913: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted

```
```
</details>"
59123,Check failure when running tensorflow.python.ops.gen_data_flow_ops.record_input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running with the following input combination:
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops
try:
  file_pattern = ""/tmp/record_input_test3nvh1t09/tmp3gauzk6b/basic.*""
  file_buffer_size = -1
  file_parallelism = -1
  file_shuffle_shift_ratio = -2
  batch_size = -1
  file_random_seed = -2
  compression_type = """"
  out = gen_data_flow_ops.record_input(file_pattern=file_pattern,file_buffer_size=file_buffer_size,file_parallelism=file_parallelism,file_shuffle_shift_ratio=file_shuffle_shift_ratio,batch_size=batch_size,file_random_seed=file_random_seed,compression_type=compression_type,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-05 22:01:15.270432: F tensorflow/core/platform/threadpool.cc:99] Check failed: num_threads >= 1 (1 vs. 0)
Aborted

```
```
</details>"
59122,Check failure when running tensorflow.python.ops.gen_array_ops.matrix_diag_part_v3 when feeding input empty tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running .python.ops.gen_array_ops.matrix_diag_part_v3 when feeding input empty tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  input_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
  input = tf.identity(input_tensor)
  k = -20
  padding_value = -20
  align = ""RIGHT_LEFT""
  out = gen_array_ops.matrix_diag_part_v3(input=input,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-05 21:40:09.377252: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted
```
```
</details>"
59121,Illegal memory access when running tensorflow.python.ops.gen_sparse_ops.sparse_split,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When .sparse_split is given zero as the first input parameter, I get illegal memory access.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0 = 0
      arg_1_tensor = tf.random.uniform([14, 2], minval=-256, maxval=257, dtype=tf.int64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([14], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4 = 4
      out = gen_sparse_ops.sparse_split(arg_0,arg_1,arg_2,arg_3,arg_4,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.int64)
      results[""res_gpu""] = gen_sparse_ops.sparse_split(arg_0,arg_1,arg_2,arg_3,arg_4,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__SparseSplit_num_split_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} Slice index 60 is larger than num_split. [Op:SparseSplit]
2023-01-05 21:32:38.376771: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-05 21:32:38.377012: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted
```
```
</details>"
59120,Tensorflow lite  memory fault,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  openharmony  aarch64
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): tag v2.3.3


**Provide the text output from tflite_convert**
Each time the program runs to the assembly portion of the NeonMatrixBatchVectorMultiplyAccumulate function in ""tensorflow\lite\kernels\internal\optimized\neon_tensor_utils.cc  "". The program will crash directly and report a memory error
"
59119,Tensorflow for C Windows GPU install is missing,https://www.tensorflow.org/install/lang_c I just wanted to let it be known that the link for the Windows GPU only tensorflow for C install is broken. I understand that the tensorflow C project isn’t as popular or heavily supported but I feel like somethings amiss. Feels like if it wasn't supposed to exist then there would be no indication via a link. Would love to see this fixed if possible
59117,Incorrect INVALID_ARGUMENT error is thrown with `iter(dataset)` call,"
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

Nightly build version: 2.12.0-dev20230105
GIT Version: v1.12.1-87220-gbf3a8ec10be

### Custom Code

Yes

### OS Platform and Distribution

Linux 4799aa259243 5.10.147 x86_64 GNU/Linux

### Mobile device

N/A

### Python version

3.8

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

libcudnn8=8.6.0.163-1+cuda11.8

### GPU model and memory

Tesla T4 15109MiB 

### Current Behaviour?

```shell
When `iter(dataset)` is executed, we run into an error that requires a value for placeholder tensor. Ideally, no value needs to be fed.
```


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='0'

import numpy as np
import tensorflow as tf

INPUT_SIZE = (1, 224, 224, 3)
tf.get_logger().setLevel('INFO')
data = tf.random.uniform(INPUT_SIZE)
dataset = tf.data.Dataset.from_tensor_slices(data)
dataset = dataset.repeat()
dataset = dataset.batch(32)
dataset = dataset.repeat()
dataset = dataset.prefetch(tf.data.AUTOTUNE)

data = next(iter(dataset))
```


### Relevant log output

```shell
2023-01-06 00:14:49.489765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1614] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10904 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5
2023-01-06 00:14:49.517784: I tensorflow/core/common_runtime/executor.cc:1195] [/device:CPU:0] Executor start aborting: INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1,224,224,3]
	 [[{{node Placeholder/_0}}]]
2023-01-06 00:14:49.518018: I tensorflow/core/common_runtime/executor.cc:1195] [/device:CPU:0] Executor start aborting: INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1,224,224,3]
	 [[{{node Placeholder/_0}}]]
```
```
"
59115,TFDefaultLogSink::Send() uses buffered writes leading to delays writing to log file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

N/A

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When setting the `TF_CPP_VLOG_FILENAME` env var it would be great to have all logs  available almost immediately when written by the TF C++ library. At the moment the output file on non-Android platforms is opened with buffering which delays output:
https://github.com/tensorflow/tensorflow/blob/548964d24666f9550e9a40249a917145bf9670fb/tensorflow/tsl/platform/default/logging.cc#L183-L191

I think this works with stderr at the moment as the default behavior of fopen with stderr on most platforms is to flush after each line.
```


### Standalone code to reproduce the issue

```shell
Here's an example using a named pipe (fifo):


rm test_fifo 2>/dev/null;
mkfifo test_fifo;
( TF_CPP_VLOG_FILENAME=$(readlink -f test_fifo) python -c 'import time; import tensorflow; time.sleep(5)' ) &
date;
cat test_fifo | ts;
rm test_fifo;
```


### Relevant log output

```shell
Produces:

[1]+  Done                    ( TF_CPP_VLOG_FILENAME=$(readlink -f test_fifo) python -c 'import time; import tensorflow; time.sleep(5)' )
[1] 20384
Thu Jan  5 14:35:28 PST 2023
Jan 05 14:35:36 2023-01-05 14:35:30.119875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:
Jan 05 14:35:36 2023-01-05 14:35:30.119923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:
Jan 05 14:35:36 2023-01-05 14:35:30.119929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Jan 05 14:35:36 2023-01-05 14:35:29.256014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
Jan 05 14:35:36 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
```

Notice the 5 second gap between the start of the Python process and the TF logs. This is due to the log buffer only being flushed on TF process close here:
https://github.com/tensorflow/tensorflow/blob/548964d24666f9550e9a40249a917145bf9670fb/tensorflow/tsl/platform/default/logging.cc#L193-L197
```
</details>"
59114,Segmentation fault when running gen_ragged_array_ops.ragged_cross,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running .ragged_cross with the following input combination, it results in segfault.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_ragged_array_ops
try:
  ragged_values_0_tensor = tf.convert_to_tensor(np.ones([3], dtype=str))
  ragged_values_0 = tf.identity(ragged_values_0_tensor)
  ragged_values = [ragged_values_0,]
  ragged_row_splits_0_tensor = tf.random.uniform([4], minval=-256, maxval=257, dtype=tf.int64)
  ragged_row_splits_0 = tf.identity(ragged_row_splits_0_tensor)
  ragged_row_splits = [ragged_row_splits_0,]
  sparse_indices = []
  sparse_values = []
  sparse_shape = []
  dense_inputs = []
  input_order = ""R""
  hashed_output = False
  num_buckets = 0
  hash_key = 956888297470
  out_values_type = 7
  out_row_splits_type = 9
  out = gen_ragged_array_ops.ragged_cross(ragged_values=ragged_values,ragged_row_splits=ragged_row_splits,sparse_indices=sparse_indices,sparse_values=sparse_values,sparse_shape=sparse_shape,dense_inputs=dense_inputs,input_order=input_order,hashed_output=hashed_output,num_buckets=num_buckets,hash_key=hash_key,out_values_type=out_values_type,out_row_splits_type=out_row_splits_type,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
The only log message is:


Segmentation fault
```
```
</details>"
59112,benchmark_model elinux_aarch64 build with define CL_DELEGATE_NO_GL fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Linux ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

NA

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This is to build benchmark_model to run on linux embedded system.
This command builds fine:     bazel build -c opt --config elinux_aarch64 --config monolithic - tensorflow/lite/tools/benchmark:benchmark_model

However when run on ARM device, there's error:
The GPU delegate compile options are only supported on Android or iOS platforms or when the tool was built with -DCL_DELEGATE_NO_GL.

So building with this defined:
    bazel build -c opt --config elinux_aarch64 --config monolithic --copt -DCL_DELEGATE_NO_GL=true tensorflow/lite/tools/benchmark:benchmark_model

Got compile error:
ERROR: /workspace/tensorflow/tensorflow/lite/tools/delegates/BUILD:113:11: Compiling tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc failed: undeclared inclusion(s) in rule '//tensorflow/lite/tools/delegates:hexagon_delegate_provider':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc':
  'tensorflow/lite/delegates/gpu/delegate.h'
  'tensorflow/lite/delegates/gpu/delegate_options.h'
In file included from ./tensorflow/lite/tools/evaluation/utils.h:35,
                 from tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc:19:
./tensorflow/lite/delegates/hexagon/hexagon_delegate.h:111:72: warning: 'visibility' attribute ignored on non-class types [-Wattributes]
 TfLiteHexagonDelegateCreate(const TfLiteHexagonDelegateOptions* options);
                                                                        ^

I think this define (CL_DELEGATE_NO_GL) includes new header files which is not in BUILD, for example:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/utils.h
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
59106,gpu is not supported,
59105,i want to reduce the app size with TensorFlowLiteSelectTfOps,"### 1. System information

- ios
- tflite model
- cocoapods
   pod 'TensorFlowLiteObjC', '=2.7.0' 
   pod 'TensorFlowLiteSelectTfOps', '0.0.1-nightly.20210521' 

### 2. Code and Question
with parsing linkmap file ,I found the TensorFlowLiteSelectTfOps take 74 MB, which is too bigger.
when converting the tflite model , my model just use three tensorflow ops:FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack.
who can tell me some ways to reduce the size of app with TensorFlowLiteSelectTfOps
"
59104,"Random error after hundreds of iterations: Aborting RingReduce with Invalid argument: Incompatible shapes: [0,64] vs. [0,256]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tensorflow-rocm 2.2

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

ROCm v3.5

### GPU model and memory

2 RX 480 4Go

### Current Behaviour?

```shell
Hello Tensorflow community,

I randomly encounter this bug after few tens of iterations for no apparent reason. Sometimes after t > 400. 
The ""incompatible shapes"" also differ from execution to execution. It doesn't seems to come from input datas.

I also got those warnings at the execution:
2023-01-05 10:30:46.933216: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:435] error: Internal: Complete shape not known for allreduce/CollectiveReduce_5
2023-01-05 10:30:46.933250: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1117] error: Internal: Complete shape not known for allreduce/CollectiveReduce_5
2023-01-05 10:30:46.933351: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1134] ScopedAllocatorOptimizer: Internal: Complete shape not known for allreduce/CollectiveReduce_5
2023-01-05 10:30:46.933359: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:907] error: Internal: Complete shape not known for allreduce/CollectiveReduce_5
2023-01-05 10:30:46.934321: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:563] scoped_allocator_optimizer failed: Internal: Complete shape not known for allreduce/CollectiveReduce_5


Thank you for your help.
```


### Standalone code to reproduce the issue

```shell
import os
import random
import time

import numpy as np
import tensorflow as tf
from tqdm import tqdm
from collections import deque

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'


print(tf.config.experimental.list_physical_devices(""GPU""))
mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.RING)

window_size = 5
episodes = 20
batch_size = 32
NAME = f""Blackstonev1-LSTM-32x64x64-{int(time.time())}""
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=""logs\{}"".format(NAME))

class AIAgent:
    def __init__(self, state_size, action_space=3, model_name=NAME):  # Stay, Buy, Sell
        self.state_size = state_size
        self.action_space = action_space
        self.memory = deque(maxlen=2000)
        self.inventory = []
        self.margin_inventory = []
        self.model_name = model_name

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_final = 0.05
        self.epsilon_decay = 0.9995

        self.model = self.model_builder()

    def model_builder(self):
        with mirrored_strategy.scope():
            model = tf.keras.models.Sequential()

            model.add(tf.keras.Input(shape=(window_size, 2)))

            model.add(tf.keras.layers.LSTM(units=32, activation='relu', return_sequences=True))
            model.add(tf.keras.layers.LSTM(units=64, activation='relu', return_sequences=True))
            model.add(tf.keras.layers.LSTM(units=64, activation='relu', return_sequences=False))
            model.add(tf.keras.layers.Dense(units=self.action_space, activation='linear'))
            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))

        return model

    def trade(self, state):
        rdm = random.random()
        if rdm <= self.epsilon:
            rdm_act = random.randrange(self.action_space)
            print(f""random: {rdm_act}"")
            return rdm_act

        actions = self.model.predict(state)
        argmax = np.argmax(actions[0])
        print(f'model: {argmax}')
        return argmax

    def batch_train(self, batch_size):
        batch = []
        for i in range(len(self.memory) - batch_size + 1, len(self.memory)):
            batch.append(self.memory[i])

        for state, action, reward, next_state, done in batch:
            reward = reward

            if not done:
                reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

            target = self.model.predict(state)
            target[0][action] = reward

            self.model.fit(state, target, epochs=1, verbose=0, callbacks=[tensorboard])

        if self.epsilon > self.epsilon_final:
            self.epsilon *= self.epsilon_decay


def state_creator(data, timestep, window_size):
    starting_id = timestep - window_size + 1

    if starting_id >= 0:
        windowed_data = data[starting_id:timestep + 1]
    else:
        windowed_data = - starting_id * [data[0]] + list(data[0:timestep + 1])

    state = windowed_data

    return np.array([state])


def main(batch_size, window_size, episodes):
    data = load_data(stock_name) # Replace with your own input here
    data_samples = len(data) - 1
    agent = AIAgent(window_size)
    agent.model.summary()
    
    
    for episode in range(1, episodes + 1):
        print(""Episode: {}/{}"".format(episode, episodes))
        state = state_creator(data, 0, window_size)
    
        total_profit = 0
        agent.inventory = []
    
        for t in tqdm(range(data_samples)):
            action = agent.trade(state)
    
            next_state = state_creator(data, t + 1, window_size)
            reward = 0
    
            if action == 1:
                # Do that
                continue
            elif action == 2:
                # Do that
                continue
    
            elif action == 0:
                # Do that
                continue
    
            if t == data_samples - 1:
                done = True
            else:
                done = False
    
            agent.memory.append((state, action, reward, next_state, done))
            state = next_state
    
            if len(agent.memory) > batch_size:
                agent.batch_train(batch_size)
    
        agent.model.save(f""{agent.model_name}_{episode}.h5"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""main.py"", line 207, in <module>
    trader.batch_train(batch_size)
  File ""main.py"", line 91, in batch_train
    self.model.fit(state, target, epochs=1, verbose=0, callbacks=[tensorboard])
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1661, in _filtered_call
    return self._call_flat(
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 593, in call
    outputs = execute.execute(
  File ""/home/hugo/Documents/scripts/blackstone_ai/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Incompatible shapes: [0,64] vs. [0,256]
	 [[{{node gradient_tape/replica_1/sequential/lstm_2/while/replica_1/sequential/lstm_2/while_grad/body/_1459/gradients/lstm_cell_2/mul_grad/BroadcastGradientArgs}}]]
  (1) Invalid argument:  Incompatible shapes: [0,64] vs. [0,256]
	 [[{{node gradient_tape/replica_1/sequential/lstm_2/while/replica_1/sequential/lstm_2/while_grad/body/_1459/gradients/lstm_cell_2/mul_grad/BroadcastGradientArgs}}]]
	 [[Adam/Adam/update_1_1/AssignAddVariableOp/_323]]
0 successful operations.
1 derived errors ignored. [Op:__inference_train_function_10543]

Function call stack:
train_function -> train_function
```
</details>"
59103,Pre loads pages and crashes during transition in colab,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59102,"Selectively build TensorFlow Lite with Bazel for Android，cannot find symbol class Interpreter，dlopen failed: cannot locate symbol ""_ZNK6google8protobuf7Message11GetTypeNameEv""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS 11.5.2
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): git tag: v2.11.0 ; https://github.com/tensorflow/tensorflow/tree/v2.11.0


**Provide the text output from tflite_convert**
Caused by: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZNK6google8protobuf7Message11GetTypeNameEv"" referenced by ""/data/app/com.alihealth.game.demo-FuGH2-eR4ckz7uJ2feTDGQ==/lib/arm64/libtensorflowlite_flex_jni.so""...
        at java.lang.Runtime.loadLibrary0(Runtime.java:1071)
        at java.lang.Runtime.loadLibrary0(Runtime.java:1007)
        at java.lang.System.loadLibrary(System.java:1668)
        at org.tensorflow.lite.flex.FlexDelegate.<clinit>(FlexDelegate.java:61)
        at java.lang.Class.classForName(Native Method)
        at java.lang.Class.forName(Class.java:454)
        at java.lang.Class.forName(Class.java:379)
        at org.tensorflow.lite.NativeInterpreterWrapper.maybeCreateFlexDelegate(NativeInterpreterWrapper.java:544)
        at org.tensorflow.lite.NativeInterpreterWrapper.addDelegates(NativeInterpreterWrapper.java:484)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:96)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:197)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:185)
        at com.alihealth.game.ai.DoctorSayWordRecManager.init(DoctorSayWordRecManager.java:123)
        at com.alihealth.game.ai.MobileAiInitTask.doInBackground(MobileAiInitTask.java:23)
        at android.os.AsyncTask$3.call(AsyncTask.java:389)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:292) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641) 
        at java.lang.Thread.run(Thread.java:929) 

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.
tflite model: https://alihealth-mvm.oss-cn-shanghai.aliyuncs.com/tf_1304610_20538587_mix_quantization.tflite?OSSAccessKeyId=LTAIl4xSPK8EBNw0&Expires=1673760375&Signature=in2qD4l7eM7sO25JZliZ19%2BKy2w%3D

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
.tf_configure.bazelrc file content：
build --action_env PYTHON_BIN_PATH=""/Applications/Xcode.app/Contents/Developer/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages""
build --python_path=""/Applications/Xcode.app/Contents/Developer/usr/bin/python3""
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-Wno-sign-compare
build --action_env ANDROID_NDK_HOME=""/Users/xxx/Library/Android/sdk_lp/ndk/20.0.5594570""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""30.0.2""
build --action_env ANDROID_SDK_API_LEVEL=""31""
build --action_env ANDROID_SDK_HOME=""/Users/xxx/Library/Android/sdk_lp""
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-v1only

buid command：
sh tensorflow/lite/tools/build_aar.sh \
  --input_models=tf_1304610_20538587_mix_quantization.tflite \
  --target_archs=arm64-v8a
"
59099,TensorListScatterV2 fail to support gradient computation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11, tf2.12.0-dev20230104

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Computing gradient for `tf.raw_ops.TensorListScatterV2` fails with error `Cannot convert 1 to EagerTensor of dtype variant`. According to [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) documentation, the operator `TensorListScatterV2` has gradient.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
indices = [[0], [1]]
element_shape = [2]
num_elements = 2
output_tensor = tf.raw_ops.TensorListScatterV2(tensor=input_tensor, indices=indices, element_shape=element_shape, num_elements=num_elements)
print(output_tensor) # OK

with tf.GradientTape() as tape:
    tape.watch(input_tensor)
    output_tensor = tf.raw_ops.TensorListScatterV2(tensor=input_tensor, indices=indices, element_shape=element_shape, num_elements=num_elements)
print(tape.gradient(output_tensor, input_tensor)) # Cannot convert 1 to EagerTensor of dtype variant
```


### Relevant log output

```shell
tf.Tensor(<TensorList>, shape=(), dtype=variant)
TypeError: Cannot convert 1 to EagerTensor of dtype variant
```
</details>"
59095,[TF-Lite]Is there a way to profile a model's inference process when using tflite in python,Is there a way to profile a model's inference process when using tflite in python？
59094,TensorFlow Lite GPU segmentation fault,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 9 /13 (macOS 11.7.2)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Sony Xperia XA2 Plus / Android Virtual Device
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly
-   **Python version**: no
-   **Bazel version (if compiling from source)**: no
-   **GCC/Compiler version (~~if compiling from source~~)**: Android Studio 2021.3.1 Patch 1
-   **CUDA/cuDNN version**: no
-   **GPU model and memory**: Qualcomm Adreno 508 6GB
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```
1. download https://github.com/johnnkp/TFLClassify and open in Android Studio
2. Run the app by selecting `finish` and press the `run` button on the toolbar
3. After accepting camera permission, app will crash in 10s

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

If I import TensorFlow Lite GPU 2.11, logcat will show`java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/gpu/GpuDelegateFactory$Options`and failed to delegate GPU.
If I import TensorFlow Lite GPU nightly, app will crash because of segmentation fault.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[GpuDelegateFactory.txt](https://github.com/tensorflow/tensorflow/files/10345380/GpuDelegateFactory.txt)
[SIGSEGV.txt](https://github.com/tensorflow/tensorflow/files/10345382/SIGSEGV.txt)
"
59092,run (cd tensorflow/go/op && go generate) error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
run command success
```


### Standalone code to reproduce the issue

```shell
2023/01/04 23:07:10 failed to process ""api_def_MergeV2Checkpoints.pbtxt"": Attribute allow_missing_files not defined in base api for MergeV2Checkpoints
exit status 1
generate.go:18: running ""go"": exit status 1
```


### Relevant log output

_No response_</details>"
59091,Mirrored strategy keras model.fit failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0-dev20221202

### Custom Code

Yes

### OS Platform and Distribution

Linux 22.04

### Mobile device

_No response_

### Python version

Docker image

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

From Docker image

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Falls over with no error in model.fit using mirrored strategy with multiple gpus

I previously logged this error under issue.

 Crashes with exit code 135 in model.fit #58673 

 I have just gotten round to creating code to reproduce the bug.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(""TensorFlow version:"", tf.__version__)

mirrored_strategy = tf.distribute.MirroredStrategy()

with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])

model.compile(loss='mse', optimizer='sgd')

dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)
model.fit(dataset, epochs=2)
print('Does it work?')
model.evaluate(dataset)
```


### Relevant log output

_No response_</details>"
59090,Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT #6971,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

No GPU , Memory - 32GB on AWS Sagemaker NoteBook

### Current Behaviour?

```shell
I am facing same issue on AWS Sagemaker Notebook - 32GB RAM.
Dimension of my dataset is 408 * 241390, whenever use the initializer command -
 
self.sess.run(tf.global_variables_initializer())

it shoes me this error - 
Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT .

I want this to pass through my generator code and run the iterations.
```


### Standalone code to reproduce the issue

```shell
`train = pd.read_csv('Final_dataset.csv')
 x = tf.placeholder(tf.float32, shape=[None, 128*128*3])
 y_ = tf.placeholder(tf.float32, shape=[None, 128*256*3])
 W = tf.Variable(tf.zeros([128*128*3,128*256*3]))
 b = tf.Variable(tf.zeros([128*256*3]))`
```


### Relevant log output

_No response_</details>"
59089,Illegal memory access when running tensorflow.python.ops.gen_math_ops.sparse_segment_sum when running from terminal,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running the test case from terminal via command  test_case.py, it gives me illegal memory access. The appropriate error message prompted when running from colab.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.constant([], shape=[0], dtype=tf.float32,)
      data = tf.identity(data_tensor)
      indices_0 = 8
      indices_1 = 3
      indices_2 = 0
      indices_3 = 9
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids_0 = 1
      segment_ids_1 = 2
      segment_ids_2 = 2
      segment_ids_3 = 2
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      out = gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float32)
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      results[""res_gpu""] = gen_math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-04 02:36:20.355748: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-04 02:36:20.355774: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59088,Check failure when running gen_bitwise_ops.population_count,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running .population_count with empty input tensor.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_bitwise_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.constant([], shape=[0], dtype=tf.int64,),dtype=tf.int8)
  arg_0 = tf.identity(arg_0_tensor)
  out = gen_bitwise_ops.population_count(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_bitwise_ops
try:
  arg_0_tensor = tf.saturate_cast(tf.random.uniform([0], minval=-128, maxval=128, dtype=tf.int64), dtype=tf.int8)
  arg_0 = tf.identity(arg_0_tensor)
  out = gen_bitwise_ops.population_count(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2023-01-04 02:15:27.020510: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
</details>"
59087,Check failure when running tensorflow.python.ops.gen_math_ops.dense_bincount,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Check failure when running tensorflow.python.ops.gen_math_ops.dense_bincount probably because of feeding empty tensor as input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  input_tensor = tf.constant([], shape=[0], dtype=tf.int32,)
  input = tf.identity(input_tensor)
  weights = []
  size = 10
  binary_output = True
  out = gen_math_ops.dense_bincount(input=input,weights=weights,size=size,binary_output=binary_output,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-04 02:11:11.516596: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
</details>"
59086,Unexpected error when running math_ops.sparse_segment_sum,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I get illegal memory access error when running .sparse_segment_sum with empty tensor as input. The error is not shown when running the code from colab.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import math_ops
try:
  try:
    with tf.device('/CPU'):
      data_tensor = tf.constant([], shape=[0], dtype=tf.float32,)
      data = tf.identity(data_tensor)
      indices_0 = 8
      indices_1 = 3
      indices_2 = 0
      indices_3 = 9
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids_0 = 1
      segment_ids_1 = 2
      segment_ids_2 = 2
      segment_ids_3 = 2
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      out = math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      data = tf.identity(data_tensor)
      data = tf.cast(data, tf.float32)
      indices = [indices_0,indices_1,indices_2,indices_3,]
      segment_ids = [segment_ids_0,segment_ids_1,segment_ids_2,segment_ids_3,]
      math_ops.sparse_segment_sum(data=data,indices=indices,segment_ids=segment_ids,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-04 02:06:54.937121: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-01-04 02:06:54.937146: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
</details>"
59085,Strange behavior of tensorflow.python.ops.gen_data_flow_ops.dynamic_stitch when running from different environments,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A very strange behavior when running .python.ops.gen_data_flow_ops.dynamic_stitch. I get different output when running from different environments. Log message from different environments:

When running from linux terminal:

```
Segmentation fault
```

When running from colab:

```
Error:{{function_node __wrapped__DynamicStitch_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] is out of range [Op:DynamicStitch]
```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops

try:
    arg_0_0_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int32)
    arg_0_0 = tf.identity(arg_0_0_tensor)
    arg_0_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
    arg_0_1 = tf.identity(arg_0_1_tensor)
    arg_0 = [
        arg_0_0,
        arg_0_1,
    ]
    arg_1_0_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int32)
    arg_1_0 = tf.identity(arg_1_0_tensor)
    arg_1_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
    arg_1_1 = tf.identity(arg_1_1_tensor)
    arg_1 = [
        arg_1_0,
        arg_1_1,
    ]
    out = gen_data_flow_ops.dynamic_stitch(
        arg_0,
        arg_1,
    )
except Exception as e:
    print(""Error:"" + str(e))

```

```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_1_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int32)
      arg_0_1 = tf.identity(arg_0_1_tensor)
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
      arg_1_0 = tf.identity(arg_1_0_tensor)
      arg_1_1_tensor = tf.random.uniform([1], minval=-256, maxval=257, dtype=tf.int32)
      arg_1_1 = tf.identity(arg_1_1_tensor)
      arg_1 = [arg_1_0,arg_1_1,]
      out = gen_data_flow_ops.dynamic_stitch(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = tf.identity(arg_0_0_tensor)
      arg_0_0 = tf.cast(arg_0_0, tf.int32)
      arg_0_1 = tf.identity(arg_0_1_tensor)
      arg_0_1 = tf.cast(arg_0_1, tf.int32)
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = tf.identity(arg_1_0_tensor)
      arg_1_0 = tf.cast(arg_1_0, tf.int32)
      arg_1_1 = tf.identity(arg_1_1_tensor)
      arg_1_1 = tf.cast(arg_1_1, tf.int32)
      arg_1 = [arg_1_0,arg_1_1,]
      gen_data_flow_ops.dynamic_stitch(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

_No response_</details>"
59084,Check failure when running tensorflow.python.ops.gen_ragged_conversion_ops.ragged_tensor_to_variant,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Crash when running .ragged_tensor_to_variant
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_ragged_conversion_ops
try:
  arg_0_0_tensor = tf.random.uniform([3], minval=-256, maxval=257, dtype=tf.int64)
  arg_0_0 = tf.identity(arg_0_0_tensor)
  arg_0 = [arg_0_0,]
  arg_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2 = True
  arg_3 = None
  out = gen_ragged_conversion_ops.ragged_tensor_to_variant(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_ragged_conversion_ops

try:
    arg_0_0_tensor = tf.random.uniform([5], minval=-256, maxval=257, dtype=tf.int32)
    arg_0_0 = tf.identity(arg_0_0_tensor)
    arg_0 = [
        arg_0_0,
    ]
    arg_1_tensor = tf.random.uniform([], minval=-256, maxval=257, dtype=tf.int32)
    arg_1 = tf.identity(arg_1_tensor)
    arg_2 = True
    arg_3 = None
    out = gen_ragged_conversion_ops.ragged_tensor_to_variant(
        arg_0,
        arg_1,
        arg_2,
        arg_3,
    )
except Exception as e:
    print(""Error:"" + str(e))

```


### Relevant log output

```shell
2023-01-04 01:34:14.128060: F tensorflow/core/framework/tensor_shape.cc:585] Check failed: d < dims() (0 vs. 0)
Aborted
```
```
</details>"
59083,Crash when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

22.04

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Cuda compilation tools, release 11.5, V11.5.119

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When .max_pool_grad_with_argmax is given negative integer tensor, it crashes.
```


### Standalone code to reproduce the issue

```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.constant(-105687333925307, shape=[2, 3, 3, 1], dtype=tf.float32,)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2023-01-04 00:07:17.297598: F tensorflow/core/kernels/maxpooling_op.cc:1065] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 240, 0, 18
Aborted

```
```
</details>"
59081,"Monolithic build of libtensorflow_cc.so produces libtensorflow_framework.so, failing to run application","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.3.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.2.152

### GPU model and memory

NVIDIA RTX 3090 TI (24GB)

### Current Behaviour?

I am trying to build a monolithic version of the TensorFlow C++ library `libtensorflow_cc.so` from source for TensorFlow versions 2.10+.

I'm building inside a Docker container of the official `devel-gpu` image, which is [defined in devel-gpu.Dockerfile](https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile).

For the build, I am calling the following. The build output is attached as the log in this issue. Note that I also had to incorporate [this patch](https://github.com/tensorflow/tensorflow/issues/57826#issuecomment-1328911560) to get the build to work at all. 

```
bazel build --jobs ${JOBS} --config=cuda --config=opt --config=monolithic --verbose_failures tensorflow:libtensorflow_cc.so tensorflow:install_headers
```

The library is built successfully, I did however notice that `libtensorflow_cc.so` seems to now be linking to `libtensorflow_framework.so`, which is also built in the process. That was not the case with v2.9.2 and below. Actually I thought that `--config=monolithic` should result in only `libtensorflow_cc.so`, but it looks like `libtensorflow_framework.so` is a dependency since https://github.com/tensorflow/tensorflow/commit/843c02fe06983ac0f4382a93fff9ffd07eb93d27? I didn't find where `--config=monolithic` is coming into play at the moment.

My main problem at the moment is that I cannot successfully compile and run a test C++ program. Previously this worked:

```
g++ -I /usr/local/include/tensorflow \
    hello_tensorflow.cpp \
    -ltensorflow_cc -lprotobuf \
    -o hello_tensorflow
```

With v2.10.0+ it is giving me the following error instead.

```
/usr/bin/ld: /tmp/ccVj5SwH.o: undefined reference to symbol '_ZN10tensorflow6TensorC1ENS_8DataTypeERKNS_11TensorShapeE'
/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.2: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
```

Now if I add `libtensorflow_framework.so` to the list of libs to link against, compilation works.

```
g++ -I /usr/local/include/tensorflow \
    hello_tensorflow.cpp \
    -ltensorflow_cc -ltensorflow_framework -lprotobuf \
    -o hello_tensorflow
```

This time however, I am getting the following runtime error:

```
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: google/protobuf/any.proto
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Aborted (core dumped)
```


### Standalone code to reproduce the issue

First build the official GPU devel image with [devel-gpu.Dockerfile](https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile).

```
docker build -t tensorflow/tensorflow:2.10.0-devel-gpu -f devel-gpu.Dockerfile .
```

Then try to build the `libtensorflow_cc.so` with the following Dockerfile.

```docker
ARG TF_VERSION=2.10.0
FROM tensorflow/tensorflow:${TF_VERSION}-devel-gpu AS build

ARG TF_VERSION
ARG JOBS=""auto""

# clone TensorFlow
RUN git clone --branch v${TF_VERSION} --depth=1 https://github.com/tensorflow/tensorflow.git /tensorflow
WORKDIR /tensorflow

# fix build issue in v2.10.0
# https://github.com/tensorflow/tensorflow/issues/57826
RUN if [ ""${TF_VERSION}"" = ""2.10.0"" ] || [ ""${TF_VERSION}"" = ""2.10.1"" ]; then \
        git fetch --depth=1 origin b1bd1d6beeac169ce669f81dcbf3c48899ca1ed0 && \
        git checkout FETCH_HEAD -- tensorflow/BUILD; \
    fi

# configure compilation
ENV PYTHON_BIN_PATH=/usr/bin/python3
ENV PYTHON_LIB_PATH=/usr/lib/python3/dist-packages
ENV TF_NEED_ROCM=0
ENV TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.0,6.1,7.0,7.2,7.5,8.0,8.6
ENV TF_CUDA_CLANG=0
ENV GCC_HOST_COMPILER_PATH=/usr/bin/gcc
ENV CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
ENV TF_SET_ANDROID_WORKSPACE=0
RUN ./configure

# build C++ library
RUN bazel build --jobs ${JOBS} --config=cuda --config=opt --config=monolithic --verbose_failures tensorflow:libtensorflow_cc.so tensorflow:install_headers
```

```shell
docker build -t tensorflow/tensorflow:2.10.0-libtensorflow-cc .
```

Notice that both `libtensorflow_cc.so` and `libtensorflow_framework.so` are built.

Install the built libraries and headers to the system, e.g. in a Docker container of the built image. Try to build and run the following sample application.

```cpp
#include <iostream>
#include <vector>

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/public/version.h>

using namespace std;


int main(int argc, char **argv) {

  // create new session
  auto scope = tensorflow::Scope::NewRootScope();
  tensorflow::ClientSession session(scope);

  // define graph of operations
  auto A = tensorflow::ops::Const(scope, {{1, 2}, {3, 4}});
  auto x = tensorflow::ops::Const(scope, {{1}, {2}});
  auto b = tensorflow::ops::MatMul(scope, A, x);

  // run graph and fetch outputs of A, x, and b
  vector<tensorflow::Tensor> outputs;
  session.Run({A, x, b}, &outputs);

  // print results
  cout << ""Hello from TensorFlow C++ "" << TF_VERSION_STRING << ""!"" << endl << endl;
  cout << ""A = "" << endl << outputs[0].tensor<int, 2>() << endl << endl;
  cout << ""x = "" << endl << outputs[1].tensor<int, 2>() << endl << endl;
  cout << ""A * x = "" << endl << outputs[2].tensor<int, 2>() << endl;

  return 0;
}
```

```
g++ -I /usr/local/include/tensorflow \
    hello_tensorflow.cpp \
    -ltensorflow_cc -ltensorflow_framework -lprotobuf \
    -o hello_tensorflow
```


### Relevant log output

```shell
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: google/protobuf/any.proto
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Aborted (core dumped)
```
</details>"
59080,Unable to Build Tensorflow Lite Flex Delegate for Android,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS 12.3.1

### Mobile device

Samsung Galaxy S22 Ultra

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

clang 13.1.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to build the tensorflow lite flex delegate shared object library to use with android natvie cpp on an android arm device but I get an error with lots of undefined references to various protobuf classes. 

No errors building the regular lite.so and gpu delegate.
```


### Standalone code to reproduce the issue

```shell
bazel build -c opt --config=android_arm --verbose_failures //tensorflow/lite/delegates/flex:tensorflowlite_flex 

setting config to android_arm64, android_x86, android_x86_64 results in the same errors.
```


### Relevant log output

```shell
anonymous@Anonymouss-MacBook-Pro-2 tensorflow_src % bazel build -c opt --config=android_arm --verbose_failures //tensorflow/lite/delegates/flex:tensorflowlite_flex
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=362
INFO: Reading rc options for 'build' from /Users/anonymous/CS/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/anonymous/CS/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/anonymous/CS/tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/opt/homebrew/opt/python@3.10/bin/python3.10 --action_env PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages --python_path=/opt/homebrew/opt/python@3.10/bin/python3.10 --action_env ANDROID_NDK_HOME=/Users/anonymous/library/Android/Sdk/ndk/21.4.7075529 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=33.0.1 --action_env ANDROID_SDK_API_LEVEL=33 --action_env ANDROID_SDK_HOME=/Users/anonymous/library/Android/Sdk
INFO: Reading rc options for 'build' from /Users/anonymous/CS/tensorflow_src/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /Users/anonymous/CS/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/anonymous/CS/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm in file /Users/anonymous/CS/tensorflow_src/.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a
INFO: Found applicable config definition build:android in file /Users/anonymous/CS/tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --define=with_xla_support=false
INFO: Analyzed target //tensorflow/lite/delegates/flex:tensorflowlite_flex (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /Users/anonymous/CS/tensorflow_src/tensorflow/lite/delegates/flex/BUILD:126:27: Linking tensorflow/lite/delegates/flex/libtensorflowlite_flex.so failed: (Exit 1): clang failed: error executing command
  (cd /private/var/tmp/_bazel_anonymous/b7ad12c66379cd51bf9f1b282979b6e7/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=33.0.1 \
    ANDROID_NDK_API_LEVEL=21 \
    ANDROID_NDK_HOME=/Users/anonymous/library/Android/Sdk/ndk/21.4.7075529 \
    ANDROID_SDK_API_LEVEL=33 \
    ANDROID_SDK_HOME=/Users/anonymous/library/Android/Sdk \
    PATH=/Users/anonymous/Library/Caches/bazelisk/downloads/bazelbuild/bazel-5.3.0-darwin-arm64/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/Users/anonymous/Library/Android/sdk/platform-tools \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/opt/homebrew/opt/python@3.10/bin/python3.10 \
    PYTHON_LIB_PATH=/opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -shared -o bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libtensorflowlite_flex.so -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/libportable_tensorflow_lib.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/kernels/libportable_tensorflow_kernels.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/lib/gif/libportable_gif_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/gif/libgif.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/lib/jpeg/libportable_jpeg_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/libjpeg_turbo/libjpeg.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/libjpeg_turbo/libsimd_armv7a.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/lib/png/libpng_io.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/png/libpng.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/highwayhash/libsip_hash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/highwayhash/libarch_specific.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/third_party/icu/data/libconversion_data.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/icu/libicuuc.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf/libprotobuf.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_protobuf/libprotobuf_lite.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/zlib/libzlib.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libdelegate_symbol.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libdelegate_only_runtime.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libdelegate_data.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libbuffer_map.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libtflite_subgraph_execute.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libbuffer_map_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/flex/libutil.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/libcc_api_experimental.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/kernels/libbuiltin_ops.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libtflite_with_xnnpack_optional.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/xnnpack/libxnnpack_delegate.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/xnnpack/libquantization_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_op_kernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/liblstm_eval.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libaudio_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libkernel_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libeigen_support.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/utils/libsparsity_format_converter.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libtensor_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libportable_tensor_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libtranspose_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libcpu_backend_gemm.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libcpu_check.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libcontext_get_ctx.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libfrontend.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libkernel_arm.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libkernel_avx.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libkernel_avx2_fma.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libkernel_avx512.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libapply_multiplier.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libpack_arm.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libpack_avx.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libpack_avx2_fma.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libpack_avx512.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libprepare_packed_matrices.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libtrmul.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libblock_map.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libcpu_backend_context.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libcontext.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libctx.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/liballocator.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libhave_built_path_for_avx.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libhave_built_path_for_avx2_fma.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libhave_built_path_for_avx512.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libprepacked_cache.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libsystem_aligned_alloc.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libtune.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libcpuinfo.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libthread_pool.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libblocking_counter.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libwait.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libxnnpack_for_tflite.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libscalar_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libfp16arith_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneonfp16arith_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneondot_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libarmsimd32_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneon_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneonfp16_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneonfma_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libneonv8_prod_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libtables.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libasm_microkernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libsubgraph.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/liboperators.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libindirection.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libjit.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libmicroparams_init.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libnormalization.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libpacking.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libcache.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libmemory.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libmutex.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libpost_operation.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/liballocator.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/liblogging.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/XNNPACK/libparams.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/pthreadpool/libpthreadpool.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/cpuinfo/libcpuinfo_impl.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/cpuinfo/deps/clog/libclog.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libvariable_op_kernels.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libcc_api_stable.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/libcc_api_stable.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/libmodel_builder.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/libtelemetry.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/profiling/libplatform_profiler.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/profiling/libatrace_profiler.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/libdenormal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libexternal_cpu_backend_context.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libmutable_op_resolver.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libsignature_runner.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libstderr_reporter.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/libsubgraph.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/liballocation.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/c/libcommon_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/experimental/remat/libmetadata_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/profiling/libroot_profiler.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libarena_planner.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libgraph_info.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libsimple_memory_arena.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libtensorflow_profiler_logger_shim.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/fft2d/libfft2d.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/ruy/ruy/profiler/libinstrumentation.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/experimental/resource/libresource.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/libflatbuffers.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/api/libapi.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/api/libop_resolver.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/api/liberror_reporter.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/schema/libschema_utils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libstring_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/utils/libsimple_delegate.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libminimal_logging.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/delegates/libutils.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libutil.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libkernel_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/core/c/libcommon.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/tfrt/fallback/libop_kernel_runner.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/libportable_tensorflow_lib_lite.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/platform/default/libresource.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/util/libstats_calculator_portable.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/platform/default/liblogging.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/platform/default/libenv_time.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/platform/default/libmutex.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/framework/libdevice_type.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/status/libstatus.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libstrerror.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/example/libexample_protos_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/example/libexample_parser_configuration_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/liballocation_description_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libapi_def_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libattr_value_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libcost_graph_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libdataset_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libdataset_metadata_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libdataset_options_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libdevice_attributes_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libfull_type_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libfunction_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libgraph_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libgraph_transfer_info_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libkernel_def_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/liblog_memory_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libmodel_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libnode_def_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/liboptimized_function_graph_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libop_def_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libreader_base_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libresource_handle_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libstep_stats_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libsummary_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libtensor_description_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libtensor_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libtensor_shape_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libtensor_slice_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libtypes_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libvariable_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libversions_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/lib/core/liberror_codes_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/profiler/protobuf/libxplane_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/profiler/libprofiler_options_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/liberror_codes_proto_impl_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/libfor_core_protos_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/libevent_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/libsaved_tensor_slice_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/libmemmapped_file_system_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/quantization/libuniform_quant_ops_attr_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/libtest_log_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/core/grappler/costs/libop_performance_data_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/profiler/protobuf/libprofiler_options_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/profiler/protobuf/libxplane_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/protobuf/libcoordination_config_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/tensorflow/tsl/protobuf/libdistributed_runtime_payloads_proto_cc_impl.pic.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/double_conversion/libdouble-conversion.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/nsync/libnsync_cpp.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_googlesource_code_re2/libre2.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/hash/libhash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/hash/libcity.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/hash/liblow_level_hash.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/types/libbad_variant_access.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libcord.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libcordz_info.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libcord_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libcordz_functions.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libcordz_handle.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/container/libraw_hash_set.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/container/libhashtablez_sampler.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/profiling/libexponential_biased.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libstr_format_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/synchronization/libsynchronization.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/synchronization/libgraphcycles_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libstacktrace.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libsymbolize.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libdebugging_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/debugging/libdemangle_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libmalloc_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/time/libtime.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libstrings.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/strings/libinternal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libbase.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/libint128.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/time/internal/cctz/libtime_zone.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/time/internal/cctz/libcivil_time.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/types/libbad_optional_access.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libthrow_delegate.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/libraw_logging_internal.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/base/liblog_severity.pic.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-opt/bin/external/farmhash_archive/libfarmhash.pic.a -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libandroid_support.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++abi.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libunwind.a -Wl,-z,defs -Wl,--version-script,tensorflow/lite/delegates/flex/version_script.lds -latomic -Wl,--gc-sections -Wl,--as-needed -Wl,-s '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -Wl,-soname,libtensorflowlite_flex.so -ldl -ldl -ldl -lm -pthread -pthread -pthread -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -lm -llog -ldl -lm -pthread -llog -lm -lz -lm -pthread -pthread -pthread -static-libgcc -target armv7-none-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm')
# Configuration: cda876c88cf052b3f2903f3d2404fa9618d3a093d09766137d6d07f412fdbafd
# Execution platform: @local_execution_config_platform//:platform
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/lib/core/liberror_codes_proto_cc_impl.pic.lo(error_codes.pb.pic.o):error_codes.pb.cc:descriptor_table_tensorflow_2fcore_2flib_2fcore_2ferror_5fcodes_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2ferror_5fcodes_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/liberror_codes_proto_impl_cc_impl.pic.lo(error_codes.pb.pic.o):error_codes.pb.cc:descriptor_table_tensorflow_2fcore_2fprotobuf_2ferror_5fcodes_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2ferror_5fcodes_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/libfor_core_protos_cc_impl.pic.lo(bfc_memory_map.pb.pic.o):bfc_memory_map.pb.cc:descriptor_table_tensorflow_2fcore_2fprotobuf_2fbfc_5fmemory_5fmap_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2fbfc_5fmemory_5fmap_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/libfor_core_protos_cc_impl.pic.lo(rpc_options.pb.pic.o):rpc_options.pb.cc:descriptor_table_tensorflow_2fcore_2fprotobuf_2frpc_5foptions_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2frpc_5foptions_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/util/libtest_log_proto_cc_impl.pic.lo(test_log.pb.pic.o):test_log.pb.cc:descriptor_table_tensorflow_2fcore_2futil_2ftest_5flog_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2ftest_5flog_2eproto'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::HistogramProto* google::protobuf::Arena::CreateMaybeMessage<tensorflow::HistogramProto>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::HistogramProto* google::protobuf::Arena::CreateMaybeMessage<tensorflow::HistogramProto>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::HistogramProto* google::protobuf::Arena::CreateMaybeMessage<tensorflow::HistogramProto>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::HistogramProto* google::protobuf::Arena::CreateMaybeMessage<tensorflow::HistogramProto>(google::protobuf::Arena*)'
tensorflow/tsl/framework/bfc_allocator.cc:1134: error: undefined reference to 'tensorflow::MemoryDump::~MemoryDump()'
tensorflow/tsl/framework/bfc_allocator.cc:1143: error: undefined reference to 'tensorflow::MemoryDump::MemoryDump()'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::MemAllocatorStats* google::protobuf::Arena::CreateMaybeMessage<tensorflow::MemAllocatorStats>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/repeated_field.h:683: error: undefined reference to 'tensorflow::BinSummary* google::protobuf::Arena::CreateMaybeMessage<tensorflow::BinSummary>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/repeated_field.h:683: error: undefined reference to 'tensorflow::MemChunk* google::protobuf::Arena::CreateMaybeMessage<tensorflow::MemChunk>(google::protobuf::Arena*)'
tensorflow/tsl/lib/histogram/histogram.cc:206: error: undefined reference to 'tensorflow::HistogramProto::Clear()'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:0: error: undefined reference to 'tensorflow::_HistogramProto_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:0: error: undefined reference to 'tensorflow::_HistogramProto_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:2467: error: undefined reference to 'tensorflow::HistogramProto::MergeFrom(tensorflow::HistogramProto const&)'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:0: error: undefined reference to 'tensorflow::_HistogramProto_default_instance_'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1264: error: undefined reference to 'tensorflow::HistogramProto::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1696: error: undefined reference to 'tensorflow::HistogramProto::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1761: error: undefined reference to 'tensorflow::HistogramProto::ByteSizeLong() const'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:3102: error: undefined reference to 'tensorflow::HistogramProto::MergeFrom(tensorflow::HistogramProto const&)'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/summary.pb.cc:0: error: undefined reference to 'tensorflow::_HistogramProto_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libsummary_proto_cc_impl.pic.lo(summary.pb.pic.o):summary.pb.cc:scc_info_Summary_Value_tensorflow_2fcore_2fframework_2fsummary_2eproto: error: undefined reference to 'scc_info_HistogramProto_tensorflow_2ftsl_2fprotobuf_2fhistogram_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/framework/libsummary_proto_cc_impl.pic.lo(summary.pb.pic.o):summary.pb.cc:descriptor_table_tensorflow_2fcore_2fframework_2fsummary_2eproto_deps: error: undefined reference to 'descriptor_table_tensorflow_2ftsl_2fprotobuf_2fhistogram_2eproto'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/config.pb.cc:0: error: undefined reference to 'tensorflow::_RPCOptions_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/config.pb.cc:0: error: undefined reference to 'tensorflow::_RPCOptions_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/config.pb.cc:6027: error: undefined reference to 'tensorflow::RPCOptions::RPCOptions(tensorflow::RPCOptions const&)'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::RPCOptions* google::protobuf::Arena::CreateMaybeMessage<tensorflow::RPCOptions>(google::protobuf::Arena*)'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1264: error: undefined reference to 'tensorflow::RPCOptions::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*)'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1696: error: undefined reference to 'tensorflow::RPCOptions::InternalSerializeWithCachedSizesToArray(unsigned char*) const'
external/com_google_protobuf/src/google/protobuf/wire_format_lite.h:1761: error: undefined reference to 'tensorflow::RPCOptions::ByteSizeLong() const'
external/com_google_protobuf/src/google/protobuf/message_lite.h:462: error: undefined reference to 'tensorflow::RPCOptions* google::protobuf::Arena::CreateMaybeMessage<tensorflow::RPCOptions>(google::protobuf::Arena*)'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/config.pb.cc:7000: error: undefined reference to 'tensorflow::RPCOptions::MergeFrom(tensorflow::RPCOptions const&)'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/config.pb.cc:0: error: undefined reference to 'tensorflow::_RPCOptions_default_instance_'
bazel-out/armeabi-v7a-opt/bin/tensorflow/core/protobuf/libfor_core_protos_cc_impl.pic.lo(config.pb.pic.o):config.pb.cc:scc_info_ConfigProto_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto: error: undefined reference to 'scc_info_RPCOptions_tensorflow_2ftsl_2fprotobuf_2frpc_5foptions_2eproto'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/lite/delegates/flex:tensorflowlite_flex failed to build
INFO: Elapsed time: 13.913s, Critical Path: 13.28s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```
</details>"
59079,SIMJO77 ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
59078,Issue created for Rollback of PR #58391: OP_REQUIRES for dtype check in AssignAddVariableOp,"Merged PR #58391 is rolled back in 105d5f3137d75f873fe3d7ad9aab1cb98356488d.
    Please follow up with the reviewer and close this issue once its resolved."
59077,tflite_model_maker-0.4.2 - Getting nightly for ever,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tflite_model_maker-0.4.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04.1 LTS and Windows 10 (21H1)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In Ubuntu and Windows same situation:
COMMAND:
    pip install tflite-model-maker    (as https://pypi.org/project/tflite-model-maker/)

OUTPUT:
    Download, build and install several dependencies, without problem or error.
    Collecting pycocotools>=2.0.2
    Downloading pycocotools-2.0.6.tar.gz (24 kB)
    Installing build dependencies ... done
    Getting requirements to build wheel ... done
    Preparing metadata (pyproject.toml) ... done
    Collecting tflite-model-maker    (<=== DOWNLOADING OLD VERSION ?)
    Downloading tflite_model_maker-0.2.4-py3-none-any.whl (190 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.1/190.1 KB 41.0 MB/s eta 0:00:00
    Downloading tflite_model_maker-0.2.3-py3-none-any.whl (114 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 KB 28.1 MB/s eta 0:00:00
    Downloading tflite_model_maker-0.2.2-py3-none-any.whl (103 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 KB 36.0 MB/s eta 0:00:00
    Downloading tflite_model_maker-0.2.1-py3-none-any.whl (102 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.9/102.9 KB 33.6 MB/s eta 0:00:00
    Downloading tflite_model_maker-0.2.0-py3-none-any.whl (102 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 KB 33.3 MB/s eta 0:00:00
    Downloading tflite_model_maker-0.1.2-py3-none-any.whl (104 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 KB 36.7 MB/s eta 0:00:00

   Collecting tf-nightly
    Downloading tf_nightly-2.12.0.dev20230103-cp310-cp310- 
    manylinux_2_17_x86_64.manylinux2014_x86_64.whl (563.8 MB)
    ...
    Still downloading tf_nightly versions, for ever. I had canceled after 2 months of tf_nightly.

I HAD TRY:
   1) Install Windows e Ubuntu.
   2) Remove all pip package e install minimum necessary
   3) Try on python virtual enviroment
```


### Standalone code to reproduce the issue

```shell
pip install tflite-model-maker
```


### Relevant log output

_No response_</details>"
59076,Issue created for Rollback of PR #58358: Fix unigram assert,"Merged PR #58358 is rolled back in 4981cacfe5da6e43b08f92a08d8d8a9c54b41197.
    Please follow up with the reviewer and close this issue once its resolved."
59075,TypeError: can't pickle weakref objects,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TypeError: can't pickle weakref objects
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle
output_file=open('save.dat','wb')
pickle.dump(tf.keras.optimizers.Adam(),output_file)
```


### Relevant log output

_No response_</details>"
59074,a bug in tensorflow,"<details><su

### Standalone code to reproduce the issue



### Relevant log output

_No response_</details>"
59073,fatal error: rocm/include/rocblas.h: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

nightly

### Custom Code

Yes

### OS Platform and Distribution

Linux Manjaro

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

12.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

AMD Vega 10

### Current Behaviour?

```shell
While compiling from sources I have the error

´fatal error: rocm/include/rocblas.h: No such file or directory´

I can fix that replacing

´#include ""rocm/include/rocblas.h""´

by

´#include ""rocm/include/rocblas/rocblas.h""´

at tensorflow/compiler/xla/stream_executor/rocm/rocblas_wrapper.h
```


### Standalone code to reproduce the issue

```shell
./configure
bazel build --config=rocm //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/pepe/CoreMarine/pinn/tensorflow/tensorflow/compiler/xla/stream_executor/rocm/BUILD:171:11: Compiling tensorflow/compiler/xla/stream_executor/rocm/rocm_blas.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer ... (remaining 128 arguments skipped)
In file included from tensorflow/compiler/xla/stream_executor/rocm/rocm_blas.cc:18:
./tensorflow/compiler/xla/stream_executor/rocm/rocblas_wrapper.h:23:10: fatal error: rocm/include/rocblas.h: No such file or directory
   23 | #include ""rocm/include/rocblas.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 40.598s, Critical Path: 30.80s
INFO: 810 processes: 370 internal, 440 local.
FAILED: Build did NOT complete successfully
```
</details>"
59072,Distribute training bug while using tf.data,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: [ngc](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-22-10.html#rel-22-10)
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.8
-   **GPU model and memory**: A100 80G (8 Gpus)
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using 8 Gpus to train a model with custome dataset generator loader. However, when I am trying to train the model, at the final batch it will throw error as ""INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds."". I tried the same scripts and data with only one GPUs and it goes fine. 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
The code I uses to train is:
```
val = pd.read_csv('data/val.csv')
window_length = 40
feats = 4
def get_LSTM_AE_model():
    model = keras.Sequential()
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_length, feats), return_sequences=True, name='encoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.RepeatVector(window_length, name='encoder_decoder_bridge'))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(feats)))
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), loss=""mse"")
    model.summary()
    
    return model

#distribute training
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BATCH_SIZE_PER_REPLICA = 4096
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
with strategy.scope():
    model = get_LSTM_AE_model()

val_events = []
val.groupby('vin').apply(lambda x:val_events.append(x[['a','b','v','d']].values))
def val_data_generator():
    # np.random.shuffle(val_events)
    for events in val_events:
        yield events
val_dataset = tf.data.Dataset.from_generator(
    generator=val_data_generator,
    output_types=tf.float32
)
def tensor_2_window(x):
    x = tf.data.Dataset.from_tensor_slices(x)
    x = x.window(40,shift=1,drop_remainder=True)
    x = x.flat_map(lambda window: window.batch(40))
    return x
val_dataset = val_dataset.flat_map(tensor_2_window)
val_dataset = val_dataset.map(lambda window: (window, window))
val_dataset = val_dataset.cache().batch(4096*9).prefetch(buffer_size=tf.data.AUTOTUNE)
history = model.fit(
    val_dataset,
    epochs=50,
    # validation_data=val_dataset
)
```
then after training to the last batch, error apperas:
```
2022-12-07 03:37:26.605819: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606363: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606475: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606605: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606675: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606747: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606807: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606831: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606908: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606998: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607074: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607141: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In [52], line 1
----> 1 history = model.fit(
      2     val_dataset,
      3     epochs=50,
      4     # validation_data=val_dataset
      5 )

File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'replica_5/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 994, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1052, in compute_loss
      return self.compiled_loss(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py"", line 279, in __call__
      batch_dim = tf.shape(y_t)[0]
Node: 'replica_5/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
9 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_5/strided_slice}}]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_6/mean_squared_error/cond/else/_189/replica_6/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_385]]
  (2) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_4/mean_squared_error/cond/else/_139/replica_4/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_377]]
  (3) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/strided_slice/_296]]
  (4) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/mean_squared_error/cond/then/_63/replica_1/mean_squared_error/cond/cond/then/_690/replica_1/mean_squared_error/cond/cond/remove_squeezable_dimensions/cond/pivot_t/_1455/_729]]
  (5) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[gradient_tape/replica_7/mean_squared_error/cond/StatelessIf/pivot_f/_228/_346]]
  (6) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[div_no_nan/ReadVariableOp_8/_916]]
  (7) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[Func/replica_2/mean_squared_error/cond/else/_89/replica_2/mean_squared_error/cond/remove_squeezable_dimensions/cond/else/_742/input/_1165/_468]]
  (8) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_112593]
```
"
59070,Segmentation fault when enabling TF CPP logs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf_nightly-2.12.0.dev20230101

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.1 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TensorFlow optimization pass is triggering a segmentation fault when logs are turned on i.e when the value of environment variable TF_CPP_MAX_VLOG_LEVEL is greater than 0.

This is introduced by a recent commit to tensorflow/core/common_runtime/optimization_registry.cc file.
Commit : https://github.com/tensorflow/tensorflow/commit/2d053fc2d9d8f524dcda0ec96c237f0f6aa21697
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.__version__)

with tf.compat.v1.Session() as sess:
  c = tf.constant([[12.0, 20.0], [13.0, 40.0]])
  d = tf.constant([[19.0, 15.0], [10.0, 12.0]])
  e = tf.matmul(c, d)

  result = sess.run(e)
  print(result)
```


### Relevant log output

```shell
2023-01-02 12:53:28.475438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-02 12:53:28.610981: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:303] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10
2023-01-02 12:53:28.611065: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:852] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0
2023-01-02 12:53:28.611081: I ./tensorflow/tsl/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled
2023-01-02 12:53:28.611090: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:892] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)
2023-01-02 12:53:28.611097: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:922] GCS additional header DISABLED. No environment variable set.
2023-01-02 12:53:28.611103: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:303] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10
2023-01-02 12:53:28.614427: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-01-02 12:53:28.614446: I tensorflow/tsl/cuda/cudart_stub.cc:28] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-01-02 12:53:28.661233: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:303] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10
2023-01-02 12:53:28.661294: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:852] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0
2023-01-02 12:53:28.661304: I ./tensorflow/tsl/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled
2023-01-02 12:53:28.661311: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:892] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)
2023-01-02 12:53:28.661317: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:922] GCS additional header DISABLED. No environment variable set.
2023-01-02 12:53:28.661322: I tensorflow/tsl/platform/cloud/gcs_file_system.cc:303] GCS RetryConfig: init_delay_time_us = 1000000 ; max_delay_time_us = 32000000 ; max_retries = 10
2023-01-02 12:53:28.661512: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
2023-01-02 12:53:29.248417: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer.so.8'; dlerror: libnvinfer.so.8: cannot open shared object file: No such file or directory
2023-01-02 12:53:29.248490: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libnvinfer_plugin.so.8'; dlerror: libnvinfer_plugin.so.8: cannot open shared object file: No such file or directory
2023-01-02 12:53:29.248499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2.12.0-dev20230101
2023-01-02 12:53:29.840961: I tensorflow/compiler/xla/parse_flags_from_env.cc:196] For env var TF_XLA_FLAGS found arguments:
2023-01-02 12:53:29.840997: I tensorflow/compiler/xla/parse_flags_from_env.cc:198]   argv[0] = <argv[0]>
2023-01-02 12:53:29.841008: I tensorflow/compiler/xla/parse_flags_from_env.cc:196] For env var TF_JITRT_FLAGS found arguments:
2023-01-02 12:53:29.841013: I tensorflow/compiler/xla/parse_flags_from_env.cc:198]   argv[0] = <argv[0]>
2023-01-02 12:53:29.841024: I tensorflow/compiler/jit/xla_cpu_device.cc:44] Not creating XLA devices, tf_xla_enable_xla_devices not set and XLA device creation not requested
2023-01-02 12:53:29.841120: W tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-01-02 12:53:29.841133: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-01-02 12:53:29.841156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Rome-PowerEdge-R7525): /proc/driver/nvidia/version does not exist
2023-01-02 12:53:29.841172: I tensorflow/compiler/jit/xla_gpu_device.cc:49] Not creating XLA devices, tf_xla_enable_xla_devices not set and XLA devices creation not required
2023-01-02 12:53:29.841447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-02 12:53:29.857355: I tensorflow/compiler/jit/xla_cpu_device.cc:58] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-01-02 12:53:29.857423: I tensorflow/compiler/jit/xla_gpu_device.cc:80] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-01-02 12:53:29.857503: I tensorflow/core/common_runtime/process_util.cc:159] Session inter op parallelism threads: 256
2023-01-02 12:53:29.874870: I tensorflow/core/common_runtime/optimization_registry.cc:54] Starting optimization of a group 0
2023-01-02 12:53:29.874937: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 0
2023-01-02 12:53:29.874995: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: MlirV1CompatGraphOptimizationPass
2023-01-02 12:53:29.875002: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875011: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:331] MLIR V1 optimization pass is not enabled
2023-01-02 12:53:29.875019: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 9
2023-01-02 12:53:29.875023: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: ControlFlowDepsToChainsPass
2023-01-02 12:53:29.875028: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875034: I tensorflow/core/common_runtime/control_flow_deps_to_chains.cc:37] ControlFlowDepsToChainsPass::Run
2023-01-02 12:53:29.875062: W tensorflow/core/util/dump_graph.cc:134] Failed to dump control_flow_deps_to_chains_before because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875153: W tensorflow/core/util/dump_graph.cc:134] Failed to dump control_flow_deps_to_chains_after because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875162: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 10
2023-01-02 12:53:29.875166: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: AccumulateNV2RemovePass
2023-01-02 12:53:29.875170: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875177: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: LowerFunctionalOpsPass
2023-01-02 12:53:29.875181: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875196: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: ParallelConcatRemovePass
2023-01-02 12:53:29.875200: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875208: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 35
2023-01-02 12:53:29.875272: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: IsolatePlacerInspectionRequiredOpsPass
2023-01-02 12:53:29.875337: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875405: I tensorflow/core/common_runtime/isolate_placer_inspection_required_ops_pass.cc:34] IsolatePlacerInspectionRequiredOpsPass::Run
2023-01-02 12:53:29.875417: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: IntroduceFloatingPointJitterPass
2023-01-02 12:53:29.875423: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875439: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 36
2023-01-02 12:53:29.875445: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: EncapsulateXlaComputationsPass
2023-01-02 12:53:29.875452: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 6
2023-01-02 12:53:29.875469: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_before because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875478: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:382] EncapsulateXlaComputations(): (failed to create writable file: INVALID_ARGUMENT: TF_DUMP_GRAPH_PREFIX not specified)
2023-01-02 12:53:29.875525: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_halfway because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875535: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:393] EncapsulateXlaComputations() half-way: (failed to create writable file: INVALID_ARGUMENT: TF_DUMP_GRAPH_PREFIX not specified)
2023-01-02 12:53:29.875548: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_xla_computations_after because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875556: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:399] EncapsulateXlaComputations() finished: (failed to create writable file: INVALID_ARGUMENT: TF_DUMP_GRAPH_PREFIX not specified)
2023-01-02 12:53:29.875563: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 37
2023-01-02 12:53:29.875569: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: FunctionalizeControlFlowForXlaPass
2023-01-02 12:53:29.875575: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 7
2023-01-02 12:53:29.875634: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 99999
2023-01-02 12:53:29.875641: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: WeakTypeInferencePass
2023-01-02 12:53:29.875647: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 7
2023-01-02 12:53:29.875654: I tensorflow/core/common_runtime/type_inference.cc:141] TypeInferencePass::Run
2023-01-02 12:53:29.875665: W tensorflow/core/util/dump_graph.cc:134] Failed to dump forward_type_inference_before because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875694: I tensorflow/core/common_runtime/type_inference.cc:318] Finished after 1 iterations; done 5 of 5 nodes in 5 visits
2023-01-02 12:53:29.875704: W tensorflow/core/util/dump_graph.cc:134] Failed to dump forward_type_inference_after because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.875715: I tensorflow/core/common_runtime/optimization_registry.cc:90] Finished optimization of a group 0
2023-01-02 12:53:29.875721: I tensorflow/core/common_runtime/optimization_registry.cc:92] Graph #nodes 5 #edges 7
2023-01-02 12:53:29.877381: I tensorflow/core/common_runtime/placer.cc:124] MatMul(MatMul) placed on: /job:localhost/replica:0/task:0/device:CPU:0
2023-01-02 12:53:29.877394: I tensorflow/core/common_runtime/placer.cc:124] Const(Const) placed on: /job:localhost/replica:0/task:0/device:CPU:0
2023-01-02 12:53:29.877401: I tensorflow/core/common_runtime/placer.cc:124] Const_1(Const) placed on: /job:localhost/replica:0/task:0/device:CPU:0
2023-01-02 12:53:29.877409: I tensorflow/core/common_runtime/optimization_registry.cc:54] Starting optimization of a group 1
2023-01-02 12:53:29.877415: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 0
2023-01-02 12:53:29.877421: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: NcclReplacePass
2023-01-02 12:53:29.877429: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 5 #edges 7
2023-01-02 12:53:29.877442: I tensorflow/core/common_runtime/optimization_registry.cc:90] Finished optimization of a group 1
2023-01-02 12:53:29.877448: I tensorflow/core/common_runtime/optimization_registry.cc:92] Graph #nodes 5 #edges 7
2023-01-02 12:53:29.877533: I tensorflow/core/common_runtime/graph_execution_state.cc:854] BuildGraph
2023-01-02 12:53:29.896462: I tensorflow/tsl/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2245890000 Hz
2023-01-02 12:53:29.896512: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1045] Starting optimization for grappler item: tf_graph
2023-01-02 12:53:29.896528: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1066] Deleted 0 unreachable functions from the graph (library size = 0)
2023-01-02 12:53:29.896815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1078] Optimized main graph.
2023-01-02 12:53:29.896923: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1297] Optimized 0 functions:
2023-01-02 12:53:29.896934: W tensorflow/core/util/dump_graph.cc:134] Failed to dump after_MetaOptimizer_140735811320096 because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.897034: I tensorflow/core/common_runtime/optimization_registry.cc:54] Starting optimization of a group 2
2023-01-02 12:53:29.897043: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 5
2023-01-02 12:53:29.897051: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: CloneConstantsForBetterClusteringPass
2023-01-02 12:53:29.897057: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.897072: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 9
2023-01-02 12:53:29.897078: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: ClusterScopingPass
2023-01-02 12:53:29.897084: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.897093: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 10
2023-01-02 12:53:29.897100: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: MarkForCompilationPass
2023-01-02 12:53:29.897106: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901348: I tensorflow/compiler/tf2xla/xla_op_registry.cc:52] LaunchOpHasKernelForDevice kernel_class_name: XlaLocalLaunchOp
2023-01-02 12:53:29.901365: I tensorflow/compiler/tf2xla/xla_op_registry.cc:52] LaunchOpHasKernelForDevice kernel_class_name: XlaLocalLaunchOp
2023-01-02 12:53:29.901485: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:650] DeadnessAnalysis time: 23 us (cumulative: 23 us, max: 23 us, #called: 1)
2023-01-02 12:53:29.901552: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1554] MarkForCompilationPassImpl::Run time: 408 us (cumulative: 408 us, max: 408 us, #called: 1)
2023-01-02 12:53:29.901563: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 12
2023-01-02 12:53:29.901569: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: ForceXlaConstantsOnHostPass
2023-01-02 12:53:29.901576: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901591: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 20
2023-01-02 12:53:29.901597: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: IncreaseDynamismForAutoJitPass
2023-01-02 12:53:29.901605: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901612: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 30
2023-01-02 12:53:29.901618: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: PartiallyDeclusterPass
2023-01-02 12:53:29.901625: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901648: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 40
2023-01-02 12:53:29.901654: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: ReportClusteringInfoPass
2023-01-02 12:53:29.901661: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901771: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 50
2023-01-02 12:53:29.901778: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: EncapsulateSubgraphsPass
2023-01-02 12:53:29.901784: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 8
2023-01-02 12:53:29.901792: I tensorflow/compiler/jit/encapsulate_subgraphs_pass.cc:1139] EncapsulateSubgraphsPass::Run
2023-01-02 12:53:29.901809: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_subgraphs_before because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.901880: W tensorflow/core/util/dump_graph.cc:134] Failed to dump encapsulate_subgraphs_after because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.901903: I tensorflow/compiler/jit/xla_cluster_util.cc:590] GetNodesRelatedToRefVariables() found 0 nodes
2023-01-02 12:53:29.901931: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 60
2023-01-02 12:53:29.901937: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: BuildXlaOpsPass
2023-01-02 12:53:29.901946: I tensorflow/core/common_runtime/optimization_registry.cc:70] Graph #nodes 6 #edges 9
2023-01-02 12:53:29.901953: I tensorflow/compiler/jit/build_xla_ops_pass.cc:603] print_outputs = 0
2023-01-02 12:53:29.901957: I tensorflow/compiler/jit/build_xla_ops_pass.cc:604] check_input_numerics = 0
2023-01-02 12:53:29.901961: I tensorflow/compiler/jit/build_xla_ops_pass.cc:605] check_output_numerics = 0
2023-01-02 12:53:29.901973: W tensorflow/core/util/dump_graph.cc:134] Failed to dump build_xla_ops because dump location is not  specified through either TF_DUMP_GRAPH_PREFIX environment variable or function argument.
2023-01-02 12:53:29.901983: I tensorflow/core/common_runtime/optimization_registry.cc:90] Finished optimization of a group 2
2023-01-02 12:53:29.902053: I tensorflow/core/common_runtime/optimization_registry.cc:92] Graph #nodes 6 #edges 9
2023-01-02 12:53:29.902132: I tensorflow/core/graph/graph_partition.cc:1251] Added send/recv: controls=0, data=0
2023-01-02 12:53:29.902170: I tensorflow/core/common_runtime/optimization_registry.cc:54] Starting optimization of a group 3
2023-01-02 12:53:29.902178: I tensorflow/core/common_runtime/optimization_registry.cc:67] Running optimization phase 1
2023-01-02 12:53:29.902186: I tensorflow/core/common_runtime/optimization_registry.cc:69] Running optimization pass: MklLayoutRewritePass
Segmentation fault (core dumped)
```
</details>"
59069,TypeError when modifying tf.keras.Optimizer parameter,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v1.15.0-rc3-22-g590d6eef7e 1.15.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 19042.2364

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 10.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have been trying to implement a (heavily-inspired) Gradient-Accumulation wrapper for my Adam optimizer (below) and have been receiving a strange TypeError.
```


### Standalone code to reproduce the issue

```shell
class AccumOptimizer(tf.keras.optimizers.Optimizer):

    def __init__(self, optimizer, steps_per_update=1, **kwargs):

        super(AccumOptimizer, self).__init__(name=""AccumOptimizer"", **kwargs)
        self.optimizer = optimizer
        self.steps_per_update = steps_per_update
        self.iterations = tf.Variable(0, dtype='int64', name='iterations')
        self.cond = tf.equal(self.iterations % self.steps_per_update, 0)
        self.lr = self.optimizer.learning_rate
        self.optimizer.learning_rate = tf.cond(self.cond,
                                               lambda: self.optimizer.learning_rate,
                                               lambda: tf.constant(0, tf.float32))
        ...
```


### Relevant log output

```shell
...
  File ""Q:\data\common\nets\ai_code_segmentation\siddharth\32_grad_accum\net.py"", line 455, in optimizer_fn
    optimizer = AccumOptimizer(optimizer=adam, steps_per_update=8)
  File ""Q:\data\common\nets\ai_code_segmentation\siddharth\32_grad_accum\grad_accum.py"", line 27, in __init__
    lambda: tf.constant(0, tf.float32))
  File ""c:\dlcodereaderprojects\dlearner_venv\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py"", line 557, in __setattr__
    self._set_hyper(name, value)
  File ""c:\dlcodereaderprojects\dlearner_venv\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py"", line 521, in _set_hyper
    backend.set_value(self._hyper[name], value)
  File ""c:\dlcodereaderprojects\dlearner_venv\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3199, in set_value
    value = np.asarray(value, dtype=dtype(x))
  File ""c:\dlcodereaderprojects\dlearner_venv\lib\site-packages\numpy\core\_asarray.py"", line 83, in asarray
    return array(a, dtype, copy=False, order=order)
TypeError: __array__() takes 1 positional argument but 2 were given
```
</details>"
59068,Unable to build in WSL2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04 in WSL2

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

1080Ti with 11GB memory

### Current Behaviour?

```shell
First, it got two warnings:

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/071fc48c32248196b17912cdab206f29fa6ded38.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/1ec021467da65c06cd4aa24ce81898b0759bd16b.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

Then, got errors:

ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1430, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1007, column 35, in _create_local_cuda_repository
                cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 747, column 52, in _get_cuda_config
                compute_capabilities = compute_capabilities(repository_ctx),
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 473, column 32, in compute_capabilities
                auto_configure_fail(""Invalid compute capability: %s"" % capability)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 353, column 9, in auto_configure_fail
                fail(""\n%sCuda Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail:
Cuda Configuration Error: Invalid compute capability: compute_112
ERROR: /home/someone/code/tensorflow/WORKSPACE:15:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1430, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1007, column 35, in _create_local_cuda_repository
                cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 747, column 52, in _get_cuda_config
                compute_capabilities = compute_capabilities(repository_ctx),
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 473, column 32, in compute_capabilities
                auto_configure_fail(""Invalid compute capability: %s"" % capability)
        File ""/home/someone/code/tensorflow/third_party/gpus/cuda_configure.bzl"", line 353, column 9, in auto_configure_fail
                fail(""\n%sCuda Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail:
Cuda Configuration Error: Invalid compute capability: compute_112
INFO: Found applicable config definition build:cuda in file /home/someone/code/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/someone/code/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda:
Cuda Configuration Error: Invalid compute capability: compute_112
```


### Standalone code to reproduce the issue

```shell
./configure

bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_</details>"
59067,TF 2.11.0: tf.image.ssim return_index_map=True outputs wrong shape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.1

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation for `tf.image.ssim` claims to output

> ... a tensor containing an SSIM value for each pixel for each image in batch if return_index_map is True

However, the output image is smaller than the source image (see example below).

Upon comparing with a more well-known library implemented in PyTorch, I believe the reason for such discrepency is due to the Conv2D padding used.

In PyTorch's implementation, a ""SAME"" padding is used

https://github.com/Po-Hsun-Su/pytorch-ssim/blob/3add4532d3f633316cba235da1c69e90f0dfb952/pytorch_ssim/__init__.py#L25

However, in the current Tensorflow implementation, ""VALID"" padding is used

https://github.com/tensorflow/tensorflow/blob/d5b57ca93e506df258271ea00fc29cf98383a374/tensorflow/python/ops/image_ops_impl.py#L4340

Please verify if this is the case.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf # tf.__version__ == ""2.11.0""
# B x T x n_mel
shape = (16, 2106, 80, 1)
image1 = np.arange(np.prod(shape))
image1 = (image1 / np.max(image1)) * 10 + 100
image1 = np.reshape(image1, shape)
image2 = np.linspace(0, 1, np.prod(shape))
image2 = np.exp(image2)
image2 = (image2 / np.max(image2)) * 10 + 100
image2 = np.reshape(image2, shape)

out_tf = tf.image.ssim(image1, image2, max_val=255, return_index_map=True)

```
which outputs a tensor of shape [16, 2096, 70].


### Relevant log output

_No response_</details>"
59062,something in tensor flow proses,
59061,"tf.image.extract_patches fails with XLA, float32 input and message about int64 input","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04, Mac OS X 12.6

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In some cases (see link below) tf.image.extract_patches with float32 input and enabled XLA says that forward pass OP requested INT64 dtype which is not registered.
But input dtype is float32...
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1WJwcmUZMlqQYSz_xwQ6I_5vlnlEmfkp3?usp=sharing
```


### Relevant log output

```shell
Detected unsupported operations when trying to compile graph __inference_run_step_3512[] on XLA_GPU_JIT: ExtractImagePatches (No registered 'ExtractImagePatches' OpKernel for XLA_GPU_JIT devices compatible with node {{node gradient_tape/model_2/temp_2/ExtractImagePatches}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, ksizes=[1, 16, 16, 1], padding=""SAME"", rates=[1, 1, 1, 1], strides=[1, 8, 8, 1]){{node gradient_tape/model_2/temp_2/ExtractImagePatches}}
```
</details>"
59059,Overflow error when running depthwise_conv2d_native_backprop_filter,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Sun_Feb_14_21:12:58_PST_2021 Cuda compilation tools, release 11.2, V11.2.152 Build cuda_11.2.r11.2/compiler.29618528_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If depthwise_conv2d_native_backprop_filter is given very large tensor, it results overflow error. I encountered this error when running the code in colab.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  arg_0_tensor = tf.random.uniform([4, 5, 5, 48], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([4], dtype=tf.int32, maxval=1879048192)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([4, 5, 5, 96], dtype=tf.float32)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_0 = 1
  arg_3_1 = 1
  arg_3_2 = 1
  arg_3_3 = 1
  arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,]
  arg_4 = ""SAME""
  explicit_paddings = []
  data_format = ""NHWC""
  dilations_0 = 1
  dilations_1 = 1
  dilations_2 = 1
  dilations_3 = 1
  dilations = [dilations_0,dilations_1,dilations_2,dilations_3,]
  out = gen_nn_ops.depthwise_conv2d_native_backprop_filter(arg_0,arg_1,arg_2,arg_3,arg_4,explicit_paddings=explicit_paddings,data_format=data_format,dilations=dilations,)
except Exception as e:
  print(""Error:""+str(e))
```
```

```shell
Error:{{function_node __wrapped__DepthwiseConv2dNativeBackpropFilter_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 485389066749118952 with 692120327, result: -1 [Op:DepthwiseConv2dNativeBackpropFilter]
```
</details>"
59058,tf.image.extract_patches fails to JIT-compile inside keras model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04, Mac OS X 12.6

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When trying to set jit_compile=True at model with Dense (Conv2D) layer followed by tf.image.extract_patches an error occured.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1-lXiMNp8NSe-c47-NAruypplraDId3O3?usp=sharing
```


### Relevant log output

```shell
Detected unsupported operations when trying to compile graph __inference_run_step_742[] on XLA_GPU_JIT: SparseTensorDenseMatMul (No registered 'SparseTensorDenseMatMul' OpKernel for XLA_GPU_JIT devices compatible with node {{node gradient_tape/model/temp/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}){{node gradient_tape/model/temp/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}
```
```
</details>"
59057,I want to solve this problem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

last version

### Custom Code

Yes

### OS Platform and Distribution

wendows

### Mobile device

_No response_

### Python version

python 3.11 

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""DeserializeSparse:0"", shape=(None, 2), dtype=int64), values=Tensor(""DeserializeSparse:1"", shape=(None,), dtype=float32), dense_shape=Tensor(""stack:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type
```


### Standalone code to reproduce the issue

```shell
def get_model(input_shape):
    """"""
    This function should build a Sequential model according to the above specification. Ensure the 
    weights are initialised by providing the input_shape argument in the first layer, given by the
    function argument.
    Your function should return the model.
    """"""
    model=Sequential([
        Conv2D(8 ,(3,3) , padding='same',activation='relu', input_shape=input_shape),
        MaxPool2D((2,2)),
        Flatten(),
        Dense(64,activation='relu'),
        Dense(64,activation='relu'),
        Dense(4, activation='softmax')
    ])
    return model
```


### Relevant log output

_No response_</details>"
59056,Migrate Early Stopping Code Error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf.2.6.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! I was looking for customize early stopping example to implement my model.I realized that https://www.tensorflow.org/guide/migrate/early_stopping#tensorflow_2_early_stopping_with_a_custom_training_loop  occurs.Then i realized a mistake(for me) in  'Early Stopping with a custom training loop' part.
```


### Standalone code to reproduce the issue

```shell
I saw this code in last part.

 # The early stopping strategy: stop the training if `val_loss` does not
    # decrease over a certain number of epochs.
    wait += 1
    if val_loss > best:
      best = val_loss
      wait = 0
    if wait >= patience:
      break
But in this condition,i think there is a mistake : Let's say val_loss = 0.1 for first epoch then best is equal to = 0.1 and wait = 0 normally.
In second epoch,if val_loss = 0.2 then best = 0.2 and wait = 0.
In third epoch,if val_loss = 0.3 then best = 0.3 and wait = 0
It continues until the end of training and wait cannot be bigger than patience in this training loop(if we want to decrease loss).
This condition does not behave like a early stopping.It gives a permission val_loss increases as training goes on.
I think that this code does not correspond to early stopping.If i am wrong , please tell me.

Bye !
```


### Relevant log output

_No response_</details>"
59052,StaticHashTable does not support complex values,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using a `tf.lookup.KeyValueTensorInitializer` where the `values` argument is a `tf.constant` of datatype `tf.complex64` or `tf.complex128` is passed to a `tf.lookup.StaticHashTable` as the `initializer` argument, an error is thrown
```


### Standalone code to reproduce the issue

```shell
init = tf.lookup.KeyValueTensorInitializer(
                keys=tf.constant([""a"",""b""]),
                values=tf.constant([1.0,2.0],dtype=tf.complex128))

Table = tf.lookup.StaticHashTable(
                initializer=init,
                default_value=-1)
```


### Relevant log output

_No response_</details>"
59051,Feature request for object detection android app,"Hello,

I deployed my Mobilenet trained model into your object detection android app and the detection looks good.
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

I would like to add two features to your app if possible:
1) Show on the phone screen number of detected objects of class X
2) Show on the phone screen the (x, y) coordinated of bounding box for detected objects of class X

Can you please help me to figure out how I can add these two features in the app. Look forward to hearing from you! "
59050,mixed_bfloat16 runs very slowly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.4

### GPU model and memory

A10G(24GB)

### Current Behaviour?

```shell
Setting mixed_bfloat16 policy made training ~500X slower, I've expected it to run at least as fast as float32.

I am using the tensorflow/tensorflow:latest-gpu, which currently holds tensorflow version 2.11.0
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import mixed_precision


def set_policy():
    policy = mixed_precision.Policy('mixed_bfloat16')
    mixed_precision.set_global_policy(policy)
    print('Compute dtype: %s' % policy.compute_dtype)
    print('Variable dtype: %s' % policy.variable_dtype)


def get_convnet():
    inputs = tf.keras.Input(shape=(28, 28, 1))
    conv1 = layers.Conv2D(32,
                          kernel_size=(3, 3),
                          activation='relu',
                          use_bias=False)(inputs)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = layers.Conv2D(64,
                          kernel_size=(3, 3),
                          activation='relu',
                          use_bias=False)(pool1)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)
    flatten = layers.Flatten()(pool2)
    dense = layers.Dense(10, use_bias=False)(flatten)
    outputs = layers.Activation('softmax', name='predictions',
                                dtype='float32')(dense)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model


def get_dataset():
    x_train = np.random.uniform(low=0.0, high=1.0, size=(60000, 28, 28))
    y_train = np.random.randint(low=0, high=10, size=(60000, ))
    return x_train, y_train


def train():
    model = get_convnet()
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=keras.optimizers.RMSprop(),
                  metrics=['accuracy'])
    x_train, y_train = get_dataset()
    history = model.fit(x_train,
                        y_train,
                        batch_size=8192,
                        epochs=3)

if __name__ == '__main__':
    print(f""TF version: {tf.__version__}"")
    train()
    set_policy()
    train()
```


### Relevant log output

```shell
root@debug-tf-bfloat16-tf-latest-ccggp:/# python profile_bfloat16.py
2022-12-29 14:23:22.916758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
TF version: 2.11.0
2022-12-29 14:23:24.084564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.087483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.088371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.089490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-29 14:23:24.089883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.090764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.091622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.513412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.514385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.515294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-29 14:23:24.516218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20764 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
Epoch 1/3
2022-12-29 14:23:26.157234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2022-12-29 14:23:27.455638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-12-29 14:23:27.461805: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fc53585c080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-29 14:23:27.461842: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6
2022-12-29 14:23:27.465412: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-29 14:23:27.559121: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
8/8 [==============================] - 3s 65ms/step - loss: 2.3078 - accuracy: 0.0991
Epoch 2/3
8/8 [==============================] - 0s 37ms/step - loss: 2.3031 - accuracy: 0.0983
Epoch 3/3
8/8 [==============================] - 0s 36ms/step - loss: 2.3027 - accuracy: 0.1023
Compute dtype: bfloat16
Variable dtype: float32
Epoch 1/3
8/8 [==============================] - 180s 22s/step - loss: 2.3070 - accuracy: 0.1016
Epoch 2/3
8/8 [==============================] - 180s 22s/step - loss: 2.3029 - accuracy: 0.1023
Epoch 3/3
1/8 [==>...........................] - ETA: 2:51 - loss: 2.3028 - accuracy: 0.1022
```
</details>"
59049,Training horizontally stacked layers does not happen in parallel," ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### Current Behaviour?

When using Keras to build a multiple layer model, by stacking layers horizontally instead of vertically (deep network), training happens sequentially, and adding more layers increases each epoch duration linearly, while GPU utilization and memory usage remains the same as a single layer regardless of the number of total layers.



### Standalone code to reproduce the issue

In the example below I would expect that each sub-network called though the `cnn_example()` function, would run in parallel before the `GlobalAveragePooling1D` operation is called. However this does not happen, regardless the number of times the layer is repeated. Is this expected behavior or am I missing something?
```
import tensorflow as tf
from tensorflow.keras import layers

input_img= layers.Input(shape=(112, 112, 3))

x = [cnn_example()(input_img) for _ in range(n_repeats)]
x = layers.Lambda(lambda x: tf.stack(x, axis=1))(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dense(1)(x)

model = tf.keras.Model(input_img, x)
```
"
59046,The second-order gradient of tf.math.log and tf.math.log1p is 0 when the input is NaN.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I was trying some loss functions and I find when the output of `tf.math.log1p` and `tf.math.log` is NaN, second-order gradients of one variable in these functions are still 0.0. Here is the equation of the loss function that causes this issue:

res = tf.math.log(tf.math.sqrt(1-x)-y+y)
```

When `1-x<0`, `tf.math.sqrt` outputs `NaN` and `res` is also `NaN`. However, the second-order derivative of `y` in this equation is `0.0`. Since `NaN` indicates some potential errors during model training and this special value should be manifested to the derivative, I think output `0.0` when receiving `NaN` is not optimal.

For your reference, I also tried on PyTorch and found that the second-order derivative of `y` in PyTorch is `NaN`, which I think would be better.


Please note that this issue happens in both `tf.math.log` and `tf.math.log1p` operators, while does not occur in operators such as `tf.math.sin`. Specifically, if I change the equation to `tf.math.sin(tf.math.sqrt(1-x)-y+y)`, the second-order derivative of y is `NaN`.
```


### Standalone code to reproduce the issue

```shell
Access this colab link (https://colab.research.google.com/drive/1UvQFl6wyvcrfqfu_x1lbOFVTHmO0n0bO?usp=sharing) or run below code to reproduce the issue. 


import tensorflow as tf
x = tf.Variable(100.)
y = tf.Variable(100.)
with tf.GradientTape() as g1:
  with tf.GradientTape() as g0:
    g0.watch(y)
    res = tf.math.log(tf.math.sqrt(1-x)-y+y)
  first_order_gradient = g0.gradient(res, y)
second_order_gradient = g1.gradient(first_order_gradient, y)
    
print(""TensorFlow's output is: "", first_order_gradient, second_order_gradient)
```

```
import torch

x = torch.tensor(100., requires_grad=True)
y = torch.tensor(100., requires_grad=True)
res = torch.log(torch.sqrt(1-x)-y+y)
first_order_gradient = torch.autograd.grad(res, (y,), create_graph=True)
second_order_gradient = torch.autograd.grad(first_order_gradient, (y,), create_graph=True)

print(""PyTorch's output is: "", first_order_gradient, second_order_gradient)
```
```


### Relevant log output

```shell
TensorFlow's output is:  tf.Tensor(nan, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)

PyTorch's output is:  (tensor(nan, grad_fn=<AddBackward0>),) (tensor(nan, grad_fn=<AddBackward0>),)
```
</details>"
59045,CUDA Library not being detected by Ubuntu-Preview,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu-Preview on WSL2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I expect when I import a module that has been installed for it to work.
```


### Standalone code to reproduce the issue

```shell
import keras_cv
from keras_cv import datasets
```


### Relevant log output

```shell
ModuleNotFoundError                       Traceback (most recent call last)
Cell In [2], line 2
      1 import time
----> 2 import keras_cv
      3 from tensorflow import keras
      4 import matplotlib.pyplot as plt

File ~/.local/lib/python3.10/site-packages/keras_cv/__init__.py:21
     18 version_check.check_tf_version()
     19 # isort:on
---> 21 from keras_cv import datasets
     22 from keras_cv import layers
     23 from keras_cv import losses

File ~/.local/lib/python3.10/site-packages/keras_cv/datasets/__init__.py:14
      1 # Copyright 2022 The KerasCV Authors
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
---> 14 from keras_cv.datasets import pascal_voc

File ~/.local/lib/python3.10/site-packages/keras_cv/datasets/pascal_voc/__init__.py:14
      1 # Copyright 2022 The KerasCV Authors
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
---> 14 from keras_cv.datasets.pascal_voc.load import load

File ~/.local/lib/python3.10/site-packages/keras_cv/datasets/pascal_voc/load.py:16
      1 # Copyright 2022 The KerasCV Authors
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
     15 import tensorflow as tf
---> 16 import tensorflow_datasets as tfds
     17 from tensorflow import keras
     19 from keras_cv import bounding_box

ModuleNotFoundError: No module named 'tensorflow_datasets'
```
</details>"
59043,8222c1cfc866126111f23bd9872998480cebf2c1.tar.gz    404,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://github.com/tensorflow/tensorflow/archive/8222c1cfc866126111f23bd9872998480cebf2c1.tar.gz  

this file can download at 2022.12.28,today[2022.12.29] is 404
```


### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/archive/8222c1cfc866126111f23bd9872998480cebf2c1.tar.gz  

this file can download at 2022.12.28,today[2022.12.29] is 404
```


### Relevant log output

_No response_</details>"
59042,Didn't find op for builtin opcode REVERSE_V2 version '1'.,"**System information**
- Linux
- TensorFlow installed from source:
- TensorFlow version 


**Provide the text output from tflite_convert**

```
Testing TestInvoke
Didn't find op for builtin opcode 'REVERSE_V2' version '1'. An older version of this builtin might be supported. Are you
 using an old TFLite binary with a newer model?

Failed to get registration from op code REVERSE_V2
```

**Standalone code to reproduce the issue** 
when i want to Invoke use tflite-micro,got a error:

Testing TestInvoke
Didn't find op for builtin opcode 'REVERSE_V2' version '1'. An older version of this builtin might be supported. Are you
 using an old TFLite binary with a newer model?

Failed to get registration from op code REVERSE_V2

can you tell me the reason？ thanks"
59041,"using a custom layer, loaded_model cannot give the same predicted value compared with original model [results not reproducible]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.4.3

### Custom Code

Yes

### OS Platform and Distribution

CentOS Linux release 8.2.2004

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA11, cuDNN8

### GPU model and memory

RTX 3090, 24268MiB

### Current Behaviour?

```shell
I implemented a custom layer, and use this layer to build a model. After training it, the original model gave a predicted value A, and the original model is saved as a h5 file. Then I load model from the h5 file, but the loaded model gave a different predicted value B, which means the results are not reproducible. Normally, the two models are supposed to give the same predicted value. 
    The custom layer is as simple as a Dense layer, and I have already locate that the custom layer caused the above problem. Actually, if I comment the line of custom layer, and uncomment the line below it (which is an original tf Dense layer), both the original model and the loaded_model gave the same results.
```


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3.8
import numpy as np
import random
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
os.environ['HOROVOD_FUSION_THRESHOLD'] = '0'
SEED = 1000
os.environ['PYTHONHASHSEED'] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
import tensorflow as tf
tf.config.run_functions_eagerly(True)
tf.random.set_seed(SEED)
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)

@tf.keras.utils.register_keras_serializable()
class Custom_Layer(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super(Custom_Layer, self).__init__(**kwargs)
        self.units = units
    def call(self, x):
        x = tf.keras.layers.Dense(self.units)(x)
        return x
    def get_config(self):
        config = super(Custom_Layer, self).get_config()
        config.update(units = self.units)
        return config

def build_model(input_shape, units):
    inputs = tf.keras.Input(shape=input_shape)
    x = Custom_Layer(units)(inputs) # problem is here
    # x = tf.keras.layers.Dense(units)(inputs) # if replace the above line with this line, can give the same result
    x = tf.keras.layers.Dense(1)(x)
    outputs = x[:,-1,:]
    return tf.keras.Model(inputs, outputs)

x_train = np.random.random((20000, 10, 50))
y_train = np.random.random(20000)
model = build_model(input_shape=x_train.shape[1:], units=256)
model.compile(loss='mse', optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0))
hist = model.fit(x_train, y_train, epochs=1)

x_test = np.random.random((5, 10, 50))
predicted_val = model.predict(x_test)
print (predicted_val)
h5_file_name = '/tmp/model.h5'
model.save(h5_file_name)

model2 = tf.keras.models.load_model(h5_file_name, custom_objects={'Custom_Layer':Custom_Layer})
predicted_val2 = model2.predict(x_test)
print (predicted_val2)
```


### Relevant log output

```shell
2022-12-29 10:33:52.034627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-29 10:33:53.136713: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-29 10:33:53.138072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-29 10:33:53.198743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3d:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-12-29 10:33:53.198834: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-29 10:33:53.203166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-29 10:33:53.203273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-29 10:33:53.204328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-29 10:33:53.204657: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-29 10:33:53.209031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-29 10:33:53.209857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-29 10:33:53.210030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-29 10:33:53.212456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-29 10:33:53.335970: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-29 10:33:53.348062: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-29 10:33:53.349549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3d:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-12-29 10:33:53.349604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-29 10:33:53.349644: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-29 10:33:53.349654: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-29 10:33:53.349664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-29 10:33:53.349673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-29 10:33:53.349682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-29 10:33:53.349691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-29 10:33:53.349701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-29 10:33:53.352041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-29 10:33:53.352076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-29 10:33:53.873305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-29 10:33:53.873357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-29 10:33:53.873366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-29 10:33:53.877072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22430 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3d:00.0, compute capability: 8.6)
/usr/local/lib64/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.
  warnings.warn(
2022-12-29 10:33:53.990568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2022-12-29 10:33:53.997553: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz
2022-12-29 10:33:54.015124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-29 10:33:54.679072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-29 10:33:54.679260: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
625/625 [==============================] - 5s 7ms/step - loss: 0.4327
[[0.44245386]
 [0.6916534 ]
 [0.49306032]
 [0.98741436]
 [0.7631112 ]]
[[0.48893338]
 [0.54947186]
 [0.40105245]
 [0.56347597]
 [0.32270208]]
```
</details>"
59039,model.fit() uses same chunk of validation data for all evaluation steps,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Linux - Google Colab

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
model.fit() api uses the same validation data chunk to evaluate after every epoch. Ideally, the evaluation should happen on the subsequent iter of the dataset provided.

## watch val_mean_label in log output
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255.0
labels = labels.astype(np.int32)

fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))
fmnist_train_ds = fmnist_train_ds.batch(1)

model = tf.keras.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

def mean_label(y_true, y_pred):
  return tf.reduce_mean(tf.cast(y_true, dtype=tf.float32), name='mean_label')

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[mean_label])

model.fit(fmnist_train_ds.repeat(), epochs=10, steps_per_epoch=1, 
          validation_data=fmnist_train_ds.repeat(), validation_steps=1, )
```


### Relevant log output

```shell
Epoch 1/10
1/1 [==============================] - 1s 587ms/step - loss: 2.8061 - mean_label: 9.0000 - val_loss: 2.2545 - val_mean_label: 9.0000
Epoch 2/10
1/1 [==============================] - 0s 16ms/step - loss: 2.0885 - mean_label: 0.0000e+00 - val_loss: 1.8890 - val_mean_label: 9.0000
Epoch 3/10
1/1 [==============================] - 0s 19ms/step - loss: 2.0514 - mean_label: 0.0000e+00 - val_loss: 1.6198 - val_mean_label: 9.0000
Epoch 4/10
1/1 [==============================] - 0s 24ms/step - loss: 2.0949 - mean_label: 3.0000 - val_loss: 1.4125 - val_mean_label: 9.0000
Epoch 5/10
1/1 [==============================] - 0s 19ms/step - loss: 1.2432 - mean_label: 0.0000e+00 - val_loss: 1.2543 - val_mean_label: 9.0000
Epoch 6/10
1/1 [==============================] - 0s 17ms/step - loss: 2.9918 - mean_label: 2.0000 - val_loss: 1.1256 - val_mean_label: 9.0000
Epoch 7/10
1/1 [==============================] - 0s 17ms/step - loss: 1.5534 - mean_label: 7.0000 - val_loss: 1.0380 - val_mean_label: 9.0000
Epoch 8/10
1/1 [==============================] - 0s 14ms/step - loss: 3.4063 - mean_label: 2.0000 - val_loss: 0.9567 - val_mean_label: 9.0000
Epoch 9/10
1/1 [==============================] - 0s 18ms/step - loss: 2.6006 - mean_label: 5.0000 - val_loss: 0.8956 - val_mean_label: 9.0000
Epoch 10/10
1/1 [==============================] - 0s 14ms/step - loss: 3.0951 - mean_label: 5.0000 - val_loss: 0.8475 - val_mean_label: 9.0000
<keras.callbacks.History at 0x7fb8c13942b0>
```
</details>"
59037,unable to build Tf lite .whl file for raspberry pi 0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf lite

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to generate a .whl file tensorflow lite for raspberry pi 0 architecture, using ARM cross compilation, by following the article https://www.tensorflow.org/lite/guide/build_cmake_pip. 
`tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh rpi0 ` After running this code, it throughs error in the process `cmake --build . --verbose -j 2 -t _pywrap_tensorflow_interpreter_wrapper`
```


### Standalone code to reproduce the issue

```shell
`tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh rpi0 `
```


### Relevant log output

```shell
file included from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/cmake/toolchains/arm-rpi-linux-gnueabihf/x64-gcc-6.5.0/arm-rpi-linux-gnueabihf/arm-rpi-linux-gnueabihf/sysroot/usr/include/limits.h:123:0,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/FXdiv-source/include/fxdiv.h:12,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/pthreadpool-source/src/portable-api.c:13:
/usr/include/limits.h:145:17: error: missing binary operator before token ""(""
 #if __GLIBC_USE (IEC_60559_BFP_EXT_C2X)
                 ^
make[3]: *** [pthreadpool/CMakeFiles/pthreadpool.dir/build.make:76: pthreadpool/CMakeFiles/pthreadpool.dir/src/portable-api.c.o] Error 1
make[3]: Leaving directory '/home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
make[2]: *** [CMakeFiles/Makefile2:5727: pthreadpool/CMakeFiles/pthreadpool.dir/all] Error 2
make[2]: *** Waiting for unfinished jobs....
In file included from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/cmake/toolchains/arm-rpi-linux-gnueabihf/x64-gcc-6.5.0/arm-rpi-linux-gnueabihf/arm-rpi-linux-gnueabihf/sysroot/usr/include/limits.h:123:0,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/base/config.h:52,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/base/attributes.h:37,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/base/log_severity.h:21,
                 from /home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/base/log_severity.cc:15:
/usr/include/limits.h:145:17: error: missing binary operator before token ""(""
 #if __GLIBC_USE (IEC_60559_BFP_EXT_C2X)
                 ^
make[3]: *** [_deps/abseil-cpp-build/absl/base/CMakeFiles/absl_log_severity.dir/build.make:76: _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_log_severity.dir/log_severity.cc.o] Error 1
make[3]: Leaving directory '/home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
make[2]: *** [CMakeFiles/Makefile2:2030: _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_log_severity.dir/all] Error 2
make[2]: Leaving directory '/home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
make[1]: *** [CMakeFiles/Makefile2:1283: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2
make[1]: Leaving directory '/home/athul/workspace/abdu/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
make: *** [Makefile:195: _pywrap_tensorflow_interpreter_wrapper] Error 2
```
</details>"
59036,Earlystopping 'auto' mode is garbage,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Trying to use the EarlyStopping callback, the documentation states for the 'auto' mode:
> mode: One of `{""auto"", ""min"", ""max""}`. In `min` mode,
        training will stop when the quantity
        monitored has stopped decreasing; in `""max""`
        mode it will stop when the quantity
        monitored has stopped increasing; in `""auto""`
        mode, the direction is automatically inferred
        from the name of the monitored quantity.

In fact the [tensorflow code](https://github.com/tensorflow/tensorflow/blob/cfc5ac84d80b705b8fc83350e7d5ca34e1daac17/tensorflow/python/keras/callbacks.py) is just:
```shell
if mode == 'min':
      self.monitor_op = np.less
    elif mode == 'max':
      self.monitor_op = np.greater
    else:
      if 'acc' in self.monitor:
        self.monitor_op = np.greater
      else:
        self.monitor_op = np.less
```

This means that for any metric that is basically not the accuracy the 'auto' mode will default to 'min', while a vast majority of performance metrics are designed to be increasing functions of performance.

This documentation is extremely misleading, and given this behavior I think we can safely say that almost nothing is inferred from the monitored metric.

In order to protect other users from this trap, I would advise to remove the 'auto' mode and its associated argument altogether.


### Standalone code to reproduce the issue

```shell
EarlyStopping(monitor='val_precision',
               mode='auto')
# this callback will tend to minimize precision...
```


### Relevant log output

_No response_</details>"
59035,[help wanted] tf.strings.operations raises TypeError: Value passed to parameter 'input' has DataType string not in list of allowed,"I'm just trying to create a superficial layer to pre-process data inside models. Here is an my Layer:
```
class StringLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(StringLayer, self).__init__()

  def call(self, inputs):
    return tf.strings.join([some_python_function(word) for word in tf.strings.split(tf.strings.as_string(inputs), sep="" "" )], separator="" "")
    
  

#model = tf.keras.models.Sequential()
#model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
#model.add(StringLayer())
```

But it keeps giving me: **`TypeError: Value passed to parameter 'input' has DataType string not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64, complex64, complex128, bool, variant`**

*I know I can do this thing out of the model, but my main goal is to somehow do this inside the model; this way, I'll be able to maximize the GPU utilization.*"
59032,Can't build tensorflow lite 2.10.0,"I try to cross-build tensorflow 2.10 to arm32 arch, but get next error:

```
#0 150.8 /tflite-vx-delegate/build/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c: In function 'xnn_qs8_vcvt_ukernel__armv6simd_x8':
#0 150.8 /tflite-vx-delegate/build/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:9: error: unknown type name 'int16x2_t'
#0 150.8    const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
```

<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I try to cross-build tensorflow 2.10 to arm32 arch, but get next error:

#0 150.8 /tflite-vx-delegate/build/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c: In function 'xnn_qs8_vcvt_ukernel__armv6simd_x8':
#0 150.8 /tflite-vx-delegate/build/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:9: error: unknown type name 'int16x2_t'
#0 150.8    const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
```
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
59030,"""Prelu"" in Tensorflow lite with the Hexagon Delegate not supported","Hello,

When using a 8-bits quantized tflite model with ""Prelu"", It was not running successfully.
It seems to be caused by Prelu not supporting it by TfLiteHexagonDelegate.
(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md ).
Prelu is supported ops from Hexagon nnlib, but is not supported Tensorflow lite with the Hexagon Delegate.( https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/docs/ops.txt ).  

If there is a plan to support ""prelu"" in TfLiteHexagonDelegate,
Could you shared the schedule?"
59026,TFlite 2.11.0 C API no longer compiles due to missing dependency,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am using the approach described here to integrate C Api for tensorflow lite: https://www.tensorflow.org/lite/android/development#tools_for_building_with_c_and_c

According to the description I should do the following: 


> Using this API is the recommended approach for developers using the NDK. Download the TensorFlow Lite AAR hosted at MavenCentral file, rename to tensorflow-lite-*.zip, and unzip it. You must include the four header files in the headers/tensorflow/lite/ and headers/tensorflow/lite/c/ folders and the relevant libtensorflowlite_jni.so dynamic library in the jni/ folder in your NDK project.
> 
> The c_api.h header file contains basic documentation about using the TFLite C API.


This approach works for any Tflite version prior to 2.11. When I try to upgrade to a v2.11 I get the following error: 

```
xxx/tensorflow-lite/headers/tensorflow/lite/c/c_api.h:20:10: fatal error: 'tensorflow/lite/core/c/c_api.h' file not found
```

This happens due to a recent changes that we introduced in v2.11 and replaced the content of the `tensorflow/lite/c/c_api.h` header file to the following: 

```
#ifndef TENSORFLOW_LITE_C_C_API_H_
#define TENSORFLOW_LITE_C_C_API_H_

/// For documentation, see
/// third_party/tensorflow/lite/core/c/c_api.h.
#include ""tensorflow/lite/core/c/c_api.h""  // IWYU pragma: export

#endif  // TENSORFLOW_LITE_C_C_API_H_
```

The problem is that `tensorflow/lite/core/c/c_api.h` is not included in the aar file in [Maven repo ](https://search.maven.org/artifact/org.tensorflow/tensorflow-lite/2.11.0/aar)



### Standalone code to reproduce the issue

Follow the guidelines described in the [TFLite C API](https://www.tensorflow.org/lite/android/development#tools_for_building_with_c_and_c) and use the [2.11.0 version aar from the Maven repo ](https://search.maven.org/artifact/org.tensorflow/tensorflow-lite/2.11.0/aar)


Error during build: 

```
xxx/tensorflow-lite/headers/tensorflow/lite/c/c_api.h:20:10: fatal error: 'tensorflow/lite/core/c/c_api.h' file not found
```



### Relevant log output

```
xxx/tensorflow-lite/headers/tensorflow/lite/c/c_api.h:20:10: fatal error: 'tensorflow/lite/core/c/c_api.h' file not found
```"
59025,ERROR: Could not find a version that satisfies the requirement tensorflow==1.15.2,"I use google colab and was wondering if anyone can solve this problem, if you want to fix my notebook I will leave a link down below, thanks if you solve it!
https://colab.research.google.com/drive/1ecuUD2sxfuM6IOhjZEW3ls-jvbpJEei4?usp=share_link"
59024,Support for keepdims and padding in tf.boolean_mask or tf.ragged.boolean_mask,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Given two example tensors `input` and `mask`:


```
>>> input = tf.random.normal([2,3,5])
>>> input
<tf.Tensor: shape=(2, 3, 5), dtype=float32, numpy=
array([[[ 1.1260294 , -0.05932725,  0.85893923, -1.5332409 ,
          0.6681451 ],
        [ 0.8833729 ,  0.8421117 , -0.60990584,  0.08593109,
          0.5969471 ],
        [ 0.20015325, -0.9459327 , -1.0818844 , -1.7254639 ,
         -0.51545954]],

       [[-0.36073774, -0.24315724,  1.5217028 ,  1.5075827 ,
          0.05745999],
        [-0.2570101 ,  1.5501927 , -0.17113225,  0.16063859,
         -0.95638955],
        [ 0.48955616,  0.11943919, -0.3523262 ,  0.10750653,
          1.1027677 ]]], dtype=float32)>

>>> mask = tf.constant([[0,1,0],[1,0,1]])
>>> mask
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 1, 0],
       [1, 0, 1]], dtype=int32)>

```

I need to mask out `input` according to `mask` where values are 0. However, since the number of masked out elements for each example in the batch `input` might be different, to keep the output a valid tensor, the output should be:

```
>>> masked_input
<tf.Tensor: shape=(2, 3, 5), dtype=float32, numpy=
array([[[ 0.8833729 ,  0.8421117 , -0.60990584,  0.08593109,
          0.5969471 ],
        [ 0 , 0 , 0 , 0 ,
         0],
        [ 0 , 0 , 0 , 0 ,
         0]],

       [[-0.36073774, -0.24315724,  1.5217028 ,  1.5075827 ,
          0.05745999],
        [ 0.48955616,  0.11943919, -0.3523262 ,  0.10750653,
          1.1027677 ],
        [ 0 , 0 , 0 , 0 ,
          0]]], dtype=float32)>
```
i.e. in the output, the masked input keeps only elements where `mask` is 1, and, with zero-padding at the end to ensure that the output is a valid tensor.



I've searched around and tried using:
1. `tf.gather`, however, still can't figure out how to proceed.
2. `tf.boolean_mask`, however, it doesn't support masking but just drops the first (zeroth) dimension, as shown below:
```
>>> tf.boolean_mask(input, mask)
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[ 0.8833729 ,  0.8421117 , -0.60990584,  0.08593109,  0.5969471 ],
       [-0.36073774, -0.24315724,  1.5217028 ,  1.5075827 ,  0.05745999],
       [ 0.48955616,  0.11943919, -0.3523262 ,  0.10750653,  1.1027677 ]],
      dtype=float32)>
```
3. `tf.ragged.boolean_mask`, this is by far the closest one to what I want, it keeps the dimension, however, still doesn't support masking, so the result is a ragged tensor...


Similar issues are mentioned in GitHub: https://github.com/tensorflow/tensorflow/issues/18238

In short:
```
tensor = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
mask = np.array([[True, False, True], [False, False, True], [True, True, True]])
boolean_mask(tensor, mask, keepdims=False) # [1, 3, 6, 7, 8, 9]
boolean_mask(tensor, mask, keepdims=True, pad_val=0) # [[1, 3, 0], [6, 0, 0], [7, 8, 9]] 
```


### Standalone code to reproduce the issue

```
As mentioned above
```


### Relevant log output

_No response_</details>"
59023,struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): v2.9

### 2. Code & Description
While converting a diffusion model with TFLiteConverter, it fails because the size of the model is bigger than 2gb
[reproducer gist](https://gist.github.com/kkimmk/67ca90ffe9097979619772e651b5cf18)

### I wonder if it's an inevitable issue like memory size, so that I need to modify the model(e.g. quantize in advance) to convert it. Or is there any option-like things that can allow this larger model size.
Any suggestions would be appreciated.

### 3. Failure after conversion
Conversion fails

### 5. (optional) Any other info / logs
Error log
```
2022-12-27 14:50:46.618014: E tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2015] Model size is bigger than 2gb
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
Traceback (most recent call last):
  File ""text2image.py"", line 84, in <module>
    generator.convert()
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/stable_diffusion_tf/stable_diffusion.py"", line 74, in convert
    tflite_model = converter.convert()
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 929, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 914, in _convert_and_export_metrics
    model_object = flatbuffer_utils.convert_bytearray_to_object(result)
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/venv/lib/python3.8/site-packages/tensorflow/lite/tools/flatbuffer_utils.py"", line 37, in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/venv/lib/python3.8/site-packages/tensorflow/lite/python/schema_py_generated.py"", line 5667, in GetRootAsModel
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
  File ""/home/minkyukim/projects/stable-diffusion-tensorflow/venv/lib/python3.8/site-packages/flatbuffers/encode.py"", line 26, in Get
    return packer_type.unpack_from(memoryview_type(buf), head)[0]
struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)
```
SavedModel size before conversion
```
(venv) $ du -sh *
4.0K    assets
18M     saved_model.pb
3.3G    variables
```"
59022,[NVIDIA XLA] large GPU memory overhead observed due to seems un-necessary copy operators introduced by XLA ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf r2.11

### Custom Code

No

### OS Platform and Distribution

Linux ipp1-1316 5.4.0-125-generic #141-Ubuntu SMP Wed Aug 10 13:42:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.30

### GCC/Compiler version

gcc

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I am trying to debug an OOM issue for LLM model, seems that one possible cause of memory consumption is from un-necessary copy operators in XLA. From the HLO graph, we observed that the input buffer is aliased to output buffer, so it is donated to XLA.  When entry computation uses the input buffer, it first creates a copy of the input buffer, and use the copied buffer inside computation, but the original input buffer is never used further. 

Attaching the HLO graph for it.  Here, you can see that parameter(4) of ENTRY computation is aliased. {4}: (4, {}, may-alias), but inside ENTRY computation: 
```
  Arg_4.5 = f32[51200]{0} parameter(4), sharding={replicated}
  copy.339 = f32[51200]{0} copy(Arg_4.5) 
```
i.e, input buffer is first copied, and later operators are using the copy.339, while Arg_4.5 is never used again. 

Why copy.339 is introduced?  Buffer donation is enabled for this case, and should the copy not exist?

We observed a lot of similar cases for other input parameters, due to this reason, the copy could cause additional several GB of memory waste, we really want to optimize for this case.



### Standalone code to reproduce the issue


I upload all the necessary HLO dumps required for debug here: 
https://drive.google.com/drive/folders/1wiOtUZVAXDdkCISunvjGVIrhzLhHY1M0?usp=sharing

Please check the final HLO graph: https://drive.google.com/file/d/14WtEvyOpqYFlyWP4Ez7qb4Dw6zpIE1wM/view?usp=share_link

As the HLO dump is captured from an full model, we are not able to provide the re-produce step from source code, but I think analysis the HLO graph should be enough for this bug.



### Relevant log output

_No response_</details>"
59021,tensor_util.py,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf.2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Windows 10

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! Please refer ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_util.py"" line 495, in make_tensor_proto

shape = [int(dim) for dim in shape] gives error. It is not possible to iterate with integer. Needs to be fixed.
```


### Standalone code to reproduce the issue

```shell
https://github.com/korayerbas/HDRCNN.git
It is Eilertsen's HDRCNN work from 2017. I try to rewrite the code; I try to rewrite the code in version 2 Tensorflow. As described above I absorved the bug.
```


### Relevant log output

```shell
TypeError: 'int' object is not iterable
```
</details>"
59020,RuntimeError: MetaGraphDef when converting to tflite,"### 1. System information

- OS Platform and Distribution: Windows 10 Pro 21H2
- TensorFlow installation: 2.10.0
- TensorFlow library: pip package

### 2. Code

converter = tf.lite.TFLiteConverter.from_saved_model(input_model_path) # path to the SavedModel directory
converter.allow_custom_ops = True
converter.optimizations =  [tf.lite.Optimize.DEFAULT]]
tflite_model = converter.convert()

I am using this model: https://drive.google.com/file/d/1zyEqDumc296TiZ6WxsgVPBDJVIMWvZKf/view?usp=share_link


### 3. Failure after conversion
I get this error:

C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\loader_impl.py:102: RuntimeWarning: Unexpected end-group tag: Not all data was converted
  saved_model.ParseFromString(file_content)
Traceback (most recent call last):
  File ""D:\JOSE\Documents\Proyectos C#\Pokedex\Pokedex.ModelBuilder\bin\Debug\net6.0\Workspace\tflite_converter.py"", line 16, in <module>
    convert_model('.', 'D:\JOSE\Documents\Proyectos C#\Pokedex\Pokedex.ModelBuilder\bin\Debug\net6.0\Workspace\model')
  File ""D:\JOSE\Documents\Proyectos C#\Pokedex\Pokedex.ModelBuilder\bin\Debug\net6.0\Workspace\tflite_converter.py"", line 5, in convert_model
    converter = tf.lite.TFLiteConverter.from_saved_model(input_model_path) # path to the SavedModel directory
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\lite\python\lite.py"", line 1789, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\load.py"", line 800, in load
    result = load_partial(export_dir, None, tags, options)[""root""]
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\load.py"", line 949, in load_partial
    root = load_v1_in_v2.load(export_dir, tags)
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\load_v1_in_v2.py"", line 284, in load
    result = loader.load(tags=tags)
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\load_v1_in_v2.py"", line 209, in load
    meta_graph_def = self.get_meta_graph_def_from_tags(tags)
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\load_v1_in_v2.py"", line 88, in get_meta_graph_def_from_tags
    return super(_EagerSavedModelLoader, self).get_meta_graph_def_from_tags(
  File ""C:\Users\JOSE\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 394, in get_meta_graph_def_from_tags
    raise RuntimeError(
RuntimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel, with available tags '[]'. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`.
"
59018,"tf.truncatediv not support float, but doc support.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.9.1-132-g18960c44ad3 2.9.2

### Custom Code

Yes

### OS Platform and Distribution

colab

### Mobile device

colab

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The tf.truncatediv documentation shows that float is supported, but the actual test only supports integer types.
document: https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/truncatediv
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

a = np.array([1., 2., 3.])
x = tf.convert_to_tensor(a)
b = np.array([2.])
y = tf.convert_to_tensor(b)
tf.truncatediv(x, y)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node TruncateDiv}} = TruncateDiv[T=DT_DOUBLE]
All kernels registered for op TruncateDiv:
  device='GPU'; T in [DT_UINT64]
  device='GPU'; T in [DT_UINT32]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT64]
  device='CPU'; T in [DT_UINT32]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_UINT8]
 [Op:TruncateDiv]
```
```
</details>"
59017,error: 'tf.Conv2D' op is neither a custom op nor a flex op,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): v2.10

### 2. Code
Code for conversion
```
converter = tf.lite.TFLiteConverter.from_saved_model(f'savedmodel/decoder')
tflite_model = converter.convert()

# save the model
with open(f'{name}.tflite', 'wb') as f:
      f.write(tflite_model)
```
Code for the model
```
latent = keras.layers.Input((n_h, n_w, 4))
decoder = Decoder()
decoder = keras.models.Model(latent, decoder(latent))
```
```
class Decoder(keras.Sequential):
    def __init__(self):
        super().__init__(
            [
                keras.layers.Lambda(lambda x: 1 / 0.18215 * x),
                PaddedConv2D(4, 1),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 512),
                AttentionBlock(512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 256),
                ResnetBlock(256, 256),
                ResnetBlock(256, 256),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(256, 3, padding=1),
                ResnetBlock(256, 128),
                ResnetBlock(128, 128),
                ResnetBlock(128, 128),
                tfa.layers.GroupNormalization(epsilon=1e-5),
                keras.layers.Activation(""swish""),
                PaddedConv2D(3, 3, padding=1),
            ]
        )
```

### 3. Failure after conversion
conversion fails


### 5. (optional) Any other info / logs
[error.log](https://github.com/tensorflow/tensorflow/files/10302790/error.log)
```
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Conv2D
Details:
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x512x512xf32>) -> (tensor<?x?x?x512xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x128x128xf32>) -> (tensor<?x?x?x128xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x128x3xf32>) -> (tensor<?x?x?x3xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x256x128xf32>) -> (tensor<?x?x?x128xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x256x256xf32>) -> (tensor<?x?x?x256xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x512x256xf32>) -> (tensor<?x?x?x256xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<3x3x512x512xf32>) -> (tensor<?x?x?x512xf32>) : {data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
```
According to the error message, I suspect that it can not recognize the input shape. But as you can see on the above code, input is specified for the functional API for `decoder` model. 
(FYI, The inference code is called with `predict_on_batch` method. I found out other model with `predict_on_batch` is converted successfully, but that model doesn't contain `conv2d` block inside. Can using `predict_on_batch` together with `conv2d` be a problem?)

**I'm sure `conv2d` is on the allowlist for TFLite operators. Any suggestions for this problem? Thank you.**"
59016,Window only link,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

na

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The Windows Only link is not working
```


### Standalone code to reproduce the issue

```shell
Try and download the Window only version of tensorflow and it fails
This XML file does not appear to have any style information associated with it. The document tree is shown below.
<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>No such object: tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip</Details>
</Error>
```


### Relevant log output

_No response_</details>"
59015,Crash when running tensorflow.python.ops.random_grad.add_leading_unit_dimensions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.random_grad.add_leading_unit_dimensions is given very large integer for num_dimensions parameter, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import random_grad
try:
  arg_0 = 1.0
  arg_1 = 12509
  random_grad.add_leading_unit_dimensions(arg_0,arg_1,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 15:35:53.205931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:35:54.166401: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:35:54.166953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 15:35:54.202334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.202435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:35:54.202447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:35:54.203889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:35:54.203915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:35:54.204525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:35:54.204657: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:35:54.206152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:35:54.206475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:35:54.206540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:35:54.206591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.206694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.206765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:35:54.207465: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:35:54.207516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.207592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:35:54.207602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:35:54.207613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:35:54.207623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:35:54.207632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:35:54.207642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:35:54.207651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:35:54.207665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:35:54.207675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:35:54.207705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.207796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.207861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:35:54.207877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:35:54.516903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 15:35:54.516938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 15:35:54.516943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 15:35:54.517135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.517331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.517417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:35:54.517495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4729 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 15:35:54.558727: F tensorflow/core/framework/tensor_shape.cc:346] Check failed: ndims_byte() < MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor
Aborted (core dumped)
```
</details>"
59014,Segmentation fault when running tensorflow.python.ops.nn_impl.fused_batch_norm,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.nn_impl.fused_batch_norm is given empty input parameter scale, it results in segmentation fault.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import nn_impl
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 2, 6, 1, 3], dtype=tf.float16)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = [] 
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      mean_tensor = tf.random.uniform([2], dtype=tf.float32)
      mean = tf.identity(mean_tensor)
      variance_tensor = tf.random.uniform([2], dtype=tf.float32)
      variance = tf.identity(variance_tensor)
      epsilon = 0.001
      exponential_avg_factor = 1.0
      data_format = ""NCDHW""
      is_training = False
      nn_impl.fused_batch_norm(arg_0,arg_1,arg_2,mean=mean,variance=variance,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float16)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      mean = tf.identity(mean_tensor)
      mean = tf.cast(mean, tf.float32)
      variance = tf.identity(variance_tensor)
      variance = tf.cast(variance, tf.float32)
      nn_impl.fused_batch_norm(arg_0,arg_1,arg_2,mean=mean,variance=variance,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    ""Error:""+str(e)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 15:33:36.370737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:33:37.325593: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:33:37.326080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 15:33:37.353069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.353172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:33:37.353185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:33:37.354612: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:33:37.354640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:33:37.355345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:33:37.355490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:33:37.357165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:33:37.357505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:33:37.357581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:33:37.357642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.357774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.357846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:33:37.358582: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:33:37.358640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.358719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:33:37.358729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:33:37.358741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:33:37.358750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:33:37.358758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:33:37.358766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:33:37.358774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:33:37.358782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:33:37.358791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:33:37.358821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.358913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.358978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:33:37.358995: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:33:37.668552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 15:33:37.668580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 15:33:37.668585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 15:33:37.668721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.668837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.668926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:33:37.669004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4714 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
Segmentation fault (core dumped)
```
</details>"
59013,Issue with CUDA library directory not being detected properly by Tensorflow Ubuntu 22.04,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have followed the exact instruction of installing TensorFlow under pip and made sure that I had installed the right drivers from the official Ubuntu repositories (`nvidia-driver-510`, `nvidia-cuda-toolkit`, `nvidia-cudnn`). Then ran `python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""` which was successful with only the obvious warning that I had not installed the optional [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)

However, I had this lingering issue when running simple model using [Tensorflow beginner tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner) that it kept stating that my GPU is out of memory. 

It turns out that it could not find the CUDA library directory which expected it to be installed under `/usr/local/cuda`, but Ubuntu `apt` installed it in `/usr/lib/cuda`. The temporary solution I have made was to create a symlink `sudo ln -s /usr/lib/cuda /usr/local/cuda` which made TensorFlow work and run the model (so far everything is working fine).

Furthermore, I remember having this problem back when I was Pop!_OS 20.04 in 2020.
```


### Standalone code to reproduce the issue

```shell
If you start a new Ubuntu 22.04 desktop, first use a *default* install of [Anaconda base (version 2022.10)](https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh) then do the following:


# Install nvidia drivers and the cuda + cudnn libraries
sudo apt install nvidia-driver-510 nvidia-cuda-toolkit nvidia-cudnn

# Run the [Tensorflow setup](https://www.tensorflow.org/install/pip) - Last updated 2022-12-20 UTC.
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/

# Enter Anaconda base first before continuing at the rest of the setup
conda activate base
python3 -m pip install tensorflow
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""


It then should display like the following:

```
2022-12-25 20:02:49.726066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-25 20:02:50.208855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-12-25 20:02:50.208895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-12-25 20:02:50.208901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-12-25 20:02:50.833468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 20:02:50.858985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 20:02:50.859152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

Using the [TensorFlow 2 quickstart for beginners notebook](https://www.tensorflow.org/tutorials/quickstart/beginner) - Last updated 2022-12-23 UTC - first install jupyter under the Anaconda base:

```bash
pip install notebook
```

Run all, this block should fail:

## Train and evaluate your model

Use the `Model.fit` method to adjust your model parameters and minimize the loss: 

```Python
model.fit(x_train, y_train, epochs=5)
```

It returns this error:

```
Epoch 1/5
2022-12-25 20:03:55.460107: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.460453: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.487711: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.487986: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.504571: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.504808: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.520155: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
2022-12-25 20:03:55.520379: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
Output exceeds the size limit. Open the full output data in a text editor
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
/tmp/ipykernel_16666/2168363081.py in <module>
----> 1 model.fit(x_train, y_train, epochs=5)

~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_2' defined at (most recent call last):
    File ""/home/srking501/anaconda3/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/srking501/anaconda3/lib/python3.9/runpy.py"", line 87, in _run_code
...
    File ""/home/srking501/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_2'
libdevice not found at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc
	 [[{{node StatefulPartitionedCall_2}}]] [Op:__inference_train_function_40623]
```

As you can see it is trying to access `/usr/local/cuda` however Ubuntu 22.04 default installation of Nvidia CUDA is in `/usr/lib/cuda`. Therefore, if you create a symlink to the actual place of CUDA it should work when you rerun the model: 

```
sudo ln -s /usr/lib/cuda/ /usr/local/cuda
```

(tl;dr) In conclusion, the problem is that it is trying to find CUDA in `/usr/local/cuda` but Ubuntu default location of CUDA library by Ubuntu's official repositories is in `/usr/lib/cuda`, thus requiring to make the solution of symlink (look right above this paragraph).
```


### Relevant log output

```shell
2022-12-25 20:11:21.322263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-25 20:11:21.806424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-12-25 20:11:21.806462: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-12-25 20:11:21.806467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
v2.11.0-rc2-17-gd5b57ca93e5 2.11.0
```
</details>"
59012,Crash when running tensorflow.python.ops.gen_data_flow_ops.record_input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_data_flow_ops.record_input is given input parameter file_parallelism with negative integers, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import gen_data_flow_ops
try:
  file_pattern = ""/tmp/record_input_test3nvh1t09/tmp3gauzk6b/basic.*""
  file_buffer_size = 1
  file_parallelism = -1
  file_shuffle_shift_ratio = 0
  batch_size = 1
  file_random_seed = 0
  compression_type = """"
  gen_data_flow_ops.record_input(file_pattern=file_pattern,file_buffer_size=file_buffer_size,file_parallelism=file_parallelism,file_shuffle_shift_ratio=file_shuffle_shift_ratio,batch_size=batch_size,file_random_seed=file_random_seed,compression_type=compression_type,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 15:04:55.069391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:04:56.000105: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:04:56.000646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 15:04:56.029995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.030099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:04:56.030112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:04:56.031508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:04:56.031536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:04:56.032170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:04:56.032294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:04:56.033768: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:04:56.034086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:04:56.034154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:04:56.034207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.034311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.034381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:04:56.035473: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 15:04:56.035523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.035601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 15:04:56.035611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:04:56.035621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 15:04:56.035629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 15:04:56.035637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 15:04:56.035645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 15:04:56.035653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 15:04:56.035674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 15:04:56.035682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 15:04:56.035733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.035820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.035883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 15:04:56.035899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 15:04:56.343768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 15:04:56.343794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 15:04:56.343799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 15:04:56.343930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.344044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.344131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 15:04:56.344207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4733 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 15:04:56.345386: F tensorflow/core/platform/threadpool.cc:99] Check failed: num_threads >= 1 (0 vs. 1)
Aborted (core dumped)
```
</details>"
59011,Crash when running tf.test.create_local_cluster,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tf.test.create_local_cluster is given input parameter num_workers with negative integers, it results in crash via high memory usage.
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import os
try:
  num_workers = -2
  num_ps = 2
  tf.test.create_local_cluster(num_workers=num_workers,num_ps=num_ps,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:47:09.860092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:10.804554: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:47:10.805055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:47:10.834019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.834127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:47:10.834141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:10.835582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:47:10.835611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:47:10.836274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:47:10.836408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:47:10.837944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:47:10.838270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:47:10.838382: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:47:10.838472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.838617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:10.838725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:47:10.838743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:11.149744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:47:11.149777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:47:11.149783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:47:11.149944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.150254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:ps/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:47:11.152650: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:15553, 1 -> localhost:16499}
2022-12-25 14:47:11.152660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {}
2022-12-25 14:47:11.153114: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:15553
2022-12-25 14:47:11.153309: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:47:11.153410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:47:11.153515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:47:11.153546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:47:11.153556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:47:11.153566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:47:11.153576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:47:11.153585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:47:11.153594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:47:11.153604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:47:11.153649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:47:11.153820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:47:11.153825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:47:11.153828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:47:11.153874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.153960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:47:11.154027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:ps/replica:0/task:1/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:47:11.157618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:15553, 1 -> localhost:16499}
2022-12-25 14:47:11.157632: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {}
2022-12-25 14:47:11.157808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:16499
```
</details>"
59010,Crash when running tensorflow.python.ops.gen_bitwise_ops.population_count,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When tensorflow.python.ops.gen_bitwise_ops.population_count is given empty input parameter tensor, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import gen_bitwise_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = [] 
      arg_0 = tf.identity(arg_0_tensor)
      gen_bitwise_ops.population_count(arg_0,)
  except Exception as e:
    ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int8)
      gen_bitwise_ops.population_count(arg_0,)
  except Exception as e:
    ""Error:""+str(e)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:40:39.610623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:40:40.542998: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:40:40.543474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:40:40.581144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.581307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:40:40.581330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:40:40.583691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:40:40.583735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:40:40.584684: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:40:40.584890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:40:40.587243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:40:40.587742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:40:40.587842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:40:40.587915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.588063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.588158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:40:40.589082: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:40:40.589154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.589258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:40:40.589274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:40:40.589289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:40:40.589302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:40:40.589315: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:40:40.589329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:40:40.589341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:40:40.589353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:40:40.589365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:40:40.589414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.589502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.589566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:40:40.589582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:40:40.895564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:40:40.895590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:40:40.895595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:40:40.895758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.895871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.895958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:40:40.896034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4723 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:40:40.936250: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted (core dumped)
```
</details>"
59009,Crash when running tensorflow.python.ops.gen_array_ops.upper_bound,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When tensorflow.python.ops.gen_array_ops.upper_bound is given input parameter bound with ranks other than 2, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import gen_array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 0], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = tf.int32
      arg_3 = None
      gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.int32
      gen_array_ops.upper_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    ""Error:""+str(e)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:37:30.681345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:37:31.659967: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:37:31.660460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:37:31.700159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.700326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:37:31.700349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:37:31.702845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:37:31.702893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:37:31.703997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:37:31.704197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:37:31.706532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:37:31.707062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:37:31.707172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:37:31.707255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.707422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.707533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:37:31.708628: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:37:31.708712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.708832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:37:31.708862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:37:31.708876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:37:31.708889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:37:31.708901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:37:31.708913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:37:31.708925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:37:31.708937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:37:31.708949: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:37:31.708993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.709118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:31.709207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:37:31.709229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:37:32.025546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:37:32.025572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:37:32.025577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:37:32.025707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:32.025820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:32.025909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:37:32.025987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4723 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
```
</details>"
59008,Crash when running tensorflow.python.ops.gen_collective_ops.collective_gather,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_collective_ops.collective_gather is given input parameter tensor with ranks other than 1, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import gen_collective_ops
try:
  arg_0_tensor = tf.random.uniform([], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  shape_0 = 0
  shape = [shape_0,]
  group_size = 1
  group_key = 100
  instance_key = 100
  communication_hint = ""ring""
  timeout_seconds = 0
  gen_collective_ops.collective_gather(arg_0,shape=shape,group_size=group_size,group_key=group_key,instance_key=instance_key,communication_hint=communication_hint,timeout_seconds=timeout_seconds,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:28:42.276547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:28:43.989383: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:28:43.994221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:28:44.069864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.070300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:28:44.070361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:28:44.086951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:28:44.087097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:28:44.097183: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:28:44.100602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:28:44.115457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:28:44.118696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:28:44.119993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:28:44.120152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.120438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.120903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:28:44.123572: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:28:44.123709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.123886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:28:44.123912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:28:44.123935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:28:44.123955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:28:44.123974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:28:44.123992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:28:44.124010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:28:44.124029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:28:44.124047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:28:44.124123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.124324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.124463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:28:44.124807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:28:44.716294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:28:44.716317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:28:44.716322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:28:44.716854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.716974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.717065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:28:44.717144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4726 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:28:44.761449: F tensorflow/core/framework/tensor_shape.cc:434] Check failed: d < dims() (0 vs. 0)
Aborted (core dumped)
```
</details>"
59007,Crash when running tensorflow.python.ops.ragged.ragged_string_ops.ngrams,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When tensorflow.python.ops.ragged.ragged_string_ops.ngrams is given input parameter ngram_width with very large elements, the program crashes due to high memory usage.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops.ragged import ragged_string_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = ""b'a'""
      arg_0_0_1 = ""b'z'""
      arg_0_0 = [arg_0_0_0,arg_0_0_1,]
      arg_0_1_0 = ""b'b'""
      arg_0_1_1 = ""b''""
      arg_0_1 = [arg_0_1_0,arg_0_1_1,]
      arg_0_2_0 = ""b'e'""
      arg_0_2_1 = ""b'f'""
      arg_0_2 = [arg_0_2_0,arg_0_2_1,]
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,]
      ngram_width = 12509
      separator = ""b'|'""
      pad_values_0 = ""b'LP'""
      pad_values_1 = ""b'RP'""
      pad_values = [pad_values_0,pad_values_1,]
      ragged_string_ops.ngrams(arg_0,ngram_width=ngram_width,separator=separator,pad_values=pad_values,)
  except Exception as e:
    ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,arg_0_0_1,]
      arg_0_1 = [arg_0_1_0,arg_0_1_1,]
      arg_0_2 = [arg_0_2_0,arg_0_2_1,]
      arg_0 = [arg_0_0,arg_0_1,arg_0_2,]
      pad_values = [pad_values_0,pad_values_1,]
      ragged_string_ops.ngrams(arg_0,ngram_width=ngram_width,separator=separator,pad_values=pad_values,)
  except Exception as e:
    ""Error:""+str(e)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 14:18:16.437999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.392284: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:18:17.392761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:18:17.429344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.429509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:18:17.429533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.431839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:18:17.431883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:18:17.432835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:18:17.433038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:18:17.435378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:18:17.435859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:18:17.435958: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:18:17.436039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.436195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.436294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:18:17.437215: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:18:17.437300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:18:17.437427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.437442: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:18:17.437453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:18:17.437465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:18:17.437476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:18:17.437487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:18:17.437497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:18:17.437507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:18:17.437553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.437772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:18:17.437796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:18:17.757806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:18:17.757832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:18:17.757837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:18:17.757972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:18:17.758259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4661 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
```
</details>"
59006,Crash when running tensorflow.python.ops.linalg.linalg_impl.matrix_exponential,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.linalg.linalg_impl.matrix_exponential is given input parameter a list with large negative elements, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops.linalg import linalg_impl
try:
  arg_0_0_0 = 5e-06
  arg_0_0_1 = 5e-06
  arg_0_0 = [arg_0_0_0,arg_0_0_1,]
  arg_0_1_0 = 5e-06
  arg_0_1_1 = 5e-06
  arg_0_1 = [arg_0_1_0,arg_0_1_1,]
  arg_0 = [arg_0_0,arg_0_1,]
  linalg_impl.matrix_exponential(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
2022-12-25 14:03:35.770294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:03:36.752882: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:03:36.753460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 14:03:36.782324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.782434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:03:36.782451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:03:36.784098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:03:36.784132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:03:36.784753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:03:36.784885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:03:36.786421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:03:36.786862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:03:36.786942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:03:36.787042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.787217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.787303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:03:36.788611: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 14:03:36.788720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.788836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 14:03:36.788856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:03:36.788889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:03:36.788900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:03:36.788912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 14:03:36.788922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 14:03:36.788932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:03:36.788942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 14:03:36.788953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 14:03:36.788997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.789094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:36.789162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 14:03:36.789182: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 14:03:37.118081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 14:03:37.118114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 14:03:37.118119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 14:03:37.118264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:37.118403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:37.118496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 14:03:37.118576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4729 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 14:03:37.162673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 14:03:37.427628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 14:03:37.432222: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5a303b0
2022-12-25 14:03:37.432286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 14:03:37.745682: F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.
Aborted (core dumped)
```
</details>"
59005,Crash when running tensorflow.python.ops.gen_array_ops.pad_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_array_ops.pad_v2 is given with input padding with large elements, the program crashes due to increase in RAM usage.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import gen_array_ops
import numpy as np
try:
  arg_0_tensor = tf.convert_to_tensor(np.ones([2, 2], dtype=str))
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0_0 = 12509
  arg_1_0_1 = 12509
  arg_1_0 = [arg_1_0_0,arg_1_0_1,]
  arg_1_1_0 = 12509
  arg_1_1_1 = 12509
  arg_1_1 = [arg_1_1_0,arg_1_1_1,]
  arg_1 = [arg_1_0,arg_1_1,]
  arg_2 = ""PAD""
  gen_array_ops.pad_v2(arg_0,arg_1,arg_2,)
except Exception as e:
  ""Error:""+str(e)
```


### Relevant log output

```shell
2022-12-25 13:42:02.032759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 13:42:03.012208: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 13:42:03.012782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-12-25 13:42:03.050082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.050232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 13:42:03.050254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 13:42:03.052670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 13:42:03.052731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 13:42:03.053883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 13:42:03.054132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 13:42:03.056360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 13:42:03.056874: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 13:42:03.056981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 13:42:03.057134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.057360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.057485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 13:42:03.058622: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-25 13:42:03.058707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.058823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
coreClock: 1.77GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2022-12-25 13:42:03.058842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 13:42:03.058865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2022-12-25 13:42:03.058878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2022-12-25 13:42:03.058890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2022-12-25 13:42:03.058903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2022-12-25 13:42:03.058915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2022-12-25 13:42:03.058927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2022-12-25 13:42:03.058939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2022-12-25 13:42:03.058989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.059119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.059212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2022-12-25 13:42:03.059237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2022-12-25 13:42:03.385787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-25 13:42:03.385821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2022-12-25 13:42:03.385826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2022-12-25 13:42:03.385975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.386121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.386214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-25 13:42:03.386299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4685 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-12-25 13:42:03.431772: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 15024009600 exceeds 10% of free system memory.
Killed
```
</details>"
59004,Crash when running tensorflow.python.ops.collective_ops.all_gather,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.collective_ops.all_gather is given input t with rank other than 1, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import collective_ops
try:
  arg_0_tensor = tf.random.uniform([], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 1
  arg_2 = 100
  arg_3 = 100
  communication_hint = ""RING""
collective_ops.all_gather(arg_0,arg_1,arg_2,arg_3,communication_hint=communication_hint,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Aborted (core dumped)
```
</details>"
59003,Crash when running tensorflow.python.ops.gen_image_ops.extract_glimpse_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_image_ops.extract_glimpse_v2 is given size parameter as a list of negative integers, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import gen_image_ops
try:
  input_tensor = tf.random.uniform([1, 5, 5, 1], dtype=tf.float32)
  input = tf.identity(input_tensor)
  size_0 = -3
  size_1 = -3
  size = [size_0,size_1,]
  offsets_0_0 = -2
  offsets_0_1 = -2
  offsets_0 = [offsets_0_0,offsets_0_1,]
  offsets = [offsets_0,]
  centered = False
  normalized = False
  noise = ""zero""
  uniform_noise = False
gen_image_ops.extract_glimpse_v2(input=input,size=size,offsets=offsets,centered=centered,normalized=normalized,noise=noise,uniform_noise=uniform_noise,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Aborted (Core dumped)
```
</details>"
59002,Crash when running tensorflow.python.ops.gen_linalg_ops.cholesky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_linalg_ops.cholesky is given negative input tensor, it results in crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import gen_linalg_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.complex(tf.constant(-1250, shape=[20, 20], dtype=tf.float32,),tf.constant(-1250, shape=[20, 20], dtype=tf.float32,))
      arg_0_tensor = tf.complex(tf.constant([], shape=[20, 20], dtype=tf.float32,),tf.constant([], shape=[20, 20], dtype=tf.float32,))
      arg_0 = tf.identity(arg_0_tensor)
      gen_linalg_ops.cholesky(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.complex64)
      gen_linalg_ops.cholesky(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Aborted (Core dumped)
```
</details>"
59001,Segmentation fault when running tensorflow.python.ops.gen_nn_ops.fused_batch_norm_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.4

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_nn_ops.fused_batch_norm_v3 is given empty scale input tensor, it results in segmentation fault and crash the program.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 2, 1, 6], dtype=tf.float16)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = [] 
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_3_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_4_tensor = tf.random.uniform([6], dtype=tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      epsilon = 0.001
      exponential_avg_factor = 1.0
      data_format = ""NHWC""
      is_training = False
 gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float16)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.float32)
      arg_3 = tf.identity(arg_3_tensor)
      arg_3 = tf.cast(arg_3, tf.float32)
      arg_4 = tf.identity(arg_4_tensor)
      arg_4 = tf.cast(arg_4, tf.float32)
gen_nn_ops.fused_batch_norm_v3(arg_0,arg_1,arg_2,arg_3,arg_4,epsilon=epsilon,exponential_avg_factor=exponential_avg_factor,data_format=data_format,is_training=is_training,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
</details>"
59000,Crash when running tensorflow.python.ops.gen_array_ops.quantize_v2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If tensorflow.python.ops.gen_array_ops.quantize_v2 is given negative axis parameter, it is results in a crash.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.random.uniform([2, 3, 4, 5], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = -1.0
  arg_2 = 0.8
  T = tf.qint8
  mode = ""SCALED""
  round_mode = ""HALF_TO_EVEN""
  narrow_range = False
  axis = -1
gen_array_ops.quantize_v2(arg_0,arg_1,arg_2,T=T,mode=mode,round_mode=round_mode,narrow_range=narrow_range,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Aborted (core dumped)
```
</details>"
58999,Segmentation fault in tensorflow.python.ops.gen_nn_ops.fused_batch_norm_grad_v3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

release 11.0, V11.0.194 Build cuda_11.0_bu.TC445_37.28540450_0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tensorflow.python.ops.gen_nn_ops.fused_batch_norm_grad_v3 is given a empty scale input tensor, it results in a CHECK fail that can be used to trigger a denial of service attack via segfault.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      y_backprop_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      y_backprop = tf.identity(y_backprop_tensor)
      x_tensor = tf.random.uniform([4, 10, 10, 2], dtype=tf.float32)
      x = tf.identity(x_tensor)
      scale_tensor = [] 
      scale = tf.identity(scale_tensor)
      reserve_space_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_2_tensor = tf.random.uniform([2], dtype=tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      epsilon = 0.001
      data_format = ""NHWC""
      is_training = False
      reserve_space_3_tensor = tf.random.uniform([], dtype=tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      y_backprop = tf.identity(y_backprop_tensor)
      y_backprop = tf.cast(y_backprop, tf.float32)
      x = tf.identity(x_tensor)
      x = tf.cast(x, tf.float32)
      scale = tf.identity(scale_tensor)
      scale = tf.cast(scale, tf.float32)
      reserve_space_1 = tf.identity(reserve_space_1_tensor)
      reserve_space_1 = tf.cast(reserve_space_1, tf.float32)
      reserve_space_2 = tf.identity(reserve_space_2_tensor)
      reserve_space_2 = tf.cast(reserve_space_2, tf.float32)
      reserve_space_3 = tf.identity(reserve_space_3_tensor)
      reserve_space_3 = tf.cast(reserve_space_3, tf.float32)
      gen_nn_ops.fused_batch_norm_grad_v3(y_backprop=y_backprop,x=x,scale=scale,reserve_space_1=reserve_space_1,reserve_space_2=reserve_space_2,epsilon=epsilon,data_format=data_format,is_training=is_training,reserve_space_3=reserve_space_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
</details>"
58997,Inconsistent predict results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.5.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda-11.2

### GPU model and memory

GRID A100-20C, NVIDIA-SMI 510.47.03

### Current Behaviour?

```shell
Inconsistent predict result on Ubuntu 20.04.4 with A100 GPU when using Conv2D layer.
I'm running model.predict(X) few times with the identical loaded model from file and same input, but there are numeric difference in the results.

We performed multiple inferences (starting new process each time) with the following configuration:
• Windows 10 + CPU: same results each run 
• Ubuntu 18.04 + CPU: same results each run
• Ubuntu 18.04 + V100: same results each run 
• Ubuntu 20.04 + A100: different results each run (!!) 
```


### Standalone code to reproduce the issue

```shell
# Simple model that illustrate the problem:

# First- create the model and save it. ( Relayed on: https://iq.opengenus.org/conv2d-in-tf/)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense

fashion = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255


model = Sequential()
model.add(Conv2D(32, (3, 3),input_shape=(28, 28, 1)))
# model.add(MaxPooling2D((2, 2)))
# model.add(Conv2D(64, (5, 5), activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=10)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)


model.save('model.h5')


# -----------------------------------------------------------------------------
# Then: 
# load the model and run it few times:
model= tf.keras.models.load_model('model.h5')
print(model.predict(test_images))

# Gives different predictions


# You might need to remove cache files from one run to another to notice the differences: 
# sudo rm -rf ~/.nv/
```


### Relevant log output

```shell
# for example from first predict run:
[[1.9118926e-16 2.1664609e-21 1.9678958e-13 ... 3.2384528e-13
  8.9434872e-15 1.0000000e+00]
 [1.4733767e-13 6.6338912e-17 9.9999976e-01 ... 1.9979627e-28
  6.2790036e-15 9.4947700e-20]
 [3.4300227e-15 1.0000000e+00 1.4307653e-19 ... 1.5259588e-22
  4.3525246e-16 2.7868801e-20]
 ...
 [6.6930616e-19 1.8571309e-32 4.7568495e-30 ... 0.0000000e+00
  1.0000000e+00 2.0719716e-37]
 [5.6024744e-17 1.0000000e+00 1.7536839e-21 ... 2.5832318e-19
  6.5471020e-19 4.7896584e-16]
 [3.2346847e-10 1.4122374e-10 3.9109116e-07 ... 1.5287013e-05
  3.6158244e-08 1.4269066e-05]]

# 2nd predict run:
[[1.9118926e-16 2.1664446e-21 1.9678996e-13 ... 3.2384528e-13
  8.9434533e-15 1.0000000e+00]
 [1.4733681e-13 6.6340434e-17 9.9999976e-01 ... 1.9982675e-28
  6.2802491e-15 9.4948418e-20]
 [3.4302713e-15 1.0000000e+00 1.4307872e-19 ... 1.5260113e-22
  4.3524746e-16 2.7868801e-20]
 ...
 [6.6841317e-19 1.8562527e-32 4.7557244e-30 ... 0.0000000e+00
  1.0000000e+00 2.0703124e-37]
 [5.6203482e-17 1.0000000e+00 1.7566836e-21 ... 2.5900697e-19
  6.5718233e-19 4.8014216e-16]
 [3.2395439e-10 1.4134931e-10 3.9128366e-07 ... 1.5299629e-05
  3.6214352e-08 1.4277887e-05]]

# (can be noticed on the last value)
```
</details>"
58995,`PluggableGraphOptimizer failed: NOT_FOUND: Op type not registered '_CopyFromGpuToHost'`,"### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow-cpu==2.10.0

### OS Platform and Distribution

Windows 10 Pro

### Python version

3.9.15

### GPU model and memory

AMD Radeon Rx580 8Gb

### tensorflow-direct-ml-plugin
latest

When I run large model, I get this error, how I can catch this and fix.

Code: https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/stable_diffusion

```shell
2022-12-24 01:32:58.490091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
2022-12-24 01:33:00.202302: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] PluggableGraphOptimizer failed: NOT_FOUND: Op type not registered '_CopyFromGpuToHost' in binary running on DESKTOP-NEIZVES. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
2022-12-24 01:33:00.248437: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{tfg-consolidate-attrs,tfg-toposort,tfg-shape-inference{graph-version=0},tfg-prepare-attrs-export} failed: INVALID_ARGUMENT: Unable to find OpDef for _CopyFromHostToGpu
	when importing GraphDef to MLIR module in GrapplerHook
2022-12-24 01:33:00.588281: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] PluggableGraphOptimizer failed: NOT_FOUND: Op type not registered '_CopyFromGpuToHost' in binary running on DESKTOP-NEIZVES. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
2022-12-24 01:33:00.611022: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{tfg-consolidate-attrs,tfg-functional-to-region,tfg.func(tfg-cf-sink),tfg-region-to-functional{force-control-capture=true},tfg-lift-legacy-call,symbol-privatize{},symbol-dce,tfg-prepare-attrs-export} failed: INVALID_ARGUMENT: Unable to find OpDef for _CopyFromGpuToHost
	when importing GraphDef to MLIR module in GrapplerHook
2022-12-24 01:33:00.616783: E tensorflow/core/grappler/optimizers/tfg_optimizer_hook.cc:134] tfg_optimizer{tfg-consolidate-attrs,tfg-functional-to-region,tfg.func(tfg-cf-sink),tfg-region-to-functional{force-control-capture=true},tfg-lift-legacy-call,symbol-privatize{},symbol-dce,tfg-prepare-attrs-export} failed: INVALID_ARGUMENT: Unable to find OpDef for _CopyFromGpuToHost
	when importing GraphDef to MLIR module in GrapplerHook
2022-12-24 01:33:00.645922: W tensorflow/core/common_runtime/process_function_library_runtime.cc:941] Ignoring multi-device function optimization failure: NOT_FOUND: Op type not registered '_CopyFromGpuToHost' in binary running on DESKTOP-NEIZVES. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.

Process finished with exit code -1073740791 (0xC0000409)
```
```
"
58994,Missing interger overflow check in TensorFlow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow does not check integer overflow, which is dangerous and can cause bugs for many users.
```


### Standalone code to reproduce the issue

```shell
a = 2147483648
print(tf.convert_to_tensor(a, tf.int32))
```

BUG case:
```shell
print(a* tf.constant(1, tf.int32))

```


### Relevant log output

_No response_</details>"
58993,"NotImplementedError: Cannot convert a symbolic tf.Tensor (strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.","### 1. System information

- MacOS (m1):
- Conda:
- TensorFlow library (version=2.10):
MacOS

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1) get_concrete_function()

```
import coremltools as ct
import tensorflow as tf
import talib as ta
import numpy as np
import os

tf.config.experimental_run_functions_eagerly(True)

@tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])


def gelu_tanh_activation(features):
    featureSize = 6
    xFeatures = [0]*featureSize
    for i in range(0, featureSize):
        xFeatures[i] = float(features[i])
    return ta.MA(np.array(xFeatures).astype('double'), timeperiod = 2)

conc_func = gelu_tanh_activation.get_concrete_function()

# # provide the concrete fucntion as a list
mlmodel = ct.convert([conc_func])
mlmodel.save(os.environ['HOME'] + '/ModelSHSZ/release/ConcFunc')
```



### Error and Logs

Traceback (most recent call last):
  File ""/Users/thao/Private/TFLite/covert.py"", line 20, in <module>
    conc_func = gelu_tanh_activation.get_concrete_function()
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py"", line 1239, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py"", line 1219, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py"", line 785, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/function.py"", line 2523, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/function.py"", line 2760, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/function.py"", line 2670, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1247, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py"", line 677, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1233, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1222, in autograph_handler
    return autograph.converted_call(
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/var/folders/3r/ptdgzwkd4y72zwkryhpz5h7c0000gn/T/__autograph_generated_fileja51y05y.py"", line 26, in tf__gelu_tanh_activation
    retval_ = ag__.converted_call(ag__.ld(ta).MA, (ag__.converted_call(ag__.converted_call(ag__.ld(np).array, (ag__.ld(xFeatures),), None, fscope).astype, ('double',), None, fscope),), dict(timeperiod=2), fscope)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 371, in converted_call
    return py_builtins.overload_of(f)(*args)
  File ""/Users/thao/miniforge3/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 924, in __array__
    raise NotImplementedError(
NotImplementedError: in user code:

    File ""/Users/thao/Private/TFLite/covert.py"", line 17, in gelu_tanh_activation  *
        return ta.MA(np.array(xFeatures).astype('double'), timeperiod = 2)

    NotImplementedError: Cannot convert a symbolic tf.Tensor (strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."
58992,How Do I Convert input arg tf.string to string in tf.function ," @tf.function(input_signature=[
        tf.TensorSpec(shape=[], dtype=tf.string, name=""key""),
        tf.TensorSpec(shape=[], dtype=tf.string, name=""value"")
    ])
    def infer(self, key, value):
          tf.print(key)
          xxx

I want to implement universal input, something like this
key: age#gender
value: 0#1
=> 
age_tensor = tf.constant(value=0, name=""age"")
gender_tensor = tf.constant(value=1,name=""gender"")

so I want to convert tf.string to string.
use tf.print function can normally print the input args，Do I have any way to simulate the function of tf.print and assign the printed value to a variable to implement tf.string to string?
or
Is there any other way I can do this?"
58991,Missing zip file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

not available, see ""Current Behaviour""

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

-

### Python version

-

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

NVidia GForce

### Current Behaviour?

```shell
Hi gents

I just tried to load a zip file from your server to enhance grafic speed with GPU
https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip

Unfortunatly I only get this:

<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>No such object: tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip</Details>
</Error>

Can you fix this?

https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.11.0.zip works!?
although I just saw, that the CPU zip has issues too, some files are missing.


Best regards
Roland Winde
```


### Standalone code to reproduce the issue

```shell
not available
```


### Relevant log output

```shell
<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>No such object: tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip</Details>
</Error>
```
</details>"
58990,NNAPI initialization from cache has hardly any time benefit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.10.1

### Custom Code

Yes

### OS Platform and Distribution

Android 12

### Mobile device

Pixel 3a

### Python version

N/A

### Bazel version

cmake 3.16.3

### GCC/Compiler version

NDK r19c

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to use compilation caching with the NNAPI delegate to reduce startup time.
To test this, I have written a small program that loads a tflite model from disk, applies the NNAPI delegate and runs invoke once. 
The NNAPI delegate is provided with the necessary settings (`model_token` and `cache_dir`), and indeed on first invocation I as expected receive:
```
ERROR: File /data/local/tmp/model_5156108677284121472.bin couldn't be opened for reading: No such file or directory
```
After this first execution, the relevant file was created and subsequent executions did indeed no longer show the cache miss error.

The problem is that there are no significant time savings.
I ran the test binary produced with the steps below on my Pixel3a and crudely measured the first execution takes 2.62 seconds while the second one takes 2.49 seconds (see logs).
For reference, the same execution takes only 0.29 seconds when not applying the NNAPI delegate (again see logs).

I have tried this with a few different models, but results were always similar.
Note that this same setup works fine with the GPU delegate, where the first execution takes a couple seconds and consecutive executions are drastically faster.

Is this expected behavior with the NNAPI delegate, possibly a bug, or is there something wrong with how I am trying to use compilation caching?

### Standalone code to reproduce the issue
```c++
#include ""tensorflow/lite/delegates/nnapi/nnapi_delegate.h""
#include ""tensorflow/lite/interpreter_builder.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model_builder.h""

#include <iostream>
#include <memory>

int main() {
  auto model = tflite::FlatBufferModel::BuildFromFile(""model.tflite"");
  auto resolver = std::make_unique<tflite::ops::builtin::BuiltinOpResolver>();
  auto builder = tflite::InterpreterBuilder(*model, *resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);

  tflite::StatefulNnApiDelegate::Options options;
  options.cache_dir = ""/data/local/tmp"";
  options.model_token = ""model"";
  auto delegate = tflite::StatefulNnApiDelegate(options);

  if (interpreter->ModifyGraphWithDelegate(&delegate) != kTfLiteOk) {
    std::cerr << ""Failed to apply NNAPI delegate."" << std::endl;
    return -1;
  };
  if (interpreter->AllocateTensors() != kTfLiteOk) {
    std::cerr << ""Failed to allocate tensors."" << std::endl;
    return -2;
  }
  if (interpreter->Invoke() != kTfLiteOk) {
    std::cerr << ""Failed to invoke."" << std::endl;
    return -3;
  }
  std::cout << ""Success!"" << std::endl;
  return 0;
}
```

To compile, place in same directory with this dockerfile
```dockerfile
FROM tensorflow/tensorflow:devel

RUN apt-get update &&\
    DEBIAN_FRONTEND=noninteractive apt-get install -y cmake

ARG NDK_VERSION=r19c
ARG NDK_DIR=android-ndk-${NDK_VERSION}
ARG NDK_FILE=${NDK_DIR}-linux-x86_64.zip
ARG TENSORFLOW_VERSION=2.10.1
ARG TENSORFLOW_DIR=tensorflow-${TENSORFLOW_VERSION}
RUN curl -s https://dl.google.com/android/repository/${NDK_FILE} \
        --output /${NDK_FILE}
RUN unzip -q /${NDK_FILE}
RUN curl -s https://codeload.github.com/tensorflow/tensorflow/zip/refs/tags/v${TENSORFLOW_VERSION} \
        --output /${TENSORFLOW_DIR}.zip
RUN unzip -q /${TENSORFLOW_DIR}.zip
RUN mkdir /${TENSORFLOW_DIR}/build
RUN cd /${TENSORFLOW_DIR}/build &&\
    cmake -DCMAKE_TOOLCHAIN_FILE=/${NDK_DIR}/build/cmake/android.toolchain.cmake \
        -DANDROID_ABI=arm64-v8a \
        -DTFLITE_ENABLE_GPU=ON \
        /${TENSORFLOW_DIR}/tensorflow/lite &&\
    cd /${TENSORFLOW_DIR}/build &&\
    cmake --build . -j$(nproc)
RUN mkdir /compile_cache_test
RUN find /${TENSORFLOW_DIR}/build -name ""*.a"" -exec cp {} /compile_cache_test \;
RUN mkdir /compile_cache_test/build
ADD CMakeLists.txt /compile_cache_test
ADD compile_cache_test.cpp /compile_cache_test
RUN cd /compile_cache_test/build &&\
    cmake -DCMAKE_TOOLCHAIN_FILE=/${NDK_DIR}/build/cmake/android.toolchain.cmake \
        -DANDROID_ABI=arm64-v8a \
        /compile_cache_test &&\
    cmake --build .
```

as well as this CMake file 
```cmake
cmake_minimum_required(VERSION 3.16)
project(NNAPICompileCacheTest CXX)
set(CMAKE_CXX_STANDARD 17)

set(TENSORFLOW_DIR /tensorflow-2.10.1)
find_library(TFLite_LIBRARY tensorflow-lite
  PATHS ${CMAKE_SOURCE_DIR} NO_CMAKE_FIND_ROOT_PATH)
file(GLOB_RECURSE TFLite_DEPENDENCIES ${CMAKE_SOURCE_DIR}/*.a)
list(REMOVE_ITEM TFLite_DEPENDENCIES ${TFLite_LIBRARY})
message(STATUS ""TFLite_DEPENDENCIES: ${TFLite_DEPENDENCIES}"")
add_library(tensorflow-lite UNKNOWN IMPORTED)
set_target_properties(tensorflow-lite PROPERTIES
  IMPORTED_LOCATION ""${TFLite_LIBRARY}""
  INTERFACE_INCLUDE_DIRECTORIES
    ""${TENSORFLOW_DIR};${TENSORFLOW_DIR}/build/flatbuffers/include""
  INTERFACE_LINK_LIBRARIES ""${TFLite_DEPENDENCIES};${TFLite_DEPENDENCIES}""
)

add_executable(compile_cache_test compile_cache_test.cpp)
target_link_libraries(compile_cache_test log tensorflow-lite)
```

It's a bit unorthodox, but building the image will also build the executable. So build with
```
docker build . -t compile_cache_test
```

And then run the below line to extract the binary compiled for android-arm64.
```
docker run -it --rm -v $PWD:/host_dir compile_cache_test cp /compile_cache_test/build/compile_cache_test /host_dir
```

Then, push this binary using `adb push compile_cache_test /data/local/tmp`, and also push some tflite model that can be (at least partially) used with NNAPI.
The timings above were recorded with the tflite model from [here](https://raw.githubusercontent.com/juandes/mobiledet-tflite-nnapi/master/app/src/main/assets/model.tflite), which also needs to be pushed to `/data/local/tmp`.

### Relevant log output

#### First run. Cache miss as expected.
```shell
sargo:/data/local/tmp $ time ./compile_cache_test                                             
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
ERROR: File /data/local/tmp/model_17199993934519011812.bin couldn't be opened for reading: No such file or directory
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
VERBOSE: Replacing 108 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 1 partitions.
Success!
    0m02.62s real     0m00.16s user     0m00.06s system
```

#### Second run (cache used?)
```shell
sargo:/data/local/tmp $ time ./compile_cache_test                                             
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
VERBOSE: Replacing 108 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 1 partitions.
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
Success!
    0m02.49s real     0m00.13s user     0m00.08s system
```

#### Run without NNAPI
```shell
sargo:/data/local/tmp $ time ./compile_cache_test                                             
INFO: Initialized TensorFlow Lite runtime.
Success!
    0m00.29s real     0m00.26s user     0m00.01s system
```
</details>"
58989,"By using tf.control_dependencies in my code, the custom_ops will not be execute","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.2 + tfserving2.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
1. When set dense_var_update_interval = 5000, if add ops.control_dependencies in my code, such as: 
  with ops.control_dependencies([refresh_time_op]):
     update_dense_op = control_flow_ops.group(*update_dense_ops(infer_model_id, op_name_compat))

then it will result in the code below(in function of update_dense_ops) will not be execute :
   get_op = gen_weps_ops.ps_get_dense_v2  # not be execute

2. When set dense_var_update_interval = 0, without ops.control_dependencies, everything is ok.
   get_op = gen_weps_ops.ps_get_dense_v2  # can be execute

3. My code is run is TF2.2 + tfserving2.2
```


### Standalone code to reproduce the issue

```shell
def get_timestamp_millis():
  return math_ops.cast(logging_ops.timestamp() * 1000, dtype=dtypes.int64)

def weps_inject_get_dense_ops(infer_model_id=None, op_name_compat=True, patterns=None):
    patterns = patterns if patterns else {}
    _vars = get_vars_to_sync(patterns)
    _ops = [op for op in ops.get_default_graph().get_operations()]

    def update_dense_ops(infer_model_id, op_name_compat):
        get_ops = []
        for var in _vars:
            if isinstance(var, de.TrainableWrapper):
                pass
            else:
                tf_logging.info('  get_dense_v2 for: %s(%r)' % (var.name, type(var)))
                get_op = gen_weps_ops.ps_get_dense_v2
                gt = get_op(input_name=var.name, shape=var.shape.as_list())
                get_ops.append(state_ops.assign(var, gt))
        # get_ops.append(logging_ops.Print(logging_ops.timestamp(), [logging_ops.timestamp()],
        #               message=""================ time to update dense""))
        return get_ops

    dense_var_update_interval = int(
        os.environ.get('TTF_WEPS_DENSE_VAR_UPDATE_INTERVAL_MILLIS', '5000'))
    if dense_var_update_interval > 0:
        last_update_time = variables.VariableV1(
            0,
            shape=(),
            dtype=dtypes.int64,
            name='dense_last_update_time',
            trainable=False,
            use_resource=True)

        def update_ops(infer_model_id, op_name_compat):
            refresh_time_op = state_ops.assign(last_update_time,
                                              get_timestamp_millis(),
                                              use_locking=True)
            with ops.control_dependencies([refresh_time_op]):
                update_dense_op = control_flow_ops.group(*update_dense_ops(infer_model_id, op_name_compat))
            return refresh_time_op, update_dense_op

        get_group = tf.group(update_ops(infer_model_id, op_name_compat))
    else:
        get_group = control_flow_ops.group(*update_dense_ops(infer_model_id, op_name_compat))

    for op in _ops:
        if op.type == 'Const':
            tf_logging.info('injected before: %s(%s)' % (op.name, op.type))
            op._add_control_input(get_group)
```


### Relevant log output

_No response_</details>"
58988,NOT_FOUND: could not find registered platform with id,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS Ventura

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

12.2.0

### CUDA/cuDNN version

NA

### GPU model and memory

M1 Mac, 8 GB

### Current Behaviour?

```shell
2022-12-23 01:31:15.756543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
2022-12-23 01:31:17.330784: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x12ff97980


Error:
                             Traceback (most recent call last)
Cell In[42], line 1
----> 1 hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])

File /opt/homebrew/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

NotFoundError: Graph execution error:
```


### Standalone code to reproduce the issue

```shell
[Link to pdf](https://github.com/nicknochnack/ImageClassification/blob/main/Getting%20Started.ipynb)

The code and files needed are same required here
```


### Relevant log output

_No response_</details>"
58987,Unsuported fullyconnected operator type by tflite,"**System information**
- Tested on ColabPro
- TensorFlow 2.9.2

**Issue**
Tensorflow lite does not support the following quantized fully_connected operation:
Input parameters:
-input int8
-filter int8
-bias int32
Output:
-output int16

After debugging the library, I have noticed that this case is not supported, moreover, no error is is thrown. During the evaluation step, in the file tensorflow/lite/kernels/fully_connected.cc, the model will take a default case and does not check the input type. Instead of throwing an error it casts the input and the filter to uint8 and evaluates that operator.

```
import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""fc.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

#read and reshape input data
input_data = np.fromfile(""input_int8.dat"", dtype=""int8"")
input_data = np.int8(np.reshape(input_data, (1, 25)))

output_data = interpreter.get_tensor(output_details[0]['index'])
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(""output type and data"")
print(type(output_data[0][0]))
print(output_data)

out_ref = np.fromfile(""output_int16.dat"", dtype=""int8"")
print(out_ref)

output_data = interpreter.get_tensor(output_details[0]['index'])
print(""output type and data"")
print(type(output_data[0][0]))
print(output_data)
```
*Output*
output type and data
<class 'numpy.int16'>
[[ -303 -4117 -2604 -1706  -103 -2236 -2852 -1752 -3658 -2029 -2906 -3052
  -1304  -841 -2676  -568 -1770   220 -1328 -3144   946  -138 -2438 -3258
  -2163   -83 -1152 -1862 -3949   946 -1991 -3275 -3667   869  -513 -4450
  -3700 -1207 -2309 -2831 -2757 -1710  -143 -5043 -2749 -5654 -2191 -4625
   -377 -1835  -369 -4886 -3887 -3610  1371  -564   843  -939 -2472 -3719
  -3490 -2867 -2712 -3368 -2164 -1430 -3685 -1830 -2481   836   211 -4628
  -2289  1850 -2082 -5311  -975 -2097  -113 -6531 -2485 -1459 -2250 -1654
  -3846 -2472 -2383 -2547 -3155 -1895   159  1065  1221 -2099 -2449 -1530
  -3535    -5 -4117 -2345]]
ref output
[ -45    5    8    0  -67   -5   11   -5  121   -6   26  -32  121  -11
 -102   -6   50  -20  121   -1   60  -21   -4  -20   57   21 -114   -8
 -100   -2    7   14  -44   11  -17   -2  -79  -11  -44  -10   40   -5
   56    8   46  -15  -15  -30  112    0   13   16   56   -1   15    3
  -72  -14    7   14  -35   -4  -21   -5  -95   -9 -128   12  -15    2
  -86    1  121   -7  -60   -5   81  -11   65  -16  -63    1 -100    3
   74   -4  108   -9   -6   -3  114   -9 -100   -4   32  -28   46   17
  -51    6  -22   11  116  -17  -78    4 -100  -25    
9    3  -18   -9
   34    5   -9    5   20    2   30    3  101  -35   53   -9   40   -9
   96  -12   61    3   -5    6   80  -19   -4   11    2   -3  -18   10
   63   11   95   -4   70  -22   35   19  -58  -16  -96  -18   37  -17
  -63  -29  -79   -1  -63   -7  -79  -22  -90  -13   16   -5  -50    6
    3  -10   64   20   80   -8 -100  -30  -87   -3   27  -16  -25    9
    3   14  -15   10   26   -9   17    6  -60    6  -83   -1   34   -3
   -3  -13  117  -15]


[fc.zip](https://github.com/tensorflow/tensorflow/files/10287488/fc.zip)
[output_int16.zip](https://github.com/tensorflow/tensorflow/files/10287495/output_int16.zip)"
58986,Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.10.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1..33

### GPU model and memory

NVIDIA QUADRO P620 / 4096 MB

### Current Behaviour?

```shell
The program runs when i am using only CPU but i get the CUDNN version mismatch when trying to Run the program using GPU.
```


### Standalone code to reproduce the issue

```shell
import os
import cv2
import glob
import random
import numpy as np
from tqdm import tqdm
from PIL import Image
import tensorflow as tf
from itertools import chain
import matplotlib.pyplot as plt
from skimage.transform import resize
from skimage.io import imread, imshow

# tensorboard --logdir=logs/ --host localhost --port 8088

seed = 42
np.random.seed = seed

#Resizing images is optional, CNNs are ok with large images
SIZE_X = 256        #Resize images (height  = X, width = Y)
SIZE_Y = 256
CHANNELS = 3


# TRAIN_DATA= 'SegTrackv2_Dataset/SegTrackv2/Train/Images'
# TEST_DATA = 'SegTrackv2_Dataset/SegTrackv2/Test/Images'

# TRAIN_DATA= 'Train/Images'
# TEST_DATA = 'Test/Images'

TRAIN_DATA= 'PatchTrain/Images'
TEST_DATA = 'PatchTest/Images'

train_ids = next(os.walk(TRAIN_DATA))[1]
test_ids = next(os.walk(TEST_DATA))[1]
classnames = test_ids

train_count = 0
test_count = 0
mask_count = 0

train_img_list = []
mask_img_list = []
test_img_list = []

# Get the list of all images in the training input and mask subfolders 

for names in train_ids:
    train_path = (os.path.join(f'{TRAIN_DATA}/{names}/Images'))
    train_img = glob.glob(f'{train_path}/**.png')
    train_img.extend(glob.glob(f'{train_path}/**.bmp'))
    train_img_list = list(chain(train_img, train_img_list))
    train_count = len(train_img_list)


    mask_path = (os.path.join(f'{TRAIN_DATA}/{names}/Masks'))
    mask_img = glob.glob(f'{mask_path}/**.png')
    mask_img.extend(glob.glob(f'{mask_path}/**.bmp'))
    mask_img_list = list(chain(mask_img, mask_img_list))
    mask_count =  len(mask_img_list)


    test_path = (os.path.join(f'{TEST_DATA}/{names}/Images'))
    test_img = glob.glob(f'{test_path}/**.png')
    test_img.extend(glob.glob(f'{test_path}/**.bmp'))
    test_img_list = list(chain(test_img, test_img_list))
    test_count = len(test_img_list)

# Create numpy array to store the images

X_train = np.zeros((train_count, SIZE_X, SIZE_Y,CHANNELS), dtype=np.uint8)
Y_train = np.zeros((train_count, SIZE_X, SIZE_Y), dtype=bool)    # Mask is the boolian value of segmented Images

# Resize Images and masks to the desired input size of convolutional layer

print('Resizing training images and masks')
for i in tqdm(range((100))): # train_count
    
    img = cv2.imread(train_img_list[i], cv2.IMREAD_COLOR)       
    img = cv2.resize(img, (SIZE_Y, SIZE_X))
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    X_train[i] = img                # Fill empty X_train with values from img
       
    
    mask = cv2.imread(mask_img_list[i], 0)       
    mask = cv2.resize(mask, (SIZE_Y, SIZE_X))    
    Y_train[i] = mask 
     
# Resize test images

X_test = np.zeros((test_count, SIZE_X, SIZE_Y,CHANNELS), dtype=np.uint8)
sizes_test = []
print('Resizing test images') 
for t in tqdm(range(50)): # test_count
    
    img = cv2.imread(test_img_list[t],cv2.IMREAD_COLOR)
    img = cv2.resize(img, (SIZE_Y, SIZE_X))
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    X_test[t] = img

print('Done!')

X_VAL = X_train[int(X_train.shape[0]*0.8):]
Y_VAL = Y_train[int(X_train.shape[0]*0.8):]

# Plot some random Training images and their corresponding masks

for i in range(1,7,2):
    ix = random.randint(0, train_count)
    plt.subplot(3,2,i)
    plt.title('X_train',fontsize = 5)
    plt.imshow(X_train[ix])
    plt.imshow(Y_train[ix],cmap='jet', alpha=0.5)

    plt.subplot(3,2,i+1)
    plt.title('Y_train',fontsize = 5)
    plt.imshow(np.squeeze(Y_train[ix]))


plt.show()


#Build the model

inputs = tf.keras.layers.Input((SIZE_X, SIZE_Y, CHANNELS))
s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)

#Contraction path

c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
c1 = tf.keras.layers.Dropout(0.1)(c1)
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
c2 = tf.keras.layers.Dropout(0.1)(c2)
c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
 
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
c3 = tf.keras.layers.Dropout(0.2)(c3)
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
 
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
c4 = tf.keras.layers.Dropout(0.2)(c4)
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)
 
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
c5 = tf.keras.layers.Dropout(0.3)(c5)
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)

#Expansive path 

u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
u6 = tf.keras.layers.concatenate([u6, c4])
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
c6 = tf.keras.layers.Dropout(0.2)(c6)
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
 
u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
u7 = tf.keras.layers.concatenate([u7, c3])
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
c7 = tf.keras.layers.Dropout(0.2)(c7)
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
 
u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
u8 = tf.keras.layers.concatenate([u8, c2])
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
c8 = tf.keras.layers.Dropout(0.1)(c8)
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
 
u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
u9 = tf.keras.layers.concatenate([u9, c1], axis=3)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
c9 = tf.keras.layers.Dropout(0.1)(c9)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
 
outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)
 
####################################################################

model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
model.summary()


####################################################################
#Modelcheckpoint

checkpointer = tf.keras.callbacks.ModelCheckpoint('model_for_nuclei.h5', verbose=1, save_best_only=True)

callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'), # Patience = 3
        tf.keras.callbacks.TensorBoard(log_dir='logs')]

results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=64, epochs=10, callbacks=callbacks) # val_split = 0.1

#####################################################################


idx = random.randint(0, len(X_train))

preds_train = model.predict(X_train[:int(X_train.shape[0]*0.8)], verbose=1)
preds_val = model.predict(X_VAL, verbose=1)
preds_test = model.predict(X_test, verbose=1)

 
preds_train_t = (preds_train > 0.5).astype(np.uint8)
preds_val_t =   (preds_val   > 0.5).astype(np.uint8)
preds_test_t =  (preds_test  > 0.5).astype(np.uint8)


# Perform a sanity check on some random training samples


for i in range(1,10,3):
    
    ix = random.randint(0, len(preds_train_t))
    plt.subplot(3,3,i)
    plt.title('X_train',fontsize = 5)
    plt.imshow(X_train[ix])
    plt.imshow(np.squeeze(preds_train_t[ix]),cmap = 'jet',alpha=0.5)

    plt.subplot(3,3,i+1)
    plt.title('Y_train',fontsize = 5)
    plt.imshow(Y_train[ix])

    plt.subplot(3,3,i+2)
    plt.title('Predict_Train',fontsize = 5)
    plt.imshow(np.squeeze(preds_train_t[ix]))      

plt.show()

# Perform a sanity check on some random Validation samples


for i in range(1,10,3):
    ix = random.randint(0, len(preds_val_t))
    
    plt.subplot(3,3,i)
    plt.title('X_val',fontsize = 5)
    plt.imshow(X_VAL[ix])
    plt.imshow(np.squeeze(preds_val_t[ix]),cmap = 'jet',alpha=0.5)

    plt.subplot(3,3,i+1)
    plt.title('Y_val',fontsize = 5)
    plt.imshow(Y_VAL[ix])

    plt.subplot(3,3,i+2)
    plt.title('Predict_Val',fontsize = 5)
    plt.imshow(np.squeeze(preds_val_t[ix]))
   
plt.show()

# Model Prediction on test Images 
for j in range(3):
    for i in range(1,7,2):
        
        ix = random.randint(0, len(preds_test_t))
        
        plt.subplot(3,2,i)
        plt.title('X_test',fontsize = 5)
        plt.imshow(X_test[ix])
        plt.imshow(np.squeeze(preds_test_t[ix]),cmap = 'jet',alpha=0.5)

        plt.subplot(3,2,i+1)
        plt.title('Predict_Test',fontsize = 5)
        plt.imshow(np.squeeze(preds_test_t[ix]))
        
    plt.show()

print('Done!')
```


### Relevant log output

```shell
PS C:\Users\AGIXAnilKumar\Pictures\Datasets> & C:/Users/AGIXAnilKumar/AppData/Local/Programs/Python/Python310/python.exe ""c:/Users/AGIXAnilKumar/Pictures/Datasets/Segmentation_UNET copy.py""
Resizing training images and masks
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 253.25it/s]
Resizing test images
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 390.69it/s]
Done!
2022-12-22 12:29:16.179310: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-22 12:29:16.624929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2783 MB memory:  -> device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1
Model: ""model""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 256, 256, 3  0           []
                                )]

 lambda (Lambda)                (None, 256, 256, 3)  0           ['input_1[0][0]']

 conv2d (Conv2D)                (None, 256, 256, 16  448         ['lambda[0][0]']
                                )

 dropout (Dropout)              (None, 256, 256, 16  0           ['conv2d[0][0]']
                                )

 conv2d_1 (Conv2D)              (None, 256, 256, 16  2320        ['dropout[0][0]']
                                )

 max_pooling2d (MaxPooling2D)   (None, 128, 128, 16  0           ['conv2d_1[0][0]']
                                )

 conv2d_2 (Conv2D)              (None, 128, 128, 32  4640        ['max_pooling2d[0][0]']
                                )

 dropout_1 (Dropout)            (None, 128, 128, 32  0           ['conv2d_2[0][0]']
                                )

 conv2d_3 (Conv2D)              (None, 128, 128, 32  9248        ['dropout_1[0][0]']
                                )

 max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)  0           ['conv2d_3[0][0]']

 conv2d_4 (Conv2D)              (None, 64, 64, 64)   18496       ['max_pooling2d_1[0][0]']

 dropout_2 (Dropout)            (None, 64, 64, 64)   0           ['conv2d_4[0][0]']

 conv2d_5 (Conv2D)              (None, 64, 64, 64)   36928       ['dropout_2[0][0]']

 max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)  0           ['conv2d_5[0][0]']

 conv2d_6 (Conv2D)              (None, 32, 32, 128)  73856       ['max_pooling2d_2[0][0]']

 dropout_3 (Dropout)            (None, 32, 32, 128)  0           ['conv2d_6[0][0]']

 conv2d_7 (Conv2D)              (None, 32, 32, 128)  147584      ['dropout_3[0][0]']

 max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0          ['conv2d_7[0][0]']

 conv2d_8 (Conv2D)              (None, 16, 16, 256)  295168      ['max_pooling2d_3[0][0]']

 dropout_4 (Dropout)            (None, 16, 16, 256)  0           ['conv2d_8[0][0]']

 conv2d_9 (Conv2D)              (None, 16, 16, 256)  590080      ['dropout_4[0][0]']

 conv2d_transpose (Conv2DTransp  (None, 32, 32, 128)  131200     ['conv2d_9[0][0]']
 ose)

 concatenate (Concatenate)      (None, 32, 32, 256)  0           ['conv2d_transpose[0][0]',
                                                                  'conv2d_7[0][0]']

 conv2d_10 (Conv2D)             (None, 32, 32, 128)  295040      ['concatenate[0][0]']

 dropout_5 (Dropout)            (None, 32, 32, 128)  0           ['conv2d_10[0][0]']

 conv2d_11 (Conv2D)             (None, 32, 32, 128)  147584      ['dropout_5[0][0]']

 conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 64)  32832       ['conv2d_11[0][0]']
 spose)

 concatenate_1 (Concatenate)    (None, 64, 64, 128)  0           ['conv2d_transpose_1[0][0]',
                                                                  'conv2d_5[0][0]']

 conv2d_12 (Conv2D)             (None, 64, 64, 64)   73792       ['concatenate_1[0][0]']

 dropout_6 (Dropout)            (None, 64, 64, 64)   0           ['conv2d_12[0][0]']

 conv2d_13 (Conv2D)             (None, 64, 64, 64)   36928       ['dropout_6[0][0]']

 conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 32  8224       ['conv2d_13[0][0]']
 spose)                         )

 concatenate_2 (Concatenate)    (None, 128, 128, 64  0           ['conv2d_transpose_2[0][0]',
                                )                                 'conv2d_3[0][0]']

 conv2d_14 (Conv2D)             (None, 128, 128, 32  18464       ['concatenate_2[0][0]']
                                )

 dropout_7 (Dropout)            (None, 128, 128, 32  0           ['conv2d_14[0][0]']
                                )

 conv2d_15 (Conv2D)             (None, 128, 128, 32  9248        ['dropout_7[0][0]']
                                )

 conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 16  2064       ['conv2d_15[0][0]']
 spose)                         )

 concatenate_3 (Concatenate)    (None, 256, 256, 32  0           ['conv2d_transpose_3[0][0]',
                                )                                 'conv2d_1[0][0]']

 conv2d_16 (Conv2D)             (None, 256, 256, 16  4624        ['concatenate_3[0][0]']
                                )

 dropout_8 (Dropout)            (None, 256, 256, 16  0           ['conv2d_16[0][0]']
                                )

 conv2d_17 (Conv2D)             (None, 256, 256, 16  2320        ['dropout_8[0][0]']
                                )

 conv2d_18 (Conv2D)             (None, 256, 256, 1)  17          ['conv2d_17[0][0]']

==================================================================================================
Total params: 1,941,105
Trainable params: 1,941,105
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
2022-12-22 12:29:19.513315: E tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2022-12-22 12:29:19.514953: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops_fused_impl.h:601 : UNIMPLEMENTED: DNN library is not found.
Traceback (most recent call last):
  File ""c:\Users\AGIXAnilKumar\Pictures\Datasets\Segmentation_UNET copy.py"", line 199, in <module>
    results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=64, epochs=10, callbacks=callbacks) # val_split = 0.1
  File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\eager\execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:

Detected at node 'model/conv2d/Relu' defined at (most recent call last):
    File ""c:\Users\AGIXAnilKumar\Pictures\Datasets\Segmentation_UNET copy.py"", line 199, in <module>
      results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=64, epochs=10, callbacks=callbacks) # val_split = 0.1
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 1564, in fit
      tmp_logs = self.train_function(iterator)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 1160, in train_function
      return step_function(self, iterator)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 1146, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\engine\base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\layers\convolutional\base_conv.py"", line 314, in call
      return self.activation(outputs)
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\activations.py"", line 317, in relu
      return backend.relu(
    File ""C:\Users\AGIXAnilKumar\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\backend.py"", line 5366, in relu
      x = tf.nn.relu(x)
Node: 'model/conv2d/Relu'
DNN library is not found.
         [[{{node model/conv2d/Relu}}]] [Op:__inference_train_function_3360]
```
</details>"
58985,Windows C API package is missing files,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

libtensorflow-cpu-windows-x86_64-2.11.0.zip

### Custom Code

No

### OS Platform and Distribution

MS Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The Windows C API install package for CPU only (libtensorflow-cpu-windows-x86_64-2.11.0.zip) is missing some files.
I saw issue #31567 so tried copying the missing files from the equivalent Linux package and it seems to work as
I am able to build and run my application.

The missing files are:
include/tensorflow/tf_buffer.h
include/tsl (whole folder is missing)
```


### Standalone code to reproduce the issue

```shell
To test the set-up I am using this project:
https://github.com/serizba/cppflow/tree/master/examples/tensor
```


### Relevant log output

_No response_</details>"
58983,TypeError: Could not build a TypeSpec for KerasTensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.11 & tf 2.12-nightly

### Custom Code

No

### OS Platform and Distribution

Linux Pop_OS 22.04

### Mobile device

_No response_

### Python version

3.9.15

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2 / cuDNN 8.1

### GPU model and memory

Nvidia RTX 3090 24Gb

### Current Behaviour?

```shell
Attempting to get the TF2 Mask-RCNN from here: https://github.com/ahmedfgad/Mask-RCNN-TF2 to work in TF2.11.

The tensorflow.keras.layers.Lambda layer used to get the gt_boxes is throwing a type error when attated to the input layer. 


TypeError: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=""created by layer 'tf.math.truediv'"") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>.
```

This worked fine in earlier versions of TF2.X series and Keras.
```


### Standalone code to reproduce the issue

```shell
# 2. GT Boxes in pixels (zero padded)
# [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates
input_gt_boxes = KL.Input(shape=[None, 4], name=""input_gt_boxes"", dtype=tf.float32)
# Normalize coordinates
gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_gt_boxes)

def norm_boxes_graph(boxes, shape):
    """"""Converts boxes from pixel coordinates to normalized coordinates.
    boxes: [..., (y1, x1, y2, x2)] in pixel coordinates
    shape: [..., (height, width)] in pixels

    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized
    coordinates it's inside the box.

    Returns:
        [..., (y1, x1, y2, x2)] in normalized coordinates
    """"""
    h, w = tf.split(tf.cast(shape, tf.float32), 2)
    scale = tf.concat([h, w, h, w], axis=-1) - tf.constant(1.0)
    shift = tf.constant([0., 0., 1., 1.])
    return tf.divide(boxes - shift, scale)
```


### Relevant log output

```shell
File ""/mnt/gluster-vol1/Source/d2/autoAnnotate.py"", line 387, in train
    model = MaskRCNN(mode='training',model_dir='./training_results/',config=config)
  File ""/mnt/gluster-vol1/Source/d2/mrcnn/model.py"", line 1837, in __init__
    self.keras_model = self.build(mode=mode, config=config)
  File ""/mnt/gluster-vol1/Source/d2/mrcnn/model.py"", line 1873, in build
    gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_gt_boxes)
  File ""/home/user/anaconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/user/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py"", line 928, in type_spec_from_value
    raise TypeError(f""Could not build a TypeSpec for {value} of ""
TypeError: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=""created by layer 'tf.math.truediv'"") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>.
```
</details>"
58982,pip subprocess to install build dependencies did not run successfully,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

Tf 2.3

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

3.1.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4/8

### GPU model and memory

NVIDIA RTX 3060 Ti

### Current Behaviour?

```shell
I am trying to build the and install Tensorflow from source. While I am able to successfully build through bazel, I am encountering issues while installing the build wheel package.

The installation is breaking wit subprocess-exited-with-error after trying to install scipy=1.4.1
```


### Standalone code to reproduce the issue

```shell
$ pip install /tmp/tensorflow_pkg/tensorflow-2.3.0-cp39-cp39-linux_x86_64.whl
```


### Relevant log output

```shell
Processing /tmp/tensorflow_pkg/tf_nightly-2.3.0-cp39-cp39-linux_x86_64.whl
Collecting termcolor>=1.1.0
  Using cached termcolor-2.1.1-py3-none-any.whl (6.2 kB)
Collecting astunparse==1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting wrapt>=1.11.1
  Using cached wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)
Requirement already satisfied: wheel>=0.26 in /home/intelligence/.virtualenvs/tf_dev/lib/python3.9/site-packages (from tf-nightly==2.3.0) (0.38.4)
Requirement already satisfied: six>=1.12.0 in /home/intelligence/.virtualenvs/tf_dev/lib/python3.9/site-packages (from tf-nightly==2.3.0) (1.16.0)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting scipy==1.4.1
  Using cached scipy-1.4.1.tar.gz (24.6 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error
  
  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [327 lines of output]
      Ignoring numpy: markers 'python_version == ""3.5"" and platform_system != ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version == ""3.6"" and platform_system != ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version == ""3.7"" and platform_system != ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version == ""3.5"" and platform_system == ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version == ""3.6"" and platform_system == ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version == ""3.7"" and platform_system == ""AIX""' don't match your environment
      Ignoring numpy: markers 'python_version >= ""3.8"" and platform_system == ""AIX""' don't match your environment
      Collecting wheel
        Using cached wheel-0.38.4-py3-none-any.whl (36 kB)
      Collecting setuptools
        Using cached setuptools-65.6.3-py3-none-any.whl (1.2 MB)
      Collecting Cython>=0.29.13
        Using cached Cython-0.29.32-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)
      Collecting numpy==1.17.3
        Using cached numpy-1.17.3.zip (6.4 MB)
        Preparing metadata (setup.py): started
        Preparing metadata (setup.py): finished with status 'done'
      Collecting pybind11>=2.4.0
        Using cached pybind11-2.10.2-py3-none-any.whl (222 kB)
      Building wheels for collected packages: numpy
        Building wheel for numpy (setup.py): started
        Building wheel for numpy (setup.py): finished with status 'error'
        error: subprocess-exited-with-error
      
        × python setup.py bdist_wheel did not run successfully.
        │ exit code: 1
        ╰─> [124 lines of output]
            Running from numpy source directory.
            blas_opt_info:
            blas_mkl_info:
            customize UnixCCompiler
              libraries mkl_rt not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            blis_info:
            customize UnixCCompiler
              libraries blis not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            openblas_info:
            customize UnixCCompiler
            customize UnixCCompiler
              libraries openblas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_3_10_blas_threads_info:
            Setting PTATLAS=ATLAS
            customize UnixCCompiler
              libraries tatlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_3_10_blas_info:
            customize UnixCCompiler
              libraries satlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_blas_threads_info:
            Setting PTATLAS=ATLAS
            customize UnixCCompiler
              libraries ptf77blas,ptcblas,atlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_blas_info:
            customize UnixCCompiler
            get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor']'
            customize Gnu95FCompiler
            Found executable /usr/bin/gfortran
            customize Gnu95FCompiler
            customize Gnu95FCompiler using config
            compiling '_configtest.c':
      
            /* This file is generated from numpy/distutils/system_info.py */
            void ATL_buildinfo(void);
            int main(void) {
              ATL_buildinfo();
              return 0;
            }
      
            C compiler: gcc -pthread -B /home/intelligence/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/intelligence/anaconda3/include -I/home/intelligence/anaconda3/include -fPIC -O2 -isystem /home/intelligence/anaconda3/include -fPIC
      
            compile options: '-c'
            gcc: _configtest.c
            gcc -pthread -B /home/intelligence/anaconda3/compiler_compat _configtest.o -L/usr/lib/x86_64-linux-gnu -lf77blas -lcblas -latlas -o _configtest
            /home/intelligence/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/lib/x86_64-linux-gnu/libatlas.so, not found (try using -rpath or -rpath-link)
            /home/intelligence/anaconda3/compiler_compat/ld: /usr/lib/x86_64-linux-gnu/libatlas.so: undefined reference to `sqrtl@GLIBC_2.2.5'
            /home/intelligence/anaconda3/compiler_compat/ld: /usr/lib/x86_64-linux-gnu/libatlas.so: undefined reference to `sqrt@GLIBC_2.2.5'
            collect2: error: ld returned 1 exit status
            failure.
            removing: _configtest.c _configtest.o _configtest.o.d
            Status: 255
            Output: compiling '_configtest.c':
      
            /* This file is generated from numpy/distutils/system_info.py */
            void ATL_buildinfo(void);
            int main(void) {
              ATL_buildinfo();
              return 0;
            }
      
            C compiler: gcc -pthread -B /home/intelligence/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/intelligence/anaconda3/include -I/home/intelligence/anaconda3/include -fPIC -O2 -isystem /home/intelligence/anaconda3/include -fPIC
      
            compile options: '-c'
            gcc: _configtest.c
            gcc -pthread -B /home/intelligence/anaconda3/compiler_compat _configtest.o -L/usr/lib/x86_64-linux-gnu -lf77blas -lcblas -latlas -o _configtest
      
            Traceback (most recent call last):
              File ""<string>"", line 2, in <module>
              File ""<pip-setuptools-caller>"", line 34, in <module>
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 443, in <module>
                setup_package()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 435, in setup_package
                setup(**metadata)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/core.py"", line 137, in setup
                config = configuration()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 158, in configuration
                config.add_subpackage('numpy')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 1033, in add_subpackage
                config_list = self.get_subpackage(subpackage_name, subpackage_path,
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 999, in get_subpackage
                config = self._get_configuration_from_setup_py(
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 941, in _get_configuration_from_setup_py
                config = setup_module.configuration(*args)
              File ""numpy/setup.py"", line 10, in configuration
                config.add_subpackage('core')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 1033, in add_subpackage
                config_list = self.get_subpackage(subpackage_name, subpackage_path,
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 999, in get_subpackage
                config = self._get_configuration_from_setup_py(
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 941, in _get_configuration_from_setup_py
                config = setup_module.configuration(*args)
              File ""numpy/core/setup.py"", line 764, in configuration
                blas_info = get_info('blas_opt', 0)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 444, in get_info
                return cl().get_info(notfound_action)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 690, in get_info
                self.calc_info()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1810, in calc_info
                if getattr(self, '_calc_info_{}'.format(blas))():
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1756, in _calc_info_atlas
                info = get_info('atlas_blas')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 444, in get_info
                return cl().get_info(notfound_action)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 690, in get_info
                self.calc_info()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1294, in calc_info
                atlas_version, atlas_extra_info = get_atlas_version(**atlas)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1589, in get_atlas_version
                'ATLAS_INFO', _c_string_literal(atlas_version))
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 177, in _c_string_literal
                s = s.replace('\\', r'\\')
            AttributeError: 'NoneType' object has no attribute 'replace'
            [end of output]
      
        note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed building wheel for numpy
        Running setup.py clean for numpy
        error: subprocess-exited-with-error
      
        × python setup.py clean did not run successfully.
        │ exit code: 1
        ╰─> [10 lines of output]
            Running from numpy source directory.
      
            `setup.py clean` is not supported, use one of the following instead:
      
              - `git clean -xdf` (cleans all files)
              - `git clean -Xdf` (cleans all versioned files, doesn't touch
                                  files that aren't checked into the git repo)
      
            Add `--force` to your command to use it anyway if you must (unsupported).
      
            [end of output]
      
        note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed cleaning build dir for numpy
      Failed to build numpy
      Installing collected packages: wheel, setuptools, pybind11, numpy, Cython
        Running setup.py install for numpy: started
        Running setup.py install for numpy: finished with status 'error'
        error: subprocess-exited-with-error
      
        × Running setup.py install for numpy did not run successfully.
        │ exit code: 1
        ╰─> [133 lines of output]
            Running from numpy source directory.
      
            Note: if you need reliable uninstall behavior, then install
            with pip instead of using `setup.py install`:
      
              - `pip install .`       (from a git repo or downloaded source
                                       release)
              - `pip install numpy`   (last NumPy release on PyPi)
      
      
            blas_opt_info:
            blas_mkl_info:
            customize UnixCCompiler
              libraries mkl_rt not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            blis_info:
            customize UnixCCompiler
              libraries blis not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            openblas_info:
            customize UnixCCompiler
            customize UnixCCompiler
              libraries openblas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_3_10_blas_threads_info:
            Setting PTATLAS=ATLAS
            customize UnixCCompiler
              libraries tatlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_3_10_blas_info:
            customize UnixCCompiler
              libraries satlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_blas_threads_info:
            Setting PTATLAS=ATLAS
            customize UnixCCompiler
              libraries ptf77blas,ptcblas,atlas not found in ['/home/intelligence/.virtualenvs/tf_dev/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu/atlas', '/usr/lib/x86_64-linux-gnu']
              NOT AVAILABLE
      
            atlas_blas_info:
            customize UnixCCompiler
            get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor']'
            customize Gnu95FCompiler
            Found executable /usr/bin/gfortran
            customize Gnu95FCompiler
            customize Gnu95FCompiler using config
            compiling '_configtest.c':
      
            /* This file is generated from numpy/distutils/system_info.py */
            void ATL_buildinfo(void);
            int main(void) {
              ATL_buildinfo();
              return 0;
            }
      
            C compiler: gcc -pthread -B /home/intelligence/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/intelligence/anaconda3/include -I/home/intelligence/anaconda3/include -fPIC -O2 -isystem /home/intelligence/anaconda3/include -fPIC
      
            compile options: '-c'
            gcc: _configtest.c
            gcc -pthread -B /home/intelligence/anaconda3/compiler_compat _configtest.o -L/usr/lib/x86_64-linux-gnu -lf77blas -lcblas -latlas -o _configtest
            /home/intelligence/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/lib/x86_64-linux-gnu/libatlas.so, not found (try using -rpath or -rpath-link)
            /home/intelligence/anaconda3/compiler_compat/ld: /usr/lib/x86_64-linux-gnu/libatlas.so: undefined reference to `sqrtl@GLIBC_2.2.5'
            /home/intelligence/anaconda3/compiler_compat/ld: /usr/lib/x86_64-linux-gnu/libatlas.so: undefined reference to `sqrt@GLIBC_2.2.5'
            collect2: error: ld returned 1 exit status
            failure.
            removing: _configtest.c _configtest.o _configtest.o.d
            Status: 255
            Output: compiling '_configtest.c':
      
            /* This file is generated from numpy/distutils/system_info.py */
            void ATL_buildinfo(void);
            int main(void) {
              ATL_buildinfo();
              return 0;
            }
      
            C compiler: gcc -pthread -B /home/intelligence/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/intelligence/anaconda3/include -I/home/intelligence/anaconda3/include -fPIC -O2 -isystem /home/intelligence/anaconda3/include -fPIC
      
            compile options: '-c'
            gcc: _configtest.c
            gcc -pthread -B /home/intelligence/anaconda3/compiler_compat _configtest.o -L/usr/lib/x86_64-linux-gnu -lf77blas -lcblas -latlas -o _configtest
      
            Traceback (most recent call last):
              File ""<string>"", line 2, in <module>
              File ""<pip-setuptools-caller>"", line 34, in <module>
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 443, in <module>
                setup_package()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 435, in setup_package
                setup(**metadata)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/core.py"", line 137, in setup
                config = configuration()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/setup.py"", line 158, in configuration
                config.add_subpackage('numpy')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 1033, in add_subpackage
                config_list = self.get_subpackage(subpackage_name, subpackage_path,
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 999, in get_subpackage
                config = self._get_configuration_from_setup_py(
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 941, in _get_configuration_from_setup_py
                config = setup_module.configuration(*args)
              File ""numpy/setup.py"", line 10, in configuration
                config.add_subpackage('core')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 1033, in add_subpackage
                config_list = self.get_subpackage(subpackage_name, subpackage_path,
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 999, in get_subpackage
                config = self._get_configuration_from_setup_py(
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/misc_util.py"", line 941, in _get_configuration_from_setup_py
                config = setup_module.configuration(*args)
              File ""numpy/core/setup.py"", line 764, in configuration
                blas_info = get_info('blas_opt', 0)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 444, in get_info
                return cl().get_info(notfound_action)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 690, in get_info
                self.calc_info()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1810, in calc_info
                if getattr(self, '_calc_info_{}'.format(blas))():
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1756, in _calc_info_atlas
                info = get_info('atlas_blas')
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 444, in get_info
                return cl().get_info(notfound_action)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 690, in get_info
                self.calc_info()
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1294, in calc_info
                atlas_version, atlas_extra_info = get_atlas_version(**atlas)
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 1589, in get_atlas_version
                'ATLAS_INFO', _c_string_literal(atlas_version))
              File ""/tmp/pip-install-dzf32x9v/numpy_f29f84f3685b47618f1c97bbca81883b/numpy/distutils/system_info.py"", line 177, in _c_string_literal
                s = s.replace('\\', r'\\')
            AttributeError: 'NoneType' object has no attribute 'replace'
            [end of output]
      
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: legacy-install-failure
      
      × Encountered error while trying to install package.
      ╰─> numpy
      
      note: This is an issue with the package mentioned above, not pip.
      hint: See above for output from the failure.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```
</details>"
58981,with `with strategy.scope():` BERT output loses it's shape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7.4

### Custom Code

Yes

### OS Platform and Distribution

Kaggle kernel

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
`hub.KerasLayer` loses it's shape under `strategy.scope()`
```


### Standalone code to reproduce the issue

```shell
To reproduce: https://www.kaggle.com/code/maifeeulasad/tfhub-bert-with-scope/

Code:

def get_model():
    bert_preprocess = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"")
    bert_encoder = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"")
    
    
    text_input = Input(shape=(), dtype=tf.string, name='text')
    preprocessed_text = bert_preprocess(text_input)
    outputs = bert_encoder(preprocessed_text)
    
    output_sequence = outputs['sequence_output']
    x = Flatten()(output_sequence)
    x = Dense(NUM_CLASS,  activation='sigmoid')(x)

    model = Model(inputs=[text_input], outputs = [x])
    return model


# 1
optimizer = Adam()
model = get_model()
model.compile(loss=CategoricalCrossentropy(from_logits=True),optimizer=optimizer,metrics=[Accuracy(), ],)
model.summary()

# 2 <--- issue here
with strategy.scope():
    optimizer = Adam()
    model_scoped = get_model()
    model_scoped.compile(loss=CategoricalCrossentropy(from_logits=True),optimizer=optimizer,metrics=[Accuracy(), ],)
    
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_19/3651651963.py in <module>
      1 with strategy.scope():
      2     optimizer = Adam()
----> 3     model_scoped = get_model()
      4     model_scoped.compile(loss=CategoricalCrossentropy(from_logits=True),optimizer=optimizer,metrics=[Accuracy(), ],)
      5 

/tmp/ipykernel_19/304083462.py in get_model()
     10     output_sequence = outputs['sequence_output']
     11     x = Flatten()(output_sequence)
---> 12     x = Dense(NUM_CLASS,  activation='sigmoid')(x)
     13 
     14     model = Model(inputs=[text_input], outputs = [x])

/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

/opt/conda/lib/python3.7/site-packages/keras/layers/core/dense.py in build(self, input_shape)
    137     last_dim = tf.compat.dimension_value(input_shape[-1])
    138     if last_dim is None:
--> 139       raise ValueError('The last dimension of the inputs to a Dense layer '
    140                        'should be defined. Found None. '
    141                        f'Full input shape received: {input_shape}')

ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)


Related issue: https://github.com/tensorflow/hub/issues/870
```
</details>"
58980, SavedModel load for tags { serve }; Status: fail: Not found: Op type not registered 'TFRA>CuckooHashTableOfTensors' in binary running on machine....,"I had this problem with LoadSavedModel in tensorflow
datail:
Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-12-22 10:53:51.466015: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: fail: Not found: Op type not registered 'TFRA>CuckooHashTableOfTensors' in binary running on machine. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.. Took 348271 microseconds.

I use TFRA to train the model, and use LoadSavedModel in tensor flow to load

"
58975,Installing sktime all_extras got compatible issues with tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

M1 macbook metal tf2.5

### Custom Code

Yes

### OS Platform and Distribution

Mac OS 13.1

### Mobile device

M1 macbook 2020

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

apple metal

### Current Behaviour?

```shell
A bug happened!
```


### Standalone code to reproduce the issue

```shell
I am trying to install sktime with all soft dependencies including tensorflow,
when I do developer install ""pip install -e .""[all_extras,dev]"" I got this error: ""ERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9
ERROR: Could not find a version that satisfies the requirement tensorflow; extra == ""all_extras"" (from sktime[all-extras,dev]) (from versions: none)
ERROR: No matching distribution found for tensorflow; extra == ""all_extras""""
```


### Relevant log output

_No response_</details>"
58973,cannot deepcopy Optimizer class anymore in tf>=2.11," ### Issue Type

Bug

### Current Behaviour?

from the 2.11 version, we cannot serialize optimizers (the keras.optimizers.optimizer_experimental.optimizer.Optimizer hierarchy) anymore 
due to its _distribution_strategy attribute of singleton type _DefaultDistributionStrategy



### Standalone code to reproduce the issue

this was fine in 2.10.x:

```shell

import tensorflow as tf
from copy import deepcopy

optimizer = tf.keras.optimizers.Adam()
deepcopy(optimizer)
```

with 2.11.0 we have this error:
```
RuntimeError: Should only create a single instance of _DefaultDistributionStrategy
```

### Source

binary

### Tensorflow Version

2.11.0

### OS Platform and Distribution

archlinux/python3.10


<details><summary>Click to expand!</summary> 
 

### Relevant log output

```shell
In [4]: deepcopy(optimizer)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[4], line 1
----> 1 deepcopy(optimizer)

File /usr/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File /usr/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    269 if state is not None:
    270     if deep:
--> 271         state = deepcopy(state, memo)
    272     if hasattr(y, '__setstate__'):
    273         y.__setstate__(state)

File /usr/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)
    144 copier = _deepcopy_dispatch.get(cls)
    145 if copier is not None:
--> 146     y = copier(x, memo)
    147 else:
    148     if issubclass(cls, type):

File /usr/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)
    229 memo[id(x)] = y
    230 for key, value in x.items():
--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)
    232 return y

File /usr/lib/python3.10/copy.py:153, in deepcopy(x, memo, _nil)
    151 copier = getattr(x, ""__deepcopy__"", None)
    152 if copier is not None:
--> 153     y = copier(memo)
    154 else:
    155     reductor = dispatch_table.get(cls)

File /usr/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3598, in _DefaultDistributionStrategy.__deepcopy__(***failed resolving arguments***)
   3596 def __deepcopy__(self, memo):
   3597   del memo
-> 3598   raise RuntimeError(""Should only create a single instance of ""
   3599                      ""_DefaultDistributionStrategy"")

RuntimeError: Should only create a single instance of _DefaultDistributionStrategy
```
</details>"
58972,__init__() got an unexpected keyword argument 'reduction',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Compile a model with `metrics=[tf.keras.losses.XXX]` does not raise error. The training process seems also good. However, if we save the model and try to reload it, it does not work.

Now I know we could not use `tf.keras.losses` in `metrics`, we must use some functions in `tf.keras.metrics` or some customized metrics. But the error message ""__init__() got an unexpected keyword argument 'reduction'"" gives no information. 

Why not raise an error during the training or even during the compiling? That would be much more friendly for new comers.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.keras.layers import Dense, Dropout
import tensorflow as tf

input_length = 2
latent_dim = 512
output_length = 2

model = tf.keras.Sequential([
    Dense(latent_dim, activation='relu', input_shape=(input_length,)),
    Dropout(rate=0.5),
    Dense(units=latent_dim, activation='relu'),
    Dense(units=output_length),
  ])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.losses.MeanAbsoluteError()])

x_train = np.ones(shape=(150000, 2))
y_train = np.ones(shape=(150000, 2))

history = model.fit(x_train, y_train, epochs=2, batch_size=128)

model.save('saved_model/my_model')
pretrained = tf.keras.models.load_model('saved_model/my_model')
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""d:/VScode/M2-SMC/debug.py"", line 26, in <module>
    pretrained = tf.keras.models.load_model('saved_model/my_model')
  File ""C:\Users\yunhao\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\yunhao\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 784, in from_config
    return cls(**config)
TypeError: __init__() got an unexpected keyword argument 'reduction'
```
</details>"
58971,XLA Collapse Documentation: Example Incorrect,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It seems like the XLA documentation for the Collapse operator is wrong. Given a tensor with dimensions `[4,2,3]`, collapsing over dimensions `[0,1]`, should result (as described in the text) in a tensor of dims `[8,3]`. In the example, however, you show the result to be a `[4,6]` tensor.
```


### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
58969,Almost wiped my PC trying to make Tensorboard work on windows Jupyter notebook,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I executed the following code:

logdir = pathlib.Path()
shutil.rmtree(logdir, ignore_errors=True)

I was trying to fix my tensorboard issues and ended up almost wiping my entire PC clean. Is there a way of getting my files back? It seems like there is nothing in the recycle bin. Please help?
```


### Standalone code to reproduce the issue

```shell
Lost over 100GB of files
```


### Relevant log output

```shell
n/a
```
</details>"
58966,Validate on entire validation_data if Dataset has length less than validation_steps,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We provide a `tf.data.Dataset` and `validation_steps` to `Model.fit`. The size of our Dataset varies may vary across experiments. If `validation_steps <= len(Dataset)`, then everything works as expected. However, if `validation_steps > len(Dataset)`, validation is only performed _once_ (at the first epoch). In this case, I'd expect that validation is simply performed across the entire dataset every epoch.

A hacky solution around this problem is to use `.repeat()`, but that is a poor solution since it will perform duplicate (unnecessary) evaluations on the same samples.
```


### Standalone code to reproduce the issue

```shell
Simply call `Model.fit` with `validation_steps > len(Dataset)`
```


### Relevant log output

_No response_</details>"
58965,Support for indexing a container of `Layer`s using index from `Dataset` element inside a `tf.function` or `Layer.call`,"### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

I think One-Hot based indexing / control flow inside a tf.function is a common idea in deep learning. It would be great if the TF team made this easier / doable to implement for efficient computation.

https://stackoverflow.com/questions/74856566/how-to-index-a-list-of-tf-keras-layers-layer-with-a-tensorflow-tensor-inside-a-t?noredirect=1#comment132112356_74856566 

I have a list of `tf.keras.layers.Layer` that I want to index inside a `tf.function` based off of an `index_tensor` (that comes from iterating over a `tf.data.Dataset`). Ideally like this:

```python
import tensorflow as tf
from tensorflow.keras import layers
from typing import List

@tf.function
def compute_single_output(heads: List[layers.Layer], index_tensor: tf.Tensor, input: tf.Tensor):
    logit = heads[index_tensor](input)
    return logit
```

But I get this:

```shell
    TypeError: list indices must be integers or slices, not Tensor
```

Is there any way to set this up so that I can do it? If not, this feature would be very helpful.

### What I've tried:
`tf.gather` won't work for a list of Layers. `tf.lookup` table wouldn't work either. I was able to make `tf.switch_case` work, but it then forces re-trace compile on every iteration of the training loop. I tried using a dict of heads instead of a list, also didn't work.

Only thing that works is to iterate over the list in a Python for loop, but this accumulates CPU RAM per iteration over the loop (because autograph can't convert `heads` into a tensor, it can't use `tf.while_loop`, see https://www.tensorflow.org/guide/function#loops for more info). It's also O(n):

```python
@tf.function
def compute_single_output(heads: List[layers.Layer], index_tensor: tf.Tensor, input: tf.Tensor):
    logit = tf.constant(-1, dtype=tf.float32)
    m = tf.constant(0)
    for head in heads:
        if m == index_tensor:
            logit = head(input)
        else:
            pass
        m += 1
    return logit
```

I expect to be running this function for a long time, so I can't afford to accumulate memory over time.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers
from typing import List

@tf.function
def compute_single_output(heads: List[layers.Layer], index_tensor: tf.Tensor, input: tf.Tensor):
    logit = heads[index_tensor](input)
    return logit

heads = [layers.Dense(3), layers.Dense(4)]
index_tensor = tf.constant(0, dtype=tf.int32)
input = tf.random.uniform((8, 6), dtype=tf.float32)

output = compute_single_output(heads, index_tensor, input)
```


### Relevant log output

_No response_

## Some Ideas
How hard is it to implement a ""LayerList"" or ""ModuleList"" into TF (that can be indexed with a `Tensor`)? Here's one in pytorch: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html 

Or have the Functional API allow for one-hot based control flow

I can try writing a custom op to map `index_tensor` -> `heads[index_tensor]` myself (I know C++), though I've never done that before"
58961,"Unable to use any augmentation layer within `tf.data` with XLA enabled, in `MultiWorkerMirroredStrategy`","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8, cuDNN 8.7

### GPU model and memory

RTX 3090, 24GB

### Current Behaviour?

Data Augmentation is generally offloaded to CPU to enable parallelism using `tf.data.Dataset.map(...)` operation.

This works as expected, with XLA enabled, on single-node training. But using `MultiWorkerMirroredStrategy()`, with XLA enabled, `GraphExecutionError` occurs.

With XLA disabled, `tf.data` based augmentation works fine.


### Standalone code to reproduce the issue

```shell
# NCCL AllReduce is being used.
strategy = tf.distribute.MultiWorkerMirroredStrategy(...)  # Fails with `jit_compile=True` below
strategy = tf.distribute.OneDeviceStrategy('/gpu:0')  # Works with `jit_compile=True`
(raw_train_ds, raw_test_ds), ds_info = tfds.load(...)

""""""Build model in distributed scope""""""
with strategy.scope():
  augmentation = tf.keras.Sequential([
    layers.RandomFlip(),
    layers.RandomZoom(0.1),
    layers.RandomTranslation(0.2, 0.2),
  ])
  
  backbone = tf.keras.applications.ResNet50V2(
    weights=None,
    include_top=False,
    input_shape=(*IMG_SIZE, 3),
    pooling='max'
  )
  backbone.trainable = True
  model = tf.keras.Sequential([
    layers.Input((*IMG_SIZE, 3)),
    backbone,
    layers.Dense(ds_info.features['label'].num_classes,
  ])
  optimizer = tf.optimizers.SGD(
    learning_rate=LR * strategy.num_replicas_in_sync,
    momentum=0.9,
    weight_decay=1e-4)

  model.compile(
      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=optimizer,
      metrics=['accuracy'],
      jit_compile=True)

  if WORKER_RANK == 0: model.summary()
train_ds = (raw_train_ds.shard(WORLD_SIZE, WORKER_RANK)
                          .shuffle(8192)
                          .map(decode_example(IMG_SIZE), tf.data.AUTOTUNE)
                          .map(lambda x, y: (augmentation(x, training=True), y), tf.data.AUTOTUNE)
                          .batch(global_batch_size)
                          .prefetch(tf.data.AUTOTUNE))
test_ds = (raw_test_ds.shard(WORLD_SIZE, WORKER_RANK)
                        .map(decode_example(IMG_SIZE), tf.data.AUTOTUNE)
                        .batch(global_batch_size)
                        .prefetch(tf.data.AUTOTUNE))
model.fit(train_ds, 
            epochs=100,
            validation_data=test_ds)
```


### Relevant log output

```shell
Error log (you may see duplication of lines, since this is the output of 2 nodes running the same code via MPI):

2022-12-20 21:55:47.039610: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-20 21:55:54.181560: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Traceback (most recent call last):
  File ""main.py"", line 167, in <module>
    main()
  File ""main.py"", line 160, in main
    model.fit(train_ds, 
  File ""/home/mpiuser/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/mpiuser/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
Node: 'StatefulPartitionedCall'
Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
Node: 'StatefulPartitionedCall'
2 root error(s) found.
  (0) INTERNAL:  tensorflow/compiler/xla/service/gpu/nccl_utils.cc:266: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: remote process exited or there was a network error
         [[{{node StatefulPartitionedCall}}]]
         [[Reshape_6/_62]]
  (1) INTERNAL:  tensorflow/compiler/xla/service/gpu/nccl_utils.cc:266: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: remote process exited or there was a network error
         [[{{node StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_35379]
Traceback (most recent call last):
  File ""main.py"", line 167, in <module>
    main()
  File ""main.py"", line 160, in main
    model.fit(train_ds, 
  File ""/home/mpiuser/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/mpiuser/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
Node: 'StatefulPartitionedCall'
Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
Node: 'StatefulPartitionedCall'
2 root error(s) found.
  (0) INTERNAL:  tensorflow/compiler/xla/service/gpu/nccl_utils.cc:266: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: remote process exited or there was a network error
         [[{{node StatefulPartitionedCall}}]]
         [[Reshape_6/_62]]
  (1) INTERNAL:  tensorflow/compiler/xla/service/gpu/nccl_utils.cc:266: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: remote process exited or there was a network error
         [[{{node StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_34897]
```
</details>"
58960,Not able to ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! 
This is the error while im trying to import pipeline from transformer. My tensorflow is upraded to latesr version 2.11.0 and i am running this on windows 11 on visual code using python 3.10
```


### Standalone code to reproduce the issue

```shell
from transformers import pipeline
RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): No module named 'torch.distributed'
```


### Relevant log output

```shell
File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\utils\import_utils.py:1093, in _LazyModule._get_module(self, module_name)
   1092 try:
-> 1093     return importlib.import_module(""."" + module_name, self.__name__)
   1094 except Exception as e:

File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\importlib\__init__.py:126, in import_module(name, package)
    125         level += 1
--> 126 return _bootstrap._gcd_import(name[level:], package, level)

File <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:1006, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:688, in _load_unlocked(spec)

File <frozen importlib._bootstrap_external>:883, in exec_module(self, module)

File <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args, **kwds)

File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\pipelines\__init__.py:48
     40 from ..utils import (
...
   1097         f"" traceback):\n{e}""
   1098     ) from e
```
</details>"
58958,Missing docker images for 2.5.2 and 2.5.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.5.2 and 2.5.3

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The docker images for 2.5.2 and 2.5.3 are missing here:
https://hub.docker.com/r/tensorflow/tensorflow
How can I obtain these images? I need the GPU images.
```


### Standalone code to reproduce the issue

```shell
sudo docker pull tensorflow/tensorflow:2.5.2-gpu
sudo docker pull tensorflow/tensorflow:2.5.3-gpu
```


### Relevant log output

_No response_</details>"
58957,GPUs only visible for administrators,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2.2/8.1.1

### GPU model and memory

NVIDIA RTX A5000

### Current Behaviour?

```shell
When executing tf.config.list_physical_devices(), tensorflow returns a list with all installed GPUs only when executed by an admin. Standard user accounts see only CPU devices.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices())
```


### Relevant log output

_No response_</details>"
58956, /home/work/tensorflow/test/tensorflow-2.4.0/tensorflow/compiler/xla/service/cpu/BUILD:695:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_matmul' failed (Exit 4),"use bazel build --jobs 1 --local_ram_resources=HOST_RAM*0.9 --local_cpu_resources=HOST_CPUS*1  --config=opt //tensorflow:libtensorflow_cc.so

my tensorflow is 2.4.0

ERROR: /home/work/tensorflow/test/tensorflow-2.4.0/tensorflow/compiler/xla/service/cpu/BUILD:695:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_matmul' failed (Exit 4)
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <http://bugzilla.redhat.com/bugzilla> for instructions.
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 32.611s, Critical Path: 21.70s
INFO: 39 processes: 39 local.
FAILED: Build did NOT complete successfully"
58955,tensorflow.experimental.numpy.lcm giving inconsistent results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to use the function `tf.experimental.numpy.lcm` but getting unusual results after doing a manual calculation.

Here is an example:
x = tf.constant([6], dtype=tf.int8)
y = tf.constant([22], dtype=tf.int8)

The expected lcm of the above two tensors should be a tensor `tf.Tensor([66], shape=(1,), dtype=int8)`
```


### Standalone code to reproduce the issue


Using Tensorflow

```

import tensorflow as tf

x = tf.constant([6], dtype=tf.int8)
y = tf.constant([22], dtype=tf.int8)
print(tf.experimental.numpy.lcm(x, y)) # Current output: tf.Tensor([62], shape=(1,), dtype=int8)
```
Using numpy

```
import numpy as np

x = np.array([6], dtype=np.int8)
y = np.array([22], dtype=np.int8)
z = np.lcm(x, y)
print(repr(z)) # Output: array([66], dtype=int8)
```
```


### Relevant log output

_No response_</details>"
58953,Unable to build Tensorflow 2.11.0 from source code,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

No

### OS Platform and Distribution

Amazon Linux 2

### Mobile device

_No response_

### Python version

3.7

### Bazel version

5.3.0

### GCC/Compiler version

7.3.1

### CUDA/cuDNN version

11.8.0/8.6.0

### GPU model and memory

NVIDIA 

### Current Behaviour?

```shell
Met below compile error when running command `bazel --max_idle_secs=60 build --config=opt --config=cuda --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package`.


[29,045 / 33,182] Compiling tensorflow/core/kernels/pad_op_gpu.cu.cc; 235s local ... (72 actions, 71 running)
[30,249 / 33,182] Compiling tensorflow/core/kernels/pad_op_gpu.cu.cc; 435s local ... (72 actions, 71 running)
ERROR: /codebuild/output/src557356419/src/github.com/tensorflow/tensorflow/tensorflow/compiler/mlir/quantization/tensorflow/python/BUILD:13:11: Compiling tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_release/d8567c683bdb8c91b8e55044ae41426f/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.8 \
    CUDNN_INSTALL_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/lib:/usr/local/cuda/lib64/stubs: \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/codebuild/user/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib64/python3.7/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.7,5.2,7.0 \
    TF_CUDA_VERSION=11.8 \
    TF_CUDNN_VERSION=8.6 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF ...
  ......
# Configuration: ba96b98541fcc31cd39f04dd19ffaddc2a35225a697570c451b030de703660d8
# Execution platform: @local_execution_config_platform//:platform
tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.cc: In function 'absl::lts_20220623::StatusOr<std::unique_ptr<llvm::raw_fd_ostream> > tensorflow::quantization::internal::{anonymous}::CreateMlirDumpFile(absl::lts_20220623::string_view)':
tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.cc:140:10: error: could not convert 'dump_file' from 'std::unique_ptr<llvm::raw_fd_ostream>' to 'absl::lts_20220623::StatusOr<std::unique_ptr<llvm::raw_fd_ostream> >'
   return dump_file;
          ^~~~~~~~~
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-array-parameter'
cc1plus: warning: unrecognized command line option '-Wno-unknown-warning'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1659.532s, Critical Path: 608.28s
INFO: 30987 processes: 10197 internal, 20790 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
```


### Standalone code to reproduce the issue

```shell
bazel --max_idle_secs=60 build --config=opt --config=cuda --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_</details>"
58952,TensorFlow device is mapped to multiple devices when using tf.estimator models and setting visible device using TF v1 api with TF2.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

python 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.3

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running models implemented with tf.estimator/Horovod to use multi-cards (like starting a 2ranks job in a host with 4 GTX-1080), we need to set visiable device for each process so that these processes will use different GPU cards. Just follow [link](https://github.com/horovod/horovod/blob/master/docs/tensorflow.rst#:~:text=and%20so%20forth.-,For%20TensorFlow%20v1%3A,-config%20%3D%20tf).

Following above mentioned settings, multi-cards job works well when using TF2.10, but throws Tensorflow Device mapping error when using TF2.11. This error only exists when using model implemented by tf.estimator and setting visible device with tf.ConfigProto().gpu_options.visible_device_list.

It seems that this error is relative to this [commit]( https://github.com/tensorflow/tensorflow/commit/16ea0f8995a9b087ff7e50bbf5a03e58e3c5002e#:~:text=if-,context.is_custom_device,-(device_string)%3A). When calling context.is_cutom_device(), this API would create TFE_Context that would create TF devices. However, program would also create TF devices when create Session with the aboved mentioned config, and these two TF device creation would conflicts.

To reproduce this error, you need to install horovod and using multi card machine to run below code with:

horovodrun -np N python xxx.py

where N denotes the number of processes to be started.
```


### Standalone code to reproduce the issue

```shell
import pandas as pd
import tensorflow as tf
import tensorflow.compat.v1 as tf1
global is_mpi
try:
    import horovod.tensorflow as hvd
    hvd.init()
    is_mpi = hvd.size()
except ImportError:
    is_mpi = 0
    print(""No MPI horovod support, this is running in no-MPI mode!"")

session_config = tf1.ConfigProto()
if is_mpi:
  session_config.gpu_options.visible_device_list = str(hvd.local_rank())

run_config = tf.estimator.RunConfig(session_config=session_config)

x_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
x_train['sex'].replace(('male', 'female'), (0, 1), inplace=True)
x_train['alone'].replace(('n', 'y'), (0, 1), inplace=True)
x_train['class'].replace(('First', 'Second', 'Third'), (1, 2, 3), inplace=True)
x_train.drop(['embark_town', 'deck'], axis=1, inplace=True)
y_train = x_train.pop('survived')

# Data setup for TensorFlow 1 with `tf.estimator`
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((dict(x_train), y_train)).batch(32)

FEATURE_NAMES = [
    'age', 'fare', 'sex', 'n_siblings_spouses', 'parch', 'class', 'alone'
]
feature_columns = []
for fn in FEATURE_NAMES:
  feat_col = tf1.feature_column.numeric_column(fn, dtype=tf.float32)
  feature_columns.append(feat_col)

linear_estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.BinaryClassHead(),
    feature_columns=feature_columns,
    model_dir=""./model"",
    config=run_config)
linear_estimator.train(input_fn=_input_fn, steps=1000)
```


### Relevant log output

```shell
[1]<stderr>:2022-12-19 16:12:04.169004: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: ALREADY_EXISTS: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
[1]<stderr>:2022-12-19 16:12:04.169026: E tensorflow/c/c_api.cc:2209] ALREADY_EXISTS: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
[1]<stderr>:Traceback (most recent call last):
[1]<stderr>:  File ""/home/yangshe1/lufengqing/issue_case/case.py"", line 43, in <module>
[1]<stderr>:    linear_estimator.train(input_fn=_input_fn, steps=1000)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 360, in train
[1]<stderr>:    loss = self._train_model(input_fn, hooks, saving_listeners)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1186, in _train_model
[1]<stderr>:    return self._train_model_default(input_fn, hooks, saving_listeners)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1217, in _train_model_default
[1]<stderr>:    return self._train_with_estimator_spec(estimator_spec, worker_hooks,
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1512, in _train_with_estimator_spec
[1]<stderr>:    with training.MonitoredTrainingSession(
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 606, in MonitoredTrainingSession
[1]<stderr>:    return MonitoredSession(
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1050, in __init__
[1]<stderr>:    super(MonitoredSession, self).__init__(
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 753, in __init__
[1]<stderr>:    self._sess = _RecoverableSession(self._coordinated_creator)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1259, in __init__
[1]<stderr>:    _WrappedSession.__init__(self, self._create_session())
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1264, in _create_session
[1]<stderr>:    return self._sess_creator.create_session()
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 906, in create_session
[1]<stderr>:    self.tf_sess = self._session_creator.create_session()
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 665, in create_session
[1]<stderr>:    return self._get_session_manager().prepare_session(
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py"", line 309, in prepare_session
[1]<stderr>:    sess, is_loaded_from_checkpoint = self._restore_checkpoint(
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py"", line 218, in _restore_checkpoint
[1]<stderr>:    sess = session.Session(self._target, graph=self._graph, config=config)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1604, in __init__
[1]<stderr>:    super(Session, self).__init__(target, graph, config=config)
[1]<stderr>:  File ""/home/yangshe1/miniconda3/envs/lfq_/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 712, in __init__
[1]<stderr>:    self._session = tf_session.TF_NewSessionRef(c_graph, opts)
[1]<stderr>:tensorflow.python.framework.errors_impl.AlreadyExistsError: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
```
</details>"
58950,[NVIDIA XLA] XLA generates element-wise fusion kernel with no data locality for GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf r2.11

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 16.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

gcc

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I encountered a case where XLA creates very bad layout for pointwise fused kernel,  the fused kernel are using layout {1,0,2,3},  which leads to almost no data locality across CUDA block threads, and these kernels takes really long time (45% of  training step for Stable Diffusion unit test).  

Can you confirm that this is a layout assignment issue (bug) of XLA?   

Attaching the HLO dumps of before/after optimization. please check fusion fused_computation.1116   the fused graph for fused_computation.1116 is also attached.

Attached files: 
https://drive.google.com/drive/folders/1BRA4BExE_v0xIOm5v-nRf_Xkb6gDoqhH?usp=sharing



### Standalone code to reproduce the issue



In the google drive I shared, there is a unittest to produce the issue. to run it, download the folder `unittest`, and from inside the folder, run `bash r.sh` 



### Relevant log output

_No response_</details>"
58947,BatchNormalization is not found under `from tensorflow.python.keras.layers`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Mac Monterey

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I can't import `BatchNormalization`

The following works though.
`from tensorflow.python.keras.layers import Input, Conv2D, Flatten, MaxPooling2D`
```


### Standalone code to reproduce the issue

```shell
from tensorflow.python.keras.layers import BatchNormalization
```


### Relevant log output

```shell
ImportError: cannot import name 'BatchNormalization' from 'tensorflow.python.keras.layers' (/Users/..../PycharmProjects/....../venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/__init__.py)
```
</details>"
58945,"unable to save function, WeightNormalization with Conv1DTranspose error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

RTX 3080TI 

### Current Behaviour?

```shell
From the official tutorial on melGAN here: https://keras.io/examples/audio/melgan_spectrogram_inversion/
The problem occurs with addon_layers.WeightNormalization wrapped around Conv1DTranspose layer. The normalization works on other layers such as Conv1D but not the Conv1DTranspose layer. The model trains fine but error arises when saving the model with error message:
ValueError: Unable to save function b'__inference_conv1d_transpose_3_layer_call_and_return_conditional_losses_101006' because it captures graph tensor Tensor(""weight_normalization_21/compute_weights/mul:0"", shape=(16, 32, 64), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`


class ConvBlock(layers.Layer):
    def __init__(self, conv_dim, upsampling_factor):
        super().__init__()
        self.conv_t = addon_layers.WeightNormalization(Conv1DTranspose(conv_dim, 16, upsampling_factor, padding=""same""), data_init=False)
        #self.conv_t = Conv1DTranspose(conv_dim, 16, upsampling_factor, padding=""same"")
        self.res_block = ResidualBlock(conv_dim)

    def call(self, X, training=False):
        t1 = self.conv_t(X, training=training)
        c1 = tf.nn.leaky_relu(t1, 0.3)
        out = self.res_block(c1, training=training) #I've put the leakly_relu activation into res_block
        return out
```
```


### Standalone code to reproduce the issue

```shell
https://keras.io/examples/audio/melgan_spectrogram_inversion/
```


### Relevant log output

```shell
ValueError: Unable to save function b'__inference_conv1d_transpose_3_layer_call_and_return_conditional_losses_101006' because it captures graph tensor Tensor(""weight_normalization_21/compute_weights/mul:0"", shape=(16, 32, 64), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`
```
</details>"
58944,Suggested Build Params For Older Gen Intel CPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Just wondering what the suggested build parameters would be for an Intel Xeon E5-4650V2 with no GPU? Here are the specs when running lscpu:


CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   40 bits physical, 48 bits virtual
CPU(s):                          16
On-line CPU(s) list:             0-15
Thread(s) per core:              1
Core(s) per socket:              8
Socket(s):                       2
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      15
Model:                           6
Model name:                      Common KVM processor
Stepping:                        1
CPU MHz:                         2493.988
BogoMIPS:                        4987.97
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       512 KiB
L1i cache:                       512 KiB
L2 cache:                        64 MiB
L3 cache:                        32 MiB
NUMA node0 CPU(s):               0-15
Vulnerability Itlb multihit:     KVM: Vulnerable
Vulnerability L1tf:              Mitigation; PTE Inversion
Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Mmio stale data:   Unknown: No mitigations
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx lm constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault
                                  pti
```

I would use the default `pip install tensorflow`, but I get the error that our CPU doesn't support sse4.1 and a bunch of other errors that are cause our scripts to fail.

Right now I've successfully built the `whl` file once, but I'm not sure if I used the right flags.
```


### Standalone code to reproduce the issue

```shell
`pip install tensorflow` results in errors stating that our CPU doesn't support sse4.2, sse4.1, etc.
```


### Relevant log output

_No response_</details>"
58943,Clog Copy-Paste Error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

Any

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In [workspace2.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace2.bzl#L162) on line 162 clog was copy and pasted from cpu-info (below it) so both import the same archive and clog is not included.

```bash
    tf_http_archive(
        name = ""clog"",
        strip_prefix = ""cpuinfo-5e63739504f0f8e18e941bd63b2d6d42536c7d90"",
        sha256 = ""18eca9bc8d9c4ce5496d0d2be9f456d55cbbb5f0639a551ce9c8bac2e84d85fe"",
        urls = tf_mirror_urls(""https://github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz""),
    )

    tf_http_archive(
        name = ""cpuinfo"",
        strip_prefix = ""cpuinfo-5e63739504f0f8e18e941bd63b2d6d42536c7d90"",
        sha256 = ""18eca9bc8d9c4ce5496d0d2be9f456d55cbbb5f0639a551ce9c8bac2e84d85fe"",
        urls = tf_mirror_urls(""https://github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz""),
    )
```


### Standalone code to reproduce the issue

```shell
View master branch
```


### Relevant log output

_No response_</details>"
58941,tf.custom_gradient with multiple input and output,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.9.2
-   **Python version**: 3.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
I have a function with 4 inputs (x1, x2, x3, x4) and 2 outputs (y1, y2) using Tensorflow. I would like to specify the gradients, since I perform some non-autodiff operations inside the function.

I need to specify the derivatives of the outputs with respect to the inputs. We can see these derivatives as a Jacobian of size (2,4). Regarding this, I have 8 derivatives: dy1_dx1, dy1_dx2, dy1_dx3, dy1_dx4, dy2_dx1, dy2_dx2, dy2_dx3 and dy2_dx4.

However, the grad function used in this tf.custom.gradient needs to have the same length as the inputs, this is 4. So, I do not know how Tensorflow handles with the introduction of the 8 derivatives using just 4 elements. I tried to include them as lists, but it gives the error. Here is a general code to reproduce the error:

```
import tensorflow as tf
@tf.custom_gradient
def bar(x1, x2, x3, x4):
  def grad(dy1, dy2):
    dy1_dx1 = x2**2 * x3**3 * x4**4             #360000
    dy1_dx2 = x1 * 2*x2 * x3**3 * x4**4         #480000
    dy1_dx3 = x1 * x2**2 * 3*x3**2 * x4**4      #540000
    dy1_dx4 = x1 * x2**2 * x3**3 * 4*x4**3      #576000
    dy2_dx1 = x2**2 + x3**3 + x4**4             #698
    dy2_dx2 = x1 + 2*x2 + x3**3 + x4**4         #697
    dy2_dx3 = x1 + x2**2 + 3*x3**2 + x4**4      #684
    dy2_dx4 = x1 + x2**2 + x3**3 + 4*x4**3      #575
    return [dy1_dx1, dy2_dx1], [dy1_dx2, dy2_dx2], [dy1_dx3, dy2_dx3], [dy1_dx4, dy2_dx4]

  y1 = x1 * x2**2 * x3**3 * x4**4
  y2 = x1 + x2**2 + x3**3 + x4**4
  return [y1, y2], grad

x1 = tf.constant(2.0, dtype=tf.float32)
x2 = tf.constant(3.0, dtype=tf.float32)
x3 = tf.constant(4.0, dtype=tf.float32)
x4 = tf.constant(5.0, dtype=tf.float32)
with tf.GradientTape(persistent=True) as tape:
  tape.watch(x1)
  tape.watch(x2)
  tape.watch(x3)
  tape.watch(x4)
  z = bar(x1, x2, x3, x4)

print(tape.gradient(z, x1))             #[dy1_dx1, dy2_dx1]
print(tape.gradient(z, x2))             #[dy1_dx2, dy2_dx2]
print(tape.gradient(z, x3))             #[dy1_dx3, dy2_dx3]
print(tape.gradient(z, x4))             #[dy1_dx4, dy2_dx4]

```

The  error says: ""custom_gradient function expected to return 4 gradients, but returned 8 instead"".

I expect someway to specify the correspondent 8 derivatives. Thank you in advance!

"
58940,autocast=False not respected when saving/loading Keras model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In the below code, the intention is to create a model that takes an input of type `float32` when saved, even if it is trained with `mixed_precision`. In this example, training code is omitted, and `call()` is just a placeholder for conciseness, but it still shows the issue.

The problem comes when loading the model. Because it has `autocast=False` I would expect that the model is saved with an input type of `tf.float32`. But it seems that the model is saved in an invalid state, because it loads with an error.

The error is the same even if you call `tf.keras.mixed_precision.set_global_policy(""float32"")` before loading the model.
```


### Standalone code to reproduce the issue

```shell
import keras
import tensorflow as tf

class MyModel(keras.Model):
    def __init__(self):
        super().__init__(autocast=False)

    def call(self, inputs):
        return 0

def main():
    tf.keras.mixed_precision.set_global_policy(""mixed_float16"")

    model = MyModel()

    inputs = tf.zeros(shape=(1, 1), dtype=tf.float32)

    prediction_0 = model(inputs)

    model_path = ""test_model""

    model.save(model_path)

    loaded_model = keras.models.load_model(model_path)

    prediction_1 = loaded_model(inputs)

if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
ValueError: Exception encountered when calling layer ""my_model"" (type MyModel).

Could not find matching concrete function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * <tf.Tensor 'inputs:0' shape=(1, 1) dtype=float16>
  Keyword arguments: {}

 Expected these arguments to match one of the following 2 option(s):

Option 1:
  Positional arguments (1 total):
    * TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')
  Keyword arguments: {}

Option 2:
  Positional arguments (1 total):
    * TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_1')
  Keyword arguments: {}

Call arguments received by layer ""my_model"" (type MyModel):
  • args=('tf.Tensor(shape=(1, 1), dtype=float16)',)
  • kwargs=<class 'inspect._empty'>

Process finished with exit code 1
```
</details>"
58939,Dependencies installation failure in rasa due to Tensor flow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.3.4

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04.6 LTS Kernel: Linux 5.4.0-1090

### Mobile device

_No response_

### Python version

3.7.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
During a fresh install of Tensor flow :

ERROR: Could not find a version that satisfies the requirement tensor flow-text<2.4,>=2.3; sys_platform != ""win32"" (from rasa) (from versions: none) ERROR: No matching distribution found for tensorflow-text<2.4,>=2.3; sys_platform != ""win32""

It does not work even after upgrading the tensor flow to 2.9.0
```


### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensor flow-text<2.4,>=2.3; sys_platform != ""win32"" (from rasa) (from versions: none) ERROR: No matching distribution found for tensorflow-text<2.4,>=2.3; sys_platform != ""win32""
```


### Relevant log output

_No response_</details>"
58938,tf.distribute.MultiWorkerMirroredStrategy with custom model.train_step,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **TensorFlow version (use command below)**: 2.6
-   **Python version**: 3.8

### Describe the problem

I'd like to use tf.distribute.MultiWorkerMirroredStrategy with a custom model.train_step. How should the loss be calculated if using a custom loss? As an example, let's say my custom loss is tf.keras.losses.SparseCategoricalCrossentropy. Should the `reduction` be set to tf.keras.losses.Reduction.NONE (like below)? Or do I need to set the reduction to tf.keras.losses.Reduction.SUM and divide by the global batch size?

This problem is kind of in between keras.fit and writing custom distributed training loop. So not really sure how tensorflow will aggregate the gradients when calling model.fit. Just asking here because there doesn't seem to be documentation on this.
Attached a reproducible example below

### Source code / logs
```
import os
import tensorflow as tf
import numpy as np
import json

def mnist_dataset():
    '''
    Load mnist dataset. No batching
    '''
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
    x_train = x_train / np.float32(255)
    y_train = y_train.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000)
    return train_dataset

class CustomModel(tf.keras.Model):
    def __init__(self, model):
        super(CustomModel, self).__init__()
        self.model = model
        self.loss_tracker= tf.keras.metrics.Mean(name='loss')
        
    def train_step(self, data):
        x, y = data
        with tf.GradientTape() as tape:
            # Forward pass
            y_pred = self.model(x, training=True)
            # Compute our own loss. Shape (batch_size,)
            loss = tf.keras.losses.SparseCategoricalCrossentropy(
                from_logits = True, reduction=tf.keras.losses.Reduction.NONE)(y, y_pred)
           
        # Compute gradients
        trainable_vars = self.model.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics
        self.loss_tracker.update_state(loss)
        self.compiled_metrics.update_state(y, y_pred)
        metrics = {m.name: m.result() for m in self.metrics}
        return metrics

def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(28, 28)),
        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
        tf.keras.layers.Conv2D(32, 3, activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10)])
    model = CustomModel(model = model)
    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001))
    return model

# TRAIN
per_worker_batch_size = 32
num_workers = len(tf_config['cluster']['worker'])
global_batch_size = per_worker_batch_size * num_workers
multi_worker_dataset = mnist_dataset().batch(global_batch_size)

with strategy.scope():
  # Model building/compiling need to be within `strategy.scope()`.
  multi_worker_model = build_and_compile_cnn_model()

multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
```
"
58937,AutoCast variable inside tf.function does not cast correctly when saving a model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Error when calling model.save()

You can work around the error by commenting out `tf.function` over `call()`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
import keras.layers

class CosineSimilarityLayer(keras.layers.Layer):
    def __init__(
        self, num_classes: int, name: str = None
    ):
        super().__init__(name=name)
        self.num_classes = num_classes
        self._weights = None

    def build(self, input_shape):
        self._weights = self.add_weight(
            name=""W"",
            shape=(
                input_shape[-1],
                self.num_classes,
            ),
            initializer=""glorot_normal"",
            trainable=True,
            dtype=self.dtype
        )
        super().build(input_shape)

    def compute_output_shape(self):
        return None, self.num_classes

    # If you comment out tf.function here, it works.
    @tf.function
    def call(self, inputs: tf.Tensor):
        embedding = inputs
        # normalize feature
        embedding_normalized = tf.nn.l2_normalize(embedding, axis=1)
        # get centroids
        weights_normalized = tf.nn.l2_normalize(self._weights, axis=1, )

        logits = embedding_normalized @ weights_normalized

        return logits

    def get_config(self):
        config = super().get_config().copy()
        config.update(
            {
                ""num_classes"": self.num_classes,
            }
        )
        return config

def main():
    tf.keras.mixed_precision.set_global_policy(""mixed_float16"")

    layer = CosineSimilarityLayer(num_classes=100)

    input = tf.zeros(shape=(10, 100), dtype=tf.float16)

    model = keras.models.Sequential([
        layer
    ])

    model(input)

    model.save(""autocast_issue"")

if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
File ""*\autocast_issue.py"", line 37, in call  *
        logits = embedding_normalized @ weights_normalized

    TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type float16 of argument 'a'.
```
</details>"
58935,Tensroflow recognizes only one GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf.2.5.0

### Custom Code

Yes

### OS Platform and Distribution

9.4.0

### Mobile device

9.4.0

### Python version

3.6.13

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

CUDA 11.2 / cuDNN 8.1

### GPU model and memory

NVIDIA RTX A5000

### Current Behaviour?

```shell
I am using No.0~ No.9 GPU NVIDIA RTX A5000.

But process recognizes only No.1 GPU.

cuda and cuDNN versions correct.

Error message not occur.

This is all the code related to gpu.

len(tf.config.list_physical_devices) is print out.
Num GPUs Available : 1
```


### Standalone code to reproduce the issue

```shell
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""


os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' 

from tensorflow.python.client import device_lib
#from tensorflow.contrib import rnn
import tensorflow_addons as tfa


import tensorflow_addons.rnn as rnn
logger.debug(""Num GPUs Available: "" + str(len(tf.config.list_physical_devices('GPU'))))

#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
#os.environ['CUDA_VISIBLE_DEVICES']='1'
# os.environ[""TF_CUDA_HOST_MEM_LIMIT_IN_MB""]=""10000""

logger.debug(device_lib.list_local_devices()) 
logger.debug(""##################################################################\n\n"")
```


### Relevant log output

```shell
#ifndef CUDNN_VERSION_H_
#define CUDNN_VERSION_H_

#define CUDNN_MAJOR 8
#define CUDNN_MINOR 1
#define CUDNN_PATCHLEVEL 0

#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

#endif /* CUDNN_VERSION_H */


nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:08:53_PST_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0
```
</details>

2022-12-20 10:48:12.332276: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
>>> device_lib.list_local_devices()
2022-12-20 10:48:39.803760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-20 10:48:39.811186: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-12-20 10:48:41.759404: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable peer access from 0x563425ab7f10 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.764495: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable peer access from 0x56342615d440 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.768745: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable peer access from 0x5634267434d0 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.772315: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable peer access from 0x563426d2d910 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.775159: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable peer access from 0x56342733cf70 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.777232: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable peer access from 0x563427939740 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.778723: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable peer access from 0x563427f49160 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.779552: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable peer access from 0x563428557840 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.779691: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable peer access from 0x563428b70b10 to 0x563429191b60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.779817: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563425ab7f10: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.779941: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable peer access from 0x563429191b60 to 0x56342615d440: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780062: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable peer access from 0x563429191b60 to 0x5634267434d0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780181: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563426d2d910: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780308: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable peer access from 0x563429191b60 to 0x56342733cf70: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780432: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563427939740: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780558: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563427f49160: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780682: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563428557840: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.780804: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1690] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable peer access from 0x563429191b60 to 0x563428b70b10: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2022-12-20 10:48:41.781852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:3d:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.782709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:3e:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.783514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: 
pciBusID: 0000:3f:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.784338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: 
pciBusID: 0000:40:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.785184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 4 with properties: 
pciBusID: 0000:41:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.786002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 5 with properties: 
pciBusID: 0000:60:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.786799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 6 with properties: 
pciBusID: 0000:61:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.787602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 7 with properties: 
pciBusID: 0000:62:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.788398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 8 with properties: 
pciBusID: 0000:63:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.789196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 9 with properties: 
pciBusID: 0000:64:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 715.34GiB/s
2022-12-20 10:48:41.789235: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-12-20 10:48:41.792605: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-12-20 10:48:41.792692: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-12-20 10:48:41.793788: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-12-20 10:48:41.794064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-12-20 10:48:41.797168: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-12-20 10:48:41.797911: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-12-20 10:48:41.798070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-12-20 10:48:41.813923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9
2022-12-20 10:48:41.813972: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-12-20 10:48:46.260209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-12-20 10:48:46.260262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 2 3 4 5 6 7 8 9 
2022-12-20 10:48:46.260276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y Y Y Y Y Y Y Y Y 
2022-12-20 10:48:46.260285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N Y Y Y Y Y Y Y Y 
2022-12-20 10:48:46.260292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 2:   Y Y N Y Y Y Y Y Y Y 
2022-12-20 10:48:46.260300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 3:   Y Y Y N Y Y Y Y Y Y 
2022-12-20 10:48:46.260307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 4:   Y Y Y Y N Y Y Y Y Y 
2022-12-20 10:48:46.260315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 5:   Y Y Y Y Y N Y Y Y Y 
2022-12-20 10:48:46.260323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 6:   Y Y Y Y Y Y N Y Y Y 
2022-12-20 10:48:46.260333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 7:   Y Y Y Y Y Y Y N Y Y 
2022-12-20 10:48:46.260340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 8:   Y Y Y Y Y Y Y Y N Y 
2022-12-20 10:48:46.260349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 9:   Y Y Y Y Y Y Y Y Y N 
2022-12-20 10:48:46.277191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 22352 MB memory) -> physical GPU (device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:3d:00.0, compute capability: 8.6)
2022-12-20 10:48:46.278851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:1 with 21245 MB memory) -> physical GPU (device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:3e:00.0, compute capability: 8.6)
2022-12-20 10:48:46.280300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:2 with 22352 MB memory) -> physical GPU (device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:3f:00.0, compute capability: 8.6)
2022-12-20 10:48:46.281761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:3 with 22352 MB memory) -> physical GPU (device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:40:00.0, compute capability: 8.6)
2022-12-20 10:48:46.283201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:4 with 22352 MB memory) -> physical GPU (device: 4, name: NVIDIA RTX A5000, pci bus id: 0000:41:00.0, compute capability: 8.6)
2022-12-20 10:48:46.284731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:5 with 22352 MB memory) -> physical GPU (device: 5, name: NVIDIA RTX A5000, pci bus id: 0000:60:00.0, compute capability: 8.6)
2022-12-20 10:48:46.286222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:6 with 22352 MB memory) -> physical GPU (device: 6, name: NVIDIA RTX A5000, pci bus id: 0000:61:00.0, compute capability: 8.6)
2022-12-20 10:48:46.287608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:7 with 22352 MB memory) -> physical GPU (device: 7, name: NVIDIA RTX A5000, pci bus id: 0000:62:00.0, compute capability: 8.6)
2022-12-20 10:48:46.289058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:8 with 22352 MB memory) -> physical GPU (device: 8, name: NVIDIA RTX A5000, pci bus id: 0000:63:00.0, compute capability: 8.6)
2022-12-20 10:48:46.290525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:9 with 22352 MB memory) -> physical GPU (device: 9, name: NVIDIA RTX A5000, pci bus id: 0000:64:00.0, compute capability: 8.6)
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 18013241447675411421
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 4999206545337463836
physical_device_desc: ""device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:3d:00.0, compute capability: 8.6""
, name: ""/device:GPU:1""
device_type: ""GPU""
memory_limit: 22277914624
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 5426967292628159816
physical_device_desc: ""device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:3e:00.0, compute capability: 8.6""
, name: ""/device:GPU:2""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 3104994750831215783
physical_device_desc: ""device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:3f:00.0, compute capability: 8.6""
, name: ""/device:GPU:3""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 14175974160922571756
physical_device_desc: ""device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:40:00.0, compute capability: 8.6""
, name: ""/device:GPU:4""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 17888261416412142435
physical_device_desc: ""device: 4, name: NVIDIA RTX A5000, pci bus id: 0000:41:00.0, compute capability: 8.6""
, name: ""/device:GPU:5""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 16596062142311775136
physical_device_desc: ""device: 5, name: NVIDIA RTX A5000, pci bus id: 0000:60:00.0, compute capability: 8.6""
, name: ""/device:GPU:6""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 1597885570592683115
physical_device_desc: ""device: 6, name: NVIDIA RTX A5000, pci bus id: 0000:61:00.0, compute capability: 8.6""
, name: ""/device:GPU:7""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 3391246708896791881
physical_device_desc: ""device: 7, name: NVIDIA RTX A5000, pci bus id: 0000:62:00.0, compute capability: 8.6""
, name: ""/device:GPU:8""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 9
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 18050846488731650593
physical_device_desc: ""device: 8, name: NVIDIA RTX A5000, pci bus id: 0000:63:00.0, compute capability: 8.6""
, name: ""/device:GPU:9""
device_type: ""GPU""
memory_limit: 23438688256
locality {
  bus_id: 1
  links {
    link {
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 1
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 2
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 3
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 4
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 5
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 6
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 7
      type: ""StreamExecutor""
      strength: 1
    }
    link {
      device_id: 8
      type: ""StreamExecutor""
      strength: 1
    }
  }
}
incarnation: 1009118939769110196
physical_device_desc: ""device: 9, name: NVIDIA RTX A5000, pci bus id: 0000:64:00.0, compute capability: 8.6""
]
"
58934,Unable to jit_compile and run function on CPU on tpu-vm,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.11.0-0-gd5b57ca9 2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux t1v-n-92ea8b2a-w-0 5.15.0-1022-gcp #29~20.04.1-Ubuntu SMP Sat Oct 29 18:17:56 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

On a `v3-8` TPU-VM running tensorflow version: `tpu-vm-tf-2.11.0` I am unable to `jit_compile` a basic function and run it on CPU. Can you please guide me how to run jit_compiled functions on TPU VM's CPU.


### Standalone code to reproduce the issue

```shell
import os

os.environ[""TPU_NAME""] = ""local""
os.environ[""TPU_LOAD_LIBRARY""] = ""1""

import tensorflow as tf

tf.debugging.set_log_device_placement(True)

print(""All devices: "", tf.config.list_logical_devices())

a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])
b = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])


@tf.function(jit_compile=True)
def jit_test(a, b):
    c = tf.matmul(a, b)
    return a + b + c


with tf.device("":/TPU:0""):
    print(jit_test(a, b))
    print(""Success!"")


with tf.device("":/CPU:0""):
    print(jit_test(a, b))  # This will fail
    print(""Will crash prior to getting here"")
```


### Relevant log output

```shell
2022-12-19 10:13:18.533185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-19 10:13:18.717772: I tensorflow/core/tpu/tpu_initializer_helper.cc:275] Libtpu path is: libtpu.so
D1219 10:13:18.874059745   34863 config.cc:113]              gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)
D1219 10:13:18.874080489   34863 config.cc:113]              gRPC EXPERIMENT tcp_read_chunks                     OFF (default:OFF)
D1219 10:13:18.874093652   34863 config.cc:113]              gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)
D1219 10:13:18.874100741   34863 config.cc:113]              gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)
D1219 10:13:18.874107419   34863 config.cc:113]              gRPC EXPERIMENT flow_control_fixes                  OFF (default:OFF)
D1219 10:13:18.874114099   34863 config.cc:113]              gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)
D1219 10:13:18.874121059   34863 config.cc:113]              gRPC EXPERIMENT periodic_resource_quota_reclamation ON  (default:ON)
D1219 10:13:18.874127645   34863 config.cc:113]              gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)
D1219 10:13:18.874134219   34863 config.cc:113]              gRPC EXPERIMENT new_hpack_huffman_decoder           OFF (default:OFF)
D1219 10:13:18.874140862   34863 config.cc:113]              gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)
D1219 10:13:18.874147728   34863 config.cc:113]              gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)
D1219 10:13:18.874154168   34863 config.cc:113]              gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)
I1219 10:13:18.874398506   34863 ev_epoll1_linux.cc:121]     grpc epoll fd: 6
D1219 10:13:18.874414089   34863 ev_posix.cc:141]            Using polling engine: epoll1
D1219 10:13:18.874434253   34863 dns_resolver_ares.cc:824]   Using ares dns resolver
D1219 10:13:18.874733217   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""priority_experimental""
D1219 10:13:18.874748219   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""outlier_detection_experimental""
D1219 10:13:18.874756312   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""weighted_target_experimental""
D1219 10:13:18.874763493   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""pick_first""
D1219 10:13:18.874770671   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""round_robin""
D1219 10:13:18.874783165   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""ring_hash_experimental""
D1219 10:13:18.874810477   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""grpclb""
D1219 10:13:18.874843143   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""rls_experimental""
D1219 10:13:18.874864810   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""xds_cluster_manager_experimental""
D1219 10:13:18.874872835   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""xds_cluster_impl_experimental""
D1219 10:13:18.874880753   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""cds_experimental""
D1219 10:13:18.874888414   34863 lb_policy_registry.cc:45]   registering LB policy factory for ""xds_cluster_resolver_experimental""
D1219 10:13:18.874895665   34863 certificate_provider_registry.cc:35] registering certificate provider factory for ""file_watcher""
I1219 10:13:18.895383666   34863 socket_utils_common_posix.cc:336] TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter
2022-12-19 10:13:18.913051: I tensorflow/core/tpu/tpu_initializer_helper.cc:225] GetTpuPjrtApi not found
2022-12-19 10:13:21.766915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-19 10:13:26.260445: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x63c6900 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:
2022-12-19 10:13:26.260485: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): TPU, 2a886c8
2022-12-19 10:13:26.260499: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): TPU, 2a886c8
2022-12-19 10:13:26.260511: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): TPU, 2a886c8
2022-12-19 10:13:26.260524: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): TPU, 2a886c8
2022-12-19 10:13:26.260536: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (4): TPU, 2a886c8
2022-12-19 10:13:26.260549: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (5): TPU, 2a886c8
2022-12-19 10:13:26.260561: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (6): TPU, 2a886c8
2022-12-19 10:13:26.260573: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (7): TPU, 2a886c8
All devices:  [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'), LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]
input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.286675: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.286733: I tensorflow/core/common_runtime/placer.cc:114] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:CPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.286753: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.287893: I tensorflow/core/common_runtime/eager/execute.cc:1445] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.288240: I tensorflow/core/common_runtime/eager/execute.cc:1445] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.361102: I tensorflow/core/common_runtime/eager/execute.cc:1445] Executing op __inference_jit_test_11 in device /job:localhost/replica:0/task:0/device:TPU:0
2022-12-19 10:13:26.473898: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
tf.Tensor(
[[ 32.  40.  48.]
 [ 74.  91. 108.]
 [116. 142. 168.]], shape=(3, 3), dtype=float32)
Success!
2022-12-19 10:13:26.477187: I tensorflow/core/common_runtime/eager/execute.cc:1445] Executing op __inference_jit_test_11 in device /job:localhost/replica:0/task:0/device:CPU:0
2022-12-19 10:13:26.478142: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:417 : NOT_FOUND: could not find registered transfer manager for platform Host -- check target linkage
Traceback (most recent call last):
  File ""notebooks/tpu_vm_test.py"", line 28, in <module>
    print(jit_test(a, b))  # This will fail
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError: could not find registered transfer manager for platform Host -- check target linkage [Op:__inference_jit_test_11]
D1219 10:13:26.848936447   34863 init.cc:190]                grpc_shutdown starts clean-up now
```
</details>"
58933,Bad download link for Windows GPU only binaries zip file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

x86_64-2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows x86 GPU only

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip

The zip file does not exist.  ""No such object""
```


### Standalone code to reproduce the issue

```shell
""No such object""  when clicking on the download link
```


### Relevant log output

_No response_</details>"
58927,tensorflow not found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

ewfewfew

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!dsfdsf
```


### Standalone code to reproduce the issue

```shell
sdferfgerg
```


### Relevant log output

_No response_</details>"
58926,Wrong layer name in exception message,"### Issue Type

Bug

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Python version

3.10.9

### Current Behaviour?

```shell
I have an error message that said:
`tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Conv2D as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Conv2D]`

But the error is on a keras.layers.Conv1D not on Conv2D.
I know how to solve this exception, but the message displayed is not correct.
```


### Standalone code to reproduce the issue

```py
import tensorflow as tf
import keras

model = tf.keras.models.Sequential([
    keras.layers.Input((5, 2)),
    keras.layers.Conv1D(filters=2, kernel_size=3)
])

model.layers[0].call(tf.ones(shape=(1, 3, 2), dtype=tf.int32))
```"
58925,Reproducible dataset order after shuffle from checkpoint,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

macOS Monterey 12.6, Debian GNU/Linux bookworm/sid

### Mobile device

_No response_

### Python version

3.8.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4 (Debian)

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti (Debian)

### Current Behaviour?

I would like to get reproducible results for datasets in which the data is shuffled. In PyTorch I could save rng and load rng to get the same shuffling order, but in Tensorflow I don't know how to do that. I would like to get results like this from a shuffled dataset. Additionally, I would also want to know how to do it on `Keras` datasets which are used in `model.fit()`

```shell
Iterate over dataset
  first batch of data [1,2,3,4,5]
save state of dataset

Iterate over dataset
  first batch of data [3,4,1,4,5]

load state of dataset
Iterate over dataset
  first batch of data [1,2,3,4,5]
```


### Standalone code to reproduce the issue

```python
# main imports
import random
import numpy as np
import tensorflow as tf

# set the number of threads for tensorflow explained later
tf.config.threading.set_intra_op_parallelism_threads(1)

def main():
    seed = 1
    random.seed(seed)
    tf.random.set_seed(seed)
    
    tf_dataset_train = (
        tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 3)))
        .cache()
        .shuffle(4)
        .batch(2)
        .prefetch(tf.data.AUTOTUNE)
    )
    for i, batch in enumerate(tf_dataset_train.as_numpy_iterator()):
        if i == 0:
            print(f""First batch {batch}"")
    
    # Save dataset shuffle state
    print(""Next loading of the dataset"")
    for i, batch in enumerate(tf_dataset_train.as_numpy_iterator()):
        if i == 0:
            print(f""First batch {batch}"")
    
    # Load dataset shuffle state
    print(""Get dataset order from first run"")
    for i, batch in enumerate(tf_dataset_train.as_numpy_iterator()):
        if i == 0:
            print(f""First batch {batch}"")


if __name__ == ""__main__"":
    main()
```


### Relevant log output

_No response_</details>"
58910,TF 2.11.0 not working with CUDA 12.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

RTX 2060

### Current Behaviour?

```shell
Cannot load the CUDA libraries `libcufft.so.10` or `libcusparse.so.11`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.constant(10)
```


### Relevant log output

```shell
2022-12-15 18:09:05.476656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-15 18:09:06.036558: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-12-15 18:09:06.036605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-12-15 18:09:06.036612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-12-15 18:09:07.624464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-15 18:09:07.637969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-12-15 18:09:07.651046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-12-15 18:09:07.651192: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-12-15 18:09:07.654075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
```
</details>"
58908,"Error following installation, help requested","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.11.0-cp310

### Custom Code

No

### OS Platform and Distribution

Linux Kubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Cuda framework 12.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I installed tensorflow into a new python virtual environment using the following commands:

mkdir tensorflow_files
cd tensorflow_files
python3 -m venv virtualenv
source virtualenv/bin/activate
pip install --upgrade TensorFlow
```
Having done so, tensorflow doesn't work, to the extent of not even being able to import it, see error messages below:

```
(virtualenv) adrian@arcturus:~/tensorflow_files$ python3
Python 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-12-15 23:35:07.846430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
```


### Standalone code to reproduce the issue

```shell
See installation description above.
```


### Relevant log output

```shell
(virtualenv) adrian@arcturus:~/tensorflow_files$ python3
Python 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2022-12-15 23:35:07.846430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/adrian/tensorflow_files/virtualenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
</details>"
58907,"while tflite model has Dequantize, gpu deledate performance is slower on android for  2.10 version","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.7 and 2.10

### Custom Code

Yes

### OS Platform and Distribution

android

### Mobile device

android

### Python version

3.7

### Bazel version

no

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current Behaviour?

```shell
A bug happened!
Here compare performance between 2.7 version  and 2.10 version
the 2.7 version date is 2021/11/10.
tflite model has dequantize operator （for weight and bias ,input is  f16 ,output is fp32）.
Gpu delegate (opencl)performace on 2.10 is slower than 2.7 ,the diffrence is a little big
my model is 128 ms in 2.7 ,but   78ms in 2.10.
I want to know the reason,thanks
```


### Standalone code to reproduce the issue

```shell
If model has dequantize operator,Gpu delegate performace is slower in new version for example 2.10.
```


### Relevant log output

_No response_</details>"
58906,CollectiveOpV3Kernel did not support group assignments,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.10

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
CollectiveOpV3Kernel did not support group assignments, I need a all2all OP which could dispatch tensors with different shape like Horovod.
```


### Standalone code to reproduce the issue

```shell
import os

import tensorflow as tf
from tensorflow.python.ops import collective_ops

tf.config.experimental.set_memory_growth=True

group_size = 2
group_key = 104
device = 'GPU'
communication = 'auto'

dev0 = '/device:%s:0' % device
dev1 = '/device:%s:1' % device

@tf.function
def run_all_to_all_2devices():
  collectives = []
  in0 = tf.convert_to_tensor([1,1,1,1,3,3])
  in1 = tf.convert_to_tensor([2,2,2,4])
  with tf.device(dev0):
    group_handle0 = collective_ops.initialize_communicator(
        group_key=group_key,
        rank=0,
        group_size=group_size,
        communication_hint=communication)
    collectives.append(
        collective_ops.all_to_all_v3(group_handle0, in0, [0,4]))
  with tf.device(dev1):
    group_handle1 = collective_ops.initialize_communicator(
        group_key=group_key,
        rank=1,
        group_size=group_size,
        communication_hint=communication)
    collectives.append(
        collective_ops.all_to_all_v3(group_handle1, in1, [0,3]))
  return collectives

result = run_all_to_all_2devices()
print(result[0])
print(result[1])
```


### Relevant log output

```shell
UnimplementedError                        Traceback (most recent call last)
Cell In[6], line 32
     28     collectives.append(
     29         collective_ops.all_to_all_v3(group_handle1, in1, [0,3]))
     30   return collectives
---> 32 result = run_all_to_all_2devices()
     33 print(result[0])
     34 print(result[1])

File ~/.conda/envs/hj/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/.conda/envs/hj/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:
...
	 [[CollectiveAllToAllV3_1/_11]]
  (1) UNIMPLEMENTED:  Group assignments are not supported yet.
	 [[{{node CollectiveAllToAllV3_1}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_run_all_to_all_2devices_39]
```
</details>"
58901,C API installation instructions don't work for MacOS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The [C API installation guide](https://www.tensorflow.org/install/lang_c#linker) says

> On Linux/macOS, if you extract the TensorFlow C library to a system directory, such as `/usr/local`, configure the linker with `ldconfig`

but this command doesn't seem to exist on mac. Is there an alternative or can this step be skipped?
```


### Standalone code to reproduce the issue

```shell
Look at docs https://www.tensorflow.org/install/lang_c#linker
```


### Relevant log output

_No response_</details>"
58900,"Build TF 2.11.0 with use ROCM 5.4.0 with GCC 12.2.0 failed, but succesfull build with use ROCM 5.4.0 with GCC 11.3.0","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Gentoo

### Mobile device

_No response_

### Python version

3.11.1

### Bazel version

5.3.2

### GCC/Compiler version

12.2.2

### CUDA/cuDNN version

12.0.0/8.7.0.84

### GPU model and memory

AMD Vega Frontier 16Gb

### Current Behaviour?

```shell
Building TF 2.11.0 with use ROCM 5.4.0 with GCC 12.2.0 failed, but succesfull built with use ROCM 5.4.0 with GCC 11.3.0
```


### Standalone code to reproduce the issue

```shell
[1A[K[31m[1mERROR: [0m/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/work/tensorflow-2.11.0-python3_11/tensorflow/BUILD:1137:20: Linking tensorflow/libtensorflow.so.2.11.0 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/work/tensorflow-2.11.0-python3_11-bazel-base/execroot/org_tensorflow && \
  exec env - \
    GLIBCXX_USE_CXX11_ABI=0 \
    HOME=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/homedir \
    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/.keras \
    MLIR_CRASH_REPRODUCER_DIRECTORY=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/.crash \
    PATH=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/python3.11/bin:/usr/lib/portage/python3.11/ebuild-helpers/xattr:/usr/lib/portage/python3.11/ebuild-helpers:/usr/sbin:/sbin:/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/bin:/usr/lib/llvm/15/bin:/usr/lib64/subversion/bin:/opt/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.11 \
    PYTHON_LIB_PATH=/usr/lib/python3.11/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/usr/lib64/rocblas/library \
    ROCM_PATH=/usr \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=absl_py,astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,curl,cython,dill_archive,double_conversion,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,libjpeg_turbo,lmdb,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \
  external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow.so.2.11.0-2.params)
# Configuration: 2f0fac33d35f1b8e1706c32b05ccaf2da62c75cfc452cc037134c3e368e32f12
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
collect2: error: ld returned 1 exit status
```


### Relevant log output
[log](https://gist.github.com/raw/0482ac8125d559790af0c1a0a8ed8ff5)
</details>"
58891,API installation instructions don't work for MacOS,"The [C API installation guide](https://www.tensorflow.org/install/lang_c#linker) says

> On Linux/macOS, if you extract the TensorFlow C library to a system directory, such as `/usr/local`, configure the linker with `ldconfig`

but this command doesn't seem to exist on mac. Is there an alternative or can this step be skipped?"
58889,E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

ubunto 22.04 LTS

### Mobile device

_No response_

### Python version

Python 3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.7.0.84

### GPU model and memory

A4000 16G

### Current Behaviour?

```shell
I expected two following commands to not fail.
I'm not sure that the package I build from source is valid now. It would be great if I could fix this by rebulding the tensorflow ...


sdbot@sdbot:~/ttt$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
2022-12-14 17:02:31.842114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-14 17:02:31.955856: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'
sdbot@sdbot:~/ttt$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
2022-12-14 17:02:41.867903: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-14 17:02:41.981995: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
v1.12.1-86312-g4e76e1e1330 2.12.0
```


### Standalone code to reproduce the issue

```shell
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```


### Relevant log output

```shell
2022-12-14 17:02:41.981995: E tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay
v1.12.1-86312-g4e76e1e1330 2.12.0
```
</details>"
58888,Training with Null Data in TFLITE Model Maker,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1 / tflite 0.4.2

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When training with tflite model maker, I am wondering if this the training algorithm uses null data samples (i.e. images with no annotations) for training or if this data is simply ignored?
```


### Standalone code to reproduce the issue

```shell
Na
```


### Relevant log output

_No response_</details>"
58886,Variables across different runners in TFLite and SavedModel,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: v2.11.0-rc2-17-gd5b57ca93e5
-   **Python version**: 3.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: Not used
-   **GPU model and memory**: Not used
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am unsure if this is a bug or an expected behavior. I am trying to share `tf.Variable`s between signature runners in TFLite but it does not seem to work. Executing eagerly or with a SavedModel, things work as expected, but the TFLite export somehow flattens/prunes the graph and does not share the variables globally between the signature runners.

### Source code / logs

I prepared a minimal example here:

https://colab.research.google.com/drive/1DTau1na7YfzCWtRk8RC5KHVzZaNp7Tsx?usp=sharing

With TFLite my target is more C++ but the problem appears in Python too, and it was easier to share and test with Python.
"
58884,TfLite build using CMake fails in official Docker container,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.12.0?

### Custom Code

No

### OS Platform and Distribution

Arch Linux 6.0.12-arch1-1

### Mobile device

Pixel 3a

### Python version

N/A

### Bazel version

cmake 3.16.3

### GCC/Compiler version

NDK r25b

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am using `tensorflow/tensorflow:devel` as a base image (see Dockerfile contents below), which seems to ship with tensorflow 2.12.0 (latest version listed in RELEASE.md).

I am following the [official cmake build instructions](https://www.tensorflow.org/lite/guide/build_cmake) for cross-compilation for Android.
During the build step (`cmake --build . -j`), an error occurs (see logs below).

I have also tried this with the the [latest 2.11.0 release](https://github.com/tensorflow/tensorflow/releases/tag/v2.11.0), with the same result.

Using older versions of the NDK causes different errors, about which I can post issues if needed.

If possible, I would like working instructions to build TfLite with CMake (I have unsuccessfully tried the Bazel build as well).
```


### Standalone code to reproduce the issue

Dockerfile:
```dockerfile
FROM tensorflow/tensorflow:devel

RUN apt-get update &&\
    DEBIAN_FRONTEND=noninteractive apt-get install -y cmake
RUN mkdir /downloads
RUN cd /downloads &&\
    curl -s https://dl.google.com/android/repository/android-ndk-r25b-linux.zip \
        --output android-ndk.zip
RUN cd /downloads && unzip android-ndk.zip && mv android-ndk-r25b
```

Build with
```
docker build . -t tflite-build-test
```

Run with
```
docker run -it --rm tflite-build-test bash
```

Inside container
```
mkdir tensorflow_src/build
cd tensorflow_src/build
cmake -DCMAKE_TOOLCHAIN_FILE=../../android-ndk-r25b/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a ../tensorflow/lite
cmake --build . -j
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""scripts/generate_code.py"", line 148, in <module>
    flatc(
  File ""scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 493, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.8/subprocess.py"", line 858, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/usr/lib/python3.8/subprocess.py"", line 1704, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/tensorflow_src/build/_deps/flatbuffers-build/flatc'
Scanning dependencies of target absl_raw_hash_set
[ 19%] Built target absl_random_seed_sequences
Scanning dependencies of target absl_flags_config
Scanning dependencies of target absl_cordz_info
make[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:521: _deps/flatbuffers-build/flatc] Error 1
make[2]: *** Deleting file '_deps/flatbuffers-build/flatc'
make[1]: *** [CMakeFiles/Makefile2:4962: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2
```
</details>"
58883,"Bulid TF2.4 in arm architecture, report  an error（crosstool_wrapper_driver_is_not_gcc failed, NvInferLegacyDims.h: No such file or directory）","1、 Describe the problem 

      bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package

When I executed the above compilation , I encountered the following error.

ERROR: /workspace/tensorflow/t243/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:39:11: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/tensorrt_stub/nvinfer_plugin_stub.pic.d ... (remaining 120 argument(s) skipped)
In file included from bazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferPlugin.h:53:0,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_plugin_stub.cc:17:
bazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:53:10: fatal error: NvInferLegacyDims.h: No such file or directory
 #include ""NvInferLegacyDims.h""
          ^~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /workspace/tensorflow/t243/tensorflow/tensorflow/tools/pip_package/BUILD:69:10 C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/compiler/tf2tensorrt/_objs/tensorrt_stub/nvinfer_stub.pic.d ... (remaining 120 argument(s) skipped)
INFO: Elapsed time: 933.025s, Critical Path: 447.45s
INFO: 17716 processes: 8772 internal, 8944 local.
FAILED: Build did NOT complete successfully


2、System information

OS:  cetos, aarch64
tensorflow version: 2.4.3/2.4
python version: 3.7
bazel version: 3.7.2
gcc version: 7.3.1
cuda version: 11.4
cudnn:8
tensorrt:8.2



"
58881,With compile gcc 12.2.0 tensorflow::functor::CSRSparseMatrixTranspose missing,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Gentoo

### Mobile device

_No response_

### Python version

3.11.1

### Bazel version

5.4.0

### GCC/Compiler version

12.2.0

### CUDA/cuDNN version

12.0.0/8.7.0.84

### GPU model and memory

AMD Vega Frontier 16Gb

### Current Behaviour?

```shell
Building TF 2.11.0 with use ROCM 5.4.1 with GCC 12.2.0 failed, but succesfull built with use ROCM 5.4.1 with GCC 11.3.0
```


### Standalone code to reproduce the issue

```shell
ERROR: /var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/work/tensorflow-2.11.0-python3_11/tensorflow/BUILD:1137:20: Linking tensorflow/libtensorflow.so.2.11.0 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/work/tensorflow-2.11.0-python3_11-bazel-base/execroot/org_tensorflow && \
  exec env - \
    HOME=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/homedir \
    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/.keras \
    MLIR_CRASH_REPRODUCER_DIRECTORY=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/.crash \
    PATH=/var/tmp/portage/sci-libs/tensorflow-2.11.0-r12/temp/python3.11/bin:/usr/lib/portage/python3.11/ebuild-helpers/xattr:/usr/lib/portage/python3.11/ebuild-helpers:/usr/sbin:/sbin:/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/bin:/usr/lib/llvm/15/bin:/usr/lib64/subversion/bin:/opt/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.11 \
    PYTHON_LIB_PATH=/usr/lib/python3.11/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/usr/lib64/rocblas/library \
    ROCM_PATH=/usr \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=absl_py,astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,curl,cython,dill_archive,double_conversion,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,libjpeg_turbo,lmdb,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \
  external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow.so.2.11.0-2.params)
# Configuration: 2f0fac33d35f1b8e1706c32b05ccaf2da62c75cfc452cc037134c3e368e32f12
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
collect2: error: ld returned 1 exit status
```


### Relevant log output
[log](https://gist.github.com/raw/0482ac8125d559790af0c1a0a8ed8ff5)
</details>"
58876,Strange C macro generated by tensorflow.lite.python.util.convert_bytes_to_c_source(),"### 1. System information

Although I believe the system setup does not matter in this issue, here goes:
Windows 10 and Ubuntu 22.10
Python 3.7, Python 3.8, Python 3.10
Tensorflow Lite 2.10 and 2.11 (pip package)


### 2. Code

`tensorflow.lite.python.util.convert_bytes_to_c_source()` generates the following strange C macro:
```
// We need to keep the data array aligned on some architectures.
#ifdef __has_attribute
#define HAVE_ATTRIBUTE(x) __has_attribute(x)
#else
#define HAVE_ATTRIBUTE(x) 0
#endif
#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))
#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))
#else
#define DATA_ALIGN_ATTRIBUTE
#endif
```

The strange part is: `defined(__GNUC__) && !defined(__clang__))`
If the code is compiled with the GNU C compiler, then of course it is not compiled with the Clang compiler. The `&& !defined(__clang__)` seems redundant. And if it's not redundant, can someone explain it to me?"
58875,Cannot make use of GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5

### Custom Code

Yes

### OS Platform and Distribution

Linux Manjaro 6.0.11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

RTX 4090

### Current Behaviour?

```shell
Cannot make use of the GPU when trying to use this app ( https://github.com/allo-/virtual_webcam_background ).

I changed the code to test the tensorflow lib and I followed all the steps mentioned here : https://www.tensorflow.org/install/pip . Still not working.

Including a test script below, to see what I mean.

I tried installing all the packages about tf and cuda. Both on my machine and on the conda env ( or pip ).

Any ideas how to solve this?

Thank you.
```


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import tensorflow as tf
import sys
import os
import yaml

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

cuda_devices = os.getenv('CUDA_VISIBLE_DEVICES')
print(""cuda_devices"", cuda_devices)

if cuda_devices is None:
    os.unsetenv('CUDA_VISIBLE_DEVICES')
else:
    os.putenv('CUDA_VISIBLE_DEVICES', cuda_devices)

try:
    import mediapipe as mp
    classifier = mp.solutions.selfie_segmentation.SelfieSegmentation(
            model_selection=1)
    HAS_MEDIAPIPE = True
except ImportError:
    HAS_MEDIAPIPE = False
```


### Relevant log output

```shell
python ./virtual_webcam.py
2022-12-13 11:19:04.927313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-13 11:19:05.784273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/antouank/.conda/envs/virtual-webcam/lib/
2022-12-13 11:19:05.784815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/antouank/.conda/envs/virtual-webcam/lib/
2022-12-13 11:19:05.784829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Num GPUs Available:  1
cuda_devices None
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```
</details>"
58874,Convert both model and post processing to tflite model,"Hello,

I used TFOD2 to train an SSD model. I can convert it to tflite but I don't have the same results because of pre/post processing.

I was wondering if it was possible to convert the model and pre/post processing to a global tflite model?

Here is my function:


```
 @tf.function
    def detect_fn(image):
        image, shapes = detection_model.preprocess(image)
        prediction_dict = detection_model.predict(image, shapes)
        detections = detection_model.postprocess(prediction_dict, shapes)
        return detections
```
"
58873,"compile tensorflow-lite-select-tf-ops no error show,but exit","### 1. System information

tensorflow 2.6.0
bazel 3.7.2
Android-ndk r21e
Android-sdk 30
jdk-14.0.1

### 2. Code

compile tensorflow-lite success
bazel build -c opt --fat_apk_cpu=arm64-v8a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  //tensorflow/lite/java:tensorflow-lite

compile tensorflow-lite-select-tf-ops failed
bazel build -c opt --fat_apk_cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite-select-tf-ops

### 3. logs
Analyzing: target //tensorflow/lite/java:tensorflow-lite-select-tf-ops (223 packages loaded, 11969 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f7b1fa6f5ebec5780e626aa48d582f2519a01632.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite/java:tensorflow-lite-select-tf-ops (243 packages loaded, 25811 targets configured).
INFO: Found 1 target...
[0 / 1] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[24 / 50] [Prepa] Expanding template external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/desugar/Desugar
[49 / 64] [Prepa] Desugaring tensorflow/lite/java/libtensorflowlite_java.jar for Android
[136 / 349] [Scann] Compiling flatbuffers/src/idl_gen_python.cpp
[148 / 349] [Scann] Compiling flatbuffers/grpc/src/compiler/go_generator.cc
[176 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_string_field.cc
[211 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/java/java_message_field_lite.cc
[243 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/python/python_generator.cc
[272 / 534] Compiling com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_field.cc; 2s local
[327 / 534] [Scann] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_helpers.cc
[438 / 2,292] [Scann] Compiling ruy/ruy/pack_arm.cc
[652 / 2,292] [Scann] Compiling com_google_protobuf/src/google/protobuf/reflection_ops.cc
[761 / 2,292] [Scann] Compiling nsync/internal/debug.c
[1,031 / 2,292] [Prepa] Linking external/icu/libicuuc.pic.a
[1,178 / 2,321] [Scann] Compiling tensorflow/lite/kernels/var_handle.cc
[1,242 / 2,321] [Scann] Compiling tensorflow/lite/kernels/squared_difference.cc
[1,476 / 2,321] [Scann] Compiling tensorflow/core/protobuf/rewriter_config.pb.cc
[1,531 / 2,321] [Scann] Compiling tensorflow/lite/delegates/flex/delegate_symbol.cc
[1,606 / 2,321] [Scann] Compiling tensorflow/core/common_runtime/collective_param_resolver_local.cc
[1,678 / 2,321] [Scann] Compiling tensorflow/core/framework/cancellation.cc
[1,744 / 2,321] Compiling tensorflow/core/common_runtime/function.cc; 11s local
[1,808 / 2,321] Compiling tensorflow/core/kernels/conv_ops_3d.cc; 40s local
[1,825 / 2,321] [Scann] Compiling tensorflow/core/data/dataset_utils.cc
[1,839 / 2,321] Compiling tensorflow/core/kernels/cwise_op_less.cc; 31s local
[1,877 / 2,321] Compiling tensorflow/core/ops/boosted_trees_ops.cc; 11s local
[1,963 / 2,321] Compiling tensorflow/core/kernels/resource_variable_ops.cc; 196s local
[1,967 / 2,321] Compiling tensorflow/core/kernels/cwise_op_maximum.cc; 55s local
[1,995 / 2,321] Compiling tensorflow/core/kernels/cwise_op_select.cc; 33s local
[2,051 / 2,321] [Scann] Compiling tensorflow/core/kernels/dynamic_partition_op.cc
[2,133 / 2,321] [Scann] Compiling tensorflow/core/kernels/regex_replace_op.cc

No error is reported, but the program hangs in the background.
I guess the memory is too small, but it doesn't，I increased the virtual memory
ram size 3.5G， swap size 15G
There is a phenomenon, when the program hangs, the shell also breaks down.
I need help. Can you tell me what might be the reason?
"
58872,[DTensor] Does DTensor work directly under the eager mode?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
From the [document](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial), it seems that DTensor only work with tf.function.
```


### Standalone code to reproduce the issue

```shell
The [document](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial) does not explicitly state whether DTensor works in eager or graph mode.
```


### Relevant log output

_No response_</details>"
58869,tensorflow installation not working on apple silicon macs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.0.1 Ventura

### Mobile device

apple silicon m1

### Python version
(major, minor, micro, releaselevel, serial)
(3, 10, 8, 'final', 0)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Whenever I'm importing TensorFlow it's giving me an error. it was working fine yesterday when I had tf 2.10 but it's not working with 2.11. I was hoping everything would go smoothly.
```


### Standalone code to reproduce the issue

```shell
Python 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/tensorflow/__init__.py"", line 443, in <module>
    _ll.load_library(_plugin_dir)
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '__ZN3tsl8internal10LogMessage16VmoduleActivatedEPKci'
>>> exit()
```


### Relevant log output
== check python ===================================================
python version: 3.10.8
python branch: 
python build version: ('main', 'Nov 22 2022 08:25:29')
python compiler version: Clang 14.0.6 
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
Apple clang version 14.0.0 (clang-1400.0.29.202)
Target: arm64-apple-darwin22.1.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                         1.23.2
protobuf                      3.19.4
tensorflow-estimator          2.11.0
tensorflow-macos              2.11.0
tensorflow-metal              0.7.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.11.0
tf.version.GIT_VERSION = unknown
tf.version.COMPILER_VERSION = Apple LLVM 13.1.6 (clang-1316.0.21.1)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tensorflow-script.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 10, 8, 'final', 0)

== bazel version  ==============================================="
58866,"No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", seed2=0, is_training=true] Registered devices: [CPU, GPU] Registered kernels:   <no registered kernels>  	 [[CudnnRNN]] 	 [[sequential_2/lstm/PartitionedCall]] [Op:__inference_train_function_66459]","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I am running it on AMD GPU using tensorflow-directml
when I tried to run the following code , it threw an error
```


### Standalone code to reproduce the issue

```shell
inception_v3=tf.keras.applications.InceptionV3(
    include_top=False,
    weights=""imagenet"",
    input_tensor=None,
    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),
)

lrcnn_model.add(TimeDistributed(inception_v3, input_shape = (SEQUENCE_LENGTH,IMAGE_HEIGHT, IMAGE_WIDTH, 3)))

lrcnn_model.add(TimeDistributed(Flatten()))
lrcnn_model.add(LSTM(32))
lrcnn_model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))


# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
lrcnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [""accuracy""])

# Start training the model.
LRCN_model_training_history = lrcnn_model.fit(x = features_train, y = labels_train, epochs = 7, batch_size = 4 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
Cell In [67], line 8
      5 lrcnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [""accuracy""])
      7 # Start training the model.
----> 8 LRCN_model_training_history = lrcnn_model.fit(x = features_train, y = labels_train, epochs = 7, batch_size = 4 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\eager\execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", seed2=0, is_training=true]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[CudnnRNN]]
	 [[sequential_2/lstm/PartitionedCall]] [Op:__inference_train_function_66459]
```
</details>"
58861,"Exporting Weights from Tensorflow Keras to C produces a different set of outputs, reduced accuracy in the model.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

Any

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Current Behaviour?

For some reason when I export my weights from a Tensorflow Keras model, be it a simple Sequential FNN, and then load them into C to perform forward passes I get different output results, sometimes similar and often just not the same at all.

This is how I export my weights:
```
    for layer in model.layers:
        if layer.get_weights() != []:
            numpy.savetxt(layer.name + "".csv"", layer.get_weights()[0].flatten(), delimiter="","")
            numpy.savetxt(layer.name + ""_bias.csv"", layer.get_weights()[1].flatten(), delimiter="","")
```
and
```
f.write("", /* weight */ "" + str(layer.get_weights()[0].flatten()[i]))
f.write("", /* bias */ "" + str(layer.get_weights()[1].flatten()[i]))
```
The first method writes a long E value and the second writes a smaller float value to the most significant decimal place. Although that should not cause a loss in precision over the first method? right?

I'm not sure why the saved weights would give different outputs, even with a 1 layer FNN that has 1 output,  a very simple network, all the weights are saved to float32 in C and then processed with float math functions such as tanhf() etc depending on the layer output.

I must be missing some understanding of what Tensorflow or Keras is doing behind the scenes? Something it's default configuration is doing to my network/weights that I need to disable or reproduce? Are the arrays of weights in reverse or something? Do I need to iterate them from left to right and not right to left on the inputs? What am I missing?

Here is a project that demonstrates this issue:
https://github.com/jcwml/neural_unitvector

this is a good example project. weights trained in Tensorflow Keras with the same input data produces different outputs when processed in C.

In this project a dataset is exported from main.c to ensure the C program and Tensorflow Keras program both use data generated by the same random function, then Keras trains a set of weights for a simple FNN network to some number of hidden units and layers, and then those weights are exported and loaded into the C program where the forward pass is reconstructed in C code. There are 3 types of networks in this example that have had their forward passes reconstructed, each more complicated than the last, and yet, none of them produce the same outputs as Keras predict() had, it's not even a minor accuracy loss, it is substantially different, you can tell that the weights still represent what they where trained to but with just much worse prediction accuracy.

Another important fact to note is that when I train a network with multiple layers it would seem in many cases when I export the weights to C for a forward pass that the input data passed into the network eventually becomes 0's before it gets to the deeper layers, like vanishing gradients, this would imply that Tensorflow is using a different precision float format but the documentation says it uses float32 by default and the console printouts also claim to be using float32 in my application. My only thought is that maybe numpy is not saving them in the full float32 precision but when I looked into that apparently it does export full precision, and that exporting weights this way should be fine.

I am very confused.

"
58860,tensorflow 2.2 cnn training issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.2

### Custom Code

Yes

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.1/7.6

### GPU model and memory

rtx 3070 and 8 gb memory

### Current Behaviour?

```shell
tensorflow training and validation accuracy remains at 0.5 in tensorflow-gpu 2.2 version, while in tensorflow 2.5 version it gives good training and validation accuracy. the training was done on cifar10 dataset. could you please explain why its performance decreases on tf 2.2 version
```


### Standalone code to reproduce the issue

```shell
import os

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10

physical_devices = tf.config.list_physical_devices(""GPU"")
tf.config.experimental.set_memory_growth(physical_devices[0], True)

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype(""float32"") / 255.0
x_test = x_test.astype(""float32"") / 255.0

model = keras.Sequential(
    [
        keras.Input(shape=(32, 32, 3)),
        layers.Conv2D(32, 3, padding=""valid"", activation=""relu""),
        layers.MaxPooling2D(),
        layers.Conv2D(64, 3, activation=""relu""),
        layers.MaxPooling2D(),
        layers.Conv2D(128, 3, activation=""relu""),
        layers.Flatten(),
        layers.Dense(64, activation=""relu""),
        layers.Dense(10),
    ]
)


def my_model():
    inputs = keras.Input(shape=(32, 32, 3))
    x = layers.Conv2D(32, 3)(inputs)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, 3)(x)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(128, 3)(x)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.Flatten()(x)
    x = layers.Dense(64, activation=""relu"")(x)
    outputs = layers.Dense(10)(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model


model = my_model()
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=keras.optimizers.Adam(lr=3e-4),
    metrics=[""accuracy""],
)

model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=1)
```


### Relevant log output

```shell
fewfewfewf
```
</details>"
58859,Tensorfllow issue:  local installation not working.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

Nvidia Geforce RTX 3060 mobile 6GB

### Current Behaviour?

```shell
Hi,
I've been using TensorFlow locally with GPU for several weeks now using Miniconda and Pip with the official tutorial here: https://www.tensorflow.org/install/pip, everything was running smoothly until I downloaded tensorflow-hub using pip and now TensorFlow doesn't work, I've tried a lot of stuff and I wasn't successful to make it work again, I've already tried:

1. Purging miniconda, cuda and pip cache.https://www.tensorflow.org/install/pip
2. Installing tensorflow with conda packages.
3. Using Conda instead of miniconda.
4. Using a Dcoker container.
5. Using an almost-clean Ubuntu 22 installatioon.
6. Purging Nvidia drivers.

None of them work. even though the installation seems to be successful when running `python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""` , I get:

2022-12-12 09:18:30.415090: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 09:18:30.902573: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/santiagorg2401/anaconda3/envs/tf/lib/
2022-12-12 09:18:30.902640: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/santiagorg2401/anaconda3/envs/tf/lib/
2022-12-12 09:18:30.902645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-12-12 09:18:32.306195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 09:18:32.310604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 09:18:32.310839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

Now as it is supposedly working, whenever I try to fit a model some weird error logs, which are included in Standalone code to reproduce the issue and Relevant log output.

So my goal here is to continue using my GPU to run TensorFlow,
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

# Check physical devices for GPU.
physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))

# Set random seed.
tf.random.set_seed(24)

# Build the model.
model_1 = Sequential([
    Conv2D(10, 3, activation='relu', input_shape = (224, 224, 3)),
    Conv2D(10, 3, activation='relu'),
    MaxPool2D(),
    Conv2D(10, 3, activation='relu'),
    Conv2D(10, 3, activation='relu'),
    MaxPool2D(),
    Flatten(),
    Dense(len(class_names), activation='softmax')
])

# Compile the model.
model_1.compile(loss=[""categorical_crossentropy""],
                optimizer=Adam(),
                metrics=[""accuracy""])

# Fit the model.
history_1 = model_1.fit(train_data,
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=test_data,
                        validation_steps=len(test_data))
```


### Relevant log output

```shell
Num GPUs: 1
Epoch 1/5
2022-12-12 09:21:36.469441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2022-12-12 09:21:37.381994: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-12-12 09:21:37.382426: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-12-12 09:21:37.382448: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:85] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version
2022-12-12 09:21:37.382919: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-12-12 09:21:37.382995: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-12-12 09:21:38.185932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-12-12 09:21:38.195037: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1d8bec50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 09:21:38.195067: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6
2022-12-12 09:21:38.214990: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 09:21:38.279776: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.2
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-12-12 09:21:38.289222: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.289445: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 09:21:38.289561: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.303227: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.303437: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.615605: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.615857: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.629144: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.629393: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.694810: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.695109: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.708523: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.708762: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.905484: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.905768: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:38.919993: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:38.920252: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:39.014158: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:39.014498: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2022-12-12 09:21:39.028825: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2022-12-12 09:21:39.029133: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
Cell In[7], line 31
     26 model_1.compile(loss=[""categorical_crossentropy""],
     27                 optimizer=Adam(),
     28                 metrics=[""accuracy""])
     30 # Fit the model.
---> 31 history_1 = model_1.fit(train_data,
     32                         epochs=5,
     33                         steps_per_epoch=len(train_data),
     34                         validation_data=test_data,
     35                         validation_steps=len(test_data))

File ~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_8' defined at (most recent call last):
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever
      self._run_once()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once
      handle._run()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2940, in run_cell
      result = self._run_cell(
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2995, in _run_cell
      return runner(coro)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3194, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3373, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3433, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_31926/2939559432.py"", line 31, in <module>
      history_1 = model_1.fit(train_data,
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1249, in train_function
      return step_function(self, iterator)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1222, in run_step
      outputs = model.train_step(data)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/santiagorg2401/anaconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py"", line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_8'
libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_8}}]] [Op:__inference_train_function_5724]
```
</details>"
58858,model with lstmcell gives different results before and after tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSuse TumbleWeed 
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.10

### 2. Code

When I attempt to convert a working tensorflow model to tflite, the model converts, but the results obtained are different
See code below:
```
import tensorflow as tf
from tensorflow.keras.layers import (
    Concatenate,
    Conv2D,
    Conv2DTranspose,
    Dropout,
    BatchNormalization)
import numpy as np

inp = tf.keras.Input([1,192], batch_size = 1)
state_h = tf.keras.Input([192], batch_size = 1)
state_c = tf.keras.Input([192], batch_size = 1)
num_channels = 24

xT = BatchNormalization()(inp)

lstm_in = tf.keras.activations.tanh(xT)
x, new_states = tf.keras.layers.LSTMCell(xT.shape[2])(lstm_in[:,0,:], states = [state_h, state_c])
new_state_h = new_states[0]
new_state_c = new_states[1]
out = x + xT

my_model = tf.keras.Model(inputs=[inp, state_h, state_c],outputs = [ out, 
                                                                       new_state_h,
                                                                       new_state_c])

#save model and create tflite model
my_model.save('lstm_test', include_optimizer = False)
converter = tf.lite.TFLiteConverter.from_saved_model('lstm_test')
converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
        ]

tflite_model = converter.convert()

inpt = tf.random.normal([1,10,192])
init_state_h = tf.zeros([1,192])
init_state_c = tf.zeros([1,192])
init_state_h_tfl = tf.zeros([1,192])
init_state_c_tfl = tf.zeros([1,192])

#set up tflite interpreter
interpreter = tf.lite.Interpreter(model_content = tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print('input_details')
print(input_details)
print('output details')
print(output_details)

for k in range(10):
    outpt, st_h, st_c = my_model.predict([tf.expand_dims(inpt[:,k,:], axis=1), init_state_h, init_state_c])
    init_state_h = st_h
    init_state_c = st_c
    interpreter.set_tensor(input_details[0]['index'], tf.expand_dims(inpt[:,k,:], axis = 1))
    interpreter.set_tensor(input_details[1]['index'], init_state_h_tfl)
    interpreter.set_tensor(input_details[2]['index'], init_state_c_tfl)
    interpreter.invoke()
    outpt_tfl = interpreter.get_tensor(output_details[0]['index'])
    st_h_tfl = interpreter.get_tensor(output_details[1]['index'])
    st_c_tfl = interpreter.get_tensor(output_details[2]['index'])
    init_state_h_tfl = st_h_tfl
    init_state_c_tfl = st_c_tfl
    np.testing.assert_almost_equal(outpt, outpt_tfl, decimal = 5)
```

### 3. Failure after conversion
When the above code is ran, it can be seen that there are considerable differences between the tflite model and the original tensorflow model.
"
58857,"when running tflite mode using GPU , the result on AMD is wrong","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

win64

### Mobile device

AMD

### Python version

3.7

### Bazel version

no 

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current Behaviour?

```shell
A bug happened!
device : AMD Ryzen 5 5600U with Radeon Graphics （notebook）
I run a .tflite model on notebook PC using GPU delegate (opencl backend), the inference result is wrong . 
I try other tflite models  , or other AMD notebook，both find result is wrong on GPU delagate.
Please you help see it ,thank you  very much
```


### Standalone code to reproduce the issue

```shell
my  configuration is following,
TfLiteGpuDelegateOptionsV2 gpu_options = TfLiteGpuDelegateOptionsV2Default();
  gpu_options.inference_priority1 =
      TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE;
  gpu_options.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;
  gpu_options.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
  gpu_options.experimental_flags |= TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT;

But IF I use following configuration,
   gpu_options.inference_priority1 =
        TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
    gpu_options.inference_priority2 =TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE;
    gpu_options.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY; 
the result is  right. Is it bug ?
```


### Relevant log output

_No response_</details>"
58856,Memory leak problem due to multi-line comments,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

None

### Python version

3.8.10

### Bazel version

5.3.2

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

Used CPU build

### GPU model and memory

Used CPU build

### Current Behaviour?

```shell
Multi-line comments are not ignored by the compiler(?), and loaded to memory.
Both source build installation on local machine and pip binary installation were tested.
I suggest changing all multi-line comments to single line comments.
```


### Standalone code to reproduce the issue

```shell
Following github repo. can reproduce the issue.
https://github.com/Armature/dump_memory
```


### Relevant log output

```shell
15751362 bytes of 35521408 bytes ( 44.3433% ) are alphabets.
```
</details>"
58855,How to reduce power consumption for gpu delegate on android,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

android

### Mobile device

android

### Python version

3.7

### Bazel version

no 

### GCC/Compiler version

no

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Here it is not a bug.
I want to  reduce power consumption for gpu delegate on android,because I find the execution of tflite model using gpu occupys many power（mA）.
I use opencl backend.
Do you have some optimization method in order to reduce power of model. Thank you very much
```


### Standalone code to reproduce the issue

```shell
my configuration is following,
 TfLiteGpuDelegateOptionsV2 gpu_options = TfLiteGpuDelegateOptionsV2Default();
  gpu_options.inference_priority1 =
      TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE;
  gpu_options.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;
  gpu_options.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
  gpu_options.experimental_flags |= TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_QUANT;
 gpu_options.inference_preference =
      TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;
```


### Relevant log output

_No response_</details>"
58854,How to get single UnidirectionalSequenceRnnOp in tflite model,"### Issue Type

Support

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

According to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc there is `kUnidirectionalSequenceRnnOp` as a single operation in tflite, could you give a python code example - how can I get this? For example - this code for LSTM gives tflite with one UnidirectionalSequenceLSTM Op.
```py
# NOTE tested with TF 2.8.0
import tensorflow as tf
import numpy as np

from tensorflow import keras


model = keras.Sequential()
shape = (4, 4)

model.add(keras.layers.InputLayer(input_shape=shape, batch_size=1))
model.add(keras.layers.LSTM(2, input_shape=shape))
```
![image](https://user-images.githubusercontent.com/4616940/197647526-59c63de2-df61-46a1-bd61-75baa2688376.png)
How can I do same for UnidirectionalSequenceRnn?"
58852, How to release memory when I want to change model with tf.saved_model.load already?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I use tf.saved_model.load to load a trained model. I want to change another model when it done. But the memory is still high, How to release the memory? I have test tf.keras.backend.clear_session() and gc.collect(), all of these didn't work.
```


### Standalone code to reproduce the issue

```shell
model = tf.saved_model.load(model_dir)

tf.keras.backend.clear_session()
gc.collect()
```


### Relevant log output

_No response_</details>"
58851,XLA tools build broken (//tensorflow/compiler/xla/tools:replay_computation_gpu ) on  tensorflow master branch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

build from commit: a3f3bb0eeda308acb970478034449d4d272cabae

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.5 LTS (Focal Fossa)

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
XLA tool build failure, error log: 

ERROR: /home/scratch.shawnw_inf/git/tensorflow-upstream/tensorflow-parser-compute/tensorflow/tensorflow/compiler/xla/tools/BUILD:110:14: Linking tensorflow/compiler/xla/tools/replay_computation_gpu failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /tmp/bazel_cache_2/_bazel_shawnw/6d5d44a3e4597a9a56f32dc526ea10a5/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.8 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/usr/local/nvm/versions/node/v16.15.1/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.8/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/replay_computation_gpu-2.params)
gemm_algorithm_picker.cc:(.text._ZNSt6vectorIN10tensorflow14AutotuneResultESaIS1_EE17_M_realloc_insertIJEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorIN10tensorflow14AutotuneResultESaIS1_EE17_M_realloc_insertIJEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x92): undefined reference to `tensorflow::AutotuneResult::AutotuneResult()'

```
```


### Standalone code to reproduce the issue

```shell
bazel build  --verbose_failures  //tensorflow/compiler/xla/tools:replay_computation_gpu 
```
```


### Relevant log output

```shell
ERROR: /home/scratch.shawnw_inf/git/tensorflow-upstream/tensorflow-parser-compute/tensorflow/tensorflow/compiler/xla/tools/BUILD:110:14: Linking tensorflow/compiler/xla/tools/replay_computation_gpu failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /tmp/bazel_cache_2/_bazel_shawnw/6d5d44a3e4597a9a56f32dc526ea10a5/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.8 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/usr/local/nvm/versions/node/v16.15.1/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.8/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/replay_computation_gpu-2.params)


gemm_algorithm_picker.cc:(.text._ZNSt6vectorIN10tensorflow14AutotuneResultESaIS1_EE17_M_realloc_insertIJEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorIN10tensorflow14AutotuneResultESaIS1_EE17_M_realloc_insertIJEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0xbc): undefined reference to `tensorflow::AutotuneResult::AutotuneResult()'


gpu_conv_algorithm_picker.cc:(.text._ZNSt8__detail9__variant17__gen_vtable_implILb1ENS0_12_Multi_arrayIPFNS0_16__variant_cookieEOZNS0_16_Variant_storageILb0EJSt9monostateSt10unique_ptrIN15stream_executor3dnn12LazyOpRunnerINS8_11FusedConvOpEEESt14default_deleteISB_EES6_INS9_INS8_6ConvOpEEESC_ISG_EEEE13_M_reset_implEvEUlOT_E_RSt7variantIJS5_SE_SI_EEEJEEESt5tupleIJSQ_EESt16integer_sequenceImJLm1EEEE14__visit_invokeESN_SQ_[_ZNSt8__detail9__variant17__gen_vtable_implILb1ENS0_12_Multi_arrayIPFNS0_16__variant_cookieEOZNS0_16_Variant_storageILb0EJSt9monostateSt10unique_ptrIN15stream_executor3dnn12LazyOpRunnerINS8_11FusedConvOpEEESt14default_deleteISB_EES6_INS9_INS8_6ConvOpEEESC_ISG_EEEE13_M_reset_implEvEUlOT_E_RSt7variantIJS5_SE_SI_EEEJEEESt5tupleIJSQ_EESt16integer_sequenceImJLm1EEEE14__visit_invokeESN_SQ_]+0x3a): undefined reference to `stream_executor::dnn::AlgorithmProto::~AlgorithmProto()'
```
</details>"
58849,OOM out of memory Mask RCNN,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 1.15

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to do a train for mask rcnn using Tensorflow 1.15. I'm using a GTX 1650 ti. I am using cuda 10 and cudnn 7.6.0 versions. I am getting an OOM error.

Code of my config file
class DamageConfig(Config):
    # define the name of the configuration
    NAME = ""damage""
    
    # number of classes (background + damge classes)
    NUM_CLASSES = 1 + 4
    
    # number of training steps per epoch
    STEPS_PER_EPOCH = 250
    # learning rate and momentum
    
    # regularization penalty
    BATCH_SIZE = 8
    
    # validation steps
    VALIDATION_STEPS = 25
    
    # RPN Acnhor scales and ratios to find ROI
   

# prepare train dataset.
train_set = DamageDataset()
# change the dataset 
train_set.load_dataset(""C:/Users/hasan/maskcuda/dataset"", ""train"")
train_set.prepare()

# prepare validation/test dataset
test_set = DamageDataset()
test_set.load_dataset(""C:/Users/hasan/maskcuda/dataset"", ""val"")
test_set.prepare()

# load damage config
config = DamageConfig()

# define the model
model = MaskRCNN(mode='training', model_dir='./', config=config)

# load weights mscoco model weights
weights_path = 'C:/Users/hasan/maskcuda/Mask_RCNN/mask_rcnn_coco.h5'

config = tensorflow.ConfigProto()
config.gpu_options.allow_growth = True
sess = tensorflow.Session(config=config)

# load the model weights
model.load_weights(weights_path, 
                   by_name=True, 
                   exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",""mrcnn_bbox"", ""mrcnn_mask""])

# start the training of model
# you can change epochs and layers (head or all)
model.train(train_set, 
            test_set, 
            learning_rate=config.LEARNING_RATE, 
            epochs=15, 
            layers='heads')
```


### Standalone code to reproduce the issue

```shell
2022-12-11 17:49:33.553181: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14829824 totalling 14.14MiB
2022-12-11 17:49:33.553215: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 26214400 totalling 25.00MiB
2022-12-11 17:49:33.553249: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 15 Chunks of size 51380224 totalling 735.00MiB
2022-12-11 17:49:33.553283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 186075648 totalling 177.46MiB
2022-12-11 17:49:33.553317: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 2.81GiB
2022-12-11 17:49:33.553348: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 3041551872 memory_limit_: 3041551975 available bytes: 103 curr_region_allocation_bytes_: 6083104256
2022-12-11 17:49:33.553391: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats:
Limit:                  3041551975
InUse:                  3019572480
MaxInUse:               3041504768
NumAllocs:                   22711
MaxAllocSize:           1348425984


ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[2,3,1030,1030] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node conv1_6/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[proposal_targets_6/strided_slice_17/_28717]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```


### Relevant log output

_No response_</details>"
58848,How to calculate the model's flops when I use tensorflow?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I don't find a good way to calculate the model's flops by tensorflow.
```


### Standalone code to reproduce the issue

```shell
I don't find a good way to calculate the model's flops by tensorflow.
```


### Relevant log output

```shell
I don't find a good way to calculate the model's flops by tensorflow.
```
</details>"
58846,tf.config.experimental.enable_tensor_float_32_execution(False) fails with TF_CUDNN_USE_AUTOTUNE=0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2/cuDNN 8.2

### GPU model and memory

Nvidia GeForce RTX 3090

### Current Behaviour?

```shell
After disabling autotuning with environment variable TF_CUDNN_USE_AUTOTUNE=0 and disabling tensor cores with tf.config.experimental.enable_tensor_float_32_execution(False) I get an error
tensorflow.python.framework.errors_impl.InvalidArgumentError: ... Algo requests disabled tensor op evaluation. [Op:Conv2D]
I expect the default algorithm without tensor cores to be used.
```


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'
import tensorflow as tf
tf.config.experimental.enable_tensor_float_32_execution(False)
tf.nn.conv2d(tf.zeros([1, 2, 5, 5]), tf.zeros([3, 3, 2, 4]), 1, 'VALID', 'NCHW')
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/tmp/bug.py"", line 5, in <module>
    tf.nn.conv2d(tf.zeros([1, 2, 5, 5]), tf.zeros([3, 3, 2, 4]), 1, 'VALID', 'NCHW')
  File ""/tmp/venv/tf-2.11/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/venv/tf-2.11/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 7215, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} Algo requests disabled tensor op evaluation. [Op:Conv2D]
```
</details>"
58845,"Tensorflow compilation fails on Raspberry 32bit compute module with message ""error: static assertion failed: Source and destination types should have equal sizes.""","I am trying to compile Tensorflow on Rpi 32bit 4GB board and encountered compilation failure after 14 hours. I have been following https://qengineering.eu/install-tensorflow-2.1.0-on-raspberry-pi-4.html and compiled Bezel before this.

Following is the git HEAD of Tensorflow code I sync'ed:

commit fcdf659347024dc5a3130e866ba3dde10bac72b0 (HEAD, tag: v2.5.0-rc3)
Merge: a7b36464407 efe04c1b561
Author: Mihai Maruseac <mihaimaruseac@google.com>
Date:   Tue May 4 16:35:43 2021 -0700

    Merge pull request #48904 from tensorflow-jenkins/version-numbers-2.5.0rc3-13515

    Update version numbers for TensorFlow 2.5.0-rc3


This is the command I used:

$ sudo bazel --host_jvm_args=-Xmx1624m build \
             --config=noaws \
             --config=nogcp \
             --config=nohdfs \
             --config=nonccl \
             --config=monolithic \
             --config=v2 \
             --local_cpu_resources=1 \
             --copt=-mfpu=neon-vfpv4 \
             --copt=-ftree-vectorize \
             --copt=-funsafe-math-optimizations \
             --copt=-ftree-loop-vectorize \
             --copt=-fomit-frame-pointer \
             --copt=-DRASPBERRY_PI \
             --host_copt=-DRASPBERRY_PI \
             --linkopt=-Wl,-latomic \
             --host_linkopt=-Wl,-latomic \
             //tensorflow/tools/pip_package:build_pip_package

Following is the snippet from ""lscpu"" command output of my Rpi:

Architecture:                    armv7l
Byte Order:                      Little Endian
CPU(s):                          4
On-line CPU(s) list:             0-3
Thread(s) per core:              1
Core(s) per socket:              4
Socket(s):                       1
Vendor ID:                       ARM
Model:                           3
Model name:                      Cortex-A72
Stepping:                        r0p3

Following is the compilation error:

INFO: Found 1 target...
ERROR: tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:64:11: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:llvm_util' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 180 argument(s) skipped)
In file included from tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:22:
external/com_google_absl/absl/base/casts.h: In instantiation of 'Dest absl::lts_2020_09_23::bit_cast(const Source&) [with Dest = long long int; Source = void (*)(const char*, long long int); typename std::enable_if<(! absl::lts_2020_09_23::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':
tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:382:76:   required from here
external/com_google_absl/absl/base/casts.h:176:30: error: static assertion failed: Source and destination types should have equal sizes.
  176 |   static_assert(sizeof(Dest) == sizeof(Source),
      |                 ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
external/com_google_absl/absl/base/casts.h: In instantiation of 'Dest absl::lts_2020_09_23::bit_cast(const Source&) [with Dest = long long int; Source = const char*; typename std::enable_if<(! absl::lts_2020_09_23::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':
tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:384:55:   required from here
external/com_google_absl/absl/base/casts.h:176:30: error: static assertion failed: Source and destination types should have equal sizes.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /media/sharathchandra/databox/sources/tensorflow_compilation/tensorflow/tensorflow/python/tools/BUILD:227:10 C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:llvm_util' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 180 argument(s) skipped)
INFO: Elapsed time: 1457.007s, Critical Path: 136.90s
INFO: 199 processes: 7 internal, 192 local.
FAILED: Build did NOT complete successfully
"
58844,The output_details and actual output of my converted TF lite model are of different shape ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win10**
- TensorFlow installation (pip package or built from source): **pip**
- TensorFlow library (version, if pip package or github SHA, if built from source): **2.10.1**

### 2. Code
[Colab notebook](https://colab.research.google.com/drive/1taQXvzVbskZbk8_2IRBCgzubNDd4syhZ?usp=sharing) that includes all from model creation to testing the converted model

### 3. Problem Setting
I am trying to build a BERT-based model that does token-level prediction. So the output of my model should be (batch_size, input_length, num_classes). In my case, for a single prediction, that is (1, 128, 6).
In my understanding, higher level APIs like the [BertNLClassifier API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier) do not work for this use case as they can do just one prediction for the whole input.

That is why I have to build it myself. I used TFBert from the transformers library and put a custom classification head on top. Checking the output:
```python
print(model.output)
```
> KerasTensor(type_spec=TensorSpec(shape=(None, 128, 6), dtype=tf.float32, name=None), name='dense/BiasAdd:0', description=""created by layer 'dense'"")

So output is exactly what I want, great!

After checking the output shape I converted that model to TF lite format like this:
```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS # required for Gelu activation function I think
]

tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

### 4. Failure after conversion
After converting the model I ran it in my mobile Android app and was surprised when I got this error message:
> E/tf_predict: Message: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [1, 1, 8] to a Java object with shape [1, 128, 8].

So apparently my output shape changed after conversion. Strange, I thought. So after that did not work I read about the python tf lite interpreter API ```tf.lite.Interpreter```, which I then used to dig deeper for the conversion error.

So I load the model and print the output details:
```python
# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""..\dev\model_punctuation.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(output_details)
```
> [{'name': 'StatefulPartitionedCall:0',
  'index': 654,
  'shape': array([1, 1, 8]),
  'shape_signature': array([-1, -1,  8]),
  'dtype': numpy.float32,
  'quantization': (0.0, 0),
  'quantization_parameters': {'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32),
   'quantized_dimension': 0},
  'sparsity_parameters': {}}]

Okay I though, makes sense. The shape here is already ```array([1, 1, 8])```, so the conversion must be buggy then, right?

I went a step further and [invoked the interpreter](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) just to check if the results match with the Android predictions:
```python
# Test the model on random input data.
input_shape = input_details[0]['shape'] # all three inputs are the same shape

input_data1 = np.array(np.zeros(input_shape), dtype=np.int32)
input_data2 = np.array(np.zeros(input_shape), dtype=np.int32)
input_data3 = np.array(np.zeros(input_shape), dtype=np.int32)

interpreter.set_tensor(input_details[0]['index'], input_data1)
interpreter.set_tensor(input_details[1]['index'], input_data2)
interpreter.set_tensor(input_details[2]['index'], input_data3)

# invoke the model
interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
```

**Now the bug happened** that I want to describe. After invoking the interpreter, I printed the ```output_data.shape``` and it returned ```(1, 128, 8)```, which is the output shape of the original Keras model, so the ""correct"" output shape.

So we now have an inconsistency between the output_details and the *actual* output, which is rather confusing. I would not mind as long as I could also extract the (1, 128, 6) output from my model in my Android app, but I unfortunately couldn't.

- Why are the output shapes inconsistent?
- How can I build the TF lite model to have token-level output for my Android inference?


### 5. (optional) Any other info / logs
#### Code I used in my Android app
```kotlin
import org.tensorflow.lite.Interpreter

fun printTFTensorOutputSpecs(model:Interpreter){
	println(""There are ${model.getOutputTensorCount()} many outputs"")
	for (i in 0 until model.outputTensorCount){
		println(""Output Tensor number $i"")
		println(model.getOutputTensor(i).dataType())
		println(model.getOutputTensor(i).numDimensions())
		println(model.getOutputTensor(i).numElements())
		val iShape = model.getOutputTensor(i).shape()
		println(iShape[0].toString()+"", ""+iShape[1].toString()+"", ""+iShape[2].toString())
	}
}

// serves as input tokens, attention mask, and type_ids
val exampleInput = IntArray(128) { 0 }

val outputMap: Map<Int, Any>

try {
	tfInterpreter.allocateTensors();

	val input1 = IntBuffer.allocate(tfInterpreter.getInputTensor(0).numElements());
	val input2 = IntBuffer.allocate(tfInterpreter.getInputTensor(1).numElements());
	val input3 = IntBuffer.allocate(tfInterpreter.getInputTensor(2).numElements());

	// Populate inputs...
	input1.put(exampleInput)
	input2.put(exampleInput)
	input3.put(exampleInput)

	// check this for the other output tensors. 0 might be aggregated result
	val output = FloatBuffer.allocate(tfInterpreter.getOutputTensor(0).numElements());

	val out_1_128_8 = arrayOf(
			Array(128) { floatArrayOf(0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f) }
	)

	outputMap = mapOf<Int, Any>(0 to out_1_128_8)

	printTFTensorOutputSpecs(tfInterpreter)

	// Process outputs...
	tfInterpreter.runForMultipleInputsOutputs(
			arrayOf(input1,input2,input3),
			outputMap
	)

} catch (e: Exception){
	Log.e(""tf_predict"", ""Message: ""+e.message)
}
```
----
```printTFTensorOutputSpecs(tfInterpreter)``` yielded:
> I/System.out: There are 1 many outputs
    I/System.out: Output Tensor number 0
I/System.out: FLOAT32 // dataType()
I/System.out: 3 // numDimensions()
I/System.out: 8 // numElements()
I/System.out: 1, 1, 8 // shape()

And ```tfInterpreter.runForMultipleInputsOutputs``` threw this error:
> E/tf_predict: Message: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [1, 1, 8] to a Java object with shape [1, 128, 8].
"
58843,ERROR: No matching distribution found for tensorflow==2.13.0,"Does anyone have a solution? I cannot run tacotron2 because of this issue, I tried to use a newer version of tensorflow and another error appears ""module 'tensorflow' has no attribute 'contrib' 
This is a nightmare, I would appreciate if anyone has a answer to my problem"
58840,Process finished with exit code 134 when applying random transformations for data augmentation on a dataset on apple m1 Mac,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.2

### Custom Code

Yes

### OS Platform and Distribution

macOS-13.0.1-arm64-arm-64bit

### Mobile device

No

### Python version

3.10.8

### Bazel version

No

### GCC/Compiler version

No

### CUDA/cuDNN version

No

### GPU model and memory

No

### Current Behaviour?

```shell
Training of the neural network stops abruptly when applying data augmentation using tf.image method on macOS 13.0.1 with an apple m1 chip.

I installed TensorFlow for apple silicon precisely as per the instructions from the official website: 
https://developer.apple.com/metal/tensorflow-plugin/

TensorFlow-related packages and their versions:
tensorboard               2.9.1 
tensorboard-data-server   0.6.1 
tensorboard-plugin-wit    1.8.1 
tensorflow-datasets       4.7.0  
tensorflow-deps           2.9.0 
tensorflow-estimator      2.9.0 
tensorflow-macos          2.9.2 
tensorflow-metadata       1.12.0  
tensorflow-metal          0.5.1  


The code can be quickly produced on Apple m1 by just implementing the sample code provided by TF in their documentation for the Data augmentation tutorial.
Here is the link for the TF data augmentation tutorial: 
https://www.tensorflow.org/tutorials/images/data_augmentation#overview
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras import layers

(train_datasets, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

num_classes = metadata.features['label'].num_classes
IMG_SIZE = 180
batch_size = 32
AUTOTUNE = tf.data.AUTOTUNE


def resize_and_rescale(image, label):
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
  image = (image / 255.0)
  return image, label


def augment(image_label, seed):
  image, label = image_label
  image, label = resize_and_rescale(image, label)
  image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6)
  # Make a new seed.
  new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]
  # Random crop back to the original size.
  image = tf.image.stateless_random_crop(
      image, size=[IMG_SIZE, IMG_SIZE, 3], seed=seed)
  # Random brightness.
  image = tf.image.stateless_random_brightness(
      image, max_delta=0.5, seed=new_seed)
  image = tf.clip_by_value(image, 0, 1)
  return image, label


# Create a generator.
rng = tf.random.Generator.from_seed(123, alg='philox')


# Create a wrapper function for updating seeds.
def f(x, y):
  seed = rng.make_seeds(2)[0]
  image, label = augment((x, y), seed)
  return image, label


train_ds = (
    train_datasets
    .shuffle(1000)
    .map(f, num_parallel_calls=AUTOTUNE)
    .batch(batch_size)
    .prefetch(AUTOTUNE)
)


val_ds = (
    val_ds
    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)
    .batch(batch_size)
    .prefetch(AUTOTUNE)
)

test_ds = (
    test_ds
    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)
    .batch(batch_size)
    .prefetch(AUTOTUNE)
)


model = tf.keras.Sequential([
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])


model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


epochs=5
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
```


### Relevant log output

```shell
Metal device set to: Apple M1

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB

2022-12-09 22:05:22.896356: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-12-09 22:05:22.896444: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2022-12-09 22:05:23.163669: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-12-09 22:05:23.163761: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
2022-12-09 22:05:23.175752: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
Epoch 1/5
2022-12-09 22:05:23.429391: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
 8/92 [=>............................] - ETA: 6s - loss: 2.2291 - accuracy: 0.1680
Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
</details>"
58837,`mypy` compatibility: missing `py.typed`,"### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.8, tf 2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04, macOS Ventura arm

### Mobile device

_No response_

### Python version

3.7, 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`mypy` is one of the great tools that many people use to run type checking via type annotations. Unfortunatelly, `tensorflow` does not play very nicely with it. As specified in the examples, many `tensorflow`-based objects are seen as `Any` by `mypy`. This can lead to various unexpected (and confusing) behaviour in the code that depends on `tensorflow`.

### Extra change required

IIUC this is an invalid type annotation according to `mypy`:

https://github.com/tensorflow/tensorflow/blob/bff338f421322c2ccd1d7e2c5e24ff6c1b0c133c/tensorflow/python/ops/ragged/dynamic_ragged_shape.py#L206

Changing to 


               static_inner_shape: Any = None):


should solve the problem (of course a more specific type annotation would be better).

### Proposed solution

I suspect that this is because a single file is missing to make `tensorflow` type annotations recognised by `mypy`: `py.typed`. Adding that file resolves the issue, and stops `mypy` from treating tensorflow entities as `Any`.

Please kindly let me know if I missed anything.


### Standalone code to reproduce the issue


### Basic example

```python
import tensorflow as tf

reveal_type(tf.Tensor)
```

Without `py.typed`: 

```console
error: Skipping analyzing ""tensorflow"": module is installed, but missing library stubs or py.typed marker  [import]
note: Revealed type is ""Any""
```

With `py.typed`: 

```console
Revealed type is ""def (op: Any, value_index: Any, dtype: Any) -> tensorflow.python.framework.ops.Tensor""
```

### More complicated example

(comments explain what is seen without `py.typed`)

```python
import tensorflow as tf


class A(tf.Module):
    pass


class B(A):
    def foo(self) -> str:
        return ""foo""


class C(B):
    def __init__(self, value: int) -> None:
        super().__init__()
        self.value = value


class D1(C):
    pass


class D2(C):
    def __init__(self, value: int) -> None:
        super().__init__(value)

    def do_something(self) -> int:
        return self.value


class D3(C):
    def do_something(self) -> int:
        return self.value


reveal_type(A())  # Reveals 'A' <- OK
reveal_type(B())  # Reveals 'B' <- OK
reveal_type(B().foo)  # Reveals 'str' <- OK
reveal_type(C(1))  # Reveals 'C' <- OK
reveal_type(D1(1))  # Reveals 'C' <- ??? 'D1' expected
reveal_type(D2(1))  # Reveals 'D2' <- OK but why?
reveal_type(D2(1).do_something)  # Reveals 'int` <- OK
reveal_type(D3(1))  # Reveals 'C' <- ??? 'D3' expected
reveal_type(D3(1).do_something)   # Reveals 'Any' <- ??? 'int' expected
```

With `py.typed` everything is as expected:

```console
tf_mypy.py:36: note: Revealed type is ""tf_mypy.A""
tf_mypy.py:37: note: Revealed type is ""tf_mypy.B""
tf_mypy.py:38: note: Revealed type is ""def () -> builtins.str""
tf_mypy.py:39: note: Revealed type is ""tf_mypy.C""
tf_mypy.py:40: note: Revealed type is ""tf_mypy.D1""
tf_mypy.py:41: note: Revealed type is ""tf_mypy.D2""
tf_mypy.py:42: note: Revealed type is ""def () -> builtins.int""
tf_mypy.py:43: note: Revealed type is ""tf_mypy.D3""
tf_mypy.py:44: note: Revealed type is ""def () -> builtins.int""
```
"
58835,Exporting 🤗 TF BERT model with multiple dynamic axes to MLIR fails with invalid ops generated,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Fedora 37

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The resulting MLIR cannot be validated:


<unknown>:0: error: The following illegal operations still remain: 
	tf.BatchMatMulV2 (count: 24)
	tf.StridedSlice (count: 1)
```


### Standalone code to reproduce the issue

```shell
import iree.compiler.tf as tfc
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, TensorType

tokenizer = AutoTokenizer.from_pretrained(""nlptown/bert-base-multilingual-uncased-sentiment"")
model = TFAutoModelForSequenceClassification.from_pretrained(""nlptown/bert-base-multilingual-uncased-sentiment"")
encodings = tokenizer(""My name is Morgan and I live in Paris"", return_tensors=TensorType.PYTORCH)


tfc.compile_module(
    model,
    import_only=True,
    output_mlir_debuginfo=False,
    import_extra_args=[""--output-format=mlir-ir""],
    exported_names=[""serving""],
    saved_model_tags=[""serve""],
)


Relevant part in transformers: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_tf_bert.py#L1818
```


### Relevant log output

```shell
Diagnostics:
2022-12-09 18:40:42.359313: I external/org_tensorflow/tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-12-09 18:40:42.362140: I external/org_tensorflow/tensorflow/cc/saved_model/bundle_v2.cc:44] Reading SavedModel from: mlir/tf
2022-12-09 18:40:42.425597: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: mlir/tf
2022-12-09 18:40:42.813717: I external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:1195] [/device:CPU:0] Executor start aborting: INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_attention_mask' with dtype int32 and shape [?,?]
	 [[{{node serving_default_attention_mask}}]]
2022-12-09 18:40:48.065663: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1010 = ""tf.BatchMatMulV2""(%1002, %1009) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %992 = ""tf.BatchMatMulV2""(%991, %984) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1540 = ""tf.BatchMatMulV2""(%1539, %1532) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1129 = ""tf.BatchMatMulV2""(%1128, %1121) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1558 = ""tf.BatchMatMulV2""(%1550, %1557) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1814 = ""tf.BatchMatMulV2""(%1813, %1806) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %444 = ""tf.BatchMatMulV2""(%443, %436) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %718 = ""tf.BatchMatMulV2""(%717, %710) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1677 = ""tf.BatchMatMulV2""(%1676, %1669) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %599 = ""tf.BatchMatMulV2""(%591, %598) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.StridedSlice' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:616:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:888:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/autograph/operators/conditional_expressions.py:52:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/autograph/operators/conditional_expressions.py:27:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:888:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1920 = ""tf.StridedSlice""(%1919, %21, %17, %24) {begin_mask = 1 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64} : (tensor<?x?x768xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x768xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1266 = ""tf.BatchMatMulV2""(%1265, %1258) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1832 = ""tf.BatchMatMulV2""(%1824, %1831) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %462 = ""tf.BatchMatMulV2""(%454, %461) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %873 = ""tf.BatchMatMulV2""(%865, %872) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %736 = ""tf.BatchMatMulV2""(%728, %735) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1284 = ""tf.BatchMatMulV2""(%1276, %1283) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %307 = ""tf.BatchMatMulV2""(%306, %299) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1147 = ""tf.BatchMatMulV2""(%1139, %1146) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %325 = ""tf.BatchMatMulV2""(%317, %324) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %581 = ""tf.BatchMatMulV2""(%580, %573) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1421 = ""tf.BatchMatMulV2""(%1413, %1420) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1403 = ""tf.BatchMatMulV2""(%1402, %1395) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:320:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %855 = ""tf.BatchMatMulV2""(%854, %847) {adj_x = false, adj_y = true, device = """"} : (tensor<?x12x?x64xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x?xf32>
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.BatchMatMulV2' op : illegal op still exists
.env/lib64/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:339:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:390:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:96:0: note: called from
.env/lib64/python3.10/site-packages/keras/engine/base_layer.py:1132:0: note: called from
.env/lib64/python3.10/site-packages/keras/utils/traceback_utils.py:65:0: note: called from
.env/lib64/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: see current operation: %1695 = ""tf.BatchMatMulV2""(%1687, %1694) {adj_x = false, adj_y = false, device = """"} : (tensor<?x12x?x?xf32>, tensor<?x12x?x64xf32>) -> tensor<?x12x?x64xf32>
```
</details>"
58833,Select TensorFlow op（FlexTensorArrayV3）is not supported by this interpreter,"**System information**
- OS Platform and Distribution (Win10):
- TensorFlow version (TensorFlow_v1):

**Provide the text output from tflite_convert**
I have already followed the instructions: [https://www.tensorflow.org/lite/guide/ops_select](https://www.tensorflow.org/lite/guide/ops_select)
When I'm trying to convert, I used:
```
import tensorflow as tf
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file=""D:\myhome\\fish_detection-master\models\\research\object_detection\\fish_inception_v2_graph2\\frozen_inference_graph.pb"",
                    # both `.pb` and `.pbtxt` files are accepted.
    input_arrays=['image_tensor'],
    input_shapes={'image_tensor' : [None,None,None,3]},
    output_arrays=['SecondStagePostprocessor/Softmax']
)
converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
]
tflite_model = converter.convert()
# Save the model.
with open('model.tflite', 'wb') as f:
    f.write(tflite_model) 
```
It's all right when converting.
Then, I add the ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency in Android Studio by using the following code:
`implementation 'org.tensorflow:tensorflow-lite:2.11.0`
`implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.11.0`

However,there is something wrong.

This is the model that I used [https://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb](https://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb)

Here is my error in Android Studio:
E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare.
E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite: Node number 3 (FlexTensorArrayV3) failed to prepare.
E/TaskJniUtils: Error getting native address of native library: task_vision_jni
    java.lang.IllegalStateException: Error occurred when initializing ObjectDetector: AllocateTensors() failed.
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithModelFdAndOptions(Native Method)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$000(ObjectDetector.java:88)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector$1.createHandle(ObjectDetector.java:156)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector$1.createHandle(ObjectDetector.java:149)
        at org.tensorflow.lite.task.core.TaskJniUtils$1.createHandle(TaskJniUtils.java:70)
        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)
        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromFdAndOptions(TaskJniUtils.java:66)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromFileAndOptions(ObjectDetector.java:147)
        at org.tensorflow.lite.examples.objectdetection.ObjectDetectorHelper.setupObjectDetector(ObjectDetectorHelper.kt:96)
        at org.tensorflow.lite.examples.objectdetection.ObjectDetectorHelper.detect(ObjectDetectorHelper.kt:107)
        at org.tensorflow.lite.examples.objectdetection.fragments.CameraFragment.detectObjects(CameraFragment.kt:289)
        at org.tensorflow.lite.examples.objectdetection.fragments.CameraFragment.bindCameraUseCases$lambda-9$lambda-8(CameraFragment.kt:264)
        at org.tensorflow.lite.examples.objectdetection.fragments.CameraFragment.$r8$lambda$trA1WWYM4Jg8atYGW5F6kpxoOW8(Unknown Source:0)
        at org.tensorflow.lite.examples.objectdetection.fragments.CameraFragment$$ExternalSyntheticLambda7.analyze(Unknown Source:2)
        at androidx.camera.core.ImageAnalysis.lambda$setAnalyzer$2(ImageAnalysis.java:476)
        at androidx.camera.core.ImageAnalysis$$ExternalSyntheticLambda0.analyze(Unknown Source:2)
        at androidx.camera.core.ImageAnalysisAbstractAnalyzer.lambda$analyzeImage$0$androidx-camera-core-ImageAnalysisAbstractAnalyzer(ImageAnalysisAbstractAnalyzer.java:283)
        at androidx.camera.core.ImageAnalysisAbstractAnalyzer$$ExternalSyntheticLambda1.run(Unknown Source:14)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
        at java.lang.Thread.run(Thread.java:919)
E/Test: TFLite failed to load model with error: Error getting native address of native library: task_vision_jni
D/EGL_emulation: eglMakeCurrent: 0xebf41200: ver 2 0 (tinfo 0xe0075a30)

 **Whether the op called FlexTensorArrayV3 can be used to inference on Android device by using org.tensorflow:tensorflow-lite-select-tf-ops:2.11.0 Android project .Dose it mean that FlexTensorArrayV3 is unlikely to be used in inference on Android device and  I should change my model?**

"
