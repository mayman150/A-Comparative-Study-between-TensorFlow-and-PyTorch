Issue Number,Issue Title,Issue Body
13125,Tensorflow froze 2 variables & converted to const ops,"Hi there, 
I'm new to Python & Tensorflow. Im using raspbian9 (stretch) & Python 2.7.13.

When i run retrain.py from the examples provided using these code:

`python retrain.py --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=50 model_dir=tf_files/inception output_graph=retrained_graph.pb output_labels=retrained_labels.txt --image_dir=img --input_binary=true`

There are no outputs for output_labels (retrained_labels.txt) & output_graph (retrained_graph.pb).
The codes below are the console outputs for the few last lines console output from the command above:

`INFO:tensorflow:2017-09-18 12:50:07.148873: Step 49: Train accuracy = 95.0%
INFO:tensorflow:2017-09-18 12:50:07.149888: Step 49: Cross entropy = 0.241774
INFO:tensorflow:2017-09-18 12:50:08.259520: Step 49: Validation accuracy = 97.0% (N=100)
INFO:tensorflow:Final test accuracy = 89.3% (N=28)
INFO:tensorflow:Froze 2 variables.
Converted 2 variables to const ops.`

Any solutions? Thanks!
"
13124,tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0 from master branch
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: null
- **GPU model and memory**: null
- **Exact command to reproduce**: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>

### Issue description
I saved debug data by `DumpingDebugHook` into hdfs filesys and then it failed to read the data by `python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>` with the error ""not a valid DFS filename"" as Invalid argument. But it works well for the local filesys by the same way.

#### Error info:
```
# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100
tfdbg offline: FLAGS.dump_dir = hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
^@hdfsOpenFile(/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
java.lang.IllegalArgumentException: Pathname /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 from /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 is not a valid DFS filename.
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py"", line 78, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py"", line 41, in main
    FLAGS.dump_dir, validate=FLAGS.validate_graph)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 692, in __init__
    self._load_all_device_dumps(partition_graphs, validate)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 714, in _load_all_device_dumps
    self._load_partition_graphs(partition_graphs, validate)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 983, in _load_partition_graphs
    self._dump_graph_file_paths[device_name])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 145, in _load_graph_def_from_event_file
    event.ParseFromString(f.read())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 119, in read
    self._preread_check()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 79, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512, status)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    c_api.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244; Invalid argument
```

In fact, it was able to read the hdfs dir info, as below:

```
$hdfs dfs -ls hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0
Found 7 items
-rw-r--r--   3 root supergroup     141732 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/gradients
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_1
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_2
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_out
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/loss
drwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/train
```

After that, I tried to change the name of the dir as above from `_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0` to `_tfdbg_device`, and then it was able to load the dir into CLI UI for debug, but there was nothing about debug info to show, as below:

![image](https://user-images.githubusercontent.com/28526467/30541641-dc4329d2-9cae-11e7-8e07-c84dd1a3a7a3.png)
"
13121,array_elementwise_ops_test_cpu_parallel test failure on ppc64le,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Rhel 7.3/ SLES 12 ppc64le
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7.4
- **Bazel version (if compiling from source)**: 0.4.6
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
bazel test --test_output=errors //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Test gives failure log as below. Root causing the issue, error lines are these:

ComputationBuilder builder(client_, TestName());
  **auto result = builder.IsFinite(builder.ConstantR0<float>(NAN));**
  ComputeAndCompareR0<bool>(&builder, false, {});

IsFinite(..) on ppc64le incorrectly returning True instead of False when passed a NaN. Tracing the implemnetaion of IsFinite leads to code under XLA, from here: https://www.tensorflow.org/performance/xla/ seems like XLA does not support ppc64le as a backend. 

Need some insight to help with further debugging, is this analysis correct that this test failure is due to XLA not supporting ppc64le? Any other pointers for this will help!
 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

==================== Test output for //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel:
Note: This is test shard 5 of 25.
[==========] Running 5 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32
2017-07-28 10:35:17.159242: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-28 10:35:17.160204: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-28 10:35:17.161041: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x1002d5b4cb0 executing computations on platform Host. Devices:
2017-07-28 10:35:17.161051: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:151: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected:
false
        vs actual:
true
tensorflow/compiler/xla/tests/literal_test_util.cc:151: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: false
actual:   true)
Expected: true
expected: false vs actual: true
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (41 ms)
[ RUN      ] ArrayElementwiseOpTest.CompareEqZeroElementF32s
2017-07-28 10:35:17.199474: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.CompareEqZeroElementF32s (6 ms)
[ RUN      ] ArrayElementwiseOpTest.MinZeroElementF32s
2017-07-28 10:35:17.205103: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.MinZeroElementF32s (5 ms)
[ RUN      ] ArrayElementwiseOpTest.ClampF32ScalarVector
2017-07-28 10:35:17.210559: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.ClampF32ScalarVector (15 ms)
[ RUN      ] ArrayElementwiseOpTest.3DBinaryOpF32s
2017-07-28 10:35:17.225124: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.3DBinaryOpF32s (12 ms)
[----------] 5 tests from ArrayElementwiseOpTest (79 ms total)

[----------] Global test environment tear-down
[==========] 5 tests from 1 test case ran. (79 ms total)
[  PASSED  ] 4 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32

1 FAILED TEST
================================================================================
==================== Test output for //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel:
Note: This is test shard 6 of 25.
[==========] Running 5 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 5 tests from ArrayElementwiseOpTest
[ RUN      ] ArrayElementwiseOpTest.IsFiniteR1F32s
2017-07-28 10:35:17.180614: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-28 10:35:17.181740: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-07-28 10:35:17.182605: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x100337e4cb0 executing computations on platform Host. Devices:
2017-07-28 10:35:17.182614: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>
tensorflow/compiler/xla/tests/literal_test_util.cc:151: Failure
Value of: Equal(expected, actual)
  Actual: false (expected: {010100}
actual:   {111100})
Expected: true
expected: {010100}   vs actual: {111100}
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s (15 ms)
[ RUN      ] ArrayElementwiseOpTest.CompareGeF32s
2017-07-28 10:35:17.195704: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.CompareGeF32s (10 ms)
[ RUN      ] ArrayElementwiseOpTest.MinF64s
2017-07-28 10:35:17.205905: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.MinF64s (10 ms)
[ RUN      ] ArrayElementwiseOpTest.AddTwoParametersF32s
2017-07-28 10:35:17.215280: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.AddTwoParametersF32s (8 ms)
[ RUN      ] ArrayElementwiseOpTest.Add1DTo3DTwoWaysOver2
2017-07-28 10:35:17.223646: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
[       OK ] ArrayElementwiseOpTest.Add1DTo3DTwoWaysOver2 (20 ms)
[----------] 5 tests from ArrayElementwiseOpTest (63 ms total)

[----------] Global test environment tear-down
[==========] 5 tests from 1 test case ran. (63 ms total)
[  PASSED  ] 4 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s

1 FAILED TEST
================================================================================
//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (23/25 cached) FAILED in 2"
13120,PermissionDeniedError when save model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution: Win7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu 1.3.0
- **Python version**: python 3.6 64bit
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX 1050Ti 4GB
### Describe the problem
try to load a model and then save it, same codes ran in three computersï¼Œone failed when saving (tensorflow-gpu 1.3.0 GTX 1050Ti 4GB), another two passed (tensorflow 1.3.0 and tensorflow-gpu 1.3.0 with GTX 780 Ti 4GB) .  And I ran it as Administrator, so should not lack of write permission

here is the code
` def fit(self,
            training_iters =1e2,
            learning_rate = 1e-4,
            optimizer_epsilon = 1e-10,
            max_gard_norm = 50,
            display_step = 5,
            save_path = None,
            restore_path = None):

        self._sess.run(tf.global_variables_initializer())
        self._variables_saver = tf.train.Saver()
        if restore_path is not None and os.path.exists(restore_path):
            self._variables_saver.restore(self._sess, restore_path)
            print ('restore ok')

        if self._batch_size == self._mini_batch_size:
            for scope in range(np.int(training_iters)):
                _, loss, acc, tp1, tp2 = \
                self._sess.run([self._train_step, self._cost, self._accuracy, self._pondering_cost, self._rnn_cost],
                               feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets})

                if scope % display_step == 0:
                    print (scope, '  loss--', loss, '  acc--', acc, '  pondering_cost--',tp1, '  rnn_cost--', tp2)
                    if save_path is not None:
                        self._variables_saver.save(self._sess, save_path)`

 and exception logs
`---------------------------------------------------------------------------
PermissionDeniedError                     Traceback (most recent call last)
d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-> 1327       return fn(*args)
   1328     except errors.OpError as e:

d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-> 1306                                    status, run_metadata)
   1307 

d:\anaconda3664\lib\contextlib.py in __exit__(self, type, value, traceback)
     88             try:
---> 89                 next(self.gen)
     90             except StopIteration:

d:\anaconda3664\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

PermissionDeniedError: Failed to create a directory: e:.
	 [[Node: save_5/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_5/Const_0_0, save_5/SaveV2/tensor_names, save_5/SaveV2/shape_and_slices, act_core/act_output_linear/b/_939, act_core/act_output_linear/b/RMSProp/_941, act_core/act_output_linear/b/RMSProp_1/_943, act_core/act_output_linear/w/_945, act_core/act_output_linear/w/RMSProp/_947, act_core/act_output_linear/w/RMSProp_1/_949, act_core/halting_linear/b/_951, act_core/halting_linear/b/RMSProp/_953, act_core/halting_linear/b/RMSProp_1/_955, act_core/halting_linear/w/_957, act_core/halting_linear/w/RMSProp/_959, act_core/halting_linear/w/RMSProp_1/_961, global_step, lstm/b_gates/_963, lstm/b_gates/RMSProp/_965, lstm/b_gates/RMSProp_1/_967, lstm/w_gates/_969, lstm/w_gates/RMSProp/_971, lstm/w_gates/RMSProp_1/_973, lstm_1/b_gates/_975, lstm_1/b_gates/RMSProp/_977, lstm_1/b_gates/RMSProp_1/_979, lstm_1/w_gates/_981, lstm_1/w_gates/RMSProp/_983, lstm_1/w_gates/RMSProp_1/_985, lstm_2/b_gates/_987, lstm_2/b_gates/RMSProp/_989, lstm_2/b_gates/RMSProp_1/_991, lstm_2/w_gates/_993, lstm_2/w_gates/RMSProp/_995, lstm_2/w_gates/RMSProp_1/_997)]]

During handling of the above exception, another exception occurred:

PermissionDeniedError                     Traceback (most recent call last)
<ipython-input-16-2f0c74b77fe4> in <module>()
      3               display_step = 1,
      4               save_path=""e:./dnc_model_171.ckpt"",
----> 5               restore_path=""e:./dnc_model_17.ckpt""
      6              )
      7 

D:\PyTrade\DNCore.py in fit(self, training_iters, learning_rate, optimizer_epsilon, max_gard_norm, display_step, save_path, restore_path)
    135                     
    136                     if save_path is not None:
--> 137                         self._variables_saver.save(self._sess, save_path)
    138 
    139             

d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py in save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)
   1472         model_checkpoint_path = sess.run(
   1473             self.saver_def.save_tensor_name,
-> 1474             {self.saver_def.filename_tensor_name: checkpoint_file})
   1475         model_checkpoint_path = compat.as_str(model_checkpoint_path)
   1476         if write_state:

d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1319     if handle is None:
   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1321                            options, run_metadata)
   1322     else:
   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

d:\anaconda3664\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1338         except KeyError:
   1339           pass
-> 1340       raise type(e)(node_def, op, message)
   1341 
   1342   def _extend_graph(self):

PermissionDeniedError: Failed to create a directory: e:.
	 [[Node: save_5/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_5/Const_0_0, save_5/SaveV2/tensor_names, save_5/SaveV2/shape_and_slices, act_core/act_output_linear/b/_939, act_core/act_output_linear/b/RMSProp/_941, act_core/act_output_linear/b/RMSProp_1/_943, act_core/act_output_linear/w/_945, act_core/act_output_linear/w/RMSProp/_947, act_core/act_output_linear/w/RMSProp_1/_949, act_core/halting_linear/b/_951, act_core/halting_linear/b/RMSProp/_953, act_core/halting_linear/b/RMSProp_1/_955, act_core/halting_linear/w/_957, act_core/halting_linear/w/RMSProp/_959, act_core/halting_linear/w/RMSProp_1/_961, global_step, lstm/b_gates/_963, lstm/b_gates/RMSProp/_965, lstm/b_gates/RMSProp_1/_967, lstm/w_gates/_969, lstm/w_gates/RMSProp/_971, lstm/w_gates/RMSProp_1/_973, lstm_1/b_gates/_975, lstm_1/b_gates/RMSProp/_977, lstm_1/b_gates/RMSProp_1/_979, lstm_1/w_gates/_981, lstm_1/w_gates/RMSProp/_983, lstm_1/w_gates/RMSProp_1/_985, lstm_2/b_gates/_987, lstm_2/b_gates/RMSProp/_989, lstm_2/b_gates/RMSProp_1/_991, lstm_2/w_gates/_993, lstm_2/w_gates/RMSProp/_995, lstm_2/w_gates/RMSProp_1/_997)]]

Caused by op 'save_5/SaveV2', defined at:
  File ""d:\anaconda3664\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\anaconda3664\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""d:\anaconda3664\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""d:\anaconda3664\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""d:\anaconda3664\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""d:\anaconda3664\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""d:\anaconda3664\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""d:\anaconda3664\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""d:\anaconda3664\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""d:\anaconda3664\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""d:\anaconda3664\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""d:\anaconda3664\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""d:\anaconda3664\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""d:\anaconda3664\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""d:\anaconda3664\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""d:\anaconda3664\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""d:\anaconda3664\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""d:\anaconda3664\lib\site-packages\IPython\core\interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""d:\anaconda3664\lib\site-packages\IPython\core\interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""d:\anaconda3664\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-16-2f0c74b77fe4>"", line 5, in <module>
    restore_path=""e:./dnc_model_17.ckpt""
  File ""D:\PyTrade\DNCore.py"", line 121, in fit
    self._variables_saver = tf.train.Saver()
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py"", line 1140, in __init__
    self.build()
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py"", line 1172, in build
    filename=self._filename)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py"", line 686, in build
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py"", line 276, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\training\saver.py"", line 219, in save_op
    tensors)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 768, in save_v2
    tensors=tensors, name=name)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""d:\anaconda3664\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

PermissionDeniedError (see above for traceback): Failed to create a directory: e:.
	 [[Node: save_5/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_5/Const_0_0, save_5/SaveV2/tensor_names, save_5/SaveV2/shape_and_slices, act_core/act_output_linear/b/_939, act_core/act_output_linear/b/RMSProp/_941, act_core/act_output_linear/b/RMSProp_1/_943, act_core/act_output_linear/w/_945, act_core/act_output_linear/w/RMSProp/_947, act_core/act_output_linear/w/RMSProp_1/_949, act_core/halting_linear/b/_951, act_core/halting_linear/b/RMSProp/_953, act_core/halting_linear/b/RMSProp_1/_955, act_core/halting_linear/w/_957, act_core/halting_linear/w/RMSProp/_959, act_core/halting_linear/w/RMSProp_1/_961, global_step, lstm/b_gates/_963, lstm/b_gates/RMSProp/_965, lstm/b_gates/RMSProp_1/_967, lstm/w_gates/_969, lstm/w_gates/RMSProp/_971, lstm/w_gates/RMSProp_1/_973, lstm_1/b_gates/_975, lstm_1/b_gates/RMSProp/_977, lstm_1/b_gates/RMSProp_1/_979, lstm_1/w_gates/_981, lstm_1/w_gates/RMSProp/_983, lstm_1/w_gates/RMSProp_1/_985, lstm_2/b_gates/_987, lstm_2/b_gates/RMSProp/_989, lstm_2/b_gates/RMSProp_1/_991, lstm_2/w_gates/_993, lstm_2/w_gates/RMSProp/_995, lstm_2/w_gates/RMSProp_1/_997)]]`
"
13119,Tensorflow installation ,"I am installing tensorflow on window machine with the help of Python (version python-3.5.4-amd64).

Command use to install tensorflow  :- pip3 install --upgrade tensorflow

Bellow error getting in Import tensorflow command.

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\eclipse\WorkspavePython\study\test.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
13118,Can't freeze .pbtxt to .pb file :  Multiple OpKernel registrations match NodeDef after compiled android_arm .so,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**:  8.0.61
- **GPU model and memory**: NVIDIA Corporation Device 1b06
- **Exact command to reproduce**: 

```
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=.../imagenet/train_logs/graph.pbtxt \
--input_checkpoint=.../imagenet/train_logs/model.ckpt-10000 \
--output_graph=.../imagenet/train_logs/frozen_graph.pb \
--output_node_names=MobilenetV1/Predictions/Reshape_1
```

### Describe the problem
I am doing some fine-tune , but can't convert text file to binary file after training , the following error is what I got

### Source code / logs
```
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 329, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 242, in main
    FLAGS.input_saved_model_dir, FLAGS.saved_model_tags)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 233, in freeze_graph
    input_saved_model_dir, saved_model_tags.split("",""))
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 127, in freeze_graph_with_def_protos
    saver.restore(sess, input_checkpoint)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1646, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1118, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1315, in _do_run
    options, run_metadata)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1334, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Multiple OpKernel registrations match NodeDef 'prefetch_queue/fifo_queue = FIFOQueueV2[capacity=2, component_types=[DT_FLOAT, DT_FLOAT], container="""", shapes=[[10,224,224,3], [10,3]], shared_name=""""]()': 'op: ""FIFOQueueV2"" device_type: ""CPU""' and 'op: ""FIFOQueueV2"" device_type: ""CPU""'
         [[Node: prefetch_queue/fifo_queue = FIFOQueueV2[capacity=2, component_types=[DT_FLOAT, DT_FLOAT], container="""", shapes=[[10,224,224,3], [10,3]], shared_name=""""]()]]

Caused by op u'prefetch_queue/fifo_queue', defined at:
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 329, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 242, in main
    FLAGS.input_saved_model_dir, FLAGS.saved_model_tags)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 233, in freeze_graph
    input_saved_model_dir, saved_model_tags.split("",""))
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 99, in freeze_graph_with_def_protos
    _ = importer.import_graph_def(input_graph_def, name="""")
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 3077, in create_op
    op_def=op_def)
  File ""/home/simonlee/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1627, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Multiple OpKernel registrations match NodeDef 'prefetch_queue/fifo_queue = FIFOQueueV2[capacity=2, component_types=[DT_FLOAT, DT_FLOAT], container="""", shapes=[[10,224,224,3], [10,3]], shared_name=""""]()': 'op: ""FIFOQueueV2"" device_type: ""CPU""' and 'op: ""FIFOQueueV2"" device_type: ""CPU""'
         [[Node: prefetch_queue/fifo_queue = FIFOQueueV2[capacity=2, component_types=[DT_FLOAT, DT_FLOAT], container="""", shapes=[[10,224,224,3], [10,3]], shared_name=""""]()]]
```


I know the op FIFOQueueV2 is not supported on Android , so I followed #8404 , recompiled my .so file before this occurred.
But does it result in this ? Does it only be used when model loaded on Android ? Or all bazel compilation ?"
13117,'DNNClassifier' object has no attribute '_train_model',"I got an error saying ""'DNNClassifier' object has no attribute '_train_model'"" when I run this.

```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"")
X_train = mnist.train.images
X_test = mnist.test.images
y_train = mnist.train.labels.astype(""int"")
y_test = mnist.test.labels.astype(""int"")
import tensorflow as tf

config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config
feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)
dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10, feature_columns=feature_cols, config=config)
dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1
dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)
```
"
13116,tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0 from master branch
- **Python version**: Python 2.7.12 (default, Nov 19 2016, 06:48:10)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: null
- **GPU model and memory**: null
- **Exact command to reproduce**: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>

### Issue description
I saved debug data by `DumpingDebugHook` into hdfs filesys and then it failed to read the data by `python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>`, but it works well with the local filesys by the same way.

#### Error info:
```
# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
tfdbg offline: FLAGS.dump_dir = hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
hdfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: ""G53.ad000000000427.et2/11.140.133.72""; destination host is: ""ns1"":8020;
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py"", line 78, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py"", line 41, in main
    FLAGS.dump_dir, validate=FLAGS.validate_graph)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 682, in __init__
    raise IOError(""Dump root directory %s does not exist"" % dump_root)
IOError: Dump root directory hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100 does not exist
```

Be sure that the above hdfs dir exists, which including debug data by `DumpingDebugHook`, as below:

```
# hdfs dfs -ls -d hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
drwxr-xr-x   - root supergroup          0 2017-09-18 08:01 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100
```"
13115,"Build problem :Configurable attribute ""copts"" doesn't match this configuration","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux pine64 3.10.104-2-pine64-longsleep aarch64 aarch64 aarch64 GNU/Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.3.0-rc0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:0.4.5
- **CUDA/cuDNN version**: no
- **Exact command to reproduce**: `bazel build -c opt --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --verbose_failures tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
As  tensorflow supports ARM 64-bit CPU platform,so I build it on PINE64.But after I install bazel exactly, and try to install tensorflow,there is something wrong in building process.

### Source code / logs
```
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/nsync/BUILD:401:13: Configurable attribute ""copts"" doesn't match this configuration (would a default condition help?).
Conditions checked:
 @nsync//:android_arm
 @nsync//:android_arm64
 @nsync//:android_armeabi
 @nsync//:android_x86_32
 @nsync//:android_x86_64
 @nsync//:clang_macos_x86_64
 @nsync//:gcc_linux_aarch64
 @nsync//:gcc_linux_ppc64
 @nsync//:gcc_linux_x86_64_1
 @nsync//:gcc_linux_x86_64_2
 @nsync//:ios_x86_64
 @nsync//:msvc_windows_x86_64.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
```
As the logs errors indicate the file, so I looks the code ,and it indicates the cpp library configure as follow:
 ```
# The library compiled in C++11, rather than C.
397 cc_library(
398     name = ""nsync_cpp"",
399     srcs = NSYNC_SRC_GENERIC + NSYNC_SRC_PLATFORM_CPP,
400     hdrs = NSYNC_HDR_GENERIC,
401     copts = NSYNC_OPTS_CPP,
402     includes = [""public""],
403     textual_hdrs = NSYNC_INTERNAL_HEADERS + NSYNC_INTERNAL_HEADERS_PLATFORM,
404 )
```
I try to annotation the number 401 code,the errer above seems disapperence but other errer happens.
I appreciate ever help, thank you all very much."
13114,Extremely slow first epoch.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: PIP, tensorflow-gpu==1.2.1
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: Python 3.6.2
- **CUDA/cuDNN version**: CUDA Version 8.0.61 / cuDNN Version 5.1.10
- **GPU model and memory**: GTX 1080ti x 2, 11171MiB each.

### Describe the problem
When training model, ""**only First epoch after executing training script is extremely slow** (More specifically, It takes 12 hours while second and third epoch after executed takes 2.5 hours with my environment and dataset)""

I uploaded this issue because similar issue in stackoverflow is not handled long time.
(https://stackoverflow.com/questions/44966831/tensorflow-first-epoch-is-extremely-slow-maybe-related-to-pool-allocator)

In more detail,
my code uses model with  **1d convolution layers** (tf.nn.conv1d - tf.bias_add - tf.nn.relu) as model.
**tf.ctc_loss** as loss function.
Data is served with tf.PaddingFIFOQueue && tf.QueueBase.dequeue_many and ""**each data has different size**"".

Here is what I tried,
First I also assumed that pool allocator makes this problem like [above stackoverflow link](https://stackoverflow.com/questions/44966831/tensorflow-first-epoch-is-extremely-slow-maybe-related-to-pool-allocator) (**but now I think this issue is not from pool allocator**)
1. Uses tcmalloc
`LD_PRELOAD=""/usr/lib/libtcmalloc.so""` which is suggested in [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/ja7FlGrvh-E)
2. Uses BFC allocator type.
- With `TF_CPU_ALLOCATOR_USE_BFC=true`
- And I tried `tf.ConfigProto().gpu_options. allocator_type = 'BFC'` also.
3. Build tensorflow from source with modified initial pool size limit (100 to 10000), (Which is hard coded in [here](https://github.com/tensorflow/tensorflow/blob/927f811b0303e51126531c135f8b093383de2d6d/tensorflow/core/common_runtime/gpu/process_state.cc#L187)) 

And all trying was ineffective.

Except extremely slow first epoch, everything was fine. Weights of model is trained well, save and load checkpoint and continuing training or inferencing is also fine without any warning and error.

Is it one of avoidable characteristic of Tensorflow or bugs?

Thank you for your reading.
If you need any more information, please notify me."
13112,Feature Request: API for weights values conversion,"It would be very nice if the TensorFlow developers can provide a simple API for converting the weight values, ported from other frameworks like `Theano, Torch, Caffe and Chainer`. A lot of researchers still use these frameworks. Most of the times when an individual convert weights values, obtained from some framework other than TF, the results are not reproducible which is not desirable. Providing such an API, can help the TF community to port the models from other frameworks to TF, proving that the same thing could have been done more easily in TF. The API can have a method that takes the following signature:

```
def port_weights_to_tf(weights values, framework=[Caffe or theano or Torch or chainer]):
       # Change the weight values for each layer and store in hdf5 with layer names
       return modified_weights
```
"
13106,no such package '@llvm//',"Trying to build latest git tree (`git describe --tags` calls it `v1.3.0-rc1-2262-g74cfc64734`):

>ERROR: /build/tensorflow-git/src/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: no such package '@llvm//': java.io.IOException: Error downloading [http://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz, https://github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz] to /build/.cache/bazel/_bazel_builduser/a152fcd393afbe6f0b02d283bc9e6174/external/llvm/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz: Checksum was e8f07137a3a0b95e143c0665cd19160dd5040114b34a48653fa7f5f91cf4c136 but wanted 2a6d4c23f6660d9130d8d5f16267db53a87f8d0104f9618b558c033570f110af and referenced by '//tensorflow/tools/pip_package:licenses'.

Surely the correct fix is *not* to just to change the expected checksum to match the observed one?
"
13104,Fix tf.argmax/argmin documentation,"https://www.tensorflow.org/api_docs/python/tf/argmax

- The documentation does not explain what an ""index"" is. I can imagine a billion definitions of ""index"". Is it the same as the ""index"" in tf.one_hot?
- The documentation does not explain what happens if axis is set to None.
- ""For vectors, use axis = 0."": Why?
- The documentation should clearly explain how the rank+shape of the returned tensor correlates with the rank+shape of the input tensor.
- The documentation incorrectly indicates that there is a guide if you click ""See the guide"", but it takes you to a function reference/index. A redundant 3 sentence description is not a ""guide"".

"
13103,Dropout hidden-to-hidden transition within an RNN,"`DropoutWrapper` allows to apply dropout to either the cell's inputs, outputs or states. However, I haven't seen an option to do the same thing for the recurrent weights of the cell (for example, 4 out of the 8 different matrices used in the original LSTM formulation). I specifically refer to the hidden-to-hidden transition within an RNN. Take as an example Section 2 from https://arxiv.org/abs/1708.02182"
13102,Trouble installing as well as uninstalling Tensorflow,"So as you can see here we have python and anaconda

```
Python 3.6.1 |Anaconda 4.4.0 (x86_64)| (default, May 11 2017, 13:04:09) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```
Then installed Tensorflow using pip3 and tfBinaryURL eventually testing the installation-

```
>>> #python
... import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
ModuleNotFoundError: No module named 'tensorflow'
>>> 
```
So I tried uninstalling Tensorflow using pip uninstall tensorflow but again after the y/n prompt (typed ""y"")

 ```
 Installing collected packages: tensorflow
  Found existing installation: tensorflow 1.3.0
  Uninstalling tensorflow-1.3.0:
Exception:
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py"", line 544, in move
    os.rename(src, real_dst)
PermissionError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc' -> '/var/folders/61/1b15w21n0jbc2sndhd3j7xfr0000gn/T/pip-oepk4q_b-uninstall/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_set.py"", line 778, in install
    requirement.uninstall(auto_confirm=True)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_install.py"", line 754, in uninstall
    paths_to_remove.remove(auto_confirm)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_uninstall.py"", line 115, in remove
    renames(path, new_path)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/utils/__init__.py"", line 267, in renames
    shutil.move(old, new)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py"", line 559, in move
    os.unlink(src)
PermissionError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc'
this happens.
```

If it's pre-installed as it says ideally the program #1 should work - but it doesn't.

Can you help, please?"
13101,from_generator feedback and questions,"@mrry, thank you for implementing the `from_generator` method in `tf.data`. I just wanted to provide some feedback and ask a few more questions.

# Interface

In addition to having `generator` be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable? E.g. instead of 

```python
pool = multiprocessing.Pool()
dataset = tf.contrib.data.Dataset.from_generator(
    lambda: pool.imap(some_function, some_data), dtypes, shapes
)
```

also support

```python
pool = multiprocessing.Pool()
dataset = tf.contrib.data.Dataset.from_generator(
    pool.imap(some_function, some_data), dtypes, shapes
)
```

# Return types

`from_generator` does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have. E.g. this fails

```python
def generator():
    while True:
        yield np.zeros(2, np.float32)
        
dataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))
x, y = dataset.make_one_shot_iterator().get_next()
session = tf.Session()
session.run([x, y])
```

but this runs smoothly

```python
def generator():
    while True:
        yield tuple(np.zeros(2, np.float32))
        
dataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))
x, y = dataset.make_one_shot_iterator().get_next()
session = tf.Session()
session.run([x, y])
```

# Performance

I've played around with a *very* naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using `feed_dicts`. The setup is as follows

```python
import tensorflow as tf
from tensorflow.contrib import data as tfdata
import numpy as np
from time import time

num_batches = 1000
batch_size = 100

class Generator:
    def __init__(self):
        self.times = []
    
    def __iter__(self):
        while True:
            x = np.random.normal()
            y = 3 + 5 * x
            x, y = np.asarray([x, y], np.float32)
            self.times.append(time())
            yield x, y

generator_state1 = Generator()

dataset = tfdata.Dataset.from_generator(
    lambda: generator_state1, 
    (tf.float32, tf.float32),
    (tf.TensorShape([]), tf.TensorShape([]))
)
prefetched = dataset.prefetch(3 * batch_size)
batches = prefetched.batch(batch_size)
iterator = batches.make_one_shot_iterator()

x, y = iterator.get_next()

w = tf.Variable([0, 0], dtype=tf.float32)
prediction = w[0] + w[1] * x
loss = tf.losses.mean_squared_error(y, prediction)
optimizer = tf.train.AdamOptimizer(0.1)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()

session = tf.Session()
session.run(init_op)
```

Running the optimisation gives me

```python
losses = []

start = time()
for _ in range(num_batches):
    _, _loss = session.run([train_op, loss])
    losses.append(_loss)
time() - start  # about seven seconds
```

Doing the same using feed_dicts gives

```python
losses = []

generator_state2 = Generator()
iterator = iter(generator_state2)

start = time()
for _ in range(num_batches):
    _x, _y = np.transpose([next(iterator) for _ in range(batch_size)])
    _, _loss = session.run([train_op, loss], {x: _x, y: _y})
    losses.append(_loss)
time() - start  # about one second
```

It seems that the dataset created using the `from_generator` method isn't fetching from the generator fast enough:

```python
np.mean(np.diff(generator_state1.times))  # 7.1533812683949508e-05
np.mean(np.diff(generator_state2.times))  # 1.0633696558370612e-05
```

Thanks again for this, and looking forward to hearing your thoughts."
13100,- NEON XLA CPU runtime is not compiling because of missing plog function in Eigen,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.3.0-0-g9e76bf324', '1.3.0')
- **Python version**: 3.4.3
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
WORKSPACE correctly configure for Android NDK (r12b) and SDK(latest).
bazel build -c opt --cxxopt='-std=c++11' --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon

### Describe the problem
I use Tensorflow compiled from source to use XLA aot binaries for Arm. For XLA to work, it needs to have runtime support for that architecture but with the above command it fails complaining for a missing function in Eigen.
I fixed the bug in Eigen and submitted a pull request for integration:
https://bitbucket.org/eigen/eigen/pull-requests/334/add-support-for-neon-plog-packetmath/diff

with the following patch, the compilation completes successfully. Once the pull request is accepted I will submit a pull request here to use the new Eigen version with the fix.

### Source code / logs
For the record, this is the error i get:

bazel build -c opt --cxxopt='-std=c++11' --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon
Extracting Bazel installation...
...........
INFO: Analysed target //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon (17 packages loaded).
INFO: Found 1 target...
ERROR: /home/gmichel/work/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:333:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon' failed (Exit 1)
In file included from external/eigen_archive/Eigen/Core:372:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]':
tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: error: no matching function for call to 'log(const __vector(4) __builtin_neon_sf&)'
 Packet plog(const Packet& a) { using std::log; return log(a); }
                                                            ^
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: note: candidates are:
In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,
                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,
                 from external/eigen_archive/Eigen/Core:78,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:218:8: note: double log(double)
 double log(double) __NDK_FPABI_MATH__;
        ^
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:218:8: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'double'
In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44:0,
                 from external/eigen_archive/Eigen/Core:78,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:357:3: note: constexpr float std::log(float)
   log(float __x)
   ^
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:357:3: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'float'
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:361:3: note: constexpr long double std::log(long double)
   log(long double __x)
   ^
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:361:3: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'long double'
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: note: template<class _Tp> constexpr typename __gnu_cxx::__enable_if<std::__is_integer<_Tp>::__value, double>::__type std::log(_Tp)
     log(_Tp __x)
     ^
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: note:   template argument deduction/substitution failed:
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath: In substitution of 'template<class _Tp> constexpr typename __gnu_cxx::__enable_if<std::__is_integer<_Tp>::__value, double>::__type std::log(_Tp) [with _Tp = __vector(4) __builtin_neon_sf]':
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60:   required from 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]'
tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: error: no type named '__type' in 'struct __gnu_cxx::__enable_if<false, double>'
In file included from external/eigen_archive/Eigen/Core:78:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]':
tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:790:5: note: template<class _Tp> std::complex<_Tp> std::log(const std::complex<_Tp>&)
     log(const complex<_Tp>& __z) { return __complex_log(__z); }
     ^
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:790:5: note:   template argument deduction/substitution failed:
In file included from external/eigen_archive/Eigen/Core:372:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: note:   mismatched types 'const std::complex<_Tp>' and 'const __vector(4) __builtin_neon_sf'
 Packet plog(const Packet& a) { using std::log; return log(a); }
                                                            ^
Target //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 15.227s, Critical Path: 0.76s
FAILED: Build did NOT complete successfully
"
13099,Weights and biases not being updated.,"(I've already tried stackoverflow)

I've made this neural net to figure out whether a house is a good buy or a bad buy. For some reasons the code is not updating weights and biases. My loss stays same. This is my code:

    import pandas as pd
    import tensorflow as tf
    
    data = pd.read_csv(""E:/workspace_py/datasets/good_bad_buy.csv"")
    
    features = data.drop(['index', 'good buy'], axis = 1)
    lbls = data.drop(['index', 'area', 'bathrooms', 'price', 'sq_price'], axis = 1)
    
    features = features[0:20]
    lbls = lbls[0:20]
    
    print(features)
    print(lbls)
    n_examples = len(lbls)
    
    # Model
    
    # Hyper parameters
    
    epochs = 100
    learning_rate = 0.1
    batch_size = 1
    
    input_data = tf.placeholder('float', [None, 4])
    labels = tf.placeholder('float', [None, 1])
    
    weights = {
    			'hl1': tf.Variable(tf.random_normal([4, 10])),
    			'hl2': tf.Variable(tf.random_normal([10, 10])),
    			'hl3': tf.Variable(tf.random_normal([10, 4])),
    			'ol': tf.Variable(tf.random_normal([4, 1]))
    			}
    
    biases = {
    			'hl1': tf.Variable(tf.random_normal([10])),
    			'hl2': tf.Variable(tf.random_normal([10])),
    			'hl3': tf.Variable(tf.random_normal([4])),
    			'ol': tf.Variable(tf.random_normal([1]))
    			}
    
    hl1 = tf.nn.relu(tf.add(tf.matmul(input_data, weights['hl1']), biases['hl1']))
    hl2 = tf.nn.relu(tf.add(tf.matmul(hl1, weights['hl2']), biases['hl2']))
    hl3 = tf.nn.relu(tf.add(tf.matmul(hl2, weights['hl3']), biases['hl3']))
    ol = tf.nn.sigmoid(tf.add(tf.matmul(hl3, weights['ol']), biases['ol']))
    
    loss = tf.reduce_mean((labels - ol)**2)
    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)
    
    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    
    iterations = int(n_examples/batch_size)
    
    
    for epoch_no in range(epochs):
        ptr = 0
    	for iteration_no in range(iterations):
    		epoch_input = features[ptr:ptr+batch_size]
    		epoch_label = lbls[ptr: ptr+batch_size]
    		ptr = ptr + batch_size
    		_, err = sess.run([train, loss], feed_dict={input_data: features, labels: lbls})
    	print(""Error at epoch "", epoch_no, "": "", err)
    
    print(sess.run(ol, feed_dict={input_data: [[2104, 3, 399900, 190.0665]]}))


This is the dataset:

    Features:

        area  bathrooms   price    sq_price
    0   2104          3  399900  190.066540
    1   1600          3  329900  206.187500
    2   2400          3  369000  153.750000
    3   1416          2  232000  163.841808
    4   3000          4  539900  179.966667
    5   1985          4  299900  151.083123
    6   1534          3  314900  205.280313
    7   1427          3  198999  139.452698
    8   1380          3  212000  153.623188
    9   1494          3  242500  162.315930
    10  1940          4  239999  123.710825
    11  2000          3  347000  173.500000
    12  1890          3  329999  174.602645
    13  4478          5  699900  156.297454
    14  1268          3  259900  204.968454
    15  2300          4  449900  195.608696
    16  1320          2  299900  227.196970
    17  1236          3  199900  161.731392
    18  2609          4  499998  191.643542
    19  3031          4  599000  197.624546

    labels:

        good buy
    0        1.0
    1        0.0
    2        1.0
    3        0.0
    4        1.0
    5        0.0
    6        0.0
    7        1.0
    8        0.0
    9        0.0
    10       1.0
    11       1.0
    12       1.0
    13       1.0
    14       0.0
    15       1.0
    16       0.0
    17       1.0
    18       1.0
    19       1.0

Any suggestions on how to fix this?"
13098,TensorFlow creates different node name for execution,"-----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
TF v1.3
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Tesla P40 
- **Exact command to reproduce**:

### Describe the problem
This is the tracing result when I executed ptb-lstm application. I wonder the meaning of ""sequence_loss_by_example/add/_1077"". 
I wanted to create enqueue and dequeue ops for ""sequence_loss_by_example/add"", but dequeue op is never called, and different node name appears in the tracing file. 
Can you let me know what is happened inside session.run?

<img width=""439"" alt=""2017-09-17 18 42 36"" src=""https://user-images.githubusercontent.com/2465713/30519717-24a28542-9bd8-11e7-9f2c-511275ec059c.png"">"
13097,Support 64bit float point gradient,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.1.0
- **Python version**:  3.5.3
- **CUDA/cuDNN version**:Cuda 8.0 cudnn 5.1
- **GPU model and memory**:GTX 1080Ti 11GB

### Describe the problem
I use Tensorflow to train a multilayer Convolutional network.

Since my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.

I want to update parameters using bigger batch size, so I follow [the answer](https://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients) that is, accumulate and average gradients over multiple batches.

**Scenario 1**

If I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.

**Scenario 2**

If I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.

Theoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.

And guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.

I want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.

The obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D."
13096,No OpKernel was registered to support Op 'QuantizeV2',"I was able to build tf 1.3 quantize_graph on windows 10 64 using bazel 0.53 and cuda 8 (i could not build it without cuda). when i try to run it i get:
No OpKernel was registered to support Op 'QuantizeV2' with these attrs.


bazel-bin\tensorflow\tools\quantization\quantize_graph --input d:/export/saved_model.pb --output_node_names my_output --output d:/export/saved_model_quant.pb --mode=eightbit

2017-09-17 10:48:28.292364: W C:\tools\msys64\tmp\_bazel_user\8rsmy-kr\execroot\org_tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 10:48:28.292529: W C:\tools\msys64\tmp\_bazel_user\8rsmy-kr\execroot\org_tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1327, in _do_call
    return fn(*args)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1297, in _run_fn
    self._extend_graph()
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1358, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""C:\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=""MIN_FIRST""](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 1301, in <module>
    app.run()
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 1292, in main
    output_graph = rewriter.rewrite(FLAGS.output_node_names.split("",""))
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 420, in rewrite
    self.eightbitize_nodes_recursively(output_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  [Previous line repeated 90 more times]
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 632, in eightbitize_nodes_recursively
    for n in quantize_weight_eightbit(current_node, b""MIN_FIRST""):
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 299, in quantize_weight_eightbit
    quint8_tensor = quantize_op[0].eval()
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\ops.py"", line 541, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\ops.py"", line 4085, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=""MIN_FIRST""](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]

Caused by op 'QuantizeV2', defined at:
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 1301, in <module>
    app.run()
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 1292, in main
    output_graph = rewriter.rewrite(FLAGS.output_node_names.split("",""))
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 420, in rewrite
    self.eightbitize_nodes_recursively(output_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 600, in eightbitize_nodes_recursively
    self.eightbitize_nodes_recursively(input_node)
  [Previous line repeated 90 more times]
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 632, in eightbitize_nodes_recursively
    for n in quantize_weight_eightbit(current_node, b""MIN_FIRST""):
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\tools\quantization\quantize_graph.py"", line 298, in quantize_weight_eightbit
    mode=quantization_mode)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\ops\gen_array_ops.py"", line 2381, in quantize_v2
    mode=mode, name=name)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""\\?\C:\Users\user\AppData\Local\Temp\Bazel.runfiles_5goewho4\runfiles\org_tensorflow\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=""MIN_FIRST""](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]
"
13095,About the issue: Ran out of GPU memory!,"HI every one, I wrote my first tensorflow code, it is a classic CNN. And when I run it on Ubuntu16. it report error!

2017-09-17 05:16:34.243946: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 05:16:34.243983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 05:16:34.244007: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 05:16:34.244017: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 05:16:34.244023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-09-17 05:16:35.679378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:89:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-09-17 05:16:36.490317: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xab5bff0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-09-17 05:16:36.492491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:8a:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-09-17 05:16:36.496233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2017-09-17 05:16:36.496258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2017-09-17 05:16:36.496267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2017-09-17 05:16:36.496282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0)
2017-09-17 05:16:36.496353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0)
step 0, train accuracy 0
step 100, train accuracy 0.845555
step 200, train accuracy 0.80391
step 300, train accuracy 0.913599
step 400, train accuracy 0.893777
step 500, train accuracy 0.918844
step 600, train accuracy 0.937555
2017-09-17 05:16:41.502931: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-09-17 05:16:41.503008: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-09-17 05:16:41.503035: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-09-17 05:16:41.503060: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-09-17 05:16:41.503223: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-09-17 05:16:41.503268: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of GPU memory when allocating 0 bytes for 
	 [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape_2, Reshape_3)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""check_cnn.py"", line 228, in <module>
    TrainNetwork()
  File ""check_cnn.py"", line 147, in TrainNetwork
    sess.run(train_step, feed_dict = {x: xs, y_: ys, keep_prob: 0.5})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of GPU memory when allocating 0 bytes for 
	 [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape_2, Reshape_3)]]

Caused by op 'SoftmaxCrossEntropyWithLogits', defined at:
  File ""check_cnn.py"", line 228, in <module>
    TrainNetwork()
  File ""check_cnn.py"", line 128, in TrainNetwork
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = y_)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 1597, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2385, in _softmax_cross_entropy_with_logits
    features=features, labels=labels, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): Ran out of GPU memory when allocating 0 bytes for 
	 [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape_2, Reshape_3)]]

The system is ubuntu16, tensorflow 1.2, CUDA 8, cudnn 6.
the machine is:

Sun Sep 17 06:30:45 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |
| N/A   39C    P0    34W / 300W |      0MiB / 16276MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |
| N/A   38C    P0    32W / 300W |      0MiB / 16276MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

So please help me for this, thanks!"
13094,Feature Request - SRU (Simple Recurrent Unit) Cell,"ArXiv path: https://arxiv.org/abs/1709.02755

This is a recently proposed, parallelization-friendly RNN cell. The author released his own PyTorch version of the SRU. We are looking forward to an offical tensorflow implementation with Cudnn optimizations. "
13093,"i want to run a batch of images using the label_images example,but this example is only a image,not a batch of image,what should i do,i can't find other examples to refer.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13092,Errror in creatinig training and inference logits,"I am using ubuntu 16.04
version of tensorflow is 1.3.0
installed and validated using anaconda
While trying to create training and inference logits of input[32] of the following python notebook
https://github.com/Currie32/Chatbot-from-Movie-Dialogue/blob/master/Chatbot_Attention.ipynb

I was getting errors while executing the following command:

train_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questions_vocab_to_int)

Stack trace:
Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""<stdin>"", line 6, in seq2seq_model
  File ""<stdin>"", line 10, in encoding_layer
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 405, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 598, in dynamic_rnn
    dtype=dtype)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 761, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2775, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2604, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2554, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 744, in _time_step
    skip_conditionals=True)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 236, in _rnn_step
    new_output, new_state = call_cell()
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 732, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 180, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 450, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 938, in call
    cur_inp, new_state = cell(cur_inp, cur_state)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 774, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 180, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 450, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 401, in call
    concat = _linear([inputs, h], 4 * self._num_units, True)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1039, in _linear
    initializer=kernel_initializer)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 360, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1405, in wrapped_custom_getter
    *args, **kwargs)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in _rnn_get_variable
    variable = getter(*args, **kwargs)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in _rnn_get_variable
    variable = getter(*args, **kwargs)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 664, in _get_single_variable
    name, """".join(traceback.format_list(tb))))
ValueError: Variable bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""<stdin>"", line 10, in encoding_layer
  File ""<stdin>"", line 6, in seq2seq_model
  File ""<stdin>"", line 2, in <module>
"
13091,Tensorflow not detecting my GPU ,"hi @oelmekki @yaroslavvb did you managed to resolve the issue?
I have the same problem able to use GPU before updating tensorflow to V1.3.0. I have also upgraded my Cudnn to V6. My CUDA is v8.0 so I don't seem to understand where the problem is coming from. I can verify that my tensorflow is GPU version because I used tfBInaryUrl for Python2.7-PGU support. Aside this, I have also installed several times with 'pip install tensorflow-gpu' and I still cannot run my codes on gpu. Theano works fine and I could run code with GPU if I use theano. 

When I tried to force the computation to be run on GPU , my codes wouldn't run and now, I got this mesage ""Device mapping: no known devices.

The frustrating thing is that I was using the GPU before I upgraded to v1.3.0/

I would appreciate any help as regards this problem.
"""
13090,Session creation silently failing on iOS when loading a SavedModel,"## Issue:

I am trying to integrate a TensorFlow solution into my iOS apps, but inference doesn't seem to work when I try to run simple graphs created in Python. In fact, the `tensorflow_inception_graph` from the examples is the only graph that seems to work with iOS inference. Every other inference attempt is met with the following error:

> Invalid argument: Session was not created with a graph before Run()!

So what I'm finding is that on mobile if we try to run a canned neural network like the `tensorflow_inception_graph`, inference works perfectly but if we try to run any kind of custom model like 1 + 1 = 2, the graph won't run.

To demonstrate this, I used Python to write and export to a `SavedModel` the simplest graph I can think of: it just adds 1 + 1:

```
g = tf.Graph()
with g.as_default():
    output = tf.add(tf.constant(1), 1, name=""output_tensor"")

builder = tf.saved_model.builder.SavedModelBuilder('/path/to/export')
with tf.Session(graph=g) as sess:
    builder.add_meta_graph_and_variables(sess, tags=[])

builder.save()
```

Now to test that inference is actually possible, I reload the `saved_model.pb` into Python:

```
with tf.Session() as sess:
    tf.saved_model.loader.load(sess, [], '/path/to/export')

    print(sess.run('output_tensor:0')) // prints 2
```

Following in the footsteps of the `simple` example located at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios, I go to perform inference on iOS using the static C++ library built using the makefile and the instructions provided at the link.

```
using namespace tensorflow;
using namespace std;
using namespace ::google::protobuf;
using namespace ::google::protobuf::io;

SessionOptions options;
Session* session_pointer = nullptr;
Status session_status = NewSession(options, &session_pointer);

cout << session_status.ToString() << endl; // prints OK

unique_ptr<Session> session(session_pointer);
    
GraphDef model_graph;
NSString* model_path = FilePathForResourceName(@""saved_model"", @""pb"");
PortableReadFileToProto([model_path UTF8String], &model_graph);
    
Status session_init = session->Create(model_graph);

cout << session_init.ToString() << endl; // prints OK, proves the graph was indeed created before run

vector<Tensor> outputs;
Status session_run = session->Run({}, {""output_tensor:0""}, {}, &outputs);

cout << session_run.ToString() << endl; // prints Invalid argument: Session was not created with a graph before Run()!
```

## Troubleshooting:

I've tried rebuilding multiple times from `1.3.0` (latest, nightly), `1.2.1` and tried using the `TensorFlow-experimental` Pod. (I also tried using the `TensorFlow-iOS` pod, but it seems to be empty.)

There are (unanswered) Stack Overflow questions that refer to this issue occurring on both platforms: 

- **Android**: https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not

- **iOS**: https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c

On GitHub, there's been several issues related to this reported over the last year: [#7088](https://github.com/tensorflow/tensorflow/issues/7088), [#5553](https://github.com/tensorflow/tensorflow/issues/5553), [#3480](https://github.com/tensorflow/tensorflow/issues/3480), [#6806](https://github.com/tensorflow/tensorflow/issues/6806), and [#3352](https://github.com/tensorflow/tensorflow/issues/3352). None of the resolutions (if any) for these issues provide a concrete answer of how to get around this problem. 

However, found in the comments of [#3480](https://github.com/tensorflow/tensorflow/issues/3480) , @petewarden's [post](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/) about using `optimize_for_inference`, `quantize_graph`, and `convert_graphdef_memmapped_format` on the model before running inference present a possible solution. So I ran those three commands and got the same error:

> google.protobuf.message.DecodeError: Truncated message.

Since we can perform successful inference on the model in Python (as shown above), the possibility of the model being corrupt (as @petewarden suggested in [#3002](https://github.com/tensorflow/tensorflow/issues/3002)) is ruled out. Therefore, the above _""Truncated Error""_ message may point to where the problem is on mobile: Unsuccessful parsing of general models stored in protobuf files.

## Closing thoughts on issue:
The point is this: The error message _""Invalid argument: Session was not created with a graph before Run()!""_ doesn't provide any information to me regarding what should be debugged in my implementation. Additionally, it appears to not even point to the source of the error as I have verified three things:

1. All arguments passed to the session and graph are valid (_Invalid Argument_)
2. Session was created with a graph before Run() and the `Status` was reported to be `OK` (_Session was not created with a graph before Run()!_)
3. The error is likely triggered due to unsuccessful parsing on mobile devices of custom models serialized in protobuf files

## Library Versions:
**TensorFlow Python version**: `('v1.3.0-rc2-20-g0787eee', '1.3.0')`
**TensorFlow C++ version**: 1.3.0, built using `build_all_ios.sh`. Also tried with version 1.2.1 and `TensorFlow-experimental` pod

## System information:

```
== cat /etc/issue ===============================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.1.0.post1)
tensorflow (1.3.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
"
13087,Deallocation messages broken,"The only way to calculate peak memory in OSS TensorFlow is to make memory timeline by parsing allocation/deallocation messages (possibly using helpers like [this](github.com/yaroslavvb/memory_util))

Sometime between TF 1.0.1 and TF 1.1, the deallocation messages stopped including allocation id, so you can't track when each tensor got deallocated. 

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = ''
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import tensorflow as tf
import numpy as np

sess = tf.Session()
a = tf.ones((1000,1000))
sess.run(a)
```

In TF 1.0.1 you see this
`2017-09-16 13:24:28: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 1 allocator_name: ""cpu"" }
`

In latest TF you see this

`2017-09-16 13:25:04.422108: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
`

Note that deallocation id is missing in later version

cc @panyx0718 who mentioned ongoing work for tracking memory deallocation"
13086,"Help completely removing Tensorflow, pip and virtualenv","Hey there,

I am new to all the tf and Python programming and have installed tensorflow through pip and virtualenv but now I read that in order to use Spyder for Python it is best to use Anaconda. I know that tf can be installed through conda as well but how do I go about it now? Do I have to completely remove the existing installations first and if yes, can someone explain in detail which and how I can do it?

Thanks in advance"
13083,mismatch between the reported values of tt.nn.zero_fraction,"Has anyone run into this issue before? 

[tt.nn.zero_fraction](https://www.tensorflow.org/api_docs/python/tf/nn/zero_fraction), defined in [nn_imply.py](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/nn_impl.py) reports my convs layers in [MobileNet_v1 slim](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py) net have around 0.5 sparsity (conv1-conv13). However, when I freeze the model's weights and look at them, there is none zero values:

```
>>>conv=sess.run('MobilenetV1/Logits/Conv2d_1c_1x1/weights/read:0')
>>>numpy.count_nonzero(conv)
5120
>>>numpy.count_nonzero(conv==0)
0

```

Looking at the histogram, there are many weights' values close to zero. Does that function do approximation? Can anyone verify this?"
13082,How can I change value by threshold in tensor?,"When I do some 2-class classify practices, I use sigmoid as output layer, and it return a value in [0,1], but I want 1 if the value greater than 0.5,else set the value 0. I can't find a function to finish it.  Thank you for help. 
Just like:
a=tf.constant([0.1,0.2,0.6,.0.7])--->turn to ---->a=tf.constant([0,0,1,1])
"
13081,"i m getting this error while installing tensorflow gpu ,have tried everything help me plezz","Traceback (most recent call last):
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""H:/tst.py"", line 1, in <module>
    import tensorflow
  File ""C:\python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems
"
13080,Incorrect protobuf SHA in workspace.bzl for v1.3.0,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.0 (the tagged version, specifically `9e76bf32`)
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 0.5.4
- **Exact command to reproduce**: `./configure` (with all the default answers, and the specified python version above), then `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem

When attempting to build the pip package in the repository on the latest tagged release (v1.3.0), I get the following fatal build error:

`ERROR: /home/ubuntu/tensorflow/tools/pip_package/BUILD:100:1: no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to /home/ubuntu/.cache/bazel/_bazel_whitlock/8ea56cff279f39ec6c0003641e649819/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/tools/pip_package:licenses'`

It looks to me like the SHA changed for that version was not specified correctly. However, this seems strange to me because I had built against this version within the last couple of weeks without issue. `master` does not have this problem.

Replacing the SHA in `tensorflow/workspace.bzl` with the one in the error message (`e5fdeee`...) solves this issue."
13079,saved model load method support for android,"I am trying load a saved model on android with java api.

 Session session = SavedModelBundle.load(modelDir, ""serve"").session();


Its works on PC. But on android i  got this error message.

E/tensorflow: CameraActivity: Exception!
                                                                 java.lang.UnsupportedOperationException: Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you
                                                                     at org.tensorflow.SavedModelBundle.load(Native Method) 
my reference model for training procedure is  tf estimator for iris data.
https://www.tensorflow.org/get_started/estimator

"
13078,quantized_conv3d and quantized_pool3d features,"Now I am trying to implement 3D CNN on FPGA.
Do you have plan to support quantized_conv3d and quantized_pool3d features these two months?
if yes, when will it be released?

Cheers~
"
13076,unit test tools print all failure logs if failed,"When a PR is tested failed, Jenkins just reports failure and tells where to check log. However, at least for me, I don't know whether I have the permission, and where to find the logs. So I have to reproduce the failed test on my own machine, but maintaining all kinds of environment is an unnecessary burden for each developer (cpu, gpu, py2, py3, window...)

```bash
//tensorflow/tools/test:check_futures_test                               PASSED in 1.3s
//tensorflow/user_ops:ackermann_test                                     PASSED in 2.3s
//tensorflow/user_ops:duplicate_op_test                                  PASSED in 2.2s
//tensorflow/user_ops:invalid_op_test                                    PASSED in 2.1s
//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT in 65.0s
  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log

Executed 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.
```

It will be more convenient to attach total failure content at the end for developers to debug, like reports of `py.test`:

```bash
$ pytest
======= test session starts ========
platform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.y
rootdir: $REGENDOC_TMPDIR, inifile:
collected 1 item

test_sample.py F

======= FAILURES ========
_______ test_answer ________

    def test_answer():
>       assert inc(3) == 5
E       assert 4 == 5
E        +  where 4 = inc(3)

test_sample.py:5: AssertionError
======= 1 failed in 0.12 seconds ========
```

"
13075,distributed Tensorflow assign device issue,"I recently found an interesting issue on device assignment when I ran the following simple code `test.py`:

```
import argparse
import tensorflow as tf

CLUSTER_SPEC = {""ps"": [""localhost:2222""],
                ""worker"": [""localhost:1111"", ""localhost:1112""]}


def parse_command_arguments():
    """""" Set up and parse the command line arguments. """"""
    parser = argparse.ArgumentParser(
        description=""Parameters and Arguments for the Test."")
    parser.add_argument(
        ""--job_name"", type=str, default="""", help=""One of 'ps', 'worker'"")
    parser.add_argument(
        ""--task_index"", type=int, default=0, help=""Index of task"")
    return parser.parse_args()


def start_server(job_name, task_index, tf_config):
    """""" Create a server based on a cluster spec. """"""
    cluster = tf.train.ClusterSpec(CLUSTER_SPEC)
    server = tf.train.Server(
        cluster, config=tf_config, job_name=job_name, task_index=task_index)
    return server, cluster


def model(cluster=None, worker_device=None):
    """""" Build up a simple estimator model. """"""
    with tf.device(tf.train.replica_device_setter(
            worker_device=worker_device, cluster=cluster)):
        W = tf.Variable([1.0], tf.float32)
        b = tf.Variable([10.0], tf.float32)
        x = tf.placeholder(tf.float32)
        y = W * x + b
    return W, b, x, y


if __name__ == ""__main__"":
    arguments = parse_command_arguments()
    job_name = arguments.job_name
    task_index = arguments.task_index
    # Set up tensorflow configuration.
    tf_config = tf.ConfigProto(
        allow_soft_placement=True, device_count={'GPU': 1},
        log_device_placement=True)
    # Start a server.
    server, cluster = start_server(job_name, task_index, tf_config)

    if job_name == ""ps"":
        server.join()
    else:
        worker_device = ""/gpu:0""
        W, b, x, y = model(cluster, worker_device=worker_device)
        is_chief = (arguments.task_index == 0) and (
            arguments.job_name == 'worker')
        sess = tf.train.MonitoredTrainingSession(
             master=server.target, is_chief=is_chief, config=tf_config)
        # Run the model.
        step = 0
        x_train = [1, 2, 3, 4]
        y_train = [0, -1, -2, -3]
        while not sess.should_stop() and step < 1000:
            sess.run([y], {x: x_train, y: y_train})
            step += 1
```
As the code uses `tf.train.replica_device_setter()` with `worker_device=""/gpu:0""`, I imagine all tensorflow `Variable` ops go to the parameter server, while all other type of ops stay in the worker.

However, if we run the above code:
`$ python test.py --job_name ps --task_index 0`
`$ python test.py --job_name worker --task_index 0`
`$ python test.py --job_name worker --task_index 1`
I notice that all tensorflow ops go to parameter server, for example in the worker log: 
`report_uninitialized_variables/boolean_mask/strided_slice/stack_2: (Const)/job:ps/replica:0/task:0/gpu:0`.

But if I change `worker_device=""/gpu:0""` in the code to `worker_device=""/job:worker/task:0/gpu:0""`, then the result is correct (all non-Variable ops go to worker). So does it mean there is a bug for `tf.train.replica_device_setter()` to automatically assign devices?

P.S The above code runs on single GPU (TITAN X (Pascal)/PCIe/SSE2) with Ubuntu 16.04, Python 3.5 and Tensorflow v1.2"
13069,Get stuck in the process of building from sources,"## System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.3 CPU
Bazel version (if compiling from source): 5.0
Exact command to reproduce: 

`
RUN tensorflow/tools/ci_build/builds/configured CPU \
    bazel build -c opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
        tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl 
`



## Describe the problem

I tried to build a docker image using the dockerfile provided by tensorflow repository, but every time I faced with the problem it got stuck in the process, repeating the output 

`
____[2,615 / 3,437] Still waiting for 2 jobs to complete:
      Running (standalone):
        Compiling tensorflow/core/kernels/matrix_solve_ls_op.cc, 851 s
        Compiling tensorflow/core/kernels/svd_op_double.cc, 839 s
____[2,615 / 3,437] Still waiting for 2 jobs to complete:
`


The following is the first output of this layer which may be helpful.

`
Extracting Bazel installation...
You have bazel 0.5.0 installed.
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
Using python library path: /usr/local/lib/python2.7/dist-packages
No MKL support will be enabled for TensorFlow
jemalloc enabled
No VERBS support will be enabled for TensorFlow
No OpenCL support will be enabled for TensorFlow
MPI support will not be enabled for TensorFlow
Configuration finished
/tensorflow
INFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch
TF_BUILD_INFO = {container_type: ""cpu"", command: ""bazel build -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 tensorfl
s/pip_package:build_pip_package"", source_HEAD: ""b46340f40fe5e2ec9bfcd385b07cfb914055fb51"", source_remote_origin:
//github.com/tensorflow/tensorflow.git"", OS: ""Linux"", kernel: ""4.9.41-moby"", architecture: ""x86_64"", processor: ""
5550M APU with Radeon(tm) HD Graphics"", processor_count: ""2"", memory_total: ""2027780 kB"", swap_total: ""1048572 kB
l_version: ""Build label: 0.5.0"", Java_version: ""1.8.0_131"", Python_version: ""2.7.12"", gpp_version: ""g++ (Ubuntu 5
buntu1~16.04.5) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: """", CUDA_device_count: ""0"", CUDA_device
 """", CUDA_toolkit_version: """"}
`"
13068,cuDNN 6 incompatible with Tensorflow 1.3 error,"Any help is greatly appreciated! I've spent way too much time on this. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
In tensorflow/stream_executor/cuda/cuda_gpu_executor.cc in function ""static int TryToReadNumaNode(conststring &pci_bus_id,intdevice_ordinal)"" added the following lines at the beginning of the function:
LOG(INFO) << ""ARM has no NUMA node, hardcoding to return zero"";
return 0;
modified workspace.bzl: set eigen_archive to use http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/d781c1de9834.tar.gz because of an error that kept coming up related to the current version of eigen.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
 Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.3.0
command used: git checkout v1.3.0

- **Python version**: 
2.7

- **Bazel version (if compiling from source)**:
0.4.5

- **CUDA/cuDNN version**:
CUDA version 8.0, CUDNN version 6.0.21
cudnn was downloaded from the NVIDIA website using the cuDNN v6.0 Library for Linux version

- **GPU model and memory**:
NVIDIA Pascal GPU on the TX2

- **Exact command to reproduce**:
sudo bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

- **Other*:
Using an 8GB swapfile
CUDA install path: usr/local/cuda/
CUDNN install paths:
     include: /usr/include/
     libs: /usr/lib/aarch64-linux-gnu/
     I used the following command on the libs: sudo chmod a+r /usr/lib/aarch64-linux-gnu/libcudnn*


### Describe the problem
When I try to install tensorflow 1.3.0 I get the error listed below. It goes through pretty much the entire build and fails at the very end. Tensorflow 1.0 installs just fine on the TX2 using cuda 8 and cudnn 5.1(these lib are no longer on my machine so its not an issue with having 5.1 installed). I would use Tensoflow 1.0, but the network I am working with has a reliance on 1.3. 

### Source code / logs
IERROR: /home/nvidia/tensorflow/tensorflow/python/BUILD:2762:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command 
  (cd /home/nvidia/.cache/bazel/_bazel_root/d2751a49dacf4cb14a513ec663770624/execroot/tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.2 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=6.0.21 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/local_linux-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Wl,--version-script tensorflow/tf_version_script.lds -Wl,-z,muldefs -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/local_linux-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/usr/bin/ld: skipping incompatible bazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible bazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible /usr/lib/gcc/aarch64-linux-gnu/5/../../../aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible /usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: skipping incompatible //usr/lib/aarch64-linux-gnu/libcudnn.so.6 when searching for -l:libcudnn.so.6
/usr/bin/ld: cannot find -l:libcudnn.so.6
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 5.977s, Critical Path: 4.15s
"
13066,r1.3 branch - encountered error attempting build on Pi 2,"OS is Ubuntu Mate 16.04 .. cannot quite decipher the problem yet.  Might try the Pi2 specific build line next.

### System information
```
== cat /etc/issue ===============================================
Linux mepi-desktop 4.4.38-v7+ #938 SMP Thu Dec 15 15:22:21 GMT 2016 armv7l armv7l armv7l GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux mypi-desktop 4.4.38-v7+ #938 SMP Thu Dec 15 15:22:21 GMT 2016 armv7l armv7l armv7l GNU/Linux

== check pips ===================================================
numpy (1.11.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found

== cuda libs  ===================================================

```

### Describe the problem
Attempting to following compilation instructions for RPi, getting error on `make -f ...`

https://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/contrib/makefile

```
tensorflow/contrib/makefile/download_dependencies.sh
sudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8
cd tensorflow/contrib/makefile/downloads/protobuf/
./autogen.sh
./configure
make
sudo make install
sudo ldconfig  # refresh shared library cache
cd ../../../../..

make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=""-Os"" CXX=g++-4.8
```

### Source code / logs
```
remote_fused_graph_execute_op.cc:(.text._ZN10tensorflow25RemoteFusedGraphExecuteOpC2EPNS_20OpKernelConstructionE[_ZN10tensorflow25RemoteFusedGraphExecuteOpC5EPNS_20OpKernelConstructionE]+0x198): undefined reference to `google::protobuf::internal::fixed_address_empty_string'
/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSlice*)':
saved_tensor_slice.pb_text.cc:(.text+0x630): undefined reference to `google::protobuf::internal::fixed_address_empty_string'
/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSliceMeta*)':
saved_tensor_slice.pb_text.cc:(.text+0x136c): undefined reference to `google::protobuf::internal::fixed_address_empty_string'
/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(memmapped_file_system.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::MemmappedFileSystemDirectoryElement*)':
memmapped_file_system.pb_text.cc:(.text+0x354): undefined reference to `google::protobuf::internal::fixed_address_empty_string'
/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saver.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SaverDef*)':
saver.pb_text.cc:(.text+0x698): undefined reference to `google::protobuf::internal::fixed_address_empty_string'
/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(cluster.pb_text.o):cluster.pb_text.cc:(.text+0x6e4): more undefined references to `google::protobuf::internal::fixed_address_empty_string' follow
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:552: recipe for target '/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed
make: *** [/home/mypi/Dev/git/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1
```"
13065,maxpooling error while building tf_label_image_example,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
   No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
   Windows 10 (64 bit)
- **TensorFlow installed from (source or binary)**:
   Using CMake and instructions present at (tensorflow/tensorflow/contrib/cmake)
- **Python version**: 
   3.6.1
- **CUDA/cuDNN version**:
   CUDA-8.0.61
   cuDNN-5.1
- **GPU model and memory**:
   NVIDIA GeForce GTX TITAN X (382.05)
- **Exact command to reproduce**:
   MSBuild /p:Configuration=Release tf_label_image_example.vcxproj

### Describe the problem
i am facing this error while trying to build the project tf_label_image_example. I was able to build this project without GPU support, but, while building with GPU support, i am facing the following error.

""C:\Users\alalwani\tensorflow\tensorflow\contrib\cmake\build\tf_label_image_example.vcxproj"" (default target) (1) ->
(Link target) ->
maxpooling_op.obj : error LNK2001: unresolved external symbol ""public: bool __cdecl tensorflow::functor::MaxPoolForwardWithOptionalArgmax::operator()(struct Eigen::QInt8 const *,int,int,int,int,int,int,int,int,int
,int,int,int,struct Eigen::QInt8 *,__int64 *,struct Eigen::GpuDevice const &)"" (??R?$MaxPoolForwardWithOptionalArgmax@UQInt8@Eigen@@@functor@tensorflow@@QEAA_NPEBUQInt8@Eigen@@hhhhhhhhhhhhpeau34@PEA_JAEBUGpuDevice@4@@z) [C:\Users\alalwa
ni\tensorflow\tensorflow\contrib\cmake\build\tf_label_image_example.vcxproj]
C:\Users\alalwani\tensorflow\tensorflow\contrib\cmake\build\Release\tf_label_image_example.exe : fatal error LNK1120: 1 unresolved externals [C:\Users\alalwani\tensorflow\tensorflow\contrib\cmake\build\tf_label_image_example.vcxproj]"
13064,Variance update in tf.contrib.layer.batch_norm,"Hi,

In the current implementation of batch norm, the variance to be used for update to moving variance is computed by centering the data around the current mean. The more proper way would be to compute the variance around the moving mean. With the current method, the moving variance will depend upon the batch size. With the updated method, the batch size will have less influence on the moving variance.

Thanks,
Mayank"
13063,XLA representation of batch normalization has changed since TF 1.3,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source (commit 1f19b8c)
- **TensorFlow version (use command below)**: ('v1.3.0-rc1-2173-g36813d5', '1.4.0-dev')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: Build label: 0.5.3
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: cannot provide

### Describe the problem
I use Tensorflow compiled from sources (last commit 1f19b8c) with XLA JIT compilation enabled. My **inference** network uses batch normalization. When I visualize the HLO cluster dumped into the dot file with graphviz I see that batch normalization operation is translated differently in TF 1.3 and TF master. In the master branch the result is like this:
![tf_1 3](https://user-images.githubusercontent.com/30651952/30482986-d53c9480-9a2d-11e7-8e8e-b5a48d5e95a2.png)
But in the 1.3 branch all these instructions were merged into a single broadcast with a constant parameter. Looks like the translation behavior has been changed by commit 7359fec. This introduces a kind of regression since these instructions can be pre-calculated because the input is constant (as it was done in 1.3).

### Source code / logs
In order to reproduce the issue run the Inception V3 network from slim models zoo with variables converted to constants and marked with XLA compile flag.
"
13062,inline assembly requires more registers than available while cross compiling tensroflow for arm cortex a15 using clang-3.8,"I am trying to cross compile tensorflow for armv7a cortex a15 using clang-3.8 and I am using bazel 0.5.4 to build tensorflow.

My Environment details are, Ubuntu - 16.04 Clang - 3.8 bazel - 0.5.4 tensorflow source code version - 1.3 target cpu - cortex a15 compiler_flag: ""-O2""

The command i am using is 
bazel build --copt=-Wno-c++11-narrowing --crosstool_top=//tools/arm_compiler:toolchain --cpu=armeabi-v7a --verbose_failures --sandbox_debug //tensorflow/tools/pip_package:build_pip_package


The error is external/gemmlowp/meta/transform_kernels_arm_32.h:5506:7: error: inline assembly requires more registers than available ""ldr r0, %[input_range_min]\n""

The compiler flags are configured as bellow in CROSSTOOL

 toolchain {
      abi_version: ""clang_3.8""
      abi_libc_version: ""glibc_2.19""
      builtin_sysroot: """"
      compiler: ""clang""
      host_system_name: ""armeabi-v7a""
      needsPic: true
      supports_gold_linker: false
      supports_incremental_linker: false
      supports_fission: false
      supports_interface_shared_objects: false
      supports_normalizing_ar: true
      supports_start_end_lib: false
      supports_thin_archives: true
      target_libc: ""glibc_2.19""
      target_cpu: ""armeabi-v7a""
      target_system_name: ""arm_a15""
      toolchain_identifier: ""clang_linux_armhf""

      tool_path { name: ""ar"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-ar"" }
      tool_path { name: ""compat-ld"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-ld"" }
      tool_path { name: ""cpp"" path: ""linaro_linux_gcc/clang_bin/clang"" }
      tool_path { name: ""dwp"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-dwp"" }
      tool_path { name: ""gcc"" path: ""linaro_linux_gcc/clang_bin/clang"" }
      tool_path { name: ""gcov"" path: ""arm-frc-linux-gnueabi/arm-frc-linux-gnueabi-gcov-4.9"" }
      # C(++) compiles invoke the compiler (as that is the one knowing where
      # to find libraries), but we provide LD so other rules can invoke the linker.
      tool_path { name: ""ld"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-ld"" }
      tool_path { name: ""nm"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-nm"" }
      tool_path { name: ""objcopy"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-objcopy"" }
      objcopy_embed_flag: ""-I""
      objcopy_embed_flag: ""binary""
      tool_path { name: ""objdump"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-objdump"" }
      tool_path { name: ""strip"" path: ""linaro_linux_gcc/arm-linux-gnueabihf-strip"" }

      compiler_flag: ""-target""
      compiler_flag: ""armv7a-arm-linux-gnueabihf""
      compiler_flag: ""--sysroot=external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc""
      compiler_flag: ""-mfloat-abi=hard""
      compiler_flag: ""-mcpu=cortex-a15""
      compiler_flag: ""-mfpu=neon-vfpv4""

      compiler_flag: ""-nostdinc""
      compiler_flag: ""-isystem""
      compiler_flag: ""/usr/lib/clang/3.8/include""
      compiler_flag: ""-isystem""
      compiler_flag: ""external/linaro_linux_gcc_repo/lib/gcc/arm-linux-gnueabihf/4.9.3/include""
      compiler_flag: ""-isystem""
      compiler_flag: ""external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/include""
      compiler_flag: ""-isystem""
      compiler_flag: ""external/linaro_linux_gcc_repo/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed""
      compiler_flag: ""-isystem""
      compiler_flag: ""external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/include""
      cxx_flag: ""-isystem""
      cxx_flag: ""external/linaro_linux_gcc_repo/arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf""
      cxx_flag: ""-isystem""
      cxx_flag: ""external/linaro_linux_gcc_repo/arm-linux-gnueabihf/include/c++/4.9.3""
      cxx_flag: ""-isystem""
      cxx_flag: ""external/linaro_linux_gcc_repo/include/c++/4.9.3/arm-linux-gnueabihf""
      cxx_flag: ""-isystem""
      cxx_flag: ""external/linaro_linux_gcc_repo/include/c++/4.9.3""

      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//include)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/usr/include)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/usr/lib/include)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//include)%/c++/4.9.3""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/libc/lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//lib/gcc/arm-linux-gnueabihf/4.9.3/include)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//lib/gcc/arm-linux-gnueabihf/4.9.3/include-fixed)%""
      cxx_builtin_include_directory: ""%package(@linaro_linux_gcc_repo//arm-linux-gnueabihf/include)%/c++/4.9.3""
      cxx_builtin_include_directory: ""/usr/lib/clang/3.8/include""
      cxx_builtin_include_directory: ""/usr/lib/llvm-3.8/lib/clang/3.8.0/include""
      cxx_builtin_include_directory: ""/home/gopinathr/Documents/work/tf1.2/tensorflow/bazel-out/clang_linux_armhf-opt/genfiles/external/local_config_python/python_include/arm-linux-gnueabihf/python2.7""

      cxx_flag: ""-std=c++0x""
      cxx_flag: ""-std=c++11""
      linker_flag: ""-target""
      linker_flag: ""armv7a-arm-linux-gnueabihf""
      linker_flag: ""--sysroot=external/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc""
      linker_flag: ""-lstdc++""
      linker_flag: ""-Ltools/arm_compiler/linaro_linux_gcc/clang_more_libs""
      linker_flag: ""-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/lib""
      linker_flag: ""-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/lib""
      linker_flag: ""-Lexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/libc/usr/lib""
      linker_flag: ""-Bexternal/linaro_linux_gcc_repo/arm-linux-gnueabihf/bin""
      linker_flag: ""-Wl,--dynamic-linker=/lib/ld-linux-armhf.so.3""

      # Anticipated future default.
      # This makes GCC and Clang do what we want when called through symlinks.
      unfiltered_cxx_flag: ""-no-canonical-prefixes""
      linker_flag: ""-no-canonical-prefixes""

      # Make C++ compilation deterministic. Use linkstamping instead of these
      # compiler symbols.
      unfiltered_cxx_flag: ""-Wno-builtin-macro-redefined""
      unfiltered_cxx_flag: ""-D__DATE__=\""redacted\""""
      unfiltered_cxx_flag: ""-D__TIMESTAMP__=\""redacted\""""
      unfiltered_cxx_flag: ""-D__TIME__=\""redacted\""""

      # Security hardening on by default.
      # Conservative choice; -D_FORTIFY_SOURCE=2 may be unsafe in some cases.
      # We need to undef it before redefining it as some distributions now have
      # it enabled by default.
      compiler_flag: ""-U_FORTIFY_SOURCE""
      compiler_flag: ""-fstack-protector""
      compiler_flag: ""-fPIE""
      linker_flag: ""-pie""
      linker_flag: ""-Wl,-z,relro,-z,now""

      # Enable coloring even if there's no attached terminal. Bazel removes the
      # escape sequences if --nocolor is specified.
      compiler_flag: ""-fdiagnostics-color=always""

        # All warnings are enabled. Maybe enable -Werror as well?
      compiler_flag: ""-Wall""
      # Enable a few more warnings that aren't part of -Wall.
      compiler_flag: ""-Wunused-but-set-parameter""
      # But disable some that are problematic.
      compiler_flag: ""-Wno-free-nonheap-object"" # has false positives

      # Keep stack frames for debugging, even in opt mode.
      compiler_flag: ""-fno-omit-frame-pointer""

      # Stamp the binary with a unique identifier.
      linker_flag: ""-Wl,--build-id=md5""
      linker_flag: ""-Wl,--hash-style=gnu""

      compilation_mode_flags {
        mode: DBG
        # Enable debug symbols.
        compiler_flag: ""-g""
      }
      compilation_mode_flags {
        mode: OPT

        # No debug symbols.
        # Maybe we should enable https://gcc.gnu.org/wiki/DebugFission for opt or
        # even generally? However, that can't happen here, as it requires special
        # handling in Bazel.
        compiler_flag: ""-g0""

        # Conservative choice for -O
        # -O3 can increase binary size and even slow down the resulting binaries.
        # Profile first and / or use FDO if you need better performance than this.
        compiler_flag: ""-O2""

        # Disable assertions
        compiler_flag: ""-DNDEBUG""

        # Removal of unused code and data at link time (can this increase binary size in some cases?).
        compiler_flag: ""-ffunction-sections""
        compiler_flag: ""-fdata-sections""

        #added by sachin
        compiler_flag: ""-v""

        linker_flag: ""-Wl,--gc-sections""
      }
    }

The file contents are external/gemmlowp/meta/transform_kernels_arm_32.h

template <>
inline void Transform1DKernel<uint8_t, int32_t, BiasAdd<uint8_t>, 16,
                              0>::Transform(const uint8_t* input,
                                            const BiasAdd<uint8_t>& params,
                                            int32_t* output) {
#ifdef DEBUG
#ifdef DEBUG_METAGEMM_VERBOSE
  std::cout << __FILE__ << ""("" << __LINE__
            << "") BiasAdd<uint8_t><uint8_t, int32_t, BiasAdd<uint8_t>, 16, ""
               ""0>::Transform()""
            << std::endl
            << std::flush;
#endif
#endif
  int params_rows_copy = params.rows;
  asm volatile(
      ""ldr r0, %[input_range_min]\n""
      ""vdup.32 q8, r0\n""
      ""ldr r0, %[input_range_scale]\n""
      ""vdup.32 q9, r0\n""
      ""ldr r0, %[bias_range_min]\n""
      ""vdup.32 q10, r0\n""
      ""ldr r0, %[bias_range_scale]\n""
      ""vdup.32 q11, r0\n""
      ""ldr r0, %[output_range_min]\n""
      ""vdup.32 q12, r0\n""
      ""ldr r0, %[one_over_output_range_scale]\n""
      ""vdup.32 q13, r0\n""
      ""ldr r0, %[output_range_offset]\n""
      ""vdup.32 q14, r0\n""
      ""1:""
      ""mov r0, %[count]\n""
      ""mov r1, %[bias]\n""
      ""2:""
      ""subs r0, r0, #16\n""

      // BiasAdd::Transform
      ""vld1.32 {d0, d1}, [%[input]]!\n""
      ""vld1.32 {d8, d9}, [r1]!\n""
      ""pld [%[input], #32]\n""
      ""vmovl.u8 q1, d1\n""
      ""vmovl.u8 q0, d0\n""
      ""vmovl.u8 q5, d9\n""
      ""vmovl.u8 q4, d8\n""
      ""vmovl.s16 q3, d3\n""
      ""vmovl.s16 q2, d2\n""
      ""vmovl.s16 q7, d11\n""
      ""vmovl.s16 q6, d10\n""
      ""vmovl.s16 q1, d1\n""
      ""vmovl.s16 q0, d0\n""
      ""vmovl.s16 q5, d9\n""
      ""vmovl.s16 q4, d8\n""
      ""vcvt.f32.s32 q0, q0\n""
      ""vcvt.f32.s32 q1, q1\n""
      ""vcvt.f32.s32 q2, q2\n""
      ""vcvt.f32.s32 q3, q3\n""
      ""vcvt.f32.s32 q4, q4\n""
      ""vcvt.f32.s32 q5, q5\n""
      ""vcvt.f32.s32 q6, q6\n""
      ""vcvt.f32.s32 q7, q7\n""
      ""vmul.f32 q0, q0, q9\n""
      ""vmul.f32 q1, q1, q9\n""
      ""vmul.f32 q2, q2, q9\n""
      ""vmul.f32 q3, q3, q9\n""
      ""vmul.f32 q4, q4, q11\n""
      ""vmul.f32 q5, q5, q11\n""
      ""vmul.f32 q6, q6, q11\n""
      ""vmul.f32 q7, q7, q11\n""
      ""vadd.f32 q0, q0, q8\n""
      ""vadd.f32 q1, q1, q8\n""
      ""vadd.f32 q2, q2, q8\n""
      ""vadd.f32 q3, q3, q8\n""
      ""vadd.f32 q4, q4, q10\n""
      ""vadd.f32 q5, q5, q10\n""
      ""vadd.f32 q6, q6, q10\n""
      ""vadd.f32 q7, q7, q10\n""
      ""vadd.f32 q0, q0, q4\n""
      ""vadd.f32 q1, q1, q5\n""
      ""vadd.f32 q2, q2, q6\n""
      ""vadd.f32 q3, q3, q7\n""
      ""vsub.f32 q0, q0, q12\n""
      ""vsub.f32 q1, q1, q12\n""
      ""vsub.f32 q2, q2, q12\n""
      ""vsub.f32 q3, q3, q12\n""
      ""vmul.f32 q0, q0, q13\n""
      ""vmul.f32 q1, q1, q13\n""
      ""vmul.f32 q2, q2, q13\n""
      ""vmul.f32 q3, q3, q13\n""
      ""vadd.f32 q0, q0, q14\n""
      ""vadd.f32 q1, q1, q14\n""
      ""vadd.f32 q2, q2, q14\n""
      ""vadd.f32 q3, q3, q14\n""
      ""vcvt.s32.f32 q0, q0\n""
      ""vcvt.s32.f32 q1, q1\n""
      ""vcvt.s32.f32 q2, q2\n""
      ""vcvt.s32.f32 q3, q3\n""

      ""vst1.32 {d0, d1, d2, d3}, [%[output]]!\n""
      ""vst1.32 {d4, d5, d6, d7}, [%[output]]!\n""
      ""pld [%[output]]\n""
      ""bne 2b\n""
      ""subs %[rows], %[rows], #1\n""
      ""bne 1b\n""
      : [input] ""+r""(input), [output] ""+r""(output)
      : [count] ""r""(params.count), [rows] ""r""(params_rows_copy),
        [output_range_offset] ""m""(params.output_range_offset),
        [input_range_scale] ""m""(params.input_range_scale),
        [one_over_output_range_scale] ""m""(params.one_over_output_range_scale),
        [bias_range_min] ""m""(params.bias_range_min),
        [output_range_min] ""m""(params.output_range_min),
        [bias_range_scale] ""m""(params.bias_range_scale),
        [bias] ""r""(params.bias), [input_range_min] ""m""(params.input_range_min)
      : ""r0"", ""r1"", ""d0"", ""d1"", ""d2"", ""d3"", ""d4"", ""d5"", ""d6"", ""d7"", ""d8"", ""d9"",
        ""d10"", ""d11"", ""d12"", ""d13"", ""d14"", ""d15"", ""d16"", ""d17"", ""d18"", ""d19"",
        ""d20"", ""d21"", ""d22"", ""d23"", ""d24"", ""d25"", ""d26"", ""d27"", ""d28"", ""d29"",
        ""cc"", ""memory"");
}

Does anyone can help me out with what is wrong??










"
13061,Proposal: Making the cmake build distribution friendly,"The following is a proposal, and I don't have it fully working yet, so a Pull Request is too early, and I want to gather some feedback. If an issue is not appropriate, let me know and I will look for a different medium.

### System information

Almost any Linux distribution.

### Describe the problem

Most Linux distributions have similar policies/limitations:

1. Not accepting bundling of system libraries (specially those security-sensitive).
2. Requiring building the whole from source and allowing patching the sources.
3. Requiring building offline (without Internet access).
4. Not having enough manpower to package hundred of packages in a dependency chain just to get packages like maven built in these source-chains environments.

Because of 4., cmake makes things easier. It is a build tool with a few dependencies that does not need complex bootstrapping. Building bazel itself will bring you into the Java maven dependency chain which we already proved in openSUSE to expand into hundred of packages.

Both the bazel and cmake build files do not play well with 1. 2. 3., but cmake already improves 4.

The cmake build files, also pull the sources of different projects. That is because the top `CMakeLists.txt` includes:

```cmake
set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/external)
```

into the load path.

that external/ directory is full of snippets like:

* `tensorflow/contrib/cmake/external/jpeg.cmake`
* `tensorflow/contrib/cmake/external/gif.cmake`
* ...
* etc

Those snippets use the [external project](https://cmake.org/cmake/help/v3.0/module/ExternalProject.html) cmake api to build directly from git or tarballs upstream. This clashes with
1., 2., 3.

It is a bit sad that things like curl used to be looked up in the system and developers deliberately bundled them without making the cmake snippet first look for it, and if not, configure the bundled one.

### Proposal

The proposal is to make the cmake build support both the Google/Mac user type of build with bundled sources, and the classical Linux distribution build.
This would be a gradual refactoring. Steps could be:

* Add to CMakeLists.txt an option:

```cmake
option(tensorflow_SYSTEM_LIBRARIES ""Use system libraries"" ON)
```

* Replicate the external/ directory as platform/ and conditionally make it use one or the other:

```cmake
if(tensorflow_SYSTEM_LIBRARIES)
 set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/platform)
else()
  set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/external)
endif()
```

So that then the part that does:

```cmake
# External dependencies
include(zlib)
include(gif)
include(png)
include(jpeg)
```

would just work...

* Replace incrementaly and one by one the $dep.cmake snippets in `platform/`.

Example with gif.cmake:

Using the standard `/usr/share/cmake/Modules/FindGIF.cmake` included in cmake, which may use `pkg-config` for some modules. The module itself documents it would then set: `GIF_LIBRARIES`, `GIF_INCLUDE_DIR`, etc.

Unfortunately the variable that the original set is called `gif_STATIC_LIBRARIES`, because it assumes it would be static. I think we could fix that later so that naming ends making sense, but lets not focus on that for now.

The `gif.cmake` I cited above would become a simple:

```cmake
find_package(GIF REQUIRED)
# dummy targets other targets depend on.
add_custom_target(gif)
# These can be removed if we fix CMakeLists
add_custom_target(gif_copy_headers_to_destination)

set(gif_STATIC_LIBRARIES ${GIF_LIBRARIES})
set(gif_INCLUDE_DIR ${GIF_INCLUDE_DIR})
```

And that is more or less enough to move forward with this dependency... repeat.
It may need some hacks also in the top level cmake files however...

I think this approach is doable, and could be turned into making the cmake build fully Linux distro enabled upstream. Slowly cleaning the naming, etc, removing these copy_headers targets so that the platform/ version of the .cmake files do not need to create dummy targets, etc.

- Similarly, the sqlite one becomes:

```cmake
find_package(SQLite3)
set(sqlite_STATIC_LIBRARIES ${SQLITE3_LIBRARIES})
set(sqlite_HEADERS ${SQLITE3_INCLUDE_DIR})
add_custom_target(sqlite)
add_custom_target(sqlite_copy_headers_to_destination)
```

With the only difference is that the `Find` module was not included in cmake, so I just copied it into
platform from https://raw.githubusercontent.com/LuaDist/libsqlite3/master/cmake/FindSQLite3.cmake somewhere.

* With this method I have been able to move forward and forward with the build.

### Current blockers

* cmake build does not work out of the box [PR](https://github.com/tensorflow/tensorflow/pull/12734)
* [Issue](https://github.com/tensorflow/tensorflow/issues/12018) with `Missing tensorflow/core/debug/debug_service.grpc.pb.h`

//cc @meaksh @dincamihai @ncounter"
13060,Pyinstaller with Tensorflow takes incorrect path for _checkpoint_ops.so file,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04, Pyinstaller
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: CUDA 8.0 cuDNN 6.0
- **GPU model and memory**: 3x nVidia GEForce 1080

### Describe the problem
As Tensorflow's `load_op_library` finds paths according to the OS it is being run on, I believe this is a problem with Tensorflow in Pyinstaller environment. 

I am trying to make an executable of my Python code which uses Tensorflow . The executable gets generated correctly but when I try to run it, I get the following error:
```

Traceback (most recent call last):
  File ""detection_init.py"", line 14, in <module>
    import lib.tensorboxDetector as tensorboxDetector
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""lib/tensorboxDetector.py"", line 26, in <module>
    from lib.train import build_forward
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""lib/train.py"", line 4, in <module>
    import tensorflow.contrib.slim as slim
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/__init__.py"", line 22, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/bayesflow/__init__.py"", line 24, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py"", line 26, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py"", line 42, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/framework/__init__.py"", line 89, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/framework/python/ops/__init__.py"", line 24, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/PyInstaller/loader/pyimod03_importers.py"", line 396, in load_module
    exec(bytecode, module.__dict__)
  File ""tensorflow/contrib/framework/python/ops/checkpoint_ops.py"", line 32, in <module>
  File ""tensorflow/contrib/util/loader.py"", line 55, in load_op_library
  File ""tensorflow/python/framework/load_library.py"", line 64, in load_op_library
tensorflow.python.framework.errors_impl.NotFoundError: tensorflow/contrib/util/tensorflow/contrib/framework/python/ops/_checkpoint_ops.so: cannot open shared object file: No such file or directory
[11241] Failed to execute script detection_init
```
If we look carefully, Pyinstaller is expecting the file `_checkpoint_ops.so` in directory `tensorflow/contrib/util/tensorflow/contrib/framework/python/ops/` but there's no directory like this. `_checkpoint_ops.so` is located at `tensorflow/contrib/framework/python/ops/`. How can this be fixed?"
13059,Clarifying relations of high level APIs,"There are right now several high level APIs in TensorFlow (`tf.keras`, `tf.estimators`, `tf.layers`, `tf.contrib.learn`) that seem to be sometimes complementary and sometimes redundant. It is somewhat hard for me (and other parts of the internet I have searched) to understand which API to use for which purpose and how they relate to each other.

For me it would greatly help if it could be specified:
* What is the intended high level API for the future? Even so if none of them will be discontinued, it would be nice one could just get pointed to one preferred. All I could find the documentation right now is: 
""The higher level APIs are built on top of TensorFlow Core. These higher level APIs are typically easier to learn and use than TensorFlow Core. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like tf.estimator helps you manage data sets, estimators, training and inference.""
* How different modules from `contrib` relate to those in core tensorflow. E. g. it seems that `tf.estimators` is a portion of `tf.contrib.learn` that made it into core.
* Why there are duplicate implementations like `tf.layers.contrib.conv2d` and `tf.layers.conv2d`?
* If modules have complementary or duplicate functionality. E.g. `tf.layers` can be used with `tf.estimators` to define the model, but probably one does not want to mix `tf.layers` and `tf.keras` .

It would be great if that could be specified in the Overview section of the respective documentations, instead of just saying it is a high level API
 
"
13050,MKL inference numbers are incorrect,"In [Performance Guide -> Comparing Compiler Optimizations](https://www.tensorflow.org/performance/performance_guide#comparing_compiler_optimizations), the numbers shared for inference on InceptionV3 and ResNet-50 models are exactly the same.

I'm assuming this is incorrect?"
13048,Word2vec on GPU slower than CPU,"
### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: NVIDIA GTX 1060 / 3GB
- **Docker used:** yes
- I picked up the code from the [word2vec example](https://github.com/tensorflow/models/tree/master/tutorials/embedding) on your official repo and made a few changes.The core code to train word2vec remains the same.

### Describe the problem
I have been working on benchmarking commonly used frameworks/libraries for unsupervised learning of word embeddings(word2vec). I am currently comparing tensorflow(cpu/gpu), gensim, deeplearning4j and the original c code on standard metrics like training time, peak memory usage and quality of learned vectors. Link to my [github repo](https://github.com/manneshiva/benchmark-word2vec-frameworks) (still working on it). I ran the benchmark on text8 corpus(plan to run it on a much larger corpus later for the true picture) which gave me strange results. 
- Tensorflow on GPU is much slower than CPU
- Tensorflow is much slower than other frameworks

Is this behavior expected? Would appreciate any inputs.
### Source code / logs
Link to [tensorflow code](https://github.com/manneshiva/benchmark-word2vec-frameworks/blob/master/nn_frameworks/tensorflow/word2vec.py)
Link to [results](http://nbviewer.jupyter.org/github/manneshiva/benchmark-word2vec-frameworks/blob/8223b9ff4b37869e5aef36a909dec384e08f3a05/visualize_report.ipynb) of sample benchmark on text8 corpus
"
13047,tf.graph_util.extract_sub_graph should raise a better error message,"Hi, 

I used `tf.graph_util.extract_sub_graph` to get a subgraph reaching a particular dest node. My dest_node is say `conv2d_transpose_7`. I by mistake gave it as a string in the second parameter. Python parses String too as a list and it raised an error `AssertionError: c is not in graph`. The `extract_sub_graph` should first check if the second parameter `isinstace` of a List so that it would be convenient to debug this kind of error in the future. A screenshot is attached.

Thanks, 


<img width=""613"" alt=""screenshot at sep 14 23-53-32"" src=""https://user-images.githubusercontent.com/6195312/30446912-22e74a76-99a8-11e7-81b4-3479d2e49c09.png"">
"
13046,tf.reduce_max inconsistent with numpy.max when handling NaN values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: P100 (16GB)
- **Exact command to reproduce**: [see below]

### Describe the problem
The documentation for `tf.reduce_max` states that it is ""Equivalent to `np.max`"". This is not true when the provided `input_tensor` includes NaN values.

TensorFlow ignores NaN values and returns inf (if present) or the largest finite value. Numpy will propagate NaN values in `np.max / np.amax` and has a special function `np.nanmax` for ignoring NaN values. (See the Notes section here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.amax.html)

Expected behavior is that `tf.reduce_max` returns NaN when its input includes NaN values.

### Source code / logs
```python
import numpy as np
import tensorflow as tf

vals = [float('1'), float('nan')]

np_max = np.max(vals)
tf_max = tf.reduce_max(tf.constant(vals))

with tf.Session() as sess:
    print('TF max: {}'.format(sess.run(tf_max)))
print('numpy max: {}'.format(np_max))
```
When run, this code produces the following output:
```
TF max: 1.0
numpy max: nan
```"
13045,Can I install and run TensorFlow on my machine with this much of information----->,"C:\Users\SEM>python Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>>"
13044,"Getting assertion failed: [Unable to decode bytes as JPEG, PNG, or GIF] when training using tensorflow object detection api","I tried to use `Tensorflow Object detection API` with my own dataset.   
Everything was working just fine until all of a sudden it crashed with the following error messages :   

    ...
    INFO:tensorflow:global step 10560: loss = 0.4366 (0.809 sec/step)
    INFO:tensorflow:global step 10561: loss = 0.3834 (0.822 sec/step)
    INFO:tensorflow:global step 10562: loss = 0.3611 (0.829 sec/step)
    INFO:tensorflow:global step 10563: loss = 0.3549 (0.901 sec/step)
    INFO:tensorflow:global step 10564: loss = 0.3634 (0.839 sec/step)
    INFO:tensorflow:global step 10565: loss = 0.3396 (0.813 sec/step)
    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, or GIF]
             [[Node: case/If_0/decode_image/cond_jpeg/cond_png/Assert/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/decode_image/cond_jpeg/cond_png/is_gif, case/If_0/decode_image/cond_jpeg/cond_png/Assert/Assert/data_0)]]
    INFO:tensorflow:global step 10566: loss = 0.3459 (0.889 sec/step)
    INFO:tensorflow:Finished training! Saving model to disk.
    Traceback (most recent call last):
      File ""train.py"", line 198, in <module>
        tf.app.run()
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
        _sys.exit(main(_sys.argv[:1] + flags_passthrough))
      File ""train.py"", line 194, in main
        worker_job_name, is_chief, FLAGS.train_dir)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py"", line 296, in train
        saver=saver)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 759, in train
        sv.saver.save(sess, sv.save_path, global_step=sv.global_step)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\Lib\contextlib.py"", line 66, in __exit__
        next(self.gen)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\supervisor.py"", line 964, in managed_session
        self.stop(close_summary_writer=close_summary_writer)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\supervisor.py"", line 792, in stop
        stop_grace_period_secs=self._stop_grace_secs)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
        six.reraise(*self._exc_info_to_raise)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\six.py"", line 686, in reraise
        raise value
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\queue_runner_impl.py"", line 238, in _run
        enqueue_callable()
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\client\session.py"", line 1063, in _single_operation_run
        target_list_as_strings, status, None)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\Lib\contextlib.py"", line 66, in __exit__
        next(self.gen)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
        pywrap_tensorflow.TF_GetCode(status))
    tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Unable to decode bytes as JPEG, PNG, or GIF]
             [[Node: case/If_0/decode_image/cond_jpeg/cond_png/Assert/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/decode_image/cond_jpeg/cond_png/is_gif, case/If_0/decode_image/cond_jpeg/cond_png/Assert/Assert/data_0)]]
    
    G:\Tensorflow_section\models-master\object_detection>

When I upgraded to the `1.3.0`, the error has changed, and  this is what I get now:   

    ...
    INFO:tensorflow:global step 10635: loss = 0.3392 (0.822 sec/step)
    INFO:tensorflow:global step 10636: loss = 0.3529 (0.823 sec/step)
    INFO:tensorflow:global step 10637: loss = 0.3305 (0.831 sec/step)
    2017-09-14 20:02:02.545415: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: Shape mismatch in tuple component 16. Expected [1,?,?,3], got [1,240,127,4]
    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Shape mismatch in tuple component 16. Expected [1,?,?,3], got [1,240,127,4]
             [[Node: batch/padding_fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_STRING, DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_FLOAT, DT_INT32, DT_STRING, DT_INT32, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/padding_fifo_queue, Reshape_2, Shape_5, SparseToDense_1, Shape_2, Merge_1, Shape, Merge_2, Shape_3, SparseToDense_5, Shape_8, SparseToDense_3, Shape_6, Cast_1, Shape_1, Cast_2, Shape_7, ExpandDims_5, Shape_4, Reshape_5, Shape_10, Reshape_6, Shape_9)]]
    INFO:tensorflow:global step 10638: loss = 0.3599 (0.858 sec/step)
    INFO:tensorflow:Finished training! Saving model to disk.
    Traceback (most recent call last):
      File ""train.py"", line 198, in <module>
        tf.app.run()
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
        _sys.exit(main(_sys.argv[:1] + flags_passthrough))
      File ""train.py"", line 194, in main
        worker_job_name, is_chief, FLAGS.train_dir)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py"", line 296, in train
        saver=saver)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 767, in train
        sv.stop(threads, close_summary_writer=True)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\supervisor.py"", line 792, in stop
        stop_grace_period_secs=self._stop_grace_secs)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
        six.reraise(*self._exc_info_to_raise)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\six.py"", line 686, in reraise
        raise value
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\training\queue_runner_impl.py"", line 238, in _run
        enqueue_callable()
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\client\session.py"", line 1235, in _single_operation_run
        target_list_as_strings, status, None)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\Lib\contextlib.py"", line 66, in __exit__
        next(self.gen)
      File ""C:\Users\Master\Anaconda3\envs\anaconda35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
        pywrap_tensorflow.TF_GetCode(status))
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape mismatch in tuple component 16. Expected [1,?,?,3], got [1,240,127,4]
             [[Node: batch/padding_fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_STRING, DT_INT32, DT_FLOAT, DT_INT32, DT_FLOAT, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_FLOAT, DT_INT32, DT_STRING, DT_INT32, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](batch/padding_fifo_queue, Reshape_2, Shape_5, SparseToDense_1, Shape_2, Merge_1, Shape, Merge_2, Shape_3, SparseToDense_5, Shape_8, SparseToDense_3, Shape_6, Cast_1, Shape_1, Cast_2, Shape_7, ExpandDims_5, Shape_4, Reshape_5, Shape_10, Reshape_6, Shape_9)]]
    
    G:\Tensorflow_section\models-master\object_detection>

I have no idea what is causing the issue, Could this be that some images have wrong extensions? for example, an image which was actually a png with 4 channels, has been saved as a jpg ?!
if this is the case, how to spot the faulty image file? or even better why does not TF use the correct type/number of channels by itself? 
right now, the error is not descriptive enough, it doesn't give any hint which image file is corrupted or is making the problem. 
if the cause of these errors is what I pointed out earlier, then they should be caught when the TF Record is being created. or if TF records are not the only means of inputs, then the same mechanism for knowing the culprit needs to be implemented as well
Anyway, if all my thoughts are wrong, then I would appreciate any help regarding this issue. 
 



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64 1703, Build 15063.540
- **TensorFlow installed from (source or binary)**:binary (used pip install )
- **TensorFlow version (use command below)**: 1.2.1 and 1.3.0
- **Python version**: 3.5.3(Anaconda)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda 8.0 /cudnn v5.1 and v6.0 (after upgrading to 1.3.0, cudnnv6.0 is in the PATH)
- **GPU model and memory**: GTX-1080 - 8G
"
13041,"Load model in C++ API and get ""from device: CUDA_ERROR_OUT_OF_MEMORY"" error","My model is about 2.4GBã€‚In my inference step, I  want to load model by multi-processing method in each GPU. That means I try to make two process in one GPU and each load a modelã€‚
After I make configuration of each session done, each session get about 5GB memory, But I still meet the ""from device: CUDA_ERROR_OUT_OF_MEMORY""ã€‚I am wonderingã€‚ã€‚ã€‚ Asking for help

##  **GPU information:**
[search@qrwt01 /home/s/apps/qtfserverd/bin]$ nvidia-smi
Thu Sep 14 21:42:48 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:08:00.0     Off |                    0 |
| N/A   48C    P0    61W / 149W |  11366MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 0000:09:00.0     Off |                    0 |
| N/A   32C    P0    72W / 149W |  11359MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     33056    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |
|    0     33057    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5515MiB |
|    1     33058    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |
|    1     33059    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5516MiB |
+-----------------------------------------------------------------------------+


## **Session configuration:**
 46 void* create_session(void* graph, std::string& checkpoint_path,
 47     int intra_op_threads, int inter_op_threads, std::string& device_list) {
 48     Session* session = NULL;
 49     SessionOptions sess_opts;
 50     //int NUM_THREADS = 8;
 51     if (intra_op_threads > 0) {
 52         sess_opts.config.set_intra_op_parallelism_threads(intra_op_threads);
 53     }
 54     if (inter_op_threads > 0) {
 55         sess_opts.config.set_inter_op_parallelism_threads(inter_op_threads);
 56     }
 57 
 58     sess_opts.config.set_allow_soft_placement(true);
 59     sess_opts.config.mutable_gpu_options()->set_visible_device_list(device_list);
 60     sess_opts.config.mutable_gpu_options()->set_allocator_type(""BFC"");
 61     sess_opts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);
 62     sess_opts.config.mutable_gpu_options()->set_allow_growth(true);
 63     Status status = NewSession(sess_opts, &session);
 64     if (!status.ok()) {
 65         fprintf(stderr, ""Create Session Failed %s\n"", status.ToString().c_str());
 66         return NULL;
 67     }


## **Error information**
load /home/search/tensorflow/deploy_combine.model.meta graph to /gpu:1 success
2017-09-14 21:42:31.188212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:09:00.0
totalMemory: 11.17GiB freeMemory: 11.05GiB
2017-09-14 21:42:31.188260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)
qss_switch:1, lstm_switch:1
qss_switch:1, lstm_switch:1
2017-09-14 21:42:33.826598: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.58G (1701773312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.838694: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.43G (1531596032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.893832: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.903917: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.913843: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.924008: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.935385: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.946556: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.956340: E tensorflow/stream_executor/cuda/cuda_driver.
"
13040,Feature request: Dataset.from_py_func,"I have an input pipeline that is especially complex. It is coded entirely in plain Python. While it *may* be possible to reimplement it in TensorFlow operations, that would be a huge amount of work and take far too much time to be worth it right now.

It would be great if I could hook my existing pipeline into the new Dataset approach via something like `Dataset.from_py_func`. One would pass a reference to a Python function that has no inputs and, when executed, acts as a generator that yields examples one-at-a-time as numpy arrays. For example:

    def generate():
        for example in complex_input_pipeline():
            yield example

    dataset = Dataset.from_py_func(generate)
    # Do normal things with dataset
        
I've been unable to figure out a way to do something like this using existing functionality so it would be great if it could be added."
13039,Reading tfrecord reaches deadock or crushes in one computer and works just fine on another.,"### Laptop -System information
- Linux Ubuntu 16.04
- TensorFlow installed with pip:
- TensorFlow 1.3.0
- Python 2.7.12
- Spyder 2.3.8

### Workstation -System information
- Linux Ubuntu 16.04
- TensorFlow installed with pip:
- TensorFlow-gpu 1.0.0
- Python 2.7.13

### Problem description
I want to create and read a tfrecord file with [build_cgd_dataset.py](https://github.com/tnikolla/grasp-detection/blob/master/build_cgd_dataset.py) as the writer and [reader_iter.py](https://github.com/tnikolla/grasp-detection/blob/master/reader_iter.py) as the reader. Everything works smooth in the workstation but it reaches a deadlock or gets stuck in the laptop. The dataset are from [Cornell grasping dataset](http://pr.cs.cornell.edu/grasping/rect_data/data.php).

I narrowed down the error in this special cases where it doesn't show any problem, for example:
  - if it reads a int64_list with one value, so a list with one elemnet
  - if it reads a float_list with one element

If the lists have more than one element the program freezes.
"
13038,cub::BlockReduce error while building tensorflow in windows using cmake,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    Windows 10 (64 bit)
- **TensorFlow installed from (source or binary)**:
    Using CMake and instructions present at (tensorflow/tensorflow/contrib/cmake)
- **TensorFlow version (use command below)**:
    -Using Master Branch
- **Python version**: 
    3.6.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
   CUDA-8.0.61
   cuDNN-5.1
- **GPU model and memory**:
   NVIDIA GeForce GTX TITAN X (382.05)
- **Exact command to reproduce**:
   MSBuild /p:Configuration=Release tf_label_image_example.vcxproj

### Describe the problem
I encounter the following error while building the tensorflow project for image recognition (tf_label_image_example.vcxproj). The error occurs when this image recognition project builds tf_core_gpu_kernels.vcxproj.

error : argument list for template ""cub::BlockReduce<T, BLOCK_DIM_X, ALGORITHM, BLOCK_DIM_Y, BLOCK_DIM_Z, PTX_ARCH>::Reduce [with T=std::iterator_tra its<T>::value_type, BLOCK_DIM_X=num_threads, ALGORITHM=cub::BLOCK_REDUCE_WARP_REDUCTIONS, BLOCK_DIM_Y=1, BLOCK_DIM_Z=1, PTX_ARCH=0]"" is missing.



"
13037,How to check tensorflow/inception - version number from an inception .pb-file?,"Hi, I am trying to figure out which Tensorflow version an inception .pb file is (https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip). This file is used in a OpenCV-plugin to Unity3D game engine (OpenCV for Unity, Enox-software, wraps OpenCV 3.3.0). I replaced these files with a model I re-trained with TF 1.3.0 yesterday and it made the whole Unity3D editor crash. So I assume I have the wrong tensorflow/inception-version (as the default zip-version worked, and my re-trained model worked inside docker-test), but I do not know what is the right version to use due to lack of documentation / knowledle / being tf-noobie. Does anybody here know how to check the version from the .pb-file ?"
13035,[W tensorflow/core/framework/op_kernel.cc:993] Failed precondition,"I want to run the tensorflow-deeplab-resnet,and I prepared my own training data for it. But When I try to run train.py to train data, it always did not work. And the error are as  follows:
![Uploading error.jpgâ€¦]()

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Tensor(""create_inputs/batch:0"", shape=(2, 4000, 4000, 3), dtype=float32) sssssssssssss
Tensor(""create_inputs/batch:1"", shape=(2, 4000, 4000, 1), dtype=uint8) sssssssssss
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Quadro M4000
major: 5 minor: 2 memoryClockRate (GHz) 0.7725
pciBusID 0000:03:00.0
Total memory: 7.92GiB
Free memory: 7.51GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:03:00.0)
Restored model parameters from /home/precision/code/tensorflow-deeplab-resnet-master/deeplab_resnet.ckpt
W tensorflow/core/framework/op_kernel.cc:993] Failed precondition: /home/precision/code/tensorflow-deeplab-resnet-master/dataset/VOCdevkit
W tensorflow/core/framework/op_kernel.cc:993] Failed precondition: /home/precision/code/tensorflow-deeplab-resnet-master/dataset/VOCdevkit
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_1_create_inputs/batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
     [[Node: create_inputs/batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_UINT8], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](create_inputs/batch/fifo_queue, create_inputs/batch/n)]]
Traceback (most recent call last):
  File ""train.py"", line 258, in <module>
    main()
  File ""train.py"", line 247, in main
    loss_value, images, labels, preds, summary, _ = sess.run([reduced_loss, image_batch, label_batch, pred, total_summary, train_op], feed_dict=feed_dict)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_1_create_inputs/batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
     [[Node: create_inputs/batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_UINT8], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](create_inputs/batch/fifo_queue, create_inputs/batch/n)]]

Caused by op u'create_inputs/batch', defined at:
  File ""train.py"", line 258, in <module>
    main()
  File ""train.py"", line 142, in main
    image_batch, label_batch = reader.dequeue(args.batch_size)
  File ""/home/precision/code/tensorflow-deeplab-resnet-master/deeplab_resnet/image_reader.py"", line 180, in dequeue
    num_elements)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 872, in batch
    name=name)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 667, in _batch
    dequeued = queue.dequeue_many(batch_size, name=name)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 458, in dequeue_many
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 1310, in _queue_dequeue_many_v2
    timeout_ms=timeout_ms, name=name)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/precision/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_1_create_inputs/batch/fifo_queue' is closed and has insufficient elements (requested 2, current size 0)
     [[Node: create_inputs/batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_UINT8], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](create_inputs/batch/fifo_queue, create_inputs/batch/n)]]

**[W tensorflow/core/framework/op_kernel.cc:993] Failed precondition**
I have written the path address for the training set explicitlyï¼Œbut the training program doesn't work properly.

Can you help me with this problem?
                                                                 Thanks a lot!"
13034,AttributeError: 'RunConfig' object has no attribute 'environment',"Problem with the learn_runner.run() method.

# **System Info:**

**Windows 10**
**TF 1.3.0**
**Python 3.5**

**Code :**

```python
import tensorflow as tf

import tensorflow.contrib.learn as tflearn
from tensorflow.contrib.learn.python.learn import learn_runner

print('Tensorflow Version - ', tf.__version__)
tf.logging.set_verbosity(tf.logging.INFO)

train_file = 'data/iris_training.csv'
test_file = 'data/iris_test.csv'

feature_names = [
    'SepalLength',
    'SepalWidth',
    'PetalLength',
    'PetalWidth'
]


def input_fn(file, perform_shuffle=False, repeat_count=1):
    def decode_csv(line):
        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])
        label = parsed_line[-1:]
        del parsed_line[-1]
        features = parsed_line
        parsed_data = dict(zip(feature_names, features)), label
        return parsed_data

    data_set = (tf.contrib.data.TextLineDataset(file).skip(1).map(decode_csv))

    if perform_shuffle:
        data_set = data_set.shuffle(buffer_size=256)

    data_set = data_set.repeat(repeat_count)
    data_set = data_set.batch(32)
    iterator = data_set.make_one_shot_iterator()
    batch_features, batch_label = iterator.get_next()
    return batch_features, batch_label

feature_columns = [tf.feature_column.numeric_column(feature, normalizer_fn=lambda x: normalize_fn(x)) for feature in
                   feature_names]


def json_serving_input_fn():
    """"""Build the serving inputs.""""""
    """"""Build the serving inputs.""""""

    inputs = {}
    for feat in feature_columns:
        inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)
    return tf.estimator.export.ServingInputReceiver(inputs, inputs)

def normalize_fn(feature):
    print('\n')
    print(tf.shape(feature))
    mean, variance = tf.nn.moments(feature, axes=[0])
    print('\n Mean - ', mean)
    print('\n Variance - ', variance)
    return (feature - mean) / variance


def experiment_fn(output_dir):
    classifier = tf.estimator.DNNClassifier(hidden_units=[10, 10], feature_columns=feature_columns, n_classes=3,
                                            model_dir=output_dir)

    from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils

    return tflearn.Experiment(classifier,
                              train_input_fn=lambda: input_fn(train_file, perform_shuffle=True, repeat_count=10),
                              eval_input_fn=lambda: input_fn(test_file, perform_shuffle=False, repeat_count=1),
                              eval_metrics=None,
                              export_strategies=[saved_model_export_utils.make_export_strategy(
                                  serving_input_fn=json_serving_input_fn, default_output_alternative_key=None,
                                  exports_to_keep=1
                              )],
                              train_steps=100
                              )

learn_runner.run(experiment_fn=experiment_fn, output_dir='build2/')

```

**Exception:**

File ""F:/Git/Tensorflow-Tutorials/iris/linear_classifier.py"", line 107, in <module>
    learn_runner.run(experiment_fn=experiment_fn, output_dir='build2/')
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py"", line 209, in run
    return _execute_schedule(experiment, schedule)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py"", line 46, in _execute_schedule
    return task()
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py"", line 502, in train_and_evaluate
    self.train(delay_secs=0)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py"", line 253, in train
    if (config.environment != run_config.Environment.LOCAL and
AttributeError: 'RunConfig' object has no attribute 'environment'"
13031,Has this issue been solved for all containers - ImportError: cannot import name audio_ops,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am running the tutorial ""audio recognition network""
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:('v1.0.0-rc2-15-g47bba63-dirty', '1.0.0')
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**:NA
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**: python tensorflow/examples/speech_commands/train.py

### Describe the problem
I pip-installed tensorflow using virtualenv. Then I run -  

python tensorflow/examples/speech_commands/train.py       for which the following error message appears: 

  Traceback (most recent call last):
  File ""/home/cogknit/tensorflow/tensorflow/examples/speech_commands/train.py"", line 79, in <module>
    import input_data
  File ""/home/cogknit/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
ImportError: cannot import name audio_ops

In stack overflow, I came across the same issue only with anaconda install in the following link -
https://stackoverflow.com/questions/45952387/anaconda-install-of-tensorflow-missing-audio-ops-from-contrib-framework 
There, the answer indicated that the ""audio_ops.py"" file is missing from the framework and hasn't been released yet .This indication seems to be validated by a developer in the following link and recently an update seems to have been made/released per the same link  : https://github.com/tensorflow/tensorflow/issues/11339

My question in particular is does the release solve the error in virtualenv installation too? Or hasn't it been solved at all? Is it better to move to a docker installation?Are there any alternatives/commands that   I can use to circumvent this error?

#12722  dealt with the same error but the solution seems to have worked only for tensorflow built from source. Is there anything that I should check again?

Thank you,"
13030,Crash when load Fine-tune model on Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**:  8.0.61
- **GPU model and memory**: NVIDIA Corporation Device 1b06
- **Exact command to reproduce**: 

How I do Fine-tune : 
```

# Convert imagenet data to TFrecord ( No bounding box , just for classification ) 
python build_image_data.py \
--train_directory='imagenet/train' \
--validation_directory='imagenet/train' \
--labels_file='imagenet/labels.txt' \
--output_directory='imagenet/tfrecord' 

# Train from checkpoint
python train_image_classifier.py \
    --dataset_dir=.../imagenet/tfrecord \
    --train_dir=.../imagenet/train_logs \
    --checkpoint_path=.../checkpoint/mobilenet_v1_0.50_224.ckpt \
    --model_name=mobilenet_v1_050 \
    --dataset_name=flowers \                       ( _NUM_CLASSES and sizes has been modified )
    --dataset_split_name=train \
    --max_number_of_steps=10000 \
    --checkpoint_exclude_scopes=MobilenetV1/Logits \
    --trainable_scopes=MobilenetV1/Logits \

# Produce .pb file (use .pbtxt)
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=.../imagenet/train_logs/graph.pbtxt \
--input_checkpoint=.../imagenet/train_logs/model.ckpt-10000 \
--output_graph=.../imagenet/train_logs/frozen_graph.pb \
--output_node_names=MobilenetV1/Predictions/Reshape_1

# Removes parts of a graph that are only needed for training
bazel-bin/tensorflow/python/tools/optimize_for_inference \
--input=.../imagenet/train_logs/frozen_graph.pb \
--output=.../imagenet/train_logs/optimized_graph.pb \
--frozen_graph=True \
--input_names='prefetch_queue/fifo_queue' \
--output_names='MobilenetV1/Predictions/Reshape_1'

```
### Describe the problem

Hi , I am doing some fine-tune with mobilenet , I got some trouble at beginning since the input node "" prefetch_queue/fifo_queue "" , which Op ""FIFOQueueV2"" is disabled on Android , but I think it's okay  after using ""bazel-bin/tensorflow/python/tools/optimize_for_inference"" . ( I am not sure about it or I still need to customize my own TF library like mentioned [here](https://github.com/tensorflow/tensorflow/issues/8454) ? )

But now there is a new problem , after modify the parameter at ""ClassifierActivity.java"" like this : 
( input_name and output_name is from summarize_graph tool )
```

  private static final int INPUT_SIZE = 224;
  private static final int IMAGE_MEAN = 117;
  private static final float IMAGE_STD = 1;
  private static final String INPUT_NAME = ""prefetch_queue/fifo_queue"";
  private static final String OUTPUT_NAME = ""MobilenetV1/Predictions/Reshape_1"";

  private static final String MODEL_FILE = ""file:///android_asset/tensorflow_inception_graph.pb"";
  private static final String LABEL_FILE =
      ""file:///android_asset/imagenet_comp_graph_label_strings.txt"";

```

and implement the demo on android , the TF Classify can be launched , but crash when press the volume key to turn on the debug mode. 

the crash prints the following log :
### Source code / logs

Error Message from adb logcat
```
E/InputEventSender( 4197): Exception dispatching finished signal.
E/MessageQueue-JNI( 4197): Exception in MessageQueue callback: handleReceiveCallback
E/MessageQueue-JNI( 4197): java.lang.NullPointerException: Attempt to invoke interface method 'void org.tensorflow.demo.Classifier.enableStatLogging(boolean)' on a null object reference
E/MessageQueue-JNI( 4197):      at org.tensorflow.demo.ClassifierActivity.onSetDebug(ClassifierActivity.java:183)
E/MessageQueue-JNI( 4197):      at org.tensorflow.demo.CameraActivity.onKeyDown(CameraActivity.java:381)
E/MessageQueue-JNI( 4197):      at android.view.KeyEvent.dispatch(KeyEvent.java:2758)
E/MessageQueue-JNI( 4197):      at android.app.Activity.dispatchKeyEvent(Activity.java:2755)
E/MessageQueue-JNI( 4197):      at com.android.internal.policy.impl.PhoneWindow$DecorView.dispatchKeyEvent(PhoneWindow.java:2380)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$ViewPostImeInputStage.processKeyEvent(ViewRootImpl.java:4556)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$ViewPostImeInputStage.onProcess(ViewRootImpl.java:4512)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$AsyncInputStage.forward(ViewRootImpl.java:4190)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.apply(ViewRootImpl.java:4061)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$AsyncInputStage.apply(ViewRootImpl.java:4247)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.apply(ViewRootImpl.java:4061)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$AsyncInputStage.forward(ViewRootImpl.java:4223)
E/MessageQueue-JNI( 4197):      at android.view.ViewRootImpl$ImeInputStage.onFinishedInputEvent(ViewRootImpl.java:4393)
E/MessageQueue-JNI( 4197):      at android.view.inputmethod.InputMethodManager$PendingEvent.run(InputMethodManager.java:2285)
E/MessageQueue-JNI( 4197):      at android.view.inputmethod.InputMethodManager.invokeFinishedInputEventCallback(InputMethodManager.java:1909)
E/MessageQueue-JNI( 4197):      at android.view.inputmethod.InputMethodManager.finishedInputEvent(InputMethodManager.java:1900)
E/MessageQueue-JNI( 4197):      at android.view.inputmethod.InputMethodManager$ImeInputEventSender.onInputEventFinished(InputMethodManager.java:2262)
E/MessageQueue-JNI( 4197):      at android.view.InputEventSender.dispatchInputEventFinished(InputEventSender.java:141)
E/MessageQueue-JNI( 4197):      at android.os.MessageQueue.nativePollOnce(Native Method)
E/MessageQueue-JNI( 4197):      at android.os.MessageQueue.next(MessageQueue.java:148)
E/MessageQueue-JNI( 4197):      at android.os.Looper.loop(Looper.java:151)
E/MessageQueue-JNI( 4197):      at android.app.ActivityThread.main(ActivityThread.java:5631)
E/MessageQueue-JNI( 4197):      at java.lang.reflect.Method.invoke(Native Method)
E/MessageQueue-JNI( 4197):      at java.lang.reflect.Method.invoke(Method.java:372)
E/MessageQueue-JNI( 4197):      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:959)
E/MessageQueue-JNI( 4197):      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:754)
E/AndroidRuntime( 4197): FATAL EXCEPTION: main
E/AndroidRuntime( 4197): Process: org.tensorflow.demo, PID: 4197
E/AndroidRuntime( 4197): java.lang.NullPointerException: Attempt to invoke interface method 'void org.tensorflow.demo.Classifier.enableStatLogging(boolean)' on a null object reference
E/AndroidRuntime( 4197):        at org.tensorflow.demo.ClassifierActivity.onSetDebug(ClassifierActivity.java:183)
E/AndroidRuntime( 4197):        at org.tensorflow.demo.CameraActivity.onKeyDown(CameraActivity.java:381)
E/AndroidRuntime( 4197):        at android.view.KeyEvent.dispatch(KeyEvent.java:2758)
E/AndroidRuntime( 4197):        at android.app.Activity.dispatchKeyEvent(Activity.java:2755)
E/AndroidRuntime( 4197):        at com.android.internal.policy.impl.PhoneWindow$DecorView.dispatchKeyEvent(PhoneWindow.java:2380)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$ViewPostImeInputStage.processKeyEvent(ViewRootImpl.java:4556)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$ViewPostImeInputStage.onProcess(ViewRootImpl.java:4512)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$AsyncInputStage.forward(ViewRootImpl.java:4190)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.apply(ViewRootImpl.java:4061)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$AsyncInputStage.apply(ViewRootImpl.java:4247)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.apply(ViewRootImpl.java:4061)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.deliver(ViewRootImpl.java:4034)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.onDeliverToNext(ViewRootImpl.java:4087)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$InputStage.forward(ViewRootImpl.java:4053)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$AsyncInputStage.forward(ViewRootImpl.java:4223)
E/AndroidRuntime( 4197):        at android.view.ViewRootImpl$ImeInputStage.onFinishedInputEvent(ViewRootImpl.java:4393)
E/AndroidRuntime( 4197):        at android.view.inputmethod.InputMethodManager$PendingEvent.run(InputMethodManager.java:2285)
E/AndroidRuntime( 4197):        at android.view.inputmethod.InputMethodManager.invokeFinishedInputEventCallback(InputMethodManager.java:1909)
E/AndroidRuntime( 4197):        at android.view.inputmethod.InputMethodManager.finishedInputEvent(InputMethodManager.java:1900)
E/AndroidRuntime( 4197):        at android.view.inputmethod.InputMethodManager$ImeInputEventSender.onInputEventFinished(InputMethodManager.java:2262)
E/AndroidRuntime( 4197):        at android.view.InputEventSender.dispatchInputEventFinished(InputEventSender.java:141)
E/AndroidRuntime( 4197):        at android.os.MessageQueue.nativePollOnce(Native Method)
E/AndroidRuntime( 4197):        at android.os.MessageQueue.next(MessageQueue.java:148)
E/AndroidRuntime( 4197):        at android.os.Looper.loop(Looper.java:151)
E/AndroidRuntime( 4197):        at android.app.ActivityThread.main(ActivityThread.java:5631)
E/AndroidRuntime( 4197):        at java.lang.reflect.Method.invoke(Native Method)
E/AndroidRuntime( 4197):        at java.lang.reflect.Method.invoke(Method.java:372)
E/AndroidRuntime( 4197):        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:959)
E/AndroidRuntime( 4197):        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:754)

```

I am not sure which part result in this , if any further information is needed , please tell me , thank you !

"
13029,GMM clustering example not working,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.0.0-rc2-15-g47bba63-dirty 1.0.0
- **Python version**: Python 3.6.0 |Anaconda 4.3.1 (x86_64)| (default, Dec 23 2016, 13:19:00) 
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: python gmm_test.py

### Describe the problem
I'm trying to use the gaussian mixed model clustering algorithm from contrib. However, the gmm class is really buggy and I'm unable to get the test cases in the gmm_test.py file to work. 

### Source code / logs
Using the iris dataset from sklearn, I've tried to update the gmm.fit step to use it (as opposed to an input_fn). "
13024,nightly link broken on github README.md,"On github page, nightly for Linux GPU Python 3.5 is broken, points to

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.4.0dev-cp35-cp35m-linux_x86_64.whl

![screenshot 2017-09-13 17 28 11](https://user-images.githubusercontent.com/23068/30406796-00aa9d00-98a9-11e7-8d7b-4226c2401df7.png)


![screenshot 2017-09-13 17 28 56](https://user-images.githubusercontent.com/23068/30406804-130eaa40-98a9-11e7-887d-00a39ec0bdc5.png)
"
13023,"Invalid argument: shape must be a vector of {int32,int64}, got shape []","I try to use the interface of RandomUniform as follows:

```
func TestRandomUniform(t *testing.T) {
	root := op.NewScope()

	T := op.Placeholder(root.SubScope(""input""), tf.Int64)

	seed := op.RandomUniformSeed(-1.0)
	seed2 := op.RandomUniformSeed2(1.0)
	product := op.RandomUniform(root,T,tf.Float,seed,seed2)


	graph, err := root.Finalize()
	if err != nil {
		panic(err.Error())
	}

	var sess *tf.Session
	sess, err = tf.NewSession(graph, &tf.SessionOptions{})
	if err != nil {
		panic(err.Error())
	}

	var A *tf.Tensor

	if A, err = tf.NewTensor(int64(1)); err != nil {
		panic(err.Error())
	}

	var results []*tf.Tensor
	if results, err = sess.Run(
		map[tf.Output]*tf.Tensor{
			T: A,
		},
		[]tf.Output{product}, nil); err != nil {
			panic(err.Error())
	}
	for _, result := range results {
		fmt.Println(result.Value().([]int64))
	}

}
```

```
Invalid argument: shape must be a vector of {int32,int64}, got shape []
panic: shape must be a vector of {int32,int64}, got shape []
	 [[Node: RandomUniform = RandomUniform[T=DT_INT64, _class=[], dtype=DT_FLOAT, seed=-1, seed2=1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_input/Placeholder_0_0)]] [recovered]
	panic: shape must be a vector of {int32,int64}, got shape []
	 [[Node: RandomUniform = RandomUniform[T=DT_INT64, _class=[], dtype=DT_FLOAT, seed=-1, seed2=1, _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_input/Placeholder_0_0)]]

goroutine 5 [running]
```

But it  prints the error for me. What's it means?I am not good at c.And where are the demo i can find?"
13020,tf.Session() freezes on GPU nodes of a SGE cluster,"Hello,

I'm trying to run the following script on the GPU nodes of my university's SGE cluster:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys

import tensorflow as tf

var1 = tf.get_variable('var1', [1024, 32], initializer=tf.random_normal_initializer())
var2 = tf.get_variable('var2', [32, 1024], initializer=tf.random_normal_initializer())

m = tf.matmul(var1, var2)

init_op = tf.global_variables_initializer()

with tf.Session() as session:
	session.run(init_op)
	session.run(m)
```

Everything works if, either:
- I allocate obscene amounts of memory, like 60GB, to the job;
- Or if I run it from the command line on the cluster nodes.

However, if I allocate a reasonable amount of memory to the job (8GB), it freezes on the `tf.Session()` call. A `strace -p <pid>` on the process gives the following trace, over and over, endlessly:

```
open(""/proc/self/maps"", O_RDONLY)       = 18
fstat(18, {st_mode=S_IFREG|0444, st_size=0, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x2ab462000000
read(18, ""00400000-00634000 r-xp 00000000 ""..., 1024) = 1024
read(18, ""001d000 09:01 1056305           ""..., 1024) = 1024
read(18, ""0 r--p 001b9000 09:01 1583003   ""..., 1024) = 1024
read(18, "" ---p 00041000 09:01 1056331    ""..., 1024) = 1024
read(18, ""b4aa02000-3b4aa03000 rw-p 000020""..., 1024) = 1024
close(18)                               = 0
munmap(0x2ab462000000, 4096)            = 0
mmap(0x36be1200000, 4294967296, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 1103806595072, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 554050781184, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 279172874240, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 141733920768, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 73014444032, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 38654705664, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 21474836480, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 12884901888, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 8589934592, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 6442450944, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 5368709120, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4831838208, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4563402752, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4429185024, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4362076160, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4328521728, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4311744512, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4303355904, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4299161600, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4297064448, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4296015872, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4295491584, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4295229440, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4295098368, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4295032832, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4295000064, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294983680, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294975488, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294971392, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294969344, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294968320, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967808, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967552, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967424, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967360, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967328, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967312, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967304, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967300, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967298, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
mmap(0x36be1200000, 4294967297, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
```

How comes TensorFlow (unsuccessfully) tries to allocate obscene amount of memory when instantiating a new session? It only happens when I run the script as a cluster GPU job, without an obscene amount of memory.

Thank you in advance for all your help.




"
13019,Failed to Load the native TensorFlow Runtime,"I installed tensorflow through Anaconda with Python 3.6.2 . It got installed successfully. But I am not able to import tensorflow. and I am using a 64 bit machine with Windows 10 OS. I got following error:

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\HP\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
13018,Add `Dataset.from_variable_length` to accept numpy arrays with varying length,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS x
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: `v1.3.0-rc2-20-g0787eee 1.3.0`
- **Python version**: `Python 3.5.2 :: Continuum Analytics, Inc.`
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 
```
import numpy as np
from tensorflow.contrib.data import Dataset
a=[np.arange(3), np.arange(5)]
Dataset.from_tensor_slices(np.asarray(a))
```

### Describe the problem
`Dataset` has a nice `.padded_batch` feature which allows padding batches to ensure all tensors have the same size, which is very nice for the common use-case of having sequences of different lengths.

However, it seems impossible to create such a dataset from a list of numpy array with different lengths. See command above.

It would be very nice to have a `Dataset.from_variable_length_tensor_slices` or such that could take such inputs.

I know I'm supposed to use a `TextLineDataset` or something similar, and then use a series of map functions, but I think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm, which is harder to reason about and debug. Also, my data is stored in JSON files, and requires complex pre-processing, so the `TextLineDataset/map` approach doesn't cut it."
13017,tf.extract_image_patches gradient transpose extremely slow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **TensorFlow installed from (source or binary)**: Binary, official docker image
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5

### Problem
Backprop through `extract_image_patches` is extremly slow, specifically the `transpose` call. It appears to be this specific call: [tensorflow/python/ops/array_grad.py#L747](https://github.com/tensorflow/tensorflow/blob/f282bb1/tensorflow/python/ops/array_grad.py#L747)

### Logs
Timeline of the problem:

<img width=""1723"" alt=""screen shot 2017-09-13 at 16 01 49"" src=""https://user-images.githubusercontent.com/3015996/30382640-f5a003b8-989f-11e7-871c-1aae1a51102b.png"">

Comparable timeline when using conv2d (which, in its Eigen implementation, also [extracts image patches](https://github.com/tensorflow/tensorflow/blob/f282bb1/tensorflow/core/kernels/eigen_spatial_convolutions.h#L1063-L1064)):
<img width=""1300"" alt=""screen shot 2017-09-13 at 16 08 45"" src=""https://user-images.githubusercontent.com/3015996/30382983-e898d338-98a0-11e7-9e76-1ba01e571c0d.png"">

"
13016,Distributed variable initialization never reaches some nodes (affects MonitoredTrainingSession too),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.
- **TensorFlow installed from (source or binary)**: binary, followed the pip install instructions
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.2, 3.6.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See instructions below.

### Describe the problem
In a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.

The source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:

- When foo and bar are in the same host and OS, the problem never happens.
- When ran foo in a Linux host and bar in a Mac OS X host the problem *always* happened.
- However, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.
- Making foo_variables below an empty list also avoids the problem entirely.

Additionally I also tested this in another couple of platforms in the same host.
- When ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem *always* happened.
- When reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.
- In this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.

### Source code / logs
```python
#!/usr/bin/env python

import sys
import tensorflow as tf

with tf.variable_scope('foo'), tf.device('job:foo/task:0'):
  foo_variables = [tf.get_variable('W', shape=(10, 5), dtype=tf.float32)]
  foo_init_vars = tf.variables_initializer(foo_variables)
  foo_pending_vars = tf.report_uninitialized_variables(foo_variables)
  foo_pending_vars.mark_used()


with tf.variable_scope('bar'), tf.device('job:bar/task:0'):
  # Expect more stuff and ops in bar in a real use case. This is just an example.
  bar_pending_vars = tf.report_uninitialized_variables(foo_variables)
  bar_pending_vars.mark_used()


cluster_spec = tf.train.ClusterSpec({
  ""foo"": [""<insert_ip_here>:55700""],
  ""bar"": [""<insert_ip_here>:55701""],
})


def foo_job():
  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)
  with tf.train.MonitoredTrainingSession(server.target, is_chief=True) as session:

    # This always runs fine.
    session.run(foo_init_vars)
    print(""Foo -- Variables not initialized: "", session.run(foo_pending_vars))
    server.join()


def bar_job():
  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)
  with tf.train.MonitoredTrainingSession(server.target, is_chief=False) as session:
    # On failure, this never gets executed...
    print(""**** Session started! ****"")

    vars_left = session.run(bar_pending_vars)
    if len(vars_left) == 0:
      print(""Bar -- Variables initialized!"")
      return

    print(""Bar -- Variables not initialized: "", vars_left)
    server.join()


if __name__ == '__main__':
  if len(sys.argv) < 2:
    print(""Usage: %s {foo|bar}"" % sys.argv[0])
    exit(1)

  if sys.argv[1] == 'foo':
    foo_job()
  else:
    bar_job()
```

@mrry, this looks like something you might have some intuition about. Any ideas of what might be going on?

This is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem."
13015,Ubuntu installation instruction is too obsolete!,"For successful build on Ubuntu it's required to do much more than stated on the installation instructions!

My system:
```
CUDA support: YES
Bazel: 0.5.4
gcc (Ubuntu 5.4.1-8ubuntu1) 5.4.1 20170304
Linux Desktop 4.10.0-33-generic #37-Ubuntu SMP Fri Aug 11 10:55:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
CPU i7-7820x (with AVX512 support)
```

It's required change the build command:

`bazel build --config=opt --config=cuda --cxxopt=""-fabi-version=0"" //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

And it's still not enough :( I still can't make successful build..."
13014,Can't compile tensorflow on Ubuntu with Skylake CPU (i7-7820X),"Can't compile tensorflow on Ubuntu with Skylake CPU (i7-7820X). I get the following error:

```
ERROR: /home/Dmitry/Private/Kaggle/tensorflow/tensorflow/core/kernels/BUILD:1369:1: C++ compilation of rule '//tensorflow/core/kernels:range_sampler' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/Dmitry/.cache/bazel/_bazel_Dmitry/a3a733ca304216612b7ed32f720401f1/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-4.9 \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=6 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' '-fabi-version=0' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/range_sampler/tensorflow/core/kernels/range_sampler.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/range_sampler/tensorflow/core/kernels/range_sampler.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/protobuf_archive -iquote bazel-out/local_linux-opt/genfiles/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -isystem external/nsync/public -isystem bazel-out/local_linux-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/kernels/range_sampler.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/range_sampler/tensorflow/core/kernels/range_sampler.pic.o)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:378:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/lib/random/random_distributions.h:27,
                 from ./tensorflow/core/lib/random/simple_philox.h:24,
                 from ./tensorflow/core/lib/random/distribution_sampler.h:38,
                 from ./tensorflow/core/kernels/range_sampler.h:23,
                 from tensorflow/core/kernels/range_sampler.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'Packet Eigen::internal::ploaddup(const typename Eigen::internal::unpacket_traits<Packet>::type*) [with Packet = __vector(16) float; typename Eigen::internal::unpacket_traits<Packet>::type = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:469:77: error: '_mm512_castsi512_ps' was not declared in this scope
   __m512 even_elements = _mm512_castsi512_ps(_mm512_cvtepu32_epi64(low_half));
                                                                             ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'Packet Eigen::internal::ploaddup(const typename Eigen::internal::unpacket_traits<Packet>::type*) [with Packet = __vector(8) double; typename Eigen::internal::unpacket_traits<Packet>::type = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:478:56: error: '_mm512_insertf64x2' was not declared in this scope
   x = _mm512_insertf64x2(x, _mm_loaddup_pd(&from[0]), 0);
                                                        ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/status.h:25,
                 from ./tensorflow/core/kernels/range_sampler.h:21,
                 from tensorflow/core/kernels/range_sampler.cc:16:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::string = std::basic_string<char>]':
tensorflow/core/kernels/range_sampler.cc:86:5:   required from here
./tensorflow/core/platform/default/logging.h:230:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )
                                   ^
./tensorflow/core/platform/macros.h:79:29: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (x)
                             ^
./tensorflow/core/platform/default/logging.h:230:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <= )
 ^
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::string = std::basic_string<char>]':
tensorflow/core/kernels/range_sampler.cc:244:3:   required from here
./tensorflow/core/platform/default/logging.h:228:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                         == )  // Compilation error with CHECK_EQ(NULL, x)?
                         ^
./tensorflow/core/platform/macros.h:79:29: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (x)
                             ^
./tensorflow/core/platform/default/logging.h:227:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1.894s, Critical Path: 1.15s
FAILED: Build did NOT complete successfully
```

OR the following error:

```
ERROR: /home/Dmitry/Private/Kaggle/tensorflow/tensorflow/core/BUILD:1299:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/Dmitry/.cache/bazel/_bazel_Dmitry/a3a733ca304216612b7ed32f720401f1/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-4.9 \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=6 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' '-fabi-version=0' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/random/distribution_sampler.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/random/distribution_sampler.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-py3-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/protobuf_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-py3-opt/genfiles/external/snappy -isystem external/nsync/public -isystem bazel-out/local_linux-py3-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/lib/random/distribution_sampler.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/random/distribution_sampler.pic.o)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:378:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/lib/random/random_distributions.h:27,
                 from ./tensorflow/core/lib/random/simple_philox.h:24,
                 from ./tensorflow/core/lib/random/distribution_sampler.h:38,
                 from tensorflow/core/lib/random/distribution_sampler.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'Packet Eigen::internal::ploaddup(const typename Eigen::internal::unpacket_traits<Packet>::type*) [with Packet = __vector(16) float; typename Eigen::internal::unpacket_traits<Packet>::type = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:469:77: error: '_mm512_castsi512_ps' was not declared in this scope
   __m512 even_elements = _mm512_castsi512_ps(_mm512_cvtepu32_epi64(low_half));
                                                                             ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'Packet Eigen::internal::ploaddup(const typename Eigen::internal::unpacket_traits<Packet>::type*) [with Packet = __vector(8) double; typename Eigen::internal::unpacket_traits<Packet>::type = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:478:56: error: '_mm512_insertf64x2' was not declared in this scope
   x = _mm512_insertf64x2(x, _mm_loaddup_pd(&from[0]), 0);
                                                        ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 183.759s, Critical Path: 16.18s
FAILED: Build did NOT complete successfully
```

Compilation command: 

`bazel build --config=opt --config=cuda --cxxopt=""-fabi-version=0"" //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

Is it possible to disable AVX512 but leave all other native optimization parameters?
"
13013,Android Detect demo is not working (not detecting/drawing anything),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro / Android 6
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A (using cmake)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
- **Phone**: Moto G4 Play (need to use Camera, not Camera2)

### Describe the problem
I start the ""TF Detect"" demo using my smartphone and nothing happens. It just seems that is not operating, neither trying to detect anything. No matter how long I stay or how many things I try to detect, nothing happens (and the log, as copied below, doesn't change too).

### Source code / logs
```
09-13 09:15:32.129 16678-16678/org.tensorflow.demo I/art: Late-enabling -Xcheck:jni
09-13 09:15:32.270 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_dependencies_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_dependencies_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.335 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_0_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_0_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.401 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_1_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_1_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.467 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_2_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_2_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.530 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_3_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_3_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.601 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_4_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_4_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.658 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_5_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_5_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.722 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_6_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_6_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.782 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_7_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_7_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.851 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_8_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_8_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.929 16678-16678/org.tensorflow.demo W/art: Failed execv(/system/bin/dex2oat --runtime-arg -classpath --runtime-arg  --debuggable --instruction-set=arm --runtime-arg -Xrelocate --boot-image=/system/framework/boot.art --runtime-arg -Xms64m --runtime-arg -Xmx512m --instruction-set-variant=cortex-a53 --instruction-set-features=default --dex-file=/data/app/org.tensorflow.demo-2/split_lib_slice_9_apk.apk --oat-file=/data/dalvik-cache/arm/data@app@org.tensorflow.demo-2@split_lib_slice_9_apk.apk@classes.dex) because non-0 exit status
09-13 09:15:32.937 16678-16678/org.tensorflow.demo I/InstantRun: starting instant run server: is main process
09-13 09:15:33.007 16678-16678/org.tensorflow.demo D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.DetectorActivity@838da96
09-13 09:15:33.026 16678-16678/org.tensorflow.demo I/CameraManagerGlobal: Connecting to camera service
09-13 09:15:33.046 16678-16678/org.tensorflow.demo I/tensorflow: CameraActivity: Camera API lv2?: false
09-13 09:15:33.124 16678-16678/org.tensorflow.demo D/tensorflow: CameraActivity: onStart org.tensorflow.demo.DetectorActivity@838da96
09-13 09:15:33.124 16678-16678/org.tensorflow.demo D/tensorflow: CameraActivity: onResume org.tensorflow.demo.DetectorActivity@838da96
09-13 09:15:33.138 16678-16731/org.tensorflow.demo D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true
09-13 09:15:33.187 16678-16731/org.tensorflow.demo I/Adreno-EGL: <qeglDrvAPI_eglInitialize:379>: EGL 1.4 QUALCOMM build:  (Ifd751822f5)
                                                                 OpenGL ES Shader Compiler Version: XE031.06.00.05
                                                                 Build Date: 01/26/16 Tue
                                                                 Local Branch: AU12_SBA
                                                                 Remote Branch: 
                                                                 Local Patches: 
                                                                 Reconstruct Branch: 
09-13 09:15:33.189 16678-16731/org.tensorflow.demo I/OpenGLRenderer: Initialized EGL, version 1.4
09-13 09:15:33.771 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: person
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bicycle
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: car
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: motorcycle
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: airplane
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bus
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: train
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: truck
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: boat
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: traffic light
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: fire hydrant
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: stop sign
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: parking meter
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bench
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bird
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: cat
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: dog
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: horse
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: sheep
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: cow
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: elephant
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bear
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: zebra
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: giraffe
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: backpack
09-13 09:15:33.772 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: umbrella
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: handbag
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: tie
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: suitcase
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: frisbee
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: skis
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: snowboard
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: sports ball
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: kite
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: baseball bat
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: baseball glove
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: skateboard
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: surfboard
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: tennis racket
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bottle
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: wine glass
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: cup
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: fork
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: knife
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: spoon
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bowl
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: banana
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: apple
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: sandwich
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: orange
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: broccoli
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: carrot
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: hot dog
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: pizza
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: donut
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: cake
09-13 09:15:33.773 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: chair
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: couch
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: potted plant
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: bed
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: dining table
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: toilet
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: tv
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: laptop
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: mouse
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: remote
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: keyboard
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: cell phone
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: microwave
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: oven
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: toaster
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: sink
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: refrigerator
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: book
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: clock
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: vase
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: scissors
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: teddy bear
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: hair drier
09-13 09:15:33.774 16678-16678/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: toothbrush
09-13 09:15:33.778 16678-16678/org.tensorflow.demo I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
09-13 09:15:33.779 16678-16678/org.tensorflow.demo E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
09-13 09:15:33.779 16678-16678/org.tensorflow.demo I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
09-13 09:15:33.853 16678-16678/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
09-13 09:15:35.690 16678-16678/org.tensorflow.demo I/TensorFlowInferenceInterface: Model load took 1780ms, TensorFlow version: 1.3.0
09-13 09:15:35.692 16678-16678/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'
09-13 09:15:35.695 16678-16678/org.tensorflow.demo I/tensorflow: DetectorActivity: Sensor orientation: 90, Screen orientation: 0
09-13 09:15:35.695 16678-16678/org.tensorflow.demo I/tensorflow: DetectorActivity: Initializing at size 1280x960

```"
13011,Failed to run the benchmark_model,"System information
- Have I written custom code: No 
- OS Platform and Distribution: Linux Ubuntu 14.04
- TensorFlow installed from: source
- TensorFlow version: ('v1.3.0-rc1-1951-g04c318b', '1.3.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.5.0
- CUDA/cuDNN version: 8.0/5.1
- GPU model and memory: Quadro M4000 8Gb
Exact command to reproduce: bazel-bin/tensorflow/tools/benchmark/benchmark_model   --graph=inception5h/tensorflow_inception_graph.pb   --input_layer=""input:0""   --input_layer_shape=""1,224,224,3""   --input_layer_type=""float"" --output_layer=""output:0"" --show_run_order=false --show_time=false --show_memory=false --show_summary=false --show_flops=true

Problem
I follow the instruction on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark to run the benchmark_model on desktop but it failed when I set the --show_flops=true. The tensorflow_inception_graph.pb is downloaded from the link provided on the page. 

Logs
(my_project) ubuntu@ubuntu-desktop:~/Desktop/virtualroiflow/tensorflow$ bazel-bin/tensorflow/tools/benchmark/benchmark_model   --graph=inception5h/tensorflow_inception_graph.pb   --input_layer=""input:0""   --input_layer_shape=""1,224,224,3""   --input_layer_type=""float"" --output_layer=""output:0"" --show_run_order=false --show_time=false --show_memory=false --show_summary=false --show_flops=true
2017-09-13 17:53:21.911797: I tensorflow/tools/benchmark/benchmark_model.cc:426] Graph: [inception5h/tensorflow_inception_graph.pb]
2017-09-13 17:53:21.911837: I tensorflow/tools/benchmark/benchmark_model.cc:427] Input layers: [input:0]
2017-09-13 17:53:21.911841: I tensorflow/tools/benchmark/benchmark_model.cc:428] Input shapes: [1,224,224,3]
2017-09-13 17:53:21.911844: I tensorflow/tools/benchmark/benchmark_model.cc:429] Input types: [float]
2017-09-13 17:53:21.911846: I tensorflow/tools/benchmark/benchmark_model.cc:430] Output layers: [output:0]
2017-09-13 17:53:21.911852: I tensorflow/tools/benchmark/benchmark_model.cc:431] Num runs: [1000]
2017-09-13 17:53:21.911870: I tensorflow/tools/benchmark/benchmark_model.cc:432] Inter-inference delay (seconds): [-1.0]
2017-09-13 17:53:21.911873: I tensorflow/tools/benchmark/benchmark_model.cc:433] Inter-benchmark delay (seconds): [-1.0]
2017-09-13 17:53:21.911877: I tensorflow/tools/benchmark/benchmark_model.cc:435] Num threads: [-1]
2017-09-13 17:53:21.911894: I tensorflow/tools/benchmark/benchmark_model.cc:436] Benchmark name: []
2017-09-13 17:53:21.911897: I tensorflow/tools/benchmark/benchmark_model.cc:437] Output prefix: []
2017-09-13 17:53:21.911900: I tensorflow/tools/benchmark/benchmark_model.cc:438] Show sizes: [0]
2017-09-13 17:53:21.911917: I tensorflow/tools/benchmark/benchmark_model.cc:439] Warmup runs: [2]
2017-09-13 17:53:21.911920: I tensorflow/tools/benchmark/benchmark_model.cc:54] Loading TensorFlow.
2017-09-13 17:53:21.911941: I tensorflow/tools/benchmark/benchmark_model.cc:61] Got config, 0 devices
2017-09-13 17:53:21.912358: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-09-13 17:53:22.022462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-13 17:53:22.022832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: 
name: Quadro M4000 major: 5 minor: 2 memoryClockRate(GHz): 0.7725
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.61GiB
2017-09-13 17:53:22.022864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:01:00.0, compute capability: 5.2)
2017-09-13 17:53:22.078421: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Fact"" device_type: ""CPU"" label: ""sergey""') for unknown op: Fact
2017-09-13 17:53:22.078460: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Fact"" device_type: ""CPU"" label: ""Sergey""') for unknown op: Fact
2017-09-13 17:53:22.078466: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Fact"" device_type: ""GPU"" host_memory_arg: ""fact""') for unknown op: Fact
2017-09-13 17:53:22.081269: I tensorflow/tools/benchmark/benchmark_model.cc:291] Running benchmark for max 2 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences
2017-09-13 17:53:22.173360: I tensorflow/tools/benchmark/benchmark_model.cc:324] count=2 first=59501 curr=32424 min=32424 max=59501 avg=45962.5 std=13538

2017-09-13 17:53:22.173384: I tensorflow/tools/benchmark/benchmark_model.cc:291] Running benchmark for max 1000 iterations, max 10 seconds without detailed stat logging, with -1s sleep between inferences
2017-09-13 17:53:32.199487: I tensorflow/tools/benchmark/benchmark_model.cc:324] count=316 first=33259 curr=33929 min=29881 max=34487 avg=31683.6 std=684

2017-09-13 17:53:32.199512: I tensorflow/tools/benchmark/benchmark_model.cc:291] Running benchmark for max 1000 iterations, max 10 seconds with detailed stat logging, with -1s sleep between inferences
2017-09-13 17:53:32.200083: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally
2017-09-13 17:53:42.310948: I tensorflow/tools/benchmark/benchmark_model.cc:324] count=316 first=51892 curr=32599 min=29882 max=51892 avg=31710.3 std=1338

2017-09-13 17:53:42.310986: I tensorflow/tools/benchmark/benchmark_model.cc:538] Average inference timings in us: Warmup: 45962, no stats: 31683, with stats: 31710
2017-09-13 17:53:42.311006: I tensorflow/core/util/stat_summarizer.cc:358] Number of nodes executed: 141
2017-09-13 17:53:42.311157: I tensorflow/core/util/stat_summarizer.cc:468] ============================== Summary by node type ==============================
2017-09-13 17:53:42.311162: I tensorflow/core/util/stat_summarizer.cc:468] 	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
2017-09-13 17:53:42.311166: I tensorflow/core/util/stat_summarizer.cc:468] 	                  Conv2D	       22	    35.377	    76.119%	    76.119%	 10077.888	       22
2017-09-13 17:53:42.311170: I tensorflow/core/util/stat_summarizer.cc:468] 	                     LRN	        2	     4.296	     9.243%	    85.362%	  3211.264	        2
2017-09-13 17:53:42.311173: I tensorflow/core/util/stat_summarizer.cc:468] 	                 MaxPool	        6	     3.124	     6.722%	    92.084%	  3562.496	        6
2017-09-13 17:53:42.311177: I tensorflow/core/util/stat_summarizer.cc:468] 	                 BiasAdd	       24	     1.677	     3.608%	    95.692%	     0.000	       24
2017-09-13 17:53:42.311195: I tensorflow/core/util/stat_summarizer.cc:468] 	                    Relu	       23	     0.740	     1.592%	    97.285%	     0.000	       23
2017-09-13 17:53:42.311199: I tensorflow/core/util/stat_summarizer.cc:468] 	                  MatMul	        2	     0.703	     1.513%	    98.797%	     8.128	        2
2017-09-13 17:53:42.311216: I tensorflow/core/util/stat_summarizer.cc:468] 	                  Concat	        3	     0.375	     0.807%	    99.604%	  2706.368	        3
2017-09-13 17:53:42.311220: I tensorflow/core/util/stat_summarizer.cc:468] 	                   Const	       51	     0.082	     0.176%	    99.781%	     0.000	       51
2017-09-13 17:53:42.311225: I tensorflow/core/util/stat_summarizer.cc:468] 	                 AvgPool	        1	     0.057	     0.123%	    99.903%	    32.512	        1
2017-09-13 17:53:42.311229: I tensorflow/core/util/stat_summarizer.cc:468] 	                 Softmax	        1	     0.028	     0.060%	    99.963%	     0.000	        1
2017-09-13 17:53:42.311232: I tensorflow/core/util/stat_summarizer.cc:468] 	                    NoOp	        1	     0.006	     0.013%	    99.976%	     0.000	        1
2017-09-13 17:53:42.311236: I tensorflow/core/util/stat_summarizer.cc:468] 	                    _Arg	        1	     0.004	     0.009%	    99.985%	     0.000	        1
2017-09-13 17:53:42.311252: I tensorflow/core/util/stat_summarizer.cc:468] 	                 _Retval	        1	     0.003	     0.006%	    99.991%	     0.000	        1
2017-09-13 17:53:42.311256: I tensorflow/core/util/stat_summarizer.cc:468] 	                 Reshape	        2	     0.003	     0.006%	    99.998%	     0.000	        2
2017-09-13 17:53:42.311274: I tensorflow/core/util/stat_summarizer.cc:468] 	                Identity	        1	     0.001	     0.002%	   100.000%	     0.000	        1
2017-09-13 17:53:42.311278: I tensorflow/core/util/stat_summarizer.cc:468] 
2017-09-13 17:53:42.416261: E tensorflow/tools/benchmark/benchmark_model.cc:556] FLOPs calculation failed with Invalid argument: You must feed a value for placeholder tensor 'input' with dtype float
	 [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
"
13009,'load_csv_with_header' no longer works. Please suggest a solution.,"I've been trying to run the tutorial program on IRIS data set and everytime when I try to run the program am seeing this error.

'module' object has no attribute 'load_csv_with_header'

Please suggest me how to solve this. "
13007,Feature: tf.decode_csv support NA values,"I found many people prefer to using `NA`, `null` or `NULL` or other string as missing value in HIVE table. But `tf.decode_csv` cannot handle the case. So, I suggest to add `na_value` argument like [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."
13006,pip error while training model on cloud,"The replica master 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-9_013O-build/setup.py' The replica worker 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-QNtwDr-build/setup.py' The replica worker 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-pA8eBo-build/setup.py' The replica worker 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-Xk5nGm-build/setup.py' The replica worker 3 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-3VkzR6-build/setup.py' The replica worker 4 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-NIf2Jb-build/setup.py' The replica worker 5 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-RlyPFQ-build/setup.py' The replica worker 6 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-QXzr7y-build/setup.py' The replica worker 7 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-FvkZhP-build/setup.py' The replica worker 8 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-Ue8_oJ-build/setup.py' The replica ps 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-AxLdZ1-build/setup.py' The replica ps 1 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-7nE9PF-build/setup.py' The replica ps 2 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File ""<string>"", line 1, in <module> IOError: [Errno 2] No such file or directory: '/tmp/pip-GSkkn1-build/setup.py' To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=802068032926&resource=ml_job%2Fjob_id%2Fhotifya9f40b85_e142_431b_82fc_8be28b5dc8c2&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22hotifya9f40b85_e142_431b_82fc_8be28b5dc8c2%22"
13005,tf.nn.in_top_k strange behavior,"Based on the documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) tf.nn.in_top_k treats ties as positive, which is almost never what you'd want. This is a major flaw, which leads to misleading metrics. An all zero predictions would give you 0 error with any targets. I think the default behavior should be to not count ties, "
13004,Tensorflow import breaks numpy calls,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

See source code below.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
```
% cat tf_env.txt
== cat /etc/issue ===============================================
Linux bda9acc7988e 4.12.9-300.fc26.x86_64 #1 SMP Fri Aug 25 13:09:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial
== are we in docker =============================================
Yes
== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux bda9acc7988e 4.12.9-300.fc26.x86_64 #1 SMP Fri Aug 25 13:09:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
```

- **TensorFlow installed from (source or binary)**:
`pip install tensorflow-gpu`

- **TensorFlow version (use command below)**:
```
 % pip list | grep 'numpy\|scipy\|tensorflow\|wheel'
DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(leg
acy|columns) in your pip.conf under the [list] section) to disable this warning.
numpy (1.12.1)
scipy (0.19.0)
tensorflow-gpu (1.3.0)
tensorflow-tensorboard (0.1.6)
wheel (0.29.0)
```
```
% python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.3.0-rc2-20-g0787eee 1.3.0
```
- **Python version**: 
```
 % python
Python 3.5.3 |Intel Corporation| (default, Apr 27 2017, 18:08:47)
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
```
- **Bazel version (if compiling from source)**:
Not compiling from source.

- **CUDA/cuDNN version**:
`/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61`

- **GPU model and memory**:
```
 % nvidia-smi
Tue Sep 12 21:56:13 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.69                 Driver Version: 384.69                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   56C    P8    12W / 200W |     62MiB /  6070MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
```

- **Exact command to reproduce**:
Run this test1.py in a new python terminal (below), once with the `import tensorflow` line, once without it. The code loads a stock image, and calls np.fft.fft2, and reads values on the corners of the resulting matrix. Output is as follows:

Expected:
```
 % python
Python 3.5.3 |Intel Corporation| (default, Apr 27 2017, 18:08:47)
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> exec(open('test1.py').read()) # WITHOUT TF IMPORT
(768, 1024)
-- loop 0 --
0.825823 0.593467 0.860577 0.842033
(595544+0j) (-41240.3+54651.8j) (-28777.7+57397.5j) (28693.1-15865.9j)

-- loop 1 --
0.825823 0.593467 0.860577 0.842033
(595544+0j) (-41240.3+54651.8j) (-28777.7+57397.5j) (28693.1-15865.9j)

-- loop 2 --
0.825823 0.593467 0.860577 0.842033
(595544+0j) (-41240.3+54651.8j) (-28777.7+57397.5j) (28693.1-15865.9j)
```

Problem: non-deterministic loop after TF import,
```
 % python
Python 3.5.3 |Intel Corporation| (default, Apr 27 2017, 18:08:47)
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> exec(open('test1.py').read()) # WITH TF IMPORT
(768, 1024)
-- loop 0 --
0.825823 0.593467 0.860577 0.842033
(4.01544e+06-4.30166e+06j) (-384357+1.41085e+06j) (349169+983449j) (228823-235335j)

-- loop 1 --
0.825823 0.593467 0.860577 0.842033
(6.03746e+06+42733.2j) (83067.3-1.09108e+06j) (-331458-151395j) (-87943.5+165673j)

-- loop 2 --
0.825823 0.593467 0.860577 0.842033
(475988+254405j) (11989.8+49912j) (13361.2+6137.22j) (-3503.42-15194.5j)
```

### Describe the problem
Importing tensorflow causes numpy.fft.fft2 to produce non-deterministic results. This is not expected. Why does importing tensorflow break the numpy call?

Just for reference, I have been able to tf.Session() with the GPU. In any case, tensorflow should not override / affect the numpy package. I am not sure what is going on here.
```
 % python
Python 3.5.3 |Intel Corporation| (default, Apr 27 2017, 18:08:47)
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
tf.S>>> tf.Session()
2017-09-12 22:04:13.379385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1
 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-12 22:04:13.379504: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2
 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-12 22:04:13.379534: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX in
structions, but these are available on your machine and could speed up CPU computations.
2017-09-12 22:04:13.379560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 i
nstructions, but these are available on your machine and could speed up CPU computations.
2017-09-12 22:04:13.379584: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA in
structions, but these are available on your machine and could speed up CPU computations.
2017-09-12 22:04:13.512425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had neg
ative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-12 22:04:13.512712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 5.80GiB
2017-09-12 22:04:13.512727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-12 22:04:13.512731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-12 22:04:13.512739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device:
 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
<tensorflow.python.client.session.Session object at 0x7f396c316c88>
```

### Source code / logs
test1.py
```
import numpy as np
import scipy.misc
import tensorflow as tf # comment this out to make deterministic

DTYPE = np.complex64
im_uint8 = scipy.misc.face()
im = im_uint8.astype(DTYPE) / (np.power(2,im_uint8.itemsize*8) - 1)
im = np.linalg.norm(im, axis=-1)
print(im.shape)
printcorners = lambda M: print( M[0,0], M[-1,0], M[0,-1], M[-1,-1])

for i in range(0,3):
    # THIS LOOP SHOULD BE DETERMINISTIC!
    print('-- loop',i,'--')
    B = im.copy()
    printcorners(B)
    Bhat = np.fft.fft2(B.copy())
    printcorners(Bhat)
    print('')
#
```

Thanks in advance."
13002,1_notmnist assignment1 zero mean equation in code,"Okay in tutorial its say to make zero mean do r-128/128 but in code its r-128/255
Below is the code provided in ipython notebook
image_size = 28  # Pixel width and height.
pixel_depth = 255.0  # Number of levels per pixel.

def load_letter(folder, min_num_images):
  """"""Load the data for a single letter label.""""""
  image_files = os.listdir(folder)
  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
                         dtype=np.float32)
  print(folder)
  num_images = 0
  for image in image_files:
    image_file = os.path.join(folder, image)
    try:
      image_data = (ndimage.imread(image_file).astype(float) - 
                    pixel_depth / 2) / pixel_depth
      if image_data.shape != (image_size, image_size):
        raise Exception('Unexpected image shape: %s' % str(image_data.shape))
      dataset[num_images, :, :] = image_data
      num_images = num_images + 1
    except IOError as e:
      print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')"
12999,"data_format=""NCHW""    didn't   work  for    tf.nn.max_pool","In [78]: x2 = tf.random_uniform([64,3,32,32])

In [79]: x2_pool = tf.nn.max_pool(x2,  [1,3,3,1], [1,2,2,1], padding=""VALID"", data_format=""NHWC"")

In [80]: x2_pool.shape
Out[80]: TensorShape([Dimension(64), Dimension(1), Dimension(15), Dimension(32)])

In [81]: x2_pool = tf.nn.max_pool(x2,  [1,3,3,1], [1,2,2,1], padding=""VALID"", data_format=""NCHW"")

In [82]: x2_pool.shape
Out[82]: TensorShape([Dimension(64), Dimension(1), Dimension(15), Dimension(32)])
"
12998,An error in  llvm/Object/SymbolicFile.h : expected ')' before 'PRIxPTR',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS Linux release 7.2.1511 (Core)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  git hash 1e96d54d9f928c4ea4bf0564ef9900f6bd03acd5
- **Python version**:  3.6.1
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: CUDA 8.0, cudnn : 5.1.5 
- **GPU model and memory**: Titan X (Maxwell) + Titan X (Pascal) + GTX 1080(Pascal)
- **Exact command to reproduce**: bazel build --config=opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package 

Compiler used : `gcc` 
```
-bash-4.2$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)

```

On compiling the source with the hash `1e96d54d9f928c4ea4bf0564ef9900f6bd03acd5` , the compilation goes on smoothly and towards the end produces an error as follows : 
`/home/uujjwal/opensource/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:124:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:cpu_executable' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed`

I am trying to build with XLA JIT compilation enabled. The problem comes from that. Lower down the long message, following comes as an error : 

```
In file included from external/llvm/include/llvm/Object/ObjectFile.h:23:0,
                 from external/llvm/include/llvm/DebugInfo/DIContext.h:19,
                 from external/llvm/include/llvm/ExecutionEngine/RuntimeDyld.h:19,
                 from external/llvm/include/llvm/ExecutionEngine/Orc/RTDyldObjectLinkingLayer.h:21,
                 from ./tensorflow/compiler/xla/service/cpu/simple_orc_jit.h:25,
                 from ./tensorflow/compiler/xla/service/cpu/cpu_executable.h:26,
                 from tensorflow/compiler/xla/service/cpu/cpu_executable.cc:16:
external/llvm/include/llvm/Object/SymbolicFile.h: In function 'OStream& llvm::object::operator<<(OStream&, const llvm::object::DataRefImpl&)':
external/llvm/include/llvm/Object/SymbolicFile.h:48:31: error: expected ')' before 'PRIxPTR'
   OS << ""("" << format(""0x%08"" PRIxPTR, D.p) << "" ("" << format(""0x%08x"", D.d.a)
```
It seems to be a problem with a header file and hence I'd be grateful for answering this bug."
12997,Ability to read multi-channel image format,"Though the `tf.decode_image()` function can handle a few different image encodings, all of these encodings can only support a maximum of 4 channels in an image (RGBA).   One format which can support a larger number of channels is TIFF (which is currently not supported in TF, perhaps due to its complex specification?).  It would be convenient if TF could support decoding TIFF or some other format that is able to represent an arbitrary number of channels, `N` (which could be decoded into an `[H, W, N]` shaped tensor).
"
12996,restore part of model using Estimator API,"Hi, 

I am trying to restore part of model using tensorflow high level API Estimator,  but I can not find I way.
I can only specify checkpoint_path for Estimator.

Here is an example.
I what to extract COCO images feature using tf-slim pretrained model resnet_v2, but this pretrained model does not provide ""resnet_v2_152/block1/unit_1/bottleneck_v2/conv2/biases"". So, I need to initialize it to zero.
But using Estimator, It will find that this biases not in checkpoint and raise a Not found error.

```
import tensorflow as tf

import tensorflow.contrib.slim.python.slim.nets.resnet_v2 as resnet_v2


def cnn_model_fn(features, labels, mode):
    net, end_points = resnet_v2.resnet_v2_152(inputs=features, is_training=mode == tf.estimator.ModeKeys.TRAIN)
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=net)
    else:
        raise NotImplementedError('only support predict!')


def parse_filename(filename):
    image_string = tf.read_file(filename)
    image_decoded = tf.image.decode_jpeg(image_string, channels=3)
    image_resized = tf.image.resize_images(image_decoded, [256, 256])
    return image_resized


def dataset_input_fn(dataset, num_epochs=None, batch_size=128, shuffle=False, buffer_size=1000, seed=None):
    def input_fn():
        d = dataset.repeat(num_epochs).batch(batch_size)
        if shuffle:
            d = d.shuffle(buffer_size)
        iterator = d.make_one_shot_iterator()
        next_example = iterator.get_next()
        return next_example

    return input_fn


filenames = sorted(tf.gfile.Glob('/root/data/COCO/download/val2014/*'))
dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames).map(parse_filename)

input_fn = dataset_input_fn(dataset, num_epochs=1, batch_size=1, shuffle=False)

estimator = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=None)

es = estimator.predict(input_fn=input_fn,
                       checkpoint_path='/root/data/checkpoints/resnet_v2_152_2017_04_14/resnet_v2_152.ckpt')
print(es.__next__())


print(""Done!"")

```

The error is obvious.
`2017-09-12 20:36:31.231422: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Tensor name ""resnet_v2_152/block1/unit_1/bottleneck_v2/conv2/biases"" not found in checkpoint files /root/data/checkpoints/resnet_v2_152_2017_04_14/resnet_v2_152.ckpt`


"
12994,Steps in getting started guide are incorrect.,"Hi, would appreciate if someone can help me get started. It must be something very trivial, but as a beginner, I did not find an answer anywhere. Looking for help.

1. I had to refer to stack overflow for basic installation on windows - https://stackoverflow.com/questions/38896424/tensorflow-not-found-in-pip

2. Even after that I get error as below (just trying import tensorflow as tf)- 
```
Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Program Files (x86)\Python36-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""gs.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Program Files (x86)\Python36-32\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Program Files (x86)\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
```
This is same as https://github.com/tensorflow/tensorflow/issues/1402, which is closed and I could not find a solution anywhere.

Please let me know if more information is needed, I am looking forward to get started successfully and learn tensor flow.
------------------------

### System information
- import tensorflow as tf
- **windows 10
- **binary**:
- **command below doesn't work, downloaded the latest version just now.**:
- **Python version 3.6.3**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version -- no idea**:
- **GPU model and memory - NA, 32GB TAM**:
- **Exact command to reproduce -- import tensorflow as tf**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
~~above command does not work and gives the same error.
"
12993,Different corrupted record Issue with TFRecordReader ,"Hi, I am trying to convert some 11 mn features and labels into TFRecords; I convert my examples into scipy csr/coo and the features are simple 1Dim features. I am trying to make a batch of features of 1000 in a csr and I am able to write it into TFRecordWriter. The files are generated.
 The problem is when I try to read it using parse_example, I am trying to create a tensor equivalent to compressed sparse matrix of scipy and I am running into issues. I am ONLY trying to test my tensor values using sess.run(<tensor>) and I am getting this corrupted error. I tried to change different reading formats , FixedFeature, VarLenFeature, but no breakthrough.
  
corrupted record at 0
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]

Here is how I am trying to parse the TFRecord

 features={
                'feature_data' : tf.FixedLenFeature([],tf.string),
                'feature_row' : tf.FixedLenFeature([],tf.string),
                'feature_col' : tf.FixedLenFeature([], tf.string),
                'feature_shape' : tf.FixedLenFeature([], tf.string)}
#                 'feature_dim2' : tf.FixedLenFeature([1], tf.int64)}
        print 'Reader initialized'
        tfrecord_features = tf.parse_single_example(serialized_example, features)
        fea_data = tf.decode_raw(tfrecord_features['feature_data'], tf.int32)
        fea_row = tf.decode_raw(tfrecord_features['feature_row'], tf.int32)
        fea_col = tf.decode_raw(tfrecord_features['feature_col'], tf.int32)
        fea_shape = tf.decode_raw(tfrecord_features['feature_shape'], tf.int32)
        print 'Parsed example'
        print type(fea_data)
        print type(fea_row)
        ade_features = tf.sparse_to_dense(sparse_indices=[fea_row, fea_col], 
                                    sparse_values=fea_data, 
                                    output_shape=fea_shape)
       ............
       sess.run(ade_features)

Here is how I am trying to write the feature.

 writer = tf.python_io.TFRecordWriter(filename)
        #for index in range(features_vectors.shape[0]):
        for iteration, (start, end) in enumerate(batches):
            #print('Writing label index: ' + str(index))
            feature_batch = features_vectors[start:end] # again a csr
            feature_batch = feature_batch.tocoo()
            label_vec = labels[start:end] # csr
            label_vec = label_vec.tocoo()
            example = tf.train.Example(features=tf.train.Features(feature={
                'feature_data' : self.bytes_feature(feature_batch.data.tobytes()),
                'feature_row' : self.bytes_feature(feature_batch.row.tobytes()),
                'feature_col' : self.bytes_feature(feature_batch.col.tobytes()),
                'feature_shape' : self.bytes_feature(np.array(feature_batch.shape, np.int32).tobytes())
            writer.write(example.SerializeToString())
        writer.close()

I am using tensorflow 1.2.1"
12992,Mixture model with gaussians fails to create,"Hello! 

I have written some code to model GMM and further prob some data points. I already have weights and other params for initialization. Here is the code:

```
ds = tf.contrib.distributions
gaussians = []
for i in range(0, len(weights)):
    mns = means[i]
    covs = covariances[i]
    gaussians.append(ds.MultivariateNormalFullCovariance(loc=np.array(means).astype(np.float64), covariance_matrix=np.array(covariances).astype(np.float64)))

gmm = ds.Mixture(
  cat=ds.Categorical(probs=weights.astype(np.float64)),
  components=gaussians
)
```

However in this stage:
` cat=ds.Categorical(probs=weights.astype(np.float64)),`

there is this error:

> Traceback (most recent call last):
>   File ""D:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 560, in merge_with
>     self.assert_same_rank(other)
>   File ""D:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 605, in assert_same_rank
>     other))
> ValueError: Shapes () and (32,) must have the same rank

I have 32 gaussians per mixture, data has 26 features. 
Weights is a list of floats, means - list of floats for each gaussian, covariances - matrix in form of list of lists for each gaussian.

I started to see why Shapes is just empty, and found this piece of code from tf from exception:
```
  @property
  def batch_shape(self):
    """"""Shape of a single sample from a single event index as a `TensorShape`.

    May be partially defined or unknown.

    The batch dimensions are indexes into independent, non-identical
    parameterizations of this distribution.

    Returns:
      batch_shape: `TensorShape`, possibly unknown.
    """"""
    return self._batch_shape()
```

So, the problem might be somewhere in **self._batch_shape()** function. And in categorical.py, I found this lines of code:
 
```
 def _batch_shape(self):
    return self.logits.get_shape()[:-1]
```

When debugging, I found out that this exact function is hit when Shape check is performed. And **self.logits.get_shape()** does return Shape(32,), but taking **[:-1]** from it, leads to an error. 

Is this an intended behaviour?


"
12990,Does tensorflow is used for content based recommendations?,"Hi,
I wanted to perform content based recommendation (context based) but I did not find any example of tensor flow doing it.can we use tensor flow for doing content based recommendations. "
12988,android //tensorflow:libtensorflow.so build fails due to missing build target,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
export CC_OPT_FLAGS=""-march=armv7-a -std=c++11""
yes '' | ./configure
bazel build --config=opt --verbose_failures //tensorflow/contrib/android:libtensorflow_inference.so //tensorflow:libtensorflow.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a
```

### Describe the problem
`//tensorflow/c/eager:c_api` depends on `//tensorflow/c/eager:c_api_internal` when building with `//tensorflow:android`, but `:c_api_internal` doesn't exist.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/BUILD#L19

### Source code / logs

Error:
```
ERROR: /home/travis/tensorflow/tensorflow/c/eager/BUILD:11:1: in deps attribute of cc_library rule //tensorflow/c/eager:c_api: target '//tensorflow/c/eager:c_api_internal' does not exist. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation in /home/travis/tensorflow/tensorflow/tensorflow.bzl:669:12
```

Full build: https://travis-ci.org/luk-ai/build-tensorflow/builds/274455610"
12986,boringssl sha256 checksum mismatched,"https://github.com/tensorflow/tensorflow/blob/5541ef4fbba56cf8930198373162dd3119e6ee70/tensorflow/workspace.bzl#L583

when download from https://github.com/google/boringssl/archive/e3860009a091cd1bd2bc189cdbc3c6d095abde84.tar.gz, the sha256 checksum is 
```
a9a3673b1f7bd80ef563e9de1d9ccdb5126dc0cce6377977009092148993c4fe
```

 , but expected is
```
02f5950f93c4fd3691771c07c9d04cf2999ab01383ff99da345249e93b0fcfb2 
```
.

Is there something wrong?

thanks."
12985,Possible bug in nightly build for Linux-gpu,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS Linux release 7.2.1511 (Core)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.0-rc1-2023-g4af9be9
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: cuda 8.0 , cudnn - 5.1.5
- **GPU model and memory**: Titan X (Maxwell) + Titan X (Pascal)

The compilation fails with the following error 

```
ERROR: /home/uujjwal/opensource/tensorflow/tensorflow/tools/pip_package/BUILD:139:1 C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:parallel_cpu_executable' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/uujjwal/.cache/bazel/_bazel_uujjwal/bc79cf6944ed9a65fbe038e7ed177538/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/misc/opt/cuda/8.0 \
    CUDNN_INSTALL_PATH=/misc/opt/cudnn/5.1-cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/home/uujjwal/dlang/usr/lib64:/home/uujjwal/dlang/usr/lib:/home/uujjwal/cntk/cntkfixed/CNTK/bindings/python/cntk/libs:/home/uujjwal/cntk/mpi-new/lib: \
    PATH=/home/uujjwal/collection-stars/build-sources/bazel-0.5.4/output:/home/uujjwal/dlang/usr/bin:/home/uujjwal/gocodes/bin:/home/uujjwal/opensource/go1.9/bin:/home/uujjwal/collection-stars/anaconda3/bin:/home/uujjwal/cntk/cntkfixed/CNTK/bin:/home/uujjwal/bin:/home/uujjwal/cntk/mpi-new/bin:/home/uujjwal/p33/bin:/home/uujjwal/bin:/home/uujjwal/cntk/mpi-new/bin:/home/uujjwal/anaconda3/bin:/home/uujjwal/anaconda3/bin:/data/stars/share/michal_toolbox:/usr/lib/oar/oardodo:/usr/lib/oar/oardodo:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/uujjwal/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/uujjwal/collection-stars/anaconda3/bin/python \
    PYTHON_LIB_PATH=/home/uujjwal/collection-stars/anaconda3/lib/python3.6/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=5.1.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/parallel_cpu_executable/tensorflow/compiler/xla/service/cpu/parallel_cpu_executable.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/parallel_cpu_executable/tensorflow/compiler/xla/service/cpu/parallel_cpu_executable.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-py3-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/protobuf_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-py3-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/local_linux-py3-opt/genfiles/external/llvm -isystem external/nsync/public -isystem bazel-out/local_linux-py3-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem external/llvm/lib/IR -isystem bazel-out/local_linux-py3-opt/genfiles/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/local_linux-py3-opt/genfiles/external/llvm/include/llvm/IR -isystem external/llvm/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/llvm/include -isystem external/llvm/lib/Target/PowerPC -isystem bazel-out/local_linux-py3-opt/genfiles/external/llvm/lib/Target/PowerPC -isystem external/llvm/lib/Target/X86 -isystem bazel-out/local_linux-py3-opt/genfiles/external/llvm/lib/Target/X86 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/compiler/xla/service/cpu/parallel_cpu_executable.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/parallel_cpu_executable/tensorflow/compiler/xla/service/cpu/parallel_cpu_executable.pic.o).
```
On the TensorFlow GitHub page, it shows that the nightly build is passing. I am confused by this"
12984,bazel build failed due to boringssl checksum mismatches,"No custom code.
System snapshot:
GCC 5.3.1
Centos 7
Bazel 0.5.4

Building from commit id 79517578deb4ea2a8dea641f1a8fa4363c13f76b from master.

[user@master tensorflow]$ bazel build --config=mkl --copt=""-march=knl"" --copt=""-O3"" -s -c opt //tensorflow/tools/pip_package:build_pip_package
WARNING: ignoring http_proxy in environment.
ERROR: /home/user/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: no such package '@boringssl//': java.io.IOException: Error downloading [http://mirror.bazel.build/github.com/google/boringssl/archive/e3860009a091cd1bd2bc189cdbc3c6d095abde84.tar.gz, https://github.com/google/boringssl/archive/e3860009a091cd1bd2bc189cdbc3c6d095abde84.tar.gz] to /home/user/.cache/bazel/_bazel_user/59b9d4cecf85aafc18f6c320e7734241/external/boringssl/e3860009a091cd1bd2bc189cdbc3c6d095abde84.tar.gz: Checksum was a9a3673b1f7bd80ef563e9de1d9ccdb5126dc0cce6377977009092148993c4fe but wanted 02f5950f93c4fd3691771c07c9d04cf2999ab01383ff99da345249e93b0fcfb2 and referenced by '//tensorflow/tools/pip_package:licenses'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
INFO: Elapsed time: 6.963s
[user@master tensorflow]$"
12980,Blocking of tf.contrib.StagingArea get() and put() operations,"**Work Environment**
TensorFlow release version : 1.3.0-rc2
TensorFlow git version : v1.3.0-rc1-994-gb93fd37
Operating System : CentOS Linux release 7.2.1511 (Core)

I am using TensorFlow StagingArea ops for increasing the efficiency of my input pipeline. Here is a part of my code snippet which constructs the input pipeline :
```
    train_put_op_list = []
    train_get_op_list = []
    val_put_op_list = []
    val_get_op_list = []
    with tf.variable_scope(tf.get_variable_scope()) as vscope:
        for i in range(4):
            with tf.device('/gpu:%d'%i):
                with tf.name_scope('GPU-Tower-%d'%i) as scope:
                    trainstagingarea = tf.contrib.staging.StagingArea(dtypes=[tf.float32, tf.int32],
                                                                 shapes=[[64, 221, 221, 3],[64]],
                                                                      capacity=0)
                    valstagingarea = tf.contrib.staging.StagingArea(dtypes=[tf.float32, tf.int32],
                                                                      shapes=[[128, 221, 221, 3],[128]],
                                                                      capacity=0)
                    train_put_op_list.append(trainstagingarea.put(train_iterator.get_next()))
                    val_put_op_list.append(valstagingarea.put(val_iterator.get_next()))
                    train_get_op_list.append(trainstagingarea.get())
                    val_get_op_list.append(valstagingarea.get())
                    with tf.device('/cpu:0'):
                        worktype = tf.get_variable(""wt"",[], initializer=tf.zeros_initializer(), trainable=False)
                    workcondition = tf.equal(worktype, 1)
                    #elem = tf.cond(workcondition, lambda: train_iterator.get_next(), lambda: val_iterator.get_next())
                    elem = tf.cond(workcondition, lambda: train_get_op_list[i], lambda: val_get_op_list[i])
                    # This is followed by the network construction and optimizer 
```
Now at the time of execution, I first run the put() ops a couple of times and then go on to run the iterations. It is shown below :
```
with tf.Session(config=config) as sess:
        sess.run(init_op)
        sess.run(iterator_training_op)
        sess.run(iterator_validation_op)
        sess.run(tf.assign(worktype, 0))
        for i in range(4):
            sess.run(train_put_op_list)
            sess.run(val_put_op_list)
        writer = tf.summary.FileWriter('.', graph=tf.get_default_graph())
        epoch = 0
        iter = 0
        previous = 0
        while(epoch<10):
            try:
                if(PROCESSINGTYPE is 'validation'):
                    sess.run(val_put_op_list)
                    [val_accu, summaries, numsamp] = sess.run([running_accuracy, validation_summary_op, processed])
                    previous+=numsamp
                    print(""Running Accuracy = {} : Number of sample processed = {} "".format(val_accu, previous))
                else:
                    sess.run(train_put_op_list)
                    [loss_value, _, train_accu, summaries, batch_accu, numsamp] = sess.run([total_loss, apply_gradient_op, running_accuracy, training_summary_op, batch_accuracy, pr\
ocessed])
                    #Remaining part of the code (not important for question)

```
The use of StagingArea improves the speed substantially (almost 3-4 times). However, the code hangs due to some block. I am not sure if the block comes from get() or put() operations. Here is the actual output :
```

# Validation is done first and the following is the output
Running Accuracy = 0.0 : Number of sample processed = 512
Running Accuracy = 0.00390625 : Number of sample processed = 1024
Running Accuracy = 0.0 : Number of sample processed = 1536
Running Accuracy = 0.001953125 : Number of sample processed = 2048
# The code hangs here
```
You can notice that in the beginning of `tf.Session() as sess:`, the `get()` and `put()` ops were run for `4` times. The output is limited to 4 lines as well. This means that, 
`sess.run(val_put_op_list)` within the `while` loop does not do anything. So, when the `get()` is called by `sess.run(running_accuracy)...`, the `StagingArea` is found empty after `4` lines and hence a blocking happens.

 - Am I correct in my analysis of the problem ?
 - What is the correct way to use the `get()` and `put()` ops here ?
 - If `StagingArea` is full and `put()` is blocked, would that also block the whole code ? TensorFlow documentation does not say anything about it."
12979,bazel compiliation is broken! build failure due to github checksums changing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `bazel build --verbose_failures //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`

### Describe the problem

GitHub tarball checksums have changed making it impossible to build tensorflow since the checksums don't match any more.

https://github.com/bazelbuild/bazel/issues/3722

### Source code / logs

```
ERROR: /home/travis/tensorflow/tensorflow/contrib/android/BUILD:72:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to /home/travis/.cache/bazel/_bazel_travis/c397b760afc31b444fffb10b0086dea5/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/contrib/android:libtensorflow_inference.so'
```

```
 /tmp/foo î‚° curl -L https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz | sha256sum
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   157    0   157    0     0    157      0 --:--:-- --:--:-- --:--:--   301
100 4274k  100 4274k    0     0  4274k      0  0:00:01  0:00:01 --:--:-- 8710k
e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d  -
 /tmp/foo î‚° curl http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz | sha256sum
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 4274k  100 4274k    0     0  4274k      0  0:00:01 --:--:--  0:00:01 6177k
6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93  -
```"
12978,Bazel & cmake compilations for Windows are completely broken,"Can't compile tensorflow revision 1.3 using Bazel (revision of the last successful nightly build using cmake). Error is the following:

```
ERROR: C:/tensorflow-1.3rc1/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 2): cl.exe failed: error executing command
  cd C:/msys64/tmp/_bazel_dmitry/ehgyfc-k/execroot/org_tensorflow
  SET CUDA_COMPUTE_CAPABILITIE=None
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\\shared;C:\Program Files (x86)\Windows Kits\8.1\include\\um;C:\Program Files (x86)\Windows Kits\8.1\include\\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\msys64\mingw64\bin;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Dmitry/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Dmitry/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\Dmitry\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2
    SET TF_CUDA_VERSION=8.0
    SET TF_CUDNN_VERSION=6
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL=0
    SET TMP=C:\Users\Dmitry\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -march=native -march=native /nologo /I. /Ibazel-out/msvc_x64-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/msvc_x64-py3-opt/genfiles/external/bazel_tools /Iexternal/protobuf /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/msvc_x64-py3-opt/genfiles/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/msvc_x64-py3-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/msvc_x64-py3-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/msvc_x64-py3-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/snappy /Ibazel-out/msvc_x64-py3-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda /Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/protobuf/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf/src /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include /showIncludes /DEIGEN_MPL2_ONLY /DSNAPPY /MT /O2 /c tensorflow/stream_executor/cuda/cuda_gpu_executor.cc /Fobazel-out/msvc_x64-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_gpu_executor.o.
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(274): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(274): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(289): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(289): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(372): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(372): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(620): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(620): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(639): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(639): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(666): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(666): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(680): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(680): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(694): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(694): error C2059: syntax error: '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(708): error C2589: 'constant': illegal token on right side of '::'
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc(708): error C2059: syntax error: '::'
cl : Command line warning D9002 : ignoring unknown option '-march=native'
cl : Command line warning D9002 : ignoring unknown option '-march=native'
____Building complete.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
____Elapsed time: 216.281s, Critical Path: 141.66s
```

If you try to compile master revision then compilation even won't start with the error:

```
ERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:1932:1: C++ compilation of rule '//tensorflow/core/kernels:resize_bilinear_op_gpu' failed (Exit 2).
.\tensorflow/core/util/cuda_kernel_helper.h(359): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(360): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(361): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(362): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(365): error C3861: 'atomicMax': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(366): error C3861: 'atomicMax': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'max': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(393): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(399): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(445): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(462): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(498): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(507): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(516): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__float_as_int': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(526): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(529): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(537): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(540): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(548): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(557): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(566): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__float_as_int': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(576): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(579): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(587): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(590): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(617): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(619): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(620): error C3861: '__shfl': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(621): error C3861: '__shfl': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(622): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(637): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(639): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(640): error C3861: '__shfl_up': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(641): error C3861: '__shfl_up': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(642): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(657): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(659): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(660): error C3861: '__shfl_down': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(661): error C3861: '__shfl_down': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(662): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(677): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(679): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(680): error C3861: '__shfl_xor': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(681): error C3861: '__shfl_xor': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(682): error C2059: syntax error: 'volatile'
cl : Command line warning D9002 : ignoring unknown option '-march=native'
cl : Command line warning D9002 : ignoring unknown option '-march=native'
cl : Command line warning D9002 : ignoring unknown option '-x'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=relaxed-constexpr'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=ftz=true'
cl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed
cl : Command line warning D9027 : source file 'cuda' ignored
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 3817.327s, Critical Path: 143.61s
```

I'll be glad to help to fix the issues. But I absolutely has no idea what's going on here. Can anybody help to fix Windows compilation please? Currently here is no build for Windows with AVX, AVX2 support in the whole world!"
12977,Cmake nightly builds for Windows are broken for more than a month!,"Cmake nightly builds for Windows are broken for more than a month!
Will tensorflow ever support Windows again?"
12974,"Feature missing? Ability to get Tensorboard output from provided ""estimators""","This is opened here since it relates to Estimator and canned estimator models not providing a method to get output for TB

3. It shouldn't be a TensorBoard issue. And I attempted to get an answer at Stackoverflow.. (here https://stackoverflow.com/questions/46064056/no-easy-way-to-add-tensorboard-output-to-pre-defined-estimator-functions-dnnclas/46065066#46065066 )

I have been using the estimator interface in TF 1.3 including the creation of the data input function:

`training_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_data, y=training_label, batch_size=64, shuffle=True, num_epochs=None)`

and building the NN:

`dnnclassifier = tf.estimator.DNNClassifier(
    feature_columns=dnn_features,
    hidden_units=[1024, 500, 100],
    n_classes=2, 
    model_dir='./tmp/ccsprop',
    optimizer=tf.train.ProximalAdagradOptimizer(
      learning_rate=0.001,
      l1_regularization_strength=0.01
    ))`

and executing it

`dnnclassifier.train(input_fn=training_input_fn, steps=1500)`

After much searching I see no easy way to add tensorboard output without resorting to recreating the model from scratch and indicated here https://www.tensorflow.org/extend/estimators

Following some of the help on SO:

` TBcall=tf.train.SummarySaverHook(save_steps=10, output_dir='./tb', summary_op=tf.summary.merge_all())`

`dnnclassifier.train(input_fn=training_input_fn, steps=1500, hooks=TBcall)`

that gives this error: Exactly one of scaffold or summary_op must be provided

The answer on SO has now been edited to show code from creating a model from scratch it seems.. So is there no way to get basic info from  tf.estimator.DNNClassifier to Tensorboard? And more generically from other tf.estimators ?? One would expect the canned estimators to provide this? For model introspection and tuning.


"
12973,Default keras initializers parameters have changed,"At least in Tensorflow 1.3, some of the Keras initializers have changed they default parameters since Tensorflow 1.1:

In Tensorflow 1.1:

``
from tensorflow.contrib.keras.python.keras.initializers import TruncatedNormal
TruncatedNormal().stddev  # returns 0.05
``

In Tensorflow 1.3:
``
from tensorflow.contrib.keras.python.keras.initializers import TruncatedNormal
TruncatedNormal().stddev  # returns 1
``

I assume this was not intentional since this is a breaking change and I did not see it documented in the release notes."
12972,Dataset API fromGenerator Functionality,"@mrry I was looking at the code for the `fromGenerator` function and I notice that you do `iter(generator())` to create multiple parallel iterators over the same generator. Maybe I'm not too familiar with the semantics of a Python generator, but my impression was that it represents a continuation and it's not necessarily repeatable, meaning that you would have to cache all the elements you get in order to reproduce them. For example, let's say a generator is querying an online service and yielding a different tensor at each time point, dependent on the answer it gets from that service. This would not be repeatable without caching those tensors. So, in that case, how would your implementation behave? And in more general, what does the `iter()` function applied in this setting mean?

Regarding the caching of elements, I think that if there is a problem with the current implementation, then the right way to do this would be something along the lines of:
1. Create a dataset from a generator, as it's currently done, but also store a flag in it specifying it's a `GeneratorDataset` (could also be subclassing dataset to make things simpler).
2. When `repeat` is called on such a dataset, convert the generator to something like a Scala stream, which simply memoizes elements as you obtain them, but lazily evaluates the tail of your sequence (sort of like an iterator but with memoization) and then return a dataset that uses that stream as its source.

It may be totally unnecessary depending on the semantics of the `iter()` function, but based on a quick search I did, I couldn't find enough information for this setting."
12969,All tf.sets.* ops fail to reproduce their own official examples.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a 
- **Exact command to reproduce**: n/a

### Context:
`tf.sets.*` ops documentation declares that: '_All but the last dimension of a and b must match._'.
Also, the example is provided:
```
a = [
    [
      [
        [1, 2],
        [3],
      ],
      [
        [4],
        [5, 6],
      ],
    ],
  ]
  b = [
    [
      [
        [1, 3],
        [2],
      ],
      [
        [4, 5],
        [5, 6, 7, 8],
      ],
    ],
  ]
tf.sets.set_intersection(a, b)
tf.sets. set_union(a, b)
tf.sets.set_size(a)
tf.sets.set_difference(a, b, aminusb=True)
```

### Problem:
When using `tf.sets.*` ops, the following error occurs:
```
ValueError: Argument must be a dense tensor: [[[[1, 2], [3]], [[4], [5, 6]]]] - got shape [1, 2, 2], but wanted [1, 2, 2, 2].
```
From this error it is deducible, that a dense tensor is needed. This statement contradicts the description of methods. Also, logically we would not want to have a dense tensor or any kind of padding in sets (this leads to excessive padding elements in sets - which defies the purpose).

### Statement:
Either the functionality or the description of `tf.sets` must be fixed."
12968,Feature Request / Workaround for Variable size multi-label candidate sampling in TensorFlow.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS/Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: both
- **TensorFlow version (use command below)**:  ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a


### Context:
Suppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples. 

### Problem:
When trying to use provided standard `nce_loss()` -  a static `int` value for `num_true` option is required. 
- `num_true`: An `int`. The number of target classes per training example.

This probably works well for problems where we have same amount of labels per training examples and we know them in advance.
When labels have a variable shape `[None]`, (and in our case, they are also being batched and bucketed by bucket size with `.padded_batch()` + `.group_by_window()`) it is necessary to supply a variable size `num_true` in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).

### Statement:
Is there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.

Related [stackoverflow question](https://stackoverflow.com/questions/46085454/variable-size-multi-label-candidate-sampling-in-tensorflow).

Desired behaviour:
```
nce_loss = tf.nn.nce_loss(
    weights=nce_weights,
    biases=nce_biases,
    labels=labels,
    inputs=inputs,
    num_sampled=num_sampled,
    num_true=(tf.shape(labels)[-1]), # or tf.placeholder(""int32"", [], name=""num_trve"")
    num_classes=self.num_classes)
```

Also, is it possible to add support for weighted losses to `nce_loss()` (specifically to `_compute_sampled_logits()` as it is partially generated from a .cc) in the same fashion as it is implemented in `tf.losses.sparse_softmax_cross_entropy` or `tf.losses.sigmoid_cross_entropy`?
Thanks in advance."
12967,Could not find `backports.weakref` while installing TensorFlow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: -
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: (Trying to install through package manager pip2)
- **TensorFlow version (use command below)**: 1.2.0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: `pip2 install tensorflow==1.2.0`

### Describe the problem
While I try to install TF (1.2.0) on Python2.7 through the package manager Pip (`pip2 install tensorflow==1.2.0`) I get the following error:
```
Collecting tensorflow==1.2.0
  Downloading https://build.hubteam.com/pypi-mirror/packages/bf/a8/e98871dc5bfbe590ed41a38058fab1391647dc0553d5034f5fd6746f0a37/tensorflow-1.2.0-cp27-cp27m-macosx_10_11_x86_64.whl (33.6MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.6MB 37kB/s
Requirement already satisfied: wheel in /usr/local/lib/python2.7/site-packages (from tensorflow==1.2.0)
Collecting backports.weakref==1.0rc1 (from tensorflow==1.2.0)
  Could not find a version that satisfies the requirement backports.weakref==1.0rc1 (from tensorflow==1.2.0) (from versions: )
No matching distribution found for backports.weakref==1.0rc1 (from tensorflow==1.2.0)
```

So it doesn't find the `backports.weakref` package in PyPl (even if I can see it [here](https://pypi.python.org/pypi/backports.weakref)).

Is this a known issue? What can I do to install this dependency manually?"
12966,Error when following official installation instructions ,"INFO: Found 1 target...
ERROR: /opt/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/cjliux/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=7 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/protobuf_archive -iquote bazel-out/local_linux-opt/genfiles/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/local_linux-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o)
tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1017:50:   required from here
tensorflow/stream_executor/cuda/cuda_dnn.cc:139:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'
       cudnnStatus_t retval = ::__name(args...);                    \
                                      ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:233:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'
   __macro(cudnnSetRNNDescriptor)                              \
   ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:238:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R5'
 CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)
 ^
In file included from tensorflow/stream_executor/cuda/cuda_dnn.cc:42:0:
bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include/cudnn.h:1553:8: note: class type 'cudnnRNNStruct' is incomplete
 struct cudnnRNNStruct;
        ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'int perftools::gputools::cuda::{anonymous}::CudnnDataTypeToByteSize(cudnnDataType_t)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:858:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'int perftools::gputools::cuda::CudnnRnnParamsDescriptor::GetRegionCountPerLayer() const':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1200:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNInputMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnInputMode(perftools::gputools::dnn::RnnInputMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:821:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDirectionMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnDirectionMode(perftools::gputools::dnn::RnnDirectionMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:833:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnMode(perftools::gputools::dnn::RnnMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:845:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDataType_t perftools::gputools::cuda::{anonymous}::ToCudnnDataType(perftools::gputools::dnn::DataType, perftools::gputools::dnn::DataLayout)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:809:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionFwdAlgo_t perftools::gputools::cuda::{anonymous}::ToConvForwardAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:283:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdDataAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardDataAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:305:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdFilterAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardFilterAlgo(perftools::gputools::dnn::AlgorithmType)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:327:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: At global scope:
tensorflow/stream_executor/cuda/cuda_dnn.cc:128:26: warning: 'tensorflow::thread::ThreadPool* perftools::gputools::cuda::wrap::GetCudaThreadpool()' defined but not used [-Wunused-function]
 static port::ThreadPool* GetCudaThreadpool() {
                          ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 22.367s, Critical Path: 10.38s
FAILED: Build did NOT complete successfully


I have configure nvcc to be the cuda compiler, so I have no idea why the aforementioned message means. Can any one help?"
12964,Feature request: Google Authentication support for OAuth2 AccessTokenCredentials,"
I would like to use the BigQueryReader to access  someone else's project with OAuth2 AccessTokenCredentials. I noticed that currently the google_auth_provider only uses application default credentials. I think it would be a great addition to add other methods of authentication as well."
12962,Error building freeze_graph,"I am trying to build freeze_graph with bazel like this : 

` bazel build tensorflow/python/tools:freeze_graph
`

and I am getting this error : 

```
ERROR: /home/konmon01/tensorflow/tensorflow/BUILD:187:1: //tensorflow:all_files: invalid label ']' in element 2 of attribute 'srcs' in 'filegroup' rule: invalid target name ']': target names may not contain ']'
ERROR: /home/konmon01/tensorflow/tensorflow/python/BUILD:3736:12: Target '//tensorflow:internal' contains an error and its package is in error (this is usually caused by a missing package group in the package-level visibility declaration)
ERROR: /home/konmon01/tensorflow/tensorflow/python/BUILD:97:1: Target '//tensorflow:internal' contains an error and its package is in error and referenced by '//tensorflow/python:platform'
ERROR: /home/konmon01/tensorflow/tensorflow/python/BUILD:97:1: Target '//tensorflow:internal' contains an error and its package is in error and referenced by '//tensorflow/python:platform'
ERROR: Analysis of target '//tensorflow/python/tools:freeze_graph' failed; build aborted
INFO: Elapsed time: 0.281s
FAILED: Build did NOT complete successfully (0 packages loaded)
```

I installed bazel from scratch but did not give a solution. Any suggestions on that ??

> bazel version

> Build label: 0.5.4
> Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
> Build time: Fri Aug 25 10:00:00 2017 (1503655200)
> Build timestamp: 1503655200
> Build timestamp as int: 1503655200

As it turns out I cannot build any script with bazel (????)"
12961,Different confidence with same model.pb from android and python,"I trained a model.pb with python, and put it in android. 
well, I find the confidence score that in android is always lower than python application  on my PC.
AFAIK, for android model.pb, I did not use DecodeJpeg because it not support on Android.

Is there anything wrong or different with DecodeJpeg in android example when processing image?



        Trace.beginSection(""preprocessBitmap"");
        // Preprocess the image data from 0-255 int to normalized float based
        // on the provided parameters.
        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
        for (int i = 0; i < intValues.length; ++i) {
            final int val = intValues[i];
            floatValues[i * 3 + 0] = (((val >> 16) & 0xFF) - imageMean) / imageStd;
            floatValues[i * 3 + 1] = (((val >> 8) & 0xFF) - imageMean) / imageStd;
            floatValues[i * 3 + 2] = ((val & 0xFF) - imageMean) / imageStd;
        }
        Trace.endSection();
"
12960,does new dataset api support pre read data?,"I found that currently dataset api does not support pre read data as old reading data api. If we have so much train data the feature, the speed of reading data is critical."
12959,Python tests in //tensorflow/python/keras/... are failing on Windows,"http://ci.tensorflow.org/job/tf-master-win-bzl/1552/console
```
11:11:55 //py_test_dir/tensorflow/python/keras:callbacks_test                     FAILED in 126.1s
11:11:55   C:/tmp/_bazel_system/424zmya1/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/keras/callbacks_test/test.log
11:11:55 //py_test_dir/tensorflow/python/keras:data_utils_test                    FAILED in 20.3s
11:11:55   C:/tmp/_bazel_system/424zmya1/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/keras/data_utils_test/test.log
11:11:55 //py_test_dir/tensorflow/python/keras:io_utils_test                      FAILED in 9.1s
11:11:55   C:/tmp/_bazel_system/424zmya1/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/keras/io_utils_test/test.log
11:11:55 //py_test_dir/tensorflow/python/keras:models_test                        FAILED in 33.0s
11:11:55   C:/tmp/_bazel_system/424zmya1/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/keras/models_test/test.log
11:11:55 //py_test_dir/tensorflow/python/keras:training_test                      FAILED in 53.9s
11:11:55   C:/tmp/_bazel_system/424zmya1/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/keras/training_test/test.log
```

Some of the error messages:
```
11:11:55 ERROR: test_LambdaCallback (__main__.KerasCallbacksTest)
11:11:55 ----------------------------------------------------------------------
11:11:55 Traceback (most recent call last):
11:11:55   File ""\\?\C:\tmp\Bazel.runfiles_uxjtk119\runfiles\org_tensorflow\py_test_dir\tensorflow\python\keras\_impl\keras\callbacks_test.py"", line 798, in test_LambdaCallback
11:11:55     p.start()
11:11:55   File ""C:\Program Files\Anaconda3\lib\multiprocessing\process.py"", line 105, in start
11:11:55     self._popen = self._Popen(self)
11:11:55   File ""C:\Program Files\Anaconda3\lib\multiprocessing\context.py"", line 212, in _Popen
11:11:55     return _default_context.get_context().Process._Popen(process_obj)
11:11:55   File ""C:\Program Files\Anaconda3\lib\multiprocessing\context.py"", line 313, in _Popen
11:11:55     return Popen(process_obj)
11:11:55   File ""C:\Program Files\Anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
11:11:55     reduction.dump(process_obj, to_child)
11:11:55   File ""C:\Program Files\Anaconda3\lib\multiprocessing\reduction.py"", line 59, in dump
11:11:55     ForkingPickler(file, protocol).dump(obj)
11:11:55 AttributeError: Can't pickle local object 'KerasCallbacksTest.test_LambdaCallback.<locals>.target'
11:11:55 
11:11:55 ======================================================================
11:11:55 ERROR: test_TensorBoard_histogram_freq_must_have_validation_data (__main__.KerasCallbacksTest)
11:11:55 ----------------------------------------------------------------------
11:11:55 Traceback (most recent call last):
11:11:55   File ""C:\Program Files\Anaconda3\lib\shutil.py"", line 488, in rmtree
11:11:55     return _rmtree_unsafe(path, onerror)
11:11:55   File ""C:\Program Files\Anaconda3\lib\shutil.py"", line 378, in _rmtree_unsafe
11:11:55     _rmtree_unsafe(fullname, onerror)
11:11:55   File ""C:\Program Files\Anaconda3\lib\shutil.py"", line 387, in _rmtree_unsafe
11:11:55     onerror(os.rmdir, path, sys.exc_info())
11:11:55   File ""C:\Program Files\Anaconda3\lib\shutil.py"", line 385, in _rmtree_unsafe
11:11:55     os.rmdir(path)
11:11:55 OSError: [WinError 145] The directory is not empty: 'C:\\tmp\\callbacks_test1q3mfkyu\\tmp9rjq7egq\\logs'
11:11:55 
11:11:55 ======================================================================
11:11:55 FAIL: test_stop_training_csv (__main__.KerasCallbacksTest)
11:11:55 ----------------------------------------------------------------------
11:11:55 Traceback (most recent call last):
11:11:55   File ""\\?\C:\tmp\Bazel.runfiles_uxjtk119\runfiles\org_tensorflow\py_test_dir\tensorflow\python\keras\_impl\keras\callbacks_test.py"", line 502, in test_stop_training_csv
11:11:55     assert 'nan' in values[-1], 'The last epoch was not logged.'
11:11:55 AssertionError: The last epoch was not logged.
```

Theses tests are not running in CMake build on Windows, so it's only detected by Bazel."
12957,ValueError in CTCLoss,"TF version 1.1.0, Ubunu 16.04 Cuda 8.0, Cudnn 6

loss = tf.nn.ctc_loss(targets, logits, maxT, time_major=False)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow /python/ops/ctc_ops.py"", line 145, in ctc_loss
    ctc_merge_repeated=ctc_merge_repeated)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_ctc_ops.py"", line 165, in _ctc_loss
    name=name)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2339, in create_op
    set_shapes_for_outputs(ret)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1719, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1669, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/home/ilab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,64,23], [?,?], [?], [].
"
12955,AttributeError: 'NoneType' object has no attribute 'get_tensor_by_name'**,"I trained mnist model and then create one .pb file by freeze that model. then i want  to load this freeze model  ... so my code is for load the freeze graph

    import tensorflow as tf
    import argparse 
    from tensorflow.python.platform import gfile

    def load_graph(model_filename):
	with tf.Session() as sess:
		model_filename ='output_graph.pb'
		with gfile.FastGFile(model_filename, 'rb') as f:
			graph_def = tf.GraphDef()
			graph_def.ParseFromString(f.read())
			g_in = tf.import_graph_def(graph_def)

     if __name__ == '__main__':
    
	 parser = argparse.ArgumentParser()
	 parser.add_argument(""--model_filename"", default=""results/output_graph.pb"", type=str, 
                                                    help=""model_filename to import"")
	 args = parser.parse_args()
	
	print('Loading the model')
	graph = load_graph(args.model_filename)
		
	x = graph.get_tensor_by_name(""x:0"")
	y = graph.get_tensor_by_name(""op1:0"")
	with tf.Session(graph=graph) as sess:
		 data = np.vectorize(lambda x: 255 - x)(np.ndarray.flatten(scipy.ndimage.imread
                                                                                    (""C:/Users/HP/Desktop/digi.jpg"", flatten=True)))
	         result = sess.run(tf.argmax(y,1), feed_dict={x: [data]})
                 print (' '.join(map(str, result)))


Half of the code is run successfully but in last get the error of no attribute 'get_tensor_by_name''

***Loading the model
Traceback (most recent call last):
  File ""C:\Users\HP\Desktop\programs\TensorFlow\save_export_fie.py"", line 29, in <module>
    x = graph.get_tensor_by_name(""x:0"")
AttributeError: 'NoneType' object has no attribute 'get_tensor_by_name'****

what is issue with my code..?????"
12954,Naming issue of tensorflow.python.layers.core.Dense,"Since I think this issue has nothing to do with the system information, I would temporarily ignore them.

- **TensorFlow version (use command below)**:v1.2.1-4-g4acb96a 1.2.1


### Describe the problem
The `Dense` layers defined in `tensorflow.python.layers.core` build `kernel` and `bias` in `build` function, while the `build` function is called in `__call__` function. The may cause that sometime one define a layer, but not call it immediately which causes an unexpected variable naming issue.

### Source code / logs
For example, when I try to implement a toy seq2seq model, the following code

```
            with tf.variable_scope('output'):
                self._output_layer = _core_layers.Dense(
                    self._vocab_size, name='output_layer')

            if targets is not None: # TRAINING
                embedding_targets = tf.nn.embedding_lookup(
                    self._embedding, targets)

                outputs, final_state, = tf.nn.dynamic_rnn(
                    cell=self._cell,
                    inputs=embedding_targets,
                    sequence_length=lengths,
                    dtype=_FLOAT,
                    time_major=True)

                logits = self._output_layer(outputs)
            else: # INFER, or sampling
                _, batch_size, _ = tf.unstack(tf.shape(encoder_outputs))
                eos_ids = tf.ones([batch_size], dtype=tf.int32, name='EOS')
                eos_step_embedded = tf.nn.embedding_lookup(
                    self._embedding, eos_ids)

                def loop_fn_transition(time, cell_output, cell_state,
                                       loop_state):

                    def get_input():
                        output_logits = self._output_layer(cell_output)  #<----- kernel/bias of dense defined here
                        predictions = tf.argmax(output_logits, axis=1)
                        next_input = tf.nn.embedding_lookup(
                            self._embedding, predictions)
                        return next_input

                    elements_finished = (time >= lengths)
                    emit_output = cell_output
                    cell_state = cell_state
                    loop_state = None

                    return (elements_finished, get_input(), cell_state,
                            emit_output, loop_state)

                def loop_fn(time, cell_output, cell_state, loop_state):
                    if cell_state is None:
                        elements_finished = (0 >= lengths)
                        next_input = eos_step_embedded
                        cell_state = encoder_final_state
                        emit_output = None
                        loop_state = None

                        return (elements_finished, next_input, cell_state,
                                emit_output, loop_state)
                    else:
                        return loop_fn_transition(time, cell_output, cell_state,
                                                  loop_state)

                cell_outputs, final_state, _ = tf.nn.raw_rnn(
                    self._cell, loop_fn)
```

I got (by inspecting the checkpoint file)

```
basic_seq2seq/decoder/output_layer/bias (DT_FLOAT) [1301]
basic_seq2seq/decoder/output_layer/kernel (DT_FLOAT) [200,1301]
```
in training mode while got 

```
basic_seq2seq/decoder/rnn/output_layer/bias (DT_FLOAT) [1301]
basic_seq2seq/decoder/rnn/output_layer/kernel (DT_FLOAT) [200,1301]
```
in inference mode. So I can't restore a training checkpoint when inference due to `NotFoundError (see above for traceback): Key basic_seq2seq/decoder/rnn/output_layer/kernel not found in checkpoint`
"
12951,--config=mkl leads to libmklml_intel.so: error: undefined reference to 'dladdr',"Cloning master (702d595822e9e5f5232b8140c6296683612c33a9), running `configure` with ""No"" throughout, and trying to build a pip package (Bazel 0.5.4, Ubuntu 16.10) with the new `--config=mkl` flag crashes.

```sh
bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package

ERROR: /home/carl/tensorflow/tensorflow/python/BUILD:1289:1: Linking of rule '//tensorflow/python:gen_resource_variable_ops_py_wrappers_cc' failed (Exit 1)
bazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dladdr'
```

```sh
ldd bazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so

libiomp5.so => not found
```

"
12949,Very Slow Run time on A Pre Trained Network - Inception V1,"------------------------

### System information
- **example script (Very Similar label_Image.py but simpler)**:
- **Windows 10 X64 - TensorFlow installed from source **:
- **TensorFlow version (1.3)**:
- **Python version - 3.5.3**: 
- **Cmake 3.8.1**:
- **CUDA/cuDNN version - CUDA 8.0/ cuDNN 6.1**:
- **GPU model and memory - Nvidia GTX 1050, 4GB**:
- **Exact command to reproduce**:

### Describe the problem

I encountered a disturbing issue regarding the run time for processing an arbitrary image through an inception V1/GoogleNet pre-trained model the runtime took approximately 9.2 ms

When using the same image and exactly the same model with Caffe, It was running X4 faster than Tensorflow (2.3 ms).  The models  I used (both for Caffe and Tensorflow)  : 
https://github.com/beniz/deepdetect/issues/89
Labels file is attached. 

How is it even possible ?!?!  
Am I doing something incorrectly ? 

By the way, I carefully checked (with GPU-Z) and it seems Tensorflow  properly uses the GPU resources.

### Source code / logs ###
Attahced, along with the used label file. 
In order to use the (python) script, simply put it in the same folder as the model and label file., and rename it to :  TF_Running_Inception.py

[TF_Running_Inception.txt](https://github.com/tensorflow/tensorflow/files/1290863/TF_Running_Inception.txt)
[imagenet_slim_labels.txt](https://github.com/tensorflow/tensorflow/files/1290864/imagenet_slim_labels.txt)



"
12948,feature request: shared memory concat with new allocation for subsequent operations,"DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.

This is a feature request for a new `allocation='shared'` option for operations such as `tf.concat(allocation='shared')` which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the [Memory-Efficient Implementation of DenseNets](http://arxiv.org/abs/1707.06990), a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:

![densenet shared memory](https://camo.githubusercontent.com/d3370ec4935b4bc92b736a122bc226abf48988fd/68747470733a2f2f7261772e6769746875622e636f6d2f67706c656973732f656666696369656e745f64656e73656e65745f7079746f7263682f6d61737465722f696d616765732f666f72776172642e706e67)

- [Pytorch efficient DenseNet implementation](https://github.com/gpleiss/efficient_densenet_pytorch)
- [Keras DenseNet Implementation](https://github.com/titu1994/keras-contrib/blob/ff47da56fbd54cf6cdc2ac2218529fbdadf99296/keras_contrib/applications/densenet.py) with ""naive"" allocations, works with TensorFlow backend.

This functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?
"
12944,No proper documentation to use transfer learning using pretrained model in .ckpt format,"Hi all,

Tensorflow had provided a retrain sample  code for inception v3 pretrained model to apply transfer learning usinng .pb file of the model.

But there is  no documentation on how  to use a .ckpt file for pretrained  models on Imagenet such as  Resnet,inception_resnet, etc.

Not much information of  how  to get a .pb file  for these pretrained model in .ckpt format.

Has anyone tried transfer learning on these models?
If yes, did they use .ckpt  or .pb  version oof the model?
How to get a .pb and how to perform transfer learning usinng .ckpt version ?

Please help me on the same.

thank and regards"
12942,"I am facing this issue after installing it on Windows 7 32 Bit , AMD processor, python 3.5.2, TensorFlow 1.0.0","Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\SEM>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_help
er
    return importlib.import_module(mname)
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: %1 is not a valid Win32 application.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_help
er
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_help
er
    return importlib.import_module(mname)
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: %1 is not a valid Win32 application.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_help
er
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\SEM\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
12941,float64 support for conv1d and conv2d,"This is a feature request for `float64` support with `nn.conv1d`, `nn.conv2d`, and `nn.convolution`. Also, `conv1d` does not correctly document which types it supports. See below for details.

`conv1d` (which seems to be a wrapper for `conv2d`) and `conv2d` throw a TypeError when passed `float64` tensors. (I have only tested using a CPU; I don't have GPU access.) This occurs despite the fact that the documentation for `conv1d` claims support for `float64`. It is also worth mentioning that (at least on my CPU), `conv1d` seems to support `float16` even though this isn't mentioned in the documentation.

`conv3d` does however seem to support `float64`. Fortunately, this can at least be used as a temporary workaround to obtain `float64` support for 1d and 2d convolutions.

| Command | float16 | float32 | float64 |
| --- | --- | --- | --- |
| conv1d | works, but undocumented | works | TypeError, despite documentation |
| conv2d | works | works | TypeError |
| conv3d | TypeError | works | works |

The above table summarizes the (rather inconsistent) current state of tensorflow's support for various types according to my tests on a CPU using the code below. `convolution` appears to be a wrapper for `conv1d`, `conv2d`, and `conv3d`, and thus fails on the same types for each dimension.

```python
import tensorflow as tf
# t = tf.float16
# t = tf.float32
t = tf.float64
b, fw1, fw2, fw3, ic, oc, iw1, iw2, iw3 = range(2,11)
tf.nn.conv1d(tf.zeros([b, iw1, ic], t), tf.zeros([fw1, ic, oc], t), 1, ""VALID"")
tf.nn.conv2d(tf.zeros([b, iw1, iw2, ic], t), tf.zeros([fw1, fw2, ic, oc], t), [1]*4, ""VALID"")
tf.nn.conv3d(tf.zeros([b, iw1, iw2, iw3, ic], t), tf.zeros([fw1, fw2, fw3, ic, oc], t), [1]*5, ""VALID"")
tf.nn.convolution(tf.zeros([b, iw1, ic], t), tf.zeros([fw1, ic, oc], t), ""VALID"")
tf.nn.convolution(tf.zeros([b, iw1, iw2, ic], t), tf.zeros([fw1, fw2, ic, oc], t), ""VALID"")
tf.nn.convolution(tf.zeros([b, iw1, iw2, iw3, ic], t), tf.zeros([fw1, fw2, fw3, ic, oc], t), ""VALID"")
```

### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Arch Linux (up to date)
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.3.0
- **Python version**: 3.6.2
- **Bazel version**: 0.5.4
"
12940,tf.nn.separable_conv2d is slower than conv2d on GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: TF 1.3
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA8.0 /cuDNN6
- **GPU model and memory**: GTX1080ti  11G


### Describe the problem
In theory, `separable_conv2d` should be more efficient than `conv2d`, but when I test a simple model on Cifar10, the result shows that `nn.separable_conv2d` run slower on GPU, but is indeed faster on CPU. 

Here is my test results on GPU: 
```
training time for normal_conv after 2000 step: 8.18395892999979 sec
time for normal_conv after one forward step:  0.003980965999289765 sec
training time for separable_conv after 2000 step: 9.158266903999902 sec
time for separable_conv after one forward step:  0.0036441169995669043 sec

```

### Source code / logs

Below is a fully self-contained example, I first define a model with two  `conv2d` , than I define another model with one `conv2d` followed by one  `separable_conv2d`, both model have 32 channels for each conv_layer and identical fc_layer.



```
import tensorflow as tf
import timeit
import numpy as np
from tensorflow.contrib.keras.python.keras.datasets.cifar10 import load_data

(x_train, y_train), (x_val, y_val) = load_data()
learning_rate = 0.001
num_steps = 1000
n_classes = 10
batch_size = 32

def reformat(labels):
    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
    labels = (np.arange(n_classes) == labels[:,None]).astype(np.float32)
    return  labels.reshape(labels.shape[0],10)
train_labels = reformat(y_train)
tf.reset_default_graph()
x = tf.placeholder(tf.float32, [None, 32, 32, 3])
y = tf.placeholder(tf.float32, [None, 10])
weights1 = {}
weights2 = {}
dtype = tf.float32
with tf.name_scope('INIT_OP'):
    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)
    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)

k = 3
kernel = 16

# Define weights for normal ConvNet
with tf.name_scope('VARIABLES_1'):
    weights1['conv1'] = tf.get_variable('conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)
    weights1['b1'] = tf.get_variable('b1', initializer=tf.zeros([kernel]))
    weights1['conv2'] = tf.get_variable('conv2', [k, k, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)
    weights1['b2'] = tf.get_variable('b2', initializer=tf.zeros([kernel]))

    weights1['wd1'] = tf.get_variable('wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)
    weights1['bd1'] = tf.get_variable('bd1',  initializer=tf.zeros([512]) )
    weights1['wd2'] = tf.get_variable('wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)
    weights1['bd2'] = tf.get_variable('bd2',  initializer=tf.zeros([10]) )


#Define weights for separable ConvNet
with tf.name_scope('VARIABLES_sep'):
    weights2['conv1'] = tf.get_variable('2_conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)
    weights2['conv_dw2'] = tf.get_variable('conv_dw2', [k, k, kernel, 1], initializer=conv_initializer, dtype=dtype, trainable=True)
    weights2['conv_pw2'] = tf.get_variable('conv_pw2', [1, 1, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)

    weights2['b1'] = tf.get_variable('2_b1', initializer=tf.zeros([kernel]))
    weights2['b2'] = tf.get_variable('2_b2', initializer=tf.zeros([kernel]))

    weights2['wd1'] = tf.get_variable('2_wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)
    weights2['bd1'] = tf.get_variable('2_bd1',  initializer=tf.zeros([512]) )
    weights2['wd2'] = tf.get_variable('2_wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)
    weights2['bd2'] = tf.get_variable('2_bd2',  initializer=tf.zeros([10]) )

def forward_conv_sep( inp, weights):
    hidden = conv_block(inp, weights2['conv1'], weights2['b1'])
    hidden = maxpool2d(hidden)
    hidden = conv_block_dw(hidden, weights2['conv_dw2'], weights2['conv_pw2'], weights2['b2'])
    hidden = maxpool2d(hidden)
    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )
    fc1 = tf.matmul(hidden, weights2['wd1']) + weights2['bd1']
    fc1 = tf.nn.relu(fc1)
    return tf.matmul(fc1, weights2['wd2']) + weights2['bd2']

def forward_conv( inp, weights):
    hidden = conv_block(inp, weights1['conv1'], weights1['b1'])
    hidden = maxpool2d(hidden)
    hidden = conv_block(hidden, weights1['conv2'], weights1['b2'])
    hidden = maxpool2d(hidden)
    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )
    fc1 = tf.matmul(hidden, weights1['wd1']) + weights1['bd1']
    fc1 = tf.nn.relu(fc1)
    return tf.matmul(fc1, weights1['wd2']) + weights1['bd2']


def conv_block_dw(inp, cweight_w, cweight_p, bweight):
    no_stride =  [1,1,1,1]
    conv_output = tf.nn.separable_conv2d(inp, cweight_w, cweight_p, no_stride, 'SAME') + bweight
    return tf.nn.relu(conv_output)

def conv_block(inp, cweight, bweight, activation=tf.nn.relu):
    no_stride =  [1,1,1,1]
    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight
    return tf.nn.relu(conv_output)

def maxpool2d(inp, k=2):
    return tf.nn.max_pool(inp, ksize=[1, k, k, 1], strides=[1, k, k, 1],
                          padding='SAME')

#logits for normal ConvNet
with tf.name_scope(""forward_conv""):
    pred1 = forward_conv(x, weights1)

#Cost for normal ConvNet
with tf.name_scope(""cost1""):
    loss1 = tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)
    cost1 = tf.reduce_mean(loss1)

#training op for normal ConvNet
with tf.name_scope('train_op1'):
    train_op1 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost1)    

#logits for separable ConvNet
with tf.name_scope(""forward_conv_sep""):
    pred2 = forward_conv_sep(x, weights2)

#Cost for separable ConvNet
with tf.name_scope(""cost2""):
    loss2 = tf.nn.softmax_cross_entropy_with_logits(logits=pred2, labels=y)
    cost2 = tf.reduce_mean(loss2)

# training op for separable ConvNet
with tf.name_scope('train_op2'):
    train_op2 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost2)


with tf.name_scope('INIT'):
    init = tf.global_variables_initializer()


with tf.Session() as sess:

    sess.run(init)

    #train normal ConvNet for 2000 steps
    start = timeit.default_timer()
    for step in range(num_steps):
        r = np.random.choice(y_train.shape[0], batch_size, replace=False)
        batch_data = x_train[r]
        batch_labels = train_labels[r]

        feed_dict = {x : batch_data, y: batch_labels}
        _ , l = sess.run([train_op1,cost1], feed_dict=feed_dict)

    stop = timeit.default_timer()
    print ('training time for normal_conv after '+str(num_steps)+' step:',stop - start) 


    start = timeit.default_timer()
    feed_dict = {x : batch_data, y: batch_labels}
    predictions1 = sess.run(pred1, feed_dict=feed_dict)
    stop = timeit.default_timer()
    print ('time for normal_conv after one forward step: ',stop - start)



    # train separable ConvNet for 2000 steps
    start = timeit.default_timer()
    for step in range(num_steps):
        r = np.random.choice(y_train.shape[0], batch_size, replace=False)
        batch_data = x_train[r]
        batch_labels = train_labels[r]

        feed_dict = {x : batch_data, y: batch_labels}
        _ , l = sess.run([train_op2,cost2], feed_dict=feed_dict)


    stop = timeit.default_timer()
    print ('training time for sep_conv after '+str(num_steps)+' step:',stop - start) 

    start = timeit.default_timer()
    feed_dict = {x : batch_data, y: batch_labels}
    predictions = sess.run(pred2, feed_dict=feed_dict)
    stop = timeit.default_timer()
    print ('time for sep_conv after one forward step: ',stop - start)
```"
12939,cudnn_rnn_ops.py cannot find _cudnn_rnn_ops.so,https://github.com/tensorflow/tensorflow/blob/702d595822e9e5f5232b8140c6296683612c33a9/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L37
12937,"Possible Bug: while_loop, map_fn do not parallize ","The `parallel_iterations` parameter of `while_loop` and `map_fn` do not reduce the runtime as expected.
In comparison to the same operations created in a python loop, `while_loop` and `map_fn` are at least 4 times slower. `while_loop` does not scale at all

More details:
For my project, I need to calculate a matrix blockwise. 
I execute the calculation in a loop, each loop iteration working on a small part of the matrix, not accessing the rest of the matrix.
To achieve maximum performance, the loop has to be executed in parallel.
In addition, I need to control the number of parallel threads because of memory limitations.
I tried to use `parallel_iterations`, but changing this values doesn't change the runtime at all.

Using a python-for loop to create the same number of operations and syncing them with `control_dependencies` is around 4 times faster.
In contrast to `while_loop`, `map_fn` is getting faster with bigger `parallel_iterations` but remains noticeable slower then the python creation.
If this issue is known (didn't find anything at stackoverflow or here) and not resolvable, 
the documentation of while_loop needs to be improved heavily - to avoid this operation if the loop should be parallized.

Benchmark-Code:
```
import tensorflow as tf
import time

if __name__ == '__main__':
      runs = 100
      N = 100
      M = 30

      KernelRow = tf.Variable(tf.zeros([M, N*M]), name='KernelRow')

      def calEntry(KernelVariable, pos):
            op = tf.assign(KernelVariable[:, pos*M:(pos+1)*M], 
                  tf.eye(M) * 123. + tf.exp(tf.random_uniform([M, M]))) #perform some operations 
            with tf.control_dependencies([op]):
                  return tf.constant(0)

      def calRow(KernelVariable, N, parallelNum):
            ops = []
            for ic in range(0, N, parallelNum):
                  with tf.control_dependencies(ops):
                        for j in range(parallelNum):
                              if ic + j < N:
                                    ops += [calEntry(KernelVariable, ic+j)]
                        
            with tf.control_dependencies(ops):
                  return tf.identity(KernelVariable)

      def calRow_while(KernelVariable, N, parallelNum):
            i = tf.constant(0)
            condition = lambda i: tf.less(i, N)
            def body(ic):
                  updateKernel = calEntry(KernelVariable, ic)
                  with tf.control_dependencies([updateKernel]):
                        return ic + 1
            return tf.while_loop(condition, body, [i], back_prop=False, parallel_iterations=parallelNum)
            
      def calRow_map(KernelVariable, N, parallelNum):
            f = lambda x: calEntry(KernelVariable, x)
            return tf.map_fn(f, tf.range(N, dtype=tf.int32), parallel_iterations=parallelNum)

      paraOps = []
      paraOpsWhile = []
      paraOpsMap = []
      for paraEntry in [1, 10, 50, 100]:
            paraOps += [(calRow(KernelRow, N, paraEntry), paraEntry )]
            paraOpsWhile += [(calRow_while(KernelRow, N, paraEntry), paraEntry )]
            paraOpsMap += [(calRow_map(KernelRow, N, paraEntry), paraEntry )]

      init = tf.global_variables_initializer()
      #perform calculation
      with tf.Session() as sess:
            sess.run(init)
            for op in paraOps:
                  runtime = []
                  sess.run(op[0]) #warm up
                  for i in range(runs):
                        startTime = time.time()
                        sess.run(op[0])
                        runtime += [time.time() - startTime]
                  print(""Calculation using {} threads took {:.4f} +- {:.4f}"".format(op[1], np.mean(runtime), np.std(runtime) ))
            
            for op in paraOpsWhile:
                  runtime = []
                  sess.run(op[0]) #warm up
                  for i in range(runs):
                        startTime = time.time()
                        sess.run(op[0])
                        runtime += [time.time() - startTime]
                  print(""WHILE: Calculation using {} threads took {:.4f} +- {:.4f}"".format(op[1], np.mean(runtime), np.std(runtime) ))
                
            for op in paraOpsMap:
                  runtime = []
                  sess.run(op[0]) #warm up
                  for i in range(runs):
                        startTime = time.time()
                        sess.run(op[0])
                        runtime += [time.time() - startTime]
                  print(""MAP: Calculation using {} threads took {:.4f} +- {:.4f}"".format(op[1], np.mean(runtime), np.std(runtime) ))
```
 Example Output:

```
Calculation using 1 threads took 0.0044 +- 0.0004
Calculation using 10 threads took 0.0026 +- 0.0006
Calculation using 50 threads took 0.0018 +- 0.0000
Calculation using 100 threads took 0.0014 +- 0.0001
WHILE: Calculation using 1 threads took 0.0063 +- 0.0001
WHILE: Calculation using 10 threads took 0.0063 +- 0.0007
WHILE: Calculation using 50 threads took 0.0063 +- 0.0001
WHILE: Calculation using 100 threads took 0.0062 +- 0.0004
MAP: Calculation using 1 threads took 0.0072 +- 0.0004
MAP: Calculation using 10 threads took 0.0035 +- 0.0003
MAP: Calculation using 50 threads took 0.0035 +- 0.0002
MAP: Calculation using 100 threads took 0.0035 +- 0.0002

```

 


### System information
Ubuntu, Tensorflow installed using pip, version 1.3
tested on two different systems"
12935,Feature request: nightly build for python 3.6,"The [`tf-nightly`](https://pypi.org/project/tf-nightly/) pip packages are great! Would it be possible to add support for python 3.6 and/or provide a wheel for python 3.6 as part of the [standard nightly build](https://github.com/tensorflow/tensorflow#installation)?

In particular, the following succeeds

```
$ docker run --rm python:3 pip install tensorflow
```

whereas the following fails

```
$ docker run --rm python:3 pip install tf-nightly
Collecting tf-nightly
  Could not find a version that satisfies the requirement tf-nightly (from versions: )
No matching distribution found for tf-nightly
```"
12932,Turning on grappler makes SLIM Resnet_v1_50 slower on AWS K80,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source binary link [here](https://s3-us-west-2.amazonaws.com/tf-benchmark/tf_binary/tensorflow-1.4.0.32ffc5a-cp27-cp27mu-linux_x86_64.whl)
- **TensorFlow version (use command below)**: 1.3+ the sha-hash is in the link to download the binary I compiled.  
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 4.5
- **CUDA/cuDNN version**: cuDNN 6 / CUDA 8
- **GPU model and memory**: K80 on AWS p2.8xlarge
- **Exact command to reproduce**: Running SLIM from tensorflow/models/slim I used the following command: `CUDA_VISIBLE_DEVICES=1 python train_image_classifier.py     --train_dir=${TRAIN_DIR}     --dataset_name=imagenet     --dataset_split_name=train     --dataset_dir=${DATASET_DIR}     --model_name=resnet_v1_50    --num_clones=1    --optimizer=sgd    --batch_size=64    --max_number_of_steps=110`


I added the following code to train_image_classifier.py:  

```python

    rewrite_options = rewriter_config_pb2.RewriterConfig()
    rewrite_options.optimizers.append('constfold')
    rewrite_options.optimizers.append('layout')
    graph_options = tf.GraphOptions(rewrite_options=rewrite_options, infer_shapes=True)
    config = tf.ConfigProto(graph_options=graph_options)

    ###########################
    # Kicks off the training. #
    ###########################
    slim.learning.train(
        train_tensor,
        session_config=config,


```

I used this binary to test and it includes the sha-hash.  The build was done from head on 09-SEP-2017 using this command `bazel build -c opt --copt=-march=""haswell"" --config=cuda //tensorflow/tools/pip_package:build_pip_package`

For /configure.  I did not include XLA I did all of the defaults with the exception of adding CUDA and cuDNN.  Nothing additional was included.  

With out grappler I get **1.5 seconds per step and with grappler 2.0 seconds per step**.  There is some variance of plus or minus .1 but it is definitely slower in my testing which was not expected.  

**Edited:** I was running v1 but I think v2 gave a very similar result.  "
12931,getting no attribute key error in ubuntu virtual box,"bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~'/dataset/dosa/'
ERROR:tensorflow:Image directory '~/dataset/dosa/' not found.
Traceback (most recent call last):
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 989, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'


**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12930,Unable to build C++ example by bazel on windows 10,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Windows 10 64Bit
- **TensorFlow installed from**: binary
- **TensorFlow version**: master
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: 0.5.4, binary

in windows powershell:
```
>>protoc --version
libprotoc 3.4.0
>>bazel version
Build label: 0.5.4
Build target: bazel-out/msvc_x64-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Aug 25 09:59:45 2017 (1503655185)
Build timestamp: 1503655185
Build timestamp as int: 1503655185
```
### Describe the problem
I want to learn tensorflow C++ api and build a example from this [page](https://www.tensorflow.org/api_guides/cc/guide), but I get the following error:
```
ERROR: E:/projects/deeplearning/tensorflow/tensorflow/cc/example/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl"", line 122
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl"", line 103, in _apply_patch
                fail(""patch command is not found, ple..."")
patch command is not found, please install it and referenced by '//tensorflow/cc/example:example'.
ERROR: Analysis of target '//tensorflow/cc/example:example' failed; build aborted.
INFO: Elapsed time: 13.336s
ERROR: Build failed. Not running target.
```
I have already run ```pacman -Syuu --noconfirm patch``` in msys2.exe.  
bazel works well in simple c++ example.
### Source code / logs
in C:/msys64/usr/bin/bash.exe:
```
$ patch --version
GNU patch 2.7.5
Copyright (C) 2003, 2009-2012 Free Software Foundation, Inc.
Copyright (C) 1988 Larry Wall

License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Written by Larry Wall and Paul Eggert
```
in windows powershell:
```
PS E:\Projects\deeplearning\tensorflow> python configure.py
You have bazel 0.5.4 installed.
Please specify the location of python. [Default is C:\Users\yanyan\Anaconda3\python.exe]:


Found possible Python library paths:
  C:\Users\yanyan\Anaconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\yanyan\Anaconda3\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: y
GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: y
VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:


Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:


Please specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:


Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.


PS E:\Projects\deeplearning\tensorflow> bazel run -c opt //tensorflow/cc/example:example
ERROR: E:/projects/deeplearning/tensorflow/tensorflow/cc/example/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl"", line 122
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl"", line 103, in _apply_patch
                fail(""patch command is not found, ple..."")
patch command is not found, please install it and referenced by '//tensorflow/cc/example:example'.
ERROR: Analysis of target '//tensorflow/cc/example:example' failed; build aborted.
INFO: Elapsed time: 15.968s
ERROR: Build failed. Not running target.
```"
12929,error when giving this command bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/home/dile/dataset/dosa,"Traceback (most recent call last):
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 108, in <module>
    import tensorflow as tf
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 76, in <module>
    from tensorflow.python.framework.ops import Graph
  File ""/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 28, in <module>
    from autograd import core as ag_core
ImportError: No module named autograd"
12927,No op named GatherTree using BeamSearchDecoder,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5
- **GPU model and memory**: GeForce GTX 1080 (8 GB)

### Describe the problem
I was implementing a seq2seq model, and inference went well using greedy algorithm(GreedyEmbeddingHelper). But when I tried to use BeamSearchDecoder to infer from a trained model, I encountered ""No op named GatherTree in defined operations."". Strangely enough, I couldn't find the same error elsewhere. 

### Error message
```
Traceback (most recent call last):
  File ""infer.py"", line 88, in <module>
    out_file='result/result.out', checkpoint=checkpoint)
  File ""infer.py"", line 48, in predict
    loader = tf.train.import_meta_graph(checkpoint + '.meta')
  File ""/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1686, in import_meta_graph
    **kwargs)
  File ""/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py"", line 504, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 283, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named GatherTree in defined operations.
```

### Source code to reproduce the problem
```
# Inference module
loaded_graph = tf.Graph()
with tf.Session(graph=loaded_graph) as sess:
	loader = tf.train.import_meta_graph(checkpoint + '.meta')
	loader.restore(sess, checkpoint)

	input_data = loaded_graph.get_tensor_by_name('inputs:0')
	logits = loaded_graph.get_tensor_by_name('inferences:0')
	src_seq_len = loaded_graph.get_tensor_by_name('source_sequence_length:0')
	tgt_seq_len = loaded_graph.get_tensor_by_name('target_sequence_length:0')

	for i in range(len(text)):
		text_seq = src2seq_word(text[i], True)
		answer = sess.run(logits, {input_data: [text_seq] * batch_size,
			                                  tgt_seq_len: [len(text_seq)] * batch_size,
			                                  src_seq_len: [len(text_seq)] * batch_size}
			                         )[:, :, 0]
```
Program failed at ```loader = tf.train.import_meta_graph(checkpoint + '.meta')```

```
# Related code
training_logits = tf.identity(train_decoder_output.rnn_output, name='logits')
inference_logits = tf.identity(infer_decoder_output.predicted_ids, name='inferences')
```"
12925,[RFE] tensorboard: provide a File->Open -like UI for saved model.pb files,"This was originally solicited as https://github.com/tensorflow/tensorflow/issues/8854 by @brandondutra but got derailed with a very nice tools contribution from @jubjamie / @brandondutra

The original request would still, imo, make for a nice addition to tensorboard; specifically, were the UI of a running instance of TB to have something akin to a File->Open function to load a saved model .pb file, users who were tasked with the use case of ""person gives them a saved model to inspect"" would appreciate it.
"
12924,How to fetch the last batch in a multigpu model,"The new dataset API is awesome, but I don't know to fetch the last batch in my model 
![image](https://user-images.githubusercontent.com/25046619/30235975-10aad7ce-9543-11e7-9b68-3c25330ee899.png)
![image](https://user-images.githubusercontent.com/25046619/30235993-3d38c9a4-9543-11e7-9698-d13b49e37050.png)

"
12921,autoconf error when running build_all_ios.sh,"When running the build_all_ios.sh script on iOS I am getting the following error:

`+ autoreconf -f -i -Wall,no-obsolete
configure.ac:30: error: possibly undefined macro: AC_PROG_LIBTOOL
      If this token and others are legitimate, please use m4_pattern_allow.
      See the Autoconf documentation.
autoreconf: /usr/local/Cellar/autoconf/2.69/bin/autoconf failed with exit status: 1`

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not getting to the point of running code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:
- **Python version**: Python 2.7.10
- **Bazel version (if compiling from source)**: release 0.5.4-homebrew
- **CUDA/cuDNN version**:
- **GPU model and memory**: NVIDIA GeForce GT 750M 2GB
- **Exact command to reproduce**: ./tensorflow/contrib/makefile/build_all_ios.sh
"
12917,"SVD on GPU: complex values, interface cleanup (Discussion)","Hi,
pull request #11878 brought an implementation of the SVD on the GPU. But at the moment, only real values are supported.

The current status when applying the SVD (M=USV') on a complex matrix M:
- The python interface declares U,V as complex, S as real
- The C++ kernel definition declares both U,V and S as complex
   (This simplified the CPU implementation using Eigen)
- The python code then immediately casts S to the reals
- The GPU solver (cuSolver) would, however, output the singular values directly as reals

This leads to the following questions / ideas / suggestions
(credit goes also to @rmlarsen for discussion this the first time with me)
- Change the kernel definition: Add a new kernel (V2 suffix) that returns S as a real type
- Implement the complex support on GPUs
- Adopt the CPU code to also use the new kernel definition
   OR
    Keep both definitions and let the python wrapper to choose between the two versions based on the target device (CPU vs GPU)
- ...?

What do you think?"
12916,inconsistent behavior in variable sharing ,"hello , 

I have been struggling for few days with an issue that I'm facing in TensorFlow , I hesitated before opening this GitHub as an issue because I'm not sure if what I'm facing is a bug or a mere confusion on my side    . 

anyways it seems that I'm experiencing an inconsistent behavior in tensorFlow with variable name/reusing where I have a function that creates some cells and attentions objects : below is the function : 

```
def create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width ):

    with tf.variable_scope(""InnerScope"" , reuse=tf.AUTO_REUSE):
        encoder_outputs_tr =tf.contrib.seq2seq.tile_batch(encoder_outputs_tr, multiplier=beam_width) 
        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width) 
        batch_size =  batch_size * beam_width 
        dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])

        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs_tr ) 

        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size , output_attention=False)
        attn_zero = attn_cell.zero_state(batch_size , tf.float32 )
        attn_zero = attn_zero.clone(cell_state = encoder_state)
    return attn_zero ,  attn_cell 
```

and then I call the function with scope reusing as shown below : 
```
with tf.variable_scope('scope' ):
    intial_train_state , train_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , 1  )
with tf.variable_scope('scope' ,reuse=True):
    intial_infer_state , infer_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width  )
print(""intial_train_state"" , intial_train_state)
print(""intial_infer_state"" , intial_infer_state)
```

the print commands parings the returned objects , first print outputs : 
`  ('intial_train_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(100, 512) dtype=float32>, time=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(100, ?) dtype=float32>, alignment_history=()))`


and the second print outputs : 
`  ('intial_infer_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(300, 512) dtype=float32>, time=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(300, ?) dtype=float32>, alignment_history=()))`

I was expecting that both output would be the same since I'm reusing the variables but as you can see that for example in the first variable the output has something like this 
**scope/InnerScope/tile_batch_1/Reshape_1:0**

and in the second variable 

**scope_1/InnerScope/tile_batch_1/Reshape_1:0**

I do not know why _1 is added to **scope** in the second call , and I'm a bit confused if the variable is being shared or not , further more when I set the reuse option to False , I get the below error in the second function call : 
`ValueError: Variable scope/memory_layer/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:`

which leads me to think that the function was reusing the variables when the reuse flag was True , but I still do not understand why is tensorFlow adding _1 to the variable name **scope_1** , and does that mean it not being reused ? and how do I fix this . 

I'm using Tensorflow 1.3 
ON MacOs 10.12.4

I have also opened a stackoverflow question : https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables

thank you "
12912,T-SNE Suddenly Not Working in Projector,"Hello,

I'm sorry this isn't a technical question, but just this week I've noticed that T-SNE in the tensorboard projector is no longer clustering data properly. I am running the same vectors I did a few weeks ago, with the same perplexity. While before I saw distinct clusters, now the points are all forming an indistinct ball. Has anyone else noticed this?"
12911,tf.gather with int32 indices yields wrong result on gpu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
$ cat /etc/issue
Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**:
Source head
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
bazel release 0.4.5
- **CUDA/cuDNN version**:
8.0, 7.0
- **GPU model and memory**:
GeForce GTX 980
- **Exact command to reproduce**:
```
import tensorflow as tf

def bug():
  params = tf.convert_to_tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=tf.float32)
  indices_32 = tf.convert_to_tensor([2, 0, 2, 4], dtype=tf.int32)
  indices_64 = tf.convert_to_tensor([2, 0, 2, 4], dtype=tf.int64)
  test_op_32 = tf.gather(params, indices_32)
  test_op_64 = tf.gather(params, indices_64)
  with tf.Session() as sess:
      res = sess.run([test_op_32, test_op_64])
  print('test_op_32', res[0], 'test_op_64', res[1])

bug()
with tf.device('/cpu:0'):
  bug()
bug()
```

### Describe the problem
When using `int32` indices for `tf.gather` on GPU, wrong (random) numbers are returned.

See code above. It does not occur if using `int64` or running on CPU.

### Output
```
2017-09-08 15:54:08.775830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:
name: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.2785
pciBusID: 0000:04:00.0
totalMemory: 3.94GiB freeMemory: 158.62MiB
2017-09-08 15:54:08.775862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)
test_op_32 [ 0.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]
2017-09-08 15:54:08.792745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)
test_op_32 [ 2.  0.  2.  4.] test_op_64 [ 2.  0.  2.  4.]
2017-09-08 15:54:08.800545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:04:00.0, compute capability: 5.2)
test_op_32 [ 3.  0.  0.  0.] test_op_64 [ 2.  0.  2.  4.]
```"
12910,is_numeric_tensor on _ref variables,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:

```python
>>> X = tf.Variable(np.random.rand(5,5))
>>> X
<tf.Variable 'Variable:0' shape=(5, 5) dtype=float64_ref>
>>> tf.is_numeric_tensor(X)
False
```

### Describe the problem
I'm not sure if this is a bug or a feature, I feel it is a bug. I want to be able to test if an input to a function was a numpy style array or a tensor. I use the tf.is_numeric_tensor to check the input, however it doesn't pick up on _ref variables such as those initialized from a numpy array. 
"
12909,Maven Packaging for GPU-backed Java Bindings,"### System information
N/A

### Describe the problem
Currently, the only `libtensorflow_jni` artifact which is deployed to Maven Central is compiled with CPU-only support. This means that (at least as far as my current understanding goes) any machine which I would like to deploy to and run GPU-backed code on has to build TensorFlow from source with GPU support. As one can imagine, this makes the logistics of such deployments much more difficult and time-consuming. 

It would be wonderful if the `libtensorflow_jni` artifact either contained a GPU-compiled TensorFlow libraries and could use them automatically when possible _or_ if there was a separate artifact (perhaps with a `gpu` artifact classifier) that contained those libraries. The former streamlines build processes at the cost of bloating the artifact a bit, while the latter at least provides an easy means of enabling GPU support for users which desire it.

I'm guessing that something like this is somewhere on the timeline (I know that the Java bindings are under active development), but I have been unable to find any documentation on the topic.

Thank you!

### Source code / logs
N/A"
12908,Subclasses of Estimator cannot override members of Estimator.,"Any reason for not allowing subclasses of `Estimator` to override members other than `['_call_input_fn', '_create_global_step']`?"
12906,Generators in graphs,"I am wondering, is there any way to place a generator in a graph that would push values to lower parts of graph or asynchronous operations of this kind should be always placed outside of tensorflow graphs?"
12905,bazel error related to python ,"bazel build tensorflow/examples/image_retraining:retrain
ERROR: /home/dile/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 310
		_create_local_python_repository(repository_ctx)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 268, in _create_local_python_repository
		_get_python_bin(repository_ctx)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 166, in _get_python_bin
		_get_env_var(repository_ctx, _PYTHON_BIN_PATH, No..., ...)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 49, in _get_env_var
		_python_configure_fail((""'%s' environment variable is n...))
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 37, in _python_configure_fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: 'PYTHON_BIN_PATH' environment variable is not set
 and referenced by '//third_party/py/numpy:headers'
ERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted
INFO: Elapsed time: 4.555s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/core ... (3 packages)



Operating system is ubuntu
and i dont know how to run configure..what and all i have to select in that
"
12904,"make ""build_all_ios.sh""  occur error","I try to build tensorflow support at Android and iOS by makefile [tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile
) in current master branch 04c318b69c5b565436cfeeaab1cb7fd5419dde27

When running the build_all_ios.sh script, the below error message show
```
Undefined symbols for architecture x86_64:
  ""nsync::nsync_mu_init(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::mutex() in env.o
      tensorflow::mutex::mutex() in random.o
  ""nsync::nsync_mu_lock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::lock() in env.o
      tensorflow::mutex::lock() in random.o
      tensorflow::mutex::lock() in histogram.o
  ""nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::unlock() in env.o
      tensorflow::mutex::unlock() in random.o
      tensorflow::mutex::unlock() in histogram.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/CSL.Peter/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'armv7 compilation failed.'
armv7 compilation failed.
+ exit 1
```
The `download_dependencies.sh` and `compile_ios_protobuf.sh` run successfully but `compile_ios_tensorflow.sh` failed. I find same issues #3191 and #4252 and seem to be fixed at #4287, but this problem still happen."
12903,Input streaming data to tensorflow,Can you please reccommend a way to input streaming data which comes in json format to tensorflow
12902,Change TanhGrad() operation definition with respect to documentation,"Hello,

TanhGrad() documentation says: ""Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy`
is the corresponding input gradient."" https://github.com/tensorflow/tensorflow/blob/bab2db47f60d59d843d661903a8d28227600dd60/tensorflow/core/ops/math_ops.cc#L323 which is correct and looks good.

But operation has following declaration of inputs:
  Input(""x: T"")                                                
  .Input(""y: T"")                                           
https://github.com/tensorflow/tensorflow/blob/bab2db47f60d59d843d661903a8d28227600dd60/tensorflow/core/ops/math_ops.cc#L200

what doesn't correlate with the documentated formula: grad = dy * (1 - y*y).
Could you please rename inputs with respect to documentation like this:

  Input(""y: T"")                                                
  .Input(""dy: T"")   

Thanks."
12901,get_session_handle has no effect if not directly fetched,"Version `v1.3.0-rc1-1612-ga2e1a5e`, recent master.

```python
handle = tf.identity(tf.get_session_handle(tf.constant(0))).eval()
gen_data_flow_ops._get_session_tensor(handle, tf.int32).eval()
# InvalidArgumentError: The tensor with handle 'GetSessionHandle;0;/job:localhost/replica:0/task:0/device:GPU:0' is not in the session store.
```

```python
handle = tf.get_session_handle(tf.constant(0)).eval().handle
gen_data_flow_ops._get_session_tensor(handle, tf.int32).eval()
# OK
```

"
12898,allocate output tensor for every operation before the first inference,"### Describe the problem
I was wondering if it's possible to allocate memory in advance, for internal output tensors of every operation in my inference workload. 
I notice TF will automatically allocate (and de-allocate) memory for every internal result in a graph. If my inference workload repeats many times, all these allocations/deallocations will also repeat. Why don't we allocate these internal output tensors just once before the first inference run, and delete them after the last?

For example, I have the following workload (which will run thousands of times):
```
    M = tf.MatMul(A, B)
    D = tf.MatMul(M, C)
````
Is it possible to allocate M before the first run and use the memory for all inference? To avoid allocation of M in every single inference?
"
12897,ImportError: cannot import name 'c_einsum',"I updated tensorflow from 1.1.0 to 1.3.0 using the wheel file: tensorflow-1.3.0-cp35-cp35m-win_amd64.whl and in the line:

`import tensorflow as tf`

I got the following error:

```
from numpy.core.multiarray import c_einsum
ImportError: cannot import name 'c_einsum'
```

Any help?"
12896,tensorflow-master version  compile build_all_ios.sh  error ( armv7 compilation failed),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12895,dlopen error when using TF_LoadLibrary,"I'm trying to use `TF_LoadLibrary` on a mac and I keep getting errors such as:
```
Caused by: org.platanios.tensorflow.jni.TensorFlow$NativeException: dlopen(/var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib, 6): Symbol not found: __ZN10tensorflow15shape_inference12UnknownShapeEPNS0_16InferenceContextE
  Referenced from: /var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib
  Expected in: flat namespace
 in /var/folders/rw/lqrc8nk52kqcc4_zq3c2b6dh0000gn/T/tensorflow_scala_native_libraries7467192138547554485/libtensorflow_ops.dylib
```
The same thing happens when I try to load the compiled dylib file through Python directly. It was compiled it using `-D_GLIBCXX_USE_CXX11_ABI=0` and `-undefined dynamic_lookup` for the linker. @asimshankar "
12894,Tensorflow gpu build error,"I am getting the following error when building Cuda enabled Tensorflow.

> ERROR: /home/ubuntu/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/                                                                      stream_executor:cuda_platform' failed (Exit 1)
> tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::w                                                                      rap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [wit                                                                      h Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRN                                                                      NMode_t, cudnnDataType_t}]':
> tensorflow/stream_executor/cuda/cuda_dnn.cc:1017:50:   required from here
> tensorflow/stream_executor/cuda/cuda_dnn.cc:139:46: **error**: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_                                                                      t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescr                                                                      iptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudn                                                                      nRNNAlgo_t, cudnnDataType_t)'
>        cudnnStatus_t retval = ::__name(args...); 

------------------------

I installed Cuda 8.0.61 (latest) with cudNN 7.0 (latest stable). It is my first time building tensorflow for the GPU so any explanation of the error would be great!

Edit: Found the answer #12052"
12892,ant(1234),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12891,build fail (looser throw specifier in EnvWrapper),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux Fedora 17

- **TensorFlow installed from (source or binary)**:

Trying to build from source.

- **TensorFlow version (use command below)**:

1.3

- **Python version**: 

2.7

- **Bazel version (if compiling from source)**:

0.5.4

- **CUDA/cuDNN version**:

Disabled.

- **GPU model and memory**:

Disabled.

- **Exact command to reproduce**:

bazel build

### Describe the problem

I'm trying to compile tensorflow from source, don't modify anything, simply following the instructions on the webpage:

built bazel 0.5.4 from source

git clone
git checkout origin/1.3
./configure     # all answers are the defaults

bazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package

The above fails with with a looser throw specifier error in core/platform/env.h

The versions of the various tools involved:

openjdk 1.8.0
bazel 0.5.4
python 2.7
gcc 4.7.2
libstdc++ 4.7.2

The problem is reproducible on my box.

### Source code / logs

The precise failure from the bazel build is this:

```
> 
> 
> ERROR: /home/fetch/tensorflow/tensorflow/core/BUILD:1244:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed (Exit 1): gcc failed: error executing command 
>   (cd /home/fetch/.cache/bazel/_bazel_fetch/7ef242de6c5f89b75c729448096cd3bb/execroot/org_tensorflow && \
>   exec env - \
>     LD_LIBRARY_PATH=/usr/lib64:/usr/lib64/openmpi/lib \
>     PATH=/opt/obuildfactory/jdk-1.8.0-openjdk-x86_64/bin:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.25.x86_64/bin:/usr/lib64/openmpi/bin:/home/fetch/opt/bin:/home/fetch/bin:/usr/lib64/openmpi/bin:/home/fetch/opt/bin:/home/fetch/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/bin/python \
>     PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages \
>     TF_NEED_CUDA=0 \
>     TF_NEED_OPENCL=0 \
>   /bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local-opt/genfiles/external/snappy -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -msse3 -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/platform/default/tracing.cc -o bazel-out/local-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/platform/default/tracing.pic.o).
> In file included from ./tensorflow/core/lib/core/threadpool.h:21:0,
>                  from ./tensorflow/core/platform/default/tracing_impl.h:25,
>                  from ./tensorflow/core/platform/tracing.h:266,
>                  from tensorflow/core/platform/default/tracing.cc:16:
> ./tensorflow/core/platform/env.h:295:11: error: looser throw specifier for 'virtual tensorflow::EnvWrapper::~EnvWrapper()'
> ./tensorflow/core/platform/env.h:50:11: error:   overriding 'virtual tensorflow::Env::~Env() noexcept (true)'
> tensorflow/core/platform/default/tracing.cc:32:19: warning: 'tensorflow::port::dummy' defined but not used [-Wunused-variable]
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 418.721s, Critical Path: 19.37s
> 
"
12888,[feature] ONNX Support,"Support exporting and loading models in ONNX format.
See: https://research.fb.com/facebook-and-microsoft-introduce-new-open-ecosystem-for-interchangeable-ai-frameworks/"
12883,Additional arguments for summary.image,"### System information
N/A 

### Describe the problem

It'd be nice to have more control over the color map applied to the image summaries. In particular having `vmin` and `vmax` arguments as in matplotlib [imshow](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.imshow.html) function.
If no value is given the default behaviour could be kept, but if specified, black would correspond to vmin and white to vmax.

### Source code / logs
N/A
"
12880,tf.map_fn handles elems differently if it's a list or tuple,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'unknown' 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 6
- **GPU model and memory**: GTX 1080, 8GB
- **Exact command to reproduce**: Just run the script


### Describe the problem
It seems from some simple experiments that `tf.map_fn` behaves differently from what documented according to the specific type of its `elems` parameter. If the parameter is a tuple of tensors, the code works. If it's a list of tensors, however, the function is applied to the list itself instead than its elements (thus raising an error in the example code I put below)

### Code:
```
import tensorflow as tf
import numpy as np

elems = tf.constant([1,2,3],dtype=tf.int64)
list_elems = [elems]
tuple_elems = (elems)

res = tf.map_fn(lambda x: x+1, tuple_elems, dtype=tf.int64)
with tf.Session() as sess:
    print(sess.run(res))
 
res = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)
with tf.Session() as sess:
    print(sess.run(res))
```
### Traceback:

```
Traceback (most recent call last):

  File ""<ipython-input-45-e7852b9d4ba6>"", line 13, in <module>
    res = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 389, in map_fn
    swap_memory=swap_memory)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2775, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2604, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2554, in _BuildLoop
    body_result = body(*packed_vars_for_body)

  File ""C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 379, in compute
    packed_fn_values = fn(packed_values)

  File ""<ipython-input-45-e7852b9d4ba6>"", line 13, in <lambda>
    res = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)

TypeError: can only concatenate list (not ""int"") to list
```"
12879,Is it a bug of tf.map_fn?,"Look at the code:

```python
import tensorflow as tf
import numpy as np

elems = [tf.constant([1,2,3],dtype=tf.int64)]
alternates = tf.map_fn(lambda x: x, elems, dtype=tf.int64)
with tf.Session() as sess:
    print(sess.run(alternates))
```

It will raise a error, but when I use tuple instead of list, it works well.

```python
import tensorflow as tf
import numpy as np

elems = (tf.constant([1,2,3],dtype=tf.int64))
alternates = tf.map_fn(lambda x: x, elems, dtype=tf.int64)
with tf.Session() as sess:
    print(sess.run(alternates))
```
Is it a bug?
If you want to know more, llo at the discussion on [stackoverflow](https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn).

"
12878,"Tutorial code in ""Logging and Monitoring Basics with tf.contrib.learn"" with multiple errors","I've tried to run the setup code in the tutorial, but I met some confusing running errors. I'm new to tensorflow, so I have no idea how to deal with it. Could anybody help me fix this problem? Thanks a lot!

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os

import numpy as np
import tensorflow as tf

# Data sets
IRIS_TRAINING = os.path.join(os.path.dirname(__file__), ""iris_training.csv"")
IRIS_TEST = os.path.join(os.path.dirname(__file__), ""iris_test.csv"")

def main(unused_argv):
    # Load datasets.
    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)

    # Specify that all features have real-value data
    feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=4)]

    # Build 3 layer DNN with 10, 20, 10 units respectively.
    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                                hidden_units=[10, 20, 10],
                                                n_classes=3,
                                                model_dir=""/tmp/iris_model"")

    # Fit model.
    classifier.fit(x=training_set.data,
                   y=training_set.target,
                   steps=2000)

    # Evaluate accuracy.
    accuracy_score = classifier.evaluate(x=test_set.data,
                                         y=test_set.target)[""accuracy""]
    print('Accuracy: {0:f}'.format(accuracy_score))

    # Classify two new flower samples.
    new_samples = np.array(
        [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)
    y = list(classifier.predict(new_samples, as_iterable=True))
    print('Predictions: {}'.format(str(y)))

if __name__ == ""__main__"":
  tf.app.run()
```

Here is what I saw:
WARNING:tensorflow:From .\logging_monitoring.py:33: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From .\logging_monitoring.py:33: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
2017-09-07 20:00:50.306221: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-07 20:00:50.306424: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-07 20:00:50.447012: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/biases not found in checkpoint
2017-09-07 20:00:50.447738: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/weights not found in checkpoint
2017-09-07 20:00:50.448265: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/biases/denlayer_0/biases/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.451033: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/multi_class_head/dnn/learning_rate not found in checkpoint
2017-09-07 20:00:50.451078: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/biases not found in checkpoint
2017-09-07 20:00:50.451715: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/weights/enlayer_0/weights/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.452925: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/biases/denlayer_1/biases/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.454670: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/logits/weights/nn/logits/weights/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.455734: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/weights/enlayer_1/weights/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.455777: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/weights not found in checkpoint
2017-09-07 20:00:50.460821: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/biases not found in checkpoint
2017-09-07 20:00:50.463001: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/logits/weights not found in checkpoint
2017-09-07 20:00:50.464352: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/biases/denlayer_2/biases/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.464794: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/weights not found in checkpoint
2017-09-07 20:00:50.467010: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/weights/enlayer_2/weights/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.467803: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/logits/biases/dnn/logits/biases/part_0/Adagrad not found in checkpoint
2017-09-07 20:00:50.472935: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\framework\op_kernel.cc:1192] Not found: Key dnn/logits/biases not found in checkpoint
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
    return fn(*args)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""C:\Program Files\Python36\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key dnn/hiddenlayer_0/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "".\logging_monitoring.py"", line 47, in <module>
    tf.app.run()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "".\logging_monitoring.py"", line 33, in main
    steps=2000)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 296, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 442, in fit
    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1353, in fit
    monitors=all_monitors)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 296, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 458, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1006, in _train_model
    config=self._session_config
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 365, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 668, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 490, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 842, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 847, in _create_session
    return self._sess_creator.create_session()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 551, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 425, in create_session
    init_fn=self._scaffold.init_fn)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\session_manager.py"", line 273, in prepare_session
    config=config)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1560, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key dnn/hiddenlayer_0/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File "".\logging_monitoring.py"", line 47, in <module>
    tf.app.run()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "".\logging_monitoring.py"", line 33, in main
    steps=2000)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 296, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 442, in fit
    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1353, in fit
    monitors=all_monitors)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 296, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 458, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1006, in _train_model
    config=self._session_config
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 365, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 668, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 490, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 842, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 847, in _create_session
    return self._sess_creator.create_session()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 551, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 416, in create_session
    self._scaffold.finalize()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 209, in finalize
    self._saver.build()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1172, in build
    filename=self._filename)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 684, in build
    restore_sequentially, reshape)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 450, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 663, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Key dnn/hiddenlayer_0/biases not found in checkpoint
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
['File "".\\logging_monitoring.py"", line 47, in <module>\n    tf.app.run()', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py"", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File "".\\logging_monitoring.py"", line 33, in main\n    steps=2000)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py"", line 296, in new_func\n    return func(*args, **kwargs)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py"", line 442, in fit\n    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py"", line 1353, in fit\n    monitors=all_monitors)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py"", line 296, in new_func\n    return func(*args, **kwargs)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py"", line 458, in fit\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py"", line 1006, in _train_model\n    config=self._session_config', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 365, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 668, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 490, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 842, in __init__\n    _WrappedSession.__init__(self, self._create_session())', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 847, in _create_session\n    return self._sess_creator.create_session()', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 551, in create_session\n    self.tf_sess = self._session_creator.create_session()', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 416, in create_session\n    self._scaffold.finalize()', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 196, in finalize\n    default_ready_for_local_init_op)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 258, in get_or_default\n    op = default_constructor()', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py"", line 193, in default_ready_for_local_init_op\n    variables.global_variables())', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py"", line 175, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py"", line 144, in _add_should_use_warning\n    wrapped = TFShouldUseWarningWrapper(x)', 'File ""C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py"", line 101, in __init__\n    stack = [s.strip() for s in traceback.format_stack()]']"
12877,"Are there any predefined image format, image size etc for training using your tutorial. I found some error for several times, such as : ""No image found in category validation"" & ""division by zero problem""","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12876,"Reading float value from tfrecord, return a not same value.","### System information
- OS Platform and Distribution: Linux Ubuntu 16.04)
- TensorFlow installed from: [binary](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp36-cp36m-linux_x86_64.whl)
- TensorFlow version: v1.3.0-rc2-20-g0787eee 1.3.0
- Python version: Python 3.6.2 :: Continuum Analytics, Inc.

### Describe the problem
When i read feature `label` with `float` type from tfrecord, it return not same value. Example:
- `3.3 -> 3.29999995`
- `3.7 -> 3.70000005`
- `4.9 -> 4.9000001`

Here, code make `Example`:
```
def make_example(s0, s1, label):
    ex = tf.train.SequenceExample()
    ex.context.feature[""len_s0""].int64_list.value.append(len(s0))
    ex.context.feature[""len_s1""].int64_list.value.append(len(s1))
    ex.context.feature[""label""].float_list.value.append(label)# HERE: the feature are `float` type

    for w in s0:
        ex.feature_lists.feature_list[""s0""].feature.add().int64_list.value.append(w)
    for w in s1:
        ex.feature_lists.feature_list[""s1""].feature.add().int64_list.value.append(w)

    return ex
```
Parsing function:
```
def _parse_function(example_proto):
    context_features = {
        ""len_s0"": tf.FixedLenFeature((), dtype=tf.int64),
        ""len_s1"": tf.FixedLenFeature((), dtype=tf.int64),
        ""label"": tf.FixedLenFeature((), dtype=tf.float32) # HERE: the feature are `float` type
    }
    sequence_features = {
        ""s0"": tf.FixedLenSequenceFeature((), dtype=tf.int64),
        ""s1"": tf.FixedLenSequenceFeature((), dtype=tf.int64)
    }

    context_parsed, sequence_parsed = tf.parse_single_sequence_example(
        serialized=example_proto,
        context_features=context_features,
        sequence_features=sequence_features
    )

    len_s0 = tf.cast(context_parsed['len_s0'], dtype=tf.int8)
    len_s1 = tf.cast(context_parsed['len_s1'], dtype=tf.int8)
    # label = tf.convert_to_tensor(context_parsed['label'], dtype=tf.float32)
    # label = tf.round(context_parsed['label'])
    label = context_parsed['label']
    s0 = tf.cast(sequence_parsed['s0'], dtype=tf.int32)
    s1 = tf.cast(sequence_parsed['s1'], dtype=tf.int32)

    return {""len_s0"": len_s0, ""s0"": s0, ""len_s1"": len_s1, ""s1"": s1}, label
```
When i call `result = tf.contrib.learn.run_n({""label"": label}) `, it return:
```
{'label': array([ 3.29999995], dtype=float32)} # right value: 3.3
{'label': array([ 3.70000005], dtype=float32)} # right value: 3.7
{'label': array([ 4.9000001], dtype=float32)} # right value: 4.9
```

Some one can help me? Plzz told me some ways to fix it."
12875,python wrapper around libtensorflow,I wish to use tensorflow as a standalone library. I  have  `libtensorflow.so`. How do I bridge it to python so I can use it for standalone purpose?or is there any github repo for the same
12874,error in Building tensor flow. [sun.security.validator.ValidatorException:],"== cat /etc/issue ===============================================
Linux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
Hello Below is compilation error i am getting.

$~/bin/bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
...........
ERROR: /home/ravi.spatil/LinuxShare/caffe_opecv_resources/mobilenet/tensorflow_ori/tensorflow/tools/pip_package/BUILD:101:1: no such package '@nsync//': Error downloading [https://github.com/google/nsync/archive/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz] to /home/ravi.spatil/.cache/bazel/_bazel_ravi.spatil/39b7c1709071717822a0ecd1200753ba/external/nsync/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted
INFO: Elapsed time: 28.497s
FAILED: Build did NOT complete successfully (106 packages loaded)

Below is system information.
--------------------------------------


== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
../tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

"
12873,Efficient way to load a model over 2GiB with C++ interface?,"Hi,all
When applying TF model with its C++ interface, freeze_graph operations, for the graph file and the cpk file, are required. This operation will then generate a dumped binary protobuffer file.

However, protobuffer does not support to dump a file that is larger than 2GiB.

In this case, what's the right way to load a big model in TF, with its C++ interface? Any clues will be appriciated.

Thank you"
12871,About Deterministic Behaviour of GPU implementation of tensorflow,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian
- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 950


Dear experts,

I have just written a python code that implements a CNN with tensorflow. Despite using tf.set_random_seed(0) throughout my code, I get different results for each different run using a GPU, while this is not happening when switching to CPU. I read different threads about stochastic behaviour of GPUs and I could find that the stochastic behaviour is happening when using AdamOptimizer. My code now uses a CPU just for the optimizer, and a GPU for all the others operations, so now the time performance of the GPU is 25% better than CPU despite the 700% of the full GPU implementation. My question is: is this avoidable? Should I really give up in having a deterministic code when using GPUs? How can I tune my hyperparameters and use a full GPU code (some forums say to make an average of different runs.. but this kills the time advantage of using GPUs).

Thanks in advance for all your kind support and for the wonderful job
"
12870,init weights from pre_train model resnet_v2_101 something wrong :    Assign requires shapes of both tensors to match. lhs shape= [19] rhs shape= [256]," **SSD generate feature map by VGG16, I replaced VGG16 with ResNet_V2_101,the code as follows:**
`  with slim.arg_scope(resnet_utils.resnet_arg_scope()):
         net , end_points =resnet_v2.resnet_v2_101(inputs,reuse=reuse,global_pool=False,is_training=True,scope='resnet_v2_101')
`
           
**when i restore the pretrain_weights from the checkpoint_path, something wrong as follows:**
`InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [19] rhs shape= [256]
	 [[Node: save_1/Assign_294 = Assign[T=DT_FLOAT, _class=[""loc:@resnet_v2_101/block3/unit_18/bottleneck_v2/conv2/BatchNorm/gamma""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](resnet_v2_101/block3/unit_18/bottleneck_v2/conv2/BatchNorm/gamma, save_1/RestoreV2_294)]]
`
**## I do not konw why, Hope for your help!**"
12864,No gradient defined for operation '..' (op type: QueueDequeueUpToV2),"When I tried to compute the gradient of the loss with 'grads = opt.compute_gradients(total_loss, update_gradient_vars)', there was an error said 'No gradient defined for operation '..' (op type: QueueDequeueUpToV2)' "
12863,[BUG] unexpect results when run init with assign,"```python
import tensorflow as tf
sess = tf.InteractiveSession()
a =tf.Variable([1])
b = tf.assign_add(a,[1])
init = tf.global_variables_initializer()
init_a=tf.variables_initializer([a])
sess.run([init])
for i in range(10):
    print(sess.run([a,b,init_a]))

# GET
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([2], dtype=int32), array([2], dtype=int32), None]
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([2], dtype=int32), array([2], dtype=int32), None]
[array([2], dtype=int32), array([2], dtype=int32), None]
[array([1], dtype=int32), array([1], dtype=int32), None]
[array([2], dtype=int32), array([2], dtype=int32), None]

# this make the metrics and stream metrics hard to use 
```

"
12860,cuda/cuda_config.h missing when compiling custom ops with nvcc,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, see below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
v1.3.0-0-g9e76bf324 1.3.0
- **Python version**: 
3.5.2
- **Bazel version (if compiling from source)**:
0.5.4
- **CUDA/cuDNN version**:
8.0.44 / 5.1.5
- **GPU model and memory**:
Any.
- **Exact command to reproduce**:
See below.

### Describe the problem

When compiling a custom op using nvcc, which includes `tensorflow/core/util/cuda_kernel_helper.h`, I get the following error:

```
/usr/local/cuda-8.0/bin/nvcc -c -o ~/Code/libspn/build/ops/gather_columns_functor_gpu.cu.cc.o ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.cc -std=c++11 -x=cu -Xcompiler -fPIC -DGOOGLE_CUDA=1 --expt-relaxed-constexpr -I ~/.local/lib/python3.5/site-packages/tensorflow/include -gencode=arch=compute_35,""code=sm_35,compute_35"" -gencode=arch=compute_52,""code=sm_52,compute_52"" -gencode=arch=compute_61,""code=sm_61,compute_61""
In file included from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/platform/default/stream_executor.h:26:0,
                 from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:24,
                 from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:26,
                 from ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.h:11,
                 from ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.cc:5:
~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
compilation terminated.
```

Copying `cuda_config.h` to `/site-packages/tensorflow/include/tensorflow/stream_executor/cuda` solves the problem.

The same issue has been observed by several other users in #6602 (see the comments added after the issue was closed).

"
12859,FailedPreconditionError when restoring initializable_iterator with Scaffold in a MonitoredTrainingSession for the second time.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: Python 3.5.1
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: - 
- **GPU model and memory**: -
- **Exact command to reproduce**: -

### Context:
Using `initializable_iterator` with `MonitoredTrainingSession` because there are stateful `lookup_ops.index_table_from_tensor()` lookup tables that don't work with `one_shot_iterator`.

`initializable_iterator` is initialized with a tf.train.Scaffold():
```
Scaffold = tf.train.Scaffold(
        init_op=control_flow_ops.group(variables.global_variables_initializer(),
                                       resources.initialize_resources(resources.shared_resources()),
                                       iter_init_op))

with tf.train.MonitoredTrainingSession(
    master=server.target,
    is_chief=hps.is_chief,
    scaffold=Scaffold,
    config=config,
    checkpoint_dir=hps.checkpoint_dir,
    hooks=hooks
) as mon_sess:
                ...
```
Where `iter_init_op` is equivalent to `iterator.initializer`.

### Problem
Upper mentioned initialization works properly when the model is initialized and created for the **first** time and some initial training can be done without problems.

If chief worker crashes or is shut down purposefully, after **restarting** `MonitoredTrainingSession` shows following error as if iterator is not initialized:
```
FailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element...
```

### Workaround
Right now the only solution that works for is to run initialization internally using `_coordinated_creator.tf_sess.run`:
```
mon_sess._coordinated_creator.tf_sess.run(iter_init_op)
```
This doesn't look like an intended use.

### Statement:
This doesn't seem as an intended behaviour.
What is a better way to use `initializable_iterator` with `MonitoredTrainingSession` or `lookup_ops.index_table_from_tensor` with `one_shot_iterator`?"
12857,Unhelpful exceptions from tf.truncated_normal with dtype=tf.int32 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 LTS 
- **TensorFlow installed from (source or binary)**: pip install tensorflow 
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A (CPU)
- **GPU model and memory**: N/A 
- **Exact command to reproduce**:

python -c ""import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.int32)""

### Describe the problem
When attempting to initialize a tf.Variable of type tf.int32 using ```tf.truncated_normal()```, invocation with simple args raise a TypeError, but the error message is ambiguous and unhelpful for debugging: 

```
python -c ""import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.int32)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 168, in truncated_normal
    mean_tensor = ops.convert_to_tensor(mean, dtype=dtype, name=""mean"")
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 376, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got 0.0 of type 'float' instead.

```

Providing more arguments to ```tf.truncated_normal``` correctly indicates that tf.int32 isn't a supported type: 

```
python -c ""import tensorflow as tf ; tf.truncated_normal([1], mean=0, stddev=1, dtype=tf.int32)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 172, in truncated_normal
    shape_tensor, dtype, seed=seed1, seed2=seed2)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 316, in _truncated_normal
    seed=seed, seed2=seed2, name=name)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 700, in apply_op
    attr_value.type = _MakeType(value, attr_def)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 179, in _MakeType
    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'dtype' has DataType int32 not in list of allowed values: float16, float32, float64
```

Contrasting this with usage of complex64, which provides the correct error message even with simple args: 

```
python -c ""import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.complex64)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py"", line 172, in truncated_normal
    shape_tensor, dtype, seed=seed1, seed2=seed2)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 316, in _truncated_normal
    seed=seed, seed2=seed2, name=name)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 700, in apply_op
    attr_value.type = _MakeType(value, attr_def)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 179, in _MakeType
    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)
  File ""/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'dtype' has DataType complex64 not in list of allowed values: float16, float32, float64
```

dtype argument checking should happen up front, earlier in the codepath of ```truncated_normal()```


### Source code / logs

See above
"
12856,C++ gradients: data_flow ops,"This set has just two ops to port - `DynamicStitch` and `DynamicPartition`. Anyone already working on it? Otherwise, I'll sign up for this.

/cc @suharshs "
12855,[Feature Request]Read the last batch in Dataset,"## The programmer guide provides an example of work flow like this


`filenames = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
dataset = tf.contrib.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

### Compute for 100 epochs.
for _ in range(100):
  sess.run(iterator.initializer)
  while True:
    try:
      sess.run(next_element)
    except tf.errors.OutOfRangeError:
      break`

## if we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model? "
12854,"tf.qint8: Quantized 8-bit signed integer (Definition, code piece, examples)","What does quantized mean in this scenario?  Could you please explain with an example and a descriptive definition along with the code used for this?

tf.qint8: Quantized 8-bit signed integer.
What does quantized mean in this scenario?

tf.quint8: Quantized 8-bit unsigned integer.
What does quantized mean in this scenario?

tf.qint16: Quantized 16-bit signed integer.
What does quantized mean in this scenario?

tf.quint16: Quantized 16-bit unsigned integer.
What does quantized mean in this scenario?

tf.qint32: Quantized 32-bit signed integer.
What does quantized mean in this scenario?

tf.resource: Handle to a mutable resource.
What does mutable resource mean?"
12853,Sub-gradient for self_adjoin_eig when eigen values are equal ,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, I have written custom code 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 14.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**: 
Python 2.7
- **Bazel version (if compiling from source)**:

- **CUDA/cuDNN version**:
8.0.61
- **GPU model and memory**:
Tesla K40c
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
This is a feature request. When any two eigen values are equal, the tf.gradients( tf.self_adjoint_eig(matrix), matrix) returns NaN.
While the gradient is not well defined, it would be useful if some valid sub-gradient is returned (which could be used in the optimization).
 
In particular, I am trying to optimize a function involving max eigen value of a matrix. 
Even when two eigen values are equal, a valid sub-gradient would be v_1 * v_1^T, 
where v_1 is the eigen vector corresponding to a max eigen value. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12851,Dataset Unzip Operation,"@mrry This is a comment I originally posted to the ""Redesigning input pipelines"" issue, but I think it went unnoticed given the amount of comments on that thread. Given that issue is now closed (for good reason :)), I decided to post it as a separate issue.

I currently cannot see a way currently to ""unzip"" a dataset. Let's say we have a trainable model that has both a train/fit method and a infer/predict method. Let's call the type of the (potentially) nested structure of inputs to our model `I` and the type of training inputs, which are only needed when training (e.g., supervision labels), `TI`. In this case, we want the train method to accept datasets with elements of type `(I, TI)` (i.e., a tuple of `I` and `TI`) and the predict method to accept datasets with elements of type `I` or `(I, TI)` (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type `I` and one with type `TI`) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type `(I, TI)` to the train method, there is no way to unzip this dataset and initialize both iterators. One has to use `Dataset.map` twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens)."
12849,Unable to compile TF 1.3 from source using full MKL,"Steps to reproduce:

    git clone https://github.com/tensorflow/tensorflow.git test
    cd test
    git checkout r1.3
    yes """" | TF_NEED_CUDA=0 TF_NEED_MKL=1 TF_DOWNLOAD_MKL=0 MKL_INSTALL_PATH=<path>/l_mkl_2017.3.196/inst/mkl ./configure
    bazel build --config=mkl -c opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package

OS version: Ubuntu Linux 14.04
Bazel version: 0.5.3

Error message:

    ERROR: missing input file '//third_party/mkl:libmklml_intel.so'
    ERROR: <path>/tensorflow/test/third_party/mkl/BUILD:16:1: //third_party/mkl:intel_binary_blob: missing input file '//third_party/mkl:libmklml_intel.so'
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
    ERROR: <path>/tensorflow/test/third_party/mkl/BUILD:16:1 1 input file(s) do not exist

The `configure` script is creating symlinks in `third_party/mkl/` for `libmkl_rt.so` ([see here](https://github.com/tensorflow/tensorflow/blob/r1.3/configure#L260)), which is fine, but not for `libmklml_intel.so` ([see here](https://github.com/tensorflow/tensorflow/blob/r1.3/configure#L264)), which doesn't exist in the full MKL distribution. However [`third_party/mkl/BUILD` references `libmklml_intel.so`](https://github.com/tensorflow/tensorflow/blob/r1.3/third_party/mkl/BUILD#L20). Is this a bug or is use of the full MKL library not supported in TensorFlow 1.3?"
12848,RenderScript support,Is there a reason for not supporting RenderScript? It's there an ETA for this?
12847,Compilation issue with AVX option,"### System information
- **OS Platform and Distribution**: Debian Buster
- **TensorFlow installed from**: source
- **TensorFlow version**: commit https://github.com/tensorflow/tensorflow/commit/12a628a623a5dae81b8fc699792eaf414e6ace41
- **Python version**: 3.5.4
- **Bazel version**: 0.5.4
- **CUDA/cuDNN version**: CUDA 8/CuDNN 6
- **GPU model and memory**: 2xTesla K80 with 12GB each
- **CPU model**: Intel Xeon E5-2683 v4
- **Exact command to reproduce**:

```
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --copt=-msse4.1 --config=mkl --config=cuda --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package
```

Or

```
bazel build -c opt --copt=-march=native --copt=-mfpmath=both --config=mkl --config=cuda --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package
```
### Describe the problem
I'm trying to compile a Tensorflow package specifically optimized for my machine. When I run the compilation with one of the command lines described above, I get some compilation errors. Doesn't matter if I let GCC deciding which optimization to make or if I force them. The kind of errors are always the same:

```
/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9220): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9231): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9244): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9255): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9268): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9279): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9292): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9303): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9316): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9327): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9340): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9352): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9365): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9376): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9389): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9401): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9410): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9419): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9428): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9437): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9445): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9454): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9463): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9472): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9481): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9490): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9499): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9508): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9517): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9526): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9535): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9544): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(55): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(63): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(73): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(81): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(91): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(100): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(109): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(117): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(127): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(136): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(145): error: argument of type ""void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(153): error: argument of type ""void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10799): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10811): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10823): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10835): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10847): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10859): error: argument of type ""const void *"" is incompatible with parameter of type ""const float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10871): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10883): error: argument of type ""const void *"" is incompatible with parameter of type ""const double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10895): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10907): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10919): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10931): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10943): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10955): error: argument of type ""const void *"" is incompatible with parameter of type ""const int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10967): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10979): error: argument of type ""const void *"" is incompatible with parameter of type ""const long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10989): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11000): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11009): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11020): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11029): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11040): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11049): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11060): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11069): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11080): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11089): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11100): error: argument of type ""void *"" is incompatible with parameter of type ""float *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11109): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11120): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11129): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11140): error: argument of type ""void *"" is incompatible with parameter of type ""double *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11149): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11160): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11169): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11180): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11189): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11200): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11209): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11220): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11229): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11240): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11249): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11260): error: argument of type ""void *"" is incompatible with parameter of type ""int *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11269): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11280): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11289): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11300): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""

92 errors detected in the compilation of ""/tmp/tmpxft_00007482_00000000-7_zero_initializer_op_gpu.cu.cpp1.ii"".
ERROR: /opt/tensorflow/tensorflow/contrib/framework/BUILD:88:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.pic.o' was not created
ERROR: /opt/tensorflow/tensorflow/contrib/framework/BUILD:88:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 331.599s, Critical Path: 63.79s
FAILED: Build did NOT complete successfully
```

Am I using a wrong command line or is it a bug in the compilation process?

Thanks in advance for any help."
12846,Unable to install tensorflow=1.0.0,"I have been trying to install tensorflow 1.0.0 using pip in Anaconda environment. I found the below error while installation. I have seen previous posts in github and stackover flow but I couldn't find the solution.

(C:\Users\naresh.kumar\AppData\Local\Continuum\Anaconda3) C:\Users\naresh.kumar>**pip install --ignore-installed --upgrade  https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_x86_64.whl**

pip = 9.0.1
python = 3.6.1
Anaconda=4.3.21
Please help me out how to solve this issue.


"
12844,//tensorflow/python:saver_test is failing in Windows Bazel build,"http://ci.tensorflow.org/job/tf-master-win-bzl/1515/console

```
17:24:51 ERROR: testSaveRestore (__main__.LenientNamesTest)
17:24:51 ----------------------------------------------------------------------
17:24:51 Traceback (most recent call last):
17:24:51   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_call
17:24:51     return fn(*args)
17:24:51   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1300, in _run_fn
17:24:51     status, run_metadata)
17:24:51   File ""C:\Program Files\Anaconda3\lib\contextlib.py"", line 66, in __exit__
17:24:51     next(self.gen)
17:24:51   File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
17:24:51     c_api.TF_GetCode(status))
17:24:51 tensorflow.python.framework.errors_impl.NotFoundError: Key v0 not found in checkpoint
17:24:51 	 [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]
```
I found out that `TF_SAVER_LENIENT_NAMES` environment variable set at
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver_test.py#L2474 is not propagated to the [C++ module](https://github.com/tensorflow/tensorflow/blob/bf6df5e2330dff8383869999840578fa5128e794/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L597).

The test passes when running with `--test_env=TF_SAVER_LENIENT_NAMES=True`.

This is only happening in the Bazel build, but not in the CMake build. We need to investigate the reason.

"
12843,Error message of scatter_update is misleading ,"When you call scatter_update with a wrong input vector, the error message will tell you that the rank is 
wrong, which is not very helpful.
An example case in the documentation (provided) would be great.

Example:
```
      testVar = tf.Variable(tf.zeros([5,1]))
      ind = tf.constant([0,3])
      data = tf.constant([5,7], dtype=tf.float32)
      up = tf.scatter_update(testVar,  ind,  data  )
```
Error Message: 
**ValueError: Shapes must be equal rank, but are 1 and 2 for 'ScatterUpdate' (op: 'ScatterUpdate') with input shapes: [5,1], [2], [2].**
That's not that helpful because you don't know which of the three input shapes are wrong, but mostly fine.
Changing the Code to:
```
      testVar = tf.Variable(tf.zeros([5,1]))
      ind = tf.expand_dims(tf.constant([0,3]),0)
      data = tf.expand_dims(tf.constant([5,7], dtype=tf.float32),0)
      up = tf.scatter_update(testVar,  ind,  data  )
```
**ValueError: Shapes must be equal rank, but are 2 and 3 for 'ScatterUpdate' (op: 'ScatterUpdate') with input shapes: [5,1], [1,2], [1,2].**
Is clearly wrong, because non of the inputs has rank 3!

In addition, it would be really awesome if you could add the following code as an example to the scatter_update documentation (https://www.tensorflow.org/api_docs/python/tf/scatter_update). 
(I couldn't find it in the repo(?), otherwise I would habe created a pull request)

Working Example:
```
      testVar = tf.Variable(tf.zeros([5,1]))
      ind = tf.constant([0,3])
      data = tf.expand_dims(tf.constant([5,7], dtype=tf.float32),1)
      up = tf.scatter_update(testVar,  ind,  data  )
```
"
12841,Tensorflow broken on importing tensorflow.contrib.tensorboard module,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**:8.0.61
- **GPU model and memory**: GTX 1070 7.92GiB
- **Exact command to reproduce**:
```
from tensorflow.examples.tutorials.mnist import input_data
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

when I import input_data module from tensorflow.examples.tutorials.mnist, python throws the following errors.And I cannot find the files related to tensorboard under tensorflow/contrib directory. 
I dont know if the issue is a tensorboard issue.
```bash
Traceback (most recent call last):
  File ""tf_mnist.py"", line 1, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""/usr/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/__init__.py"", line 21, in <modul
e>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""/usr/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/input_data.py"", line 29, in <mod
ule>
    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 38, in <module>
    from tensorflow.contrib import keras
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/__init__.py"", line 26, in <module>
    from tensorflow.contrib.keras.api.keras import *
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/api/keras/__init__.py"", line 25, in <module
>
    from tensorflow.contrib.keras.api.keras import activations
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/api/keras/activations/__init__.py"", line 22
, in <module>
    from tensorflow.contrib.keras.python.keras.activations import elu
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/__init__.py"", line 21, in <mod
ule>
    from tensorflow.contrib.keras.python.keras import activations
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/activations.py"", line 24, in <
module>
    from tensorflow.contrib.keras.python.keras.engine import Layer
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/__init__.py"", line 26,
in <module>
    from tensorflow.contrib.keras.python.keras.engine.training import Model
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/training.py"", line 28,
in <module>
    from tensorflow.contrib.keras.python.keras import callbacks as cbks
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/callbacks.py"", line 34, in <mo
dule>
    from tensorflow.contrib.tensorboard.plugins import projector
ModuleNotFoundError: No module named 'tensorflow.contrib.tensorboard'
```
And I have installed the tensorboard package.But the problem remains.
Could anyone help me?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12840,User-defined functions loaders,"I noticed user-defined functions are still [experimental](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L27). I have an idea about functions loaders that could allow us to use dynamically loaded functions from different sources.

Format of function address would be [multiaddr](https://github.com/multiformats/multiaddr#string-format)-like.

```
/<loader>/<namespace>/<function>
```

Loaders can be embed into tensorflow like kernels.

```
/tf/custom
/core/add
/ipfs/QmVv4Wz46JaZJeH5PMV4LGbRi3MKEmszPYY3g6fjGnVXBS
```"
12838,Feature Request: ADAG,"It seems difficult to add [ADAG](http://joerihermans.com/ramblings/distributed-deep-learning-part-1-an-introduction/)  as an optimizer, because by default with tf.train.replica_device_setter(), all variables get assigned to a parameter server (ps).  Thus, it is difficult to update variables locally because the optimizer first pushes updates to the ps, allowing all other workers to see its updates.  If anyone know how to create local copies of variables, I can provide an example implementation.
"
12830,Custom op linking flags,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Clean master (b5214cab6151fc9c0471829a05bab4872e2e3bc4)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: not relevant
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 7 cudnn 6
- **GPU model and memory**: 1080 Ti
- **Exact command to reproduce**: not relevant

### Describe the problem
It is a feature request :)

I'm compiling TF from source (for optimal performance). And I need custom op so I'm following (https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) by adding a bazel BUILD file in user_ops directory.

But at the same time my custom op needs some other libraries (like opencv). I could easily install opencv using my system's package manager and add a ""-lopencv_xxx"" flag during linking. Currently I hacked definition of `tf_custom_op_library` to add the extra linking flags. But I hope that (since I think this scenario is pretty common), `tf_custom_op_library` could expose something like `extra_link_flags`.

It should just be a few extra lines of code upstream. I could submit a PR if you think it's mergable
"
12824,Apparent Segmentation Violation with Go API,"Hi, I am trying to build a project that will take multiple URLs, download their images, and then use TensorFlow and the InceptionV3 pre-trained model to perform image recognition. For this, I am using the Go API. Most of the time, this process occurs without an issue, but every now and again an error occurs. I think this may be caused by a bug in TensorFlow as it appears to being thrown from the c code.

A sample of the code that I am using is below. In case it is relevant, I run ProcessImage from different workers concurrently. This obviously means that the tfSession variable is shared between different threads. However, I don't think this is the problem as I have tried having each worker create and use its own session, and the error still occurs.

```go
var tfSession *tf.Session
var modelGraph, normaliseGraph *tf.Graph
var normaliseInput, normaliseOutput tf.Output

/*
 * Sets up resources that can be shared by each request.
 */
func init() {
  model, _ := ioutil.ReadFile(""inception3/inception_v3_2016_08_28_frozen.pb"")
  modelGraph = tf.NewGraph()
  modelGraph.Import(model, """")
  tfSession, _ = tf.NewSession(modelGraph, nil)
  normaliseGraph, normaliseInput, normaliseOutput = ConstructNormaliseGraph()
}

/*
 * Processes the image and returns the probabilities of each label.
 */
func ProcessImage(url string) []float32 {
  tensor, okay := MakeTensorFromImage(url)
  if !okay {
    return nil
  }

  output, err := tfSession.Run(
    map[tf.Output]*tf.Tensor{
      modelGraph.Operation(""input"").Output(0): tensor,
    },
    []tf.Output{
      modelGraph.Operation(""InceptionV3/Predictions/Reshape_1"").Output(0),
    },
    nil)
  if err != nil {
    return nil
  }

  return output[0].Value().([][]float32)[0]
}

/*
 * Constructs graph used to normalise image to required dimensions.
 */
func ConstructNormaliseGraph() (graph *tf.Graph, input, output tf.Output) {
  s := op.NewScope()
  input = op.Placeholder(s, tf.String)
  output = op.Div(s,
		op.Sub(s,
			op.ResizeBilinear(s,
				op.ExpandDims(s,
					op.Cast(s,
						op.DecodeJpeg(s, input, op.DecodeJpegChannels(3)), tf.Float),
					op.Const(s.SubScope(""make_batch""), int32(0))),
				op.Const(s.SubScope(""size""), []int32{299, 299})),
			op.Const(s.SubScope(""mean""), float32(0))),
		op.Const(s.SubScope(""scale""), float32(255)))
	graph, _ = s.Finalize()
  return
}

/*
 * Creates a Tensor from the given image url.
 */
func MakeTensorFromImage(url string) (*tf.Tensor, bool) {
  r, err := client.Get(url)
  if err != nil {
    return nil, false
  }
  bytes, _ := ioutil.ReadAll(r.Body)
  r.Body.Close()

  stringBytes := string(bytes)
  if stringBytes == ""Content not found"" {
    return nil, false
  }
  tensor, _ := tf.NewTensor(stringBytes)

  session, _ := tf.NewSession(normaliseGraph, nil)
  defer session.Close()

  normalized, err := session.Run(
    map[tf.Output]*tf.Tensor{normaliseInput: tensor},
    []tf.Output{normaliseOutput},
    nil)
  if err != nil {
    return nil, false
  }
  return normalized[0], true
}
```

I'm using macOS Sierra, but this error also exists when I compile the project for Linux. I have included both of these Go environments below. On Sierra, I installed TensorFlow using Homebrew, and on Linux, I installed it using the instructions [here](https://www.tensorflow.org/install/install_go).

```
GOARCH=""amd64""
GOBIN=""""
GOEXE=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""darwin""
GOOS=""darwin""
GOPATH=""/Users/Jamie/Documents/Go""
GORACE=""""
GOROOT=""/usr/local/Cellar/go/1.9/libexec""
GOTOOLDIR=""/usr/local/Cellar/go/1.9/libexec/pkg/tool/darwin_amd64""
GCCGO=""gccgo""
CC=""clang""
GOGCCFLAGS=""-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=/var/folders/qq/c31qf27j2mng1xxhvk0b6b6c0000gn/T/go-build437606594=/tmp/go-build -gno-record-gcc-switches -fno-common""
CXX=""clang++""
CGO_ENABLED=""1""
CGO_CFLAGS=""-g -O2""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-g -O2""
PKG_CONFIG=""pkg-config""
```

```
GOARCH=""amd64""
GOBIN=""""
GOEXE=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""linux""
GOOS=""linux""
GOPATH=""/home/ec2-user/go""
GORACE=""""
GOROOT=""/usr/local/go""
GOTOOLDIR=""/usr/local/go/pkg/tool/linux_amd64""
GCCGO=""gccgo""
CC=""gcc""
GOGCCFLAGS=""-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build126220137=/tmp/go-build -gno-record-gcc-switches""
CXX=""g++""
CGO_ENABLED=""1""
PKG_CONFIG=""pkg-config""
CGO_CFLAGS=""-g -O2""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-g -O2""
```

The error I am getting is below. For your information, image.go line 99 is the line that contains `output, err := tfSession.Run(` in the ProcessImage function above.

```
fatal error: unexpected signal during runtime execution
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x0]

runtime stack:
runtime.throw(0x436cbee, 0x2a)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/panic.go:605 +0x95
runtime.sigpanic()
	/usr/local/Cellar/go/1.9/libexec/src/runtime/signal_unix.go:351 +0x2b8

goroutine 12 [syscall, locked to thread]:
runtime.cgocall(0x42aedc0, 0xc425d31cd8, 0x4310100)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/cgocall.go:132 +0xe4 fp=0xc425d31c90 sp=0xc425d31c50 pc=0x40044d4
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0xb250860, 0x0, 0xc425d69550, 0xc4200fe0e0, 0x1, 0xc425d69540, 0xc4200fe0d8, 0xc400000001, 0x0, 0x0, ...)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:705 +0x45 fp=0xc425d31cd8 sp=0xc425d31c90 pc=0x419af45
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0xb250860, 0x0, 0xc425d69550, 0xc4200fe0e0, 0x1, 0xc425d69540, 0xc4200fe0d8, 0xc400000001, 0x0, 0xc400000000, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a fp=0xc425d31d48 sp=0xc425d31cd8 pc=0x41a56ca
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000c0a0, 0xc4200fd770, 0xc425d31e78, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23f fp=0xc425d31de0 sp=0xc425d31d48 pc=0x419efff
github.com/jamiebaggott/vision.ProcessImage(0xc4202f2b60, 0x6d, 0xc420080301, 0x0, 0x0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:99 +0x214 fp=0xc425d31ea8 sp=0xc425d31de0 pc=0x42a8f94
github.com/jamiebaggott/vision.ImageWorker()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:174 +0x10c fp=0xc425d31fe0 sp=0xc425d31ea8 pc=0x42a9a3c
runtime.goexit()
	/usr/local/Cellar/go/1.9/libexec/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc425d31fe8 sp=0xc425d31fe0 pc=0x4059781
created by github.com/jamiebaggott/vision.init.1
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:38 +0x37c

goroutine 1 [IO wait, 2 minutes]:
internal/poll.runtime_pollWait(0x46adeb0, 0x72, 0xffffffffffffffff)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc420108118, 0x72, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc420108118, 0xffffffffffffff00, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Accept(0xc420108100, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:334 +0x1e2
net.(*netFD).accept(0xc420108100, 0x4376820, 0xc425ce1d98, 0x400439b)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:238 +0x42
net.(*TCPListener).accept(0xc4201440b0, 0x42f7d80, 0xc425ce1dc8, 0x4003137)
	/usr/local/Cellar/go/1.9/libexec/src/net/tcpsock_posix.go:136 +0x2e
net.(*TCPListener).AcceptTCP(0xc4201440b0, 0xc425ce1e10, 0xc425ce1e18, 0xc425ce1e08)
	/usr/local/Cellar/go/1.9/libexec/src/net/tcpsock.go:234 +0x49
net/http.tcpKeepAliveListener.Accept(0xc4201440b0, 0x43761e8, 0xc42019c000, 0x44dfda0, 0xc4201365d0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/server.go:3120 +0x2f
net/http.(*Server).Serve(0xc420160270, 0x44df960, 0xc4201440b0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/server.go:2695 +0x1b2
net/http.(*Server).ListenAndServe(0xc420160270, 0xc420160270, 0x7)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/server.go:2636 +0xa9
net/http.ListenAndServe(0x43608c9, 0x5, 0x0, 0x0, 0xc42004df70, 0x42ae572)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/server.go:2882 +0x7f
main.main()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/main/hashtag.go:18 +0x96

goroutine 5 [sleep]:
time.Sleep(0x1dcd6500)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
gopkg.in/mgo%2ev2.(*mongoCluster).syncServersLoop(0xc4200f6000)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/cluster.go:368 +0x424
created by gopkg.in/mgo%2ev2.newCluster
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/cluster.go:78 +0x181

goroutine 24 [sleep, 1 minutes]:
time.Sleep(0x37e11d600)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
gopkg.in/mgo%2ev2.(*mongoServer).pinger(0xc420146000, 0xc420039e01)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/server.go:301 +0x4fd
created by gopkg.in/mgo%2ev2.newServer
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/server.go:89 +0x13c

goroutine 6 [IO wait]:
internal/poll.runtime_pollWait(0x46adf70, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc420108218, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc420108218, 0xc420154000, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc420108200, 0xc420154000, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc420108200, 0xc420154000, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc42000e038, 0xc420154000, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc42000e038, 0xc420154000, 0x24, 0x24, 0x0, 0x18)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc4200f80e0)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 10 [syscall, locked to thread]:
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0xb250860, 0x0, 0xc420188120, 0xc4200fe058, 0x1, 0xc420188110, 0xc4200fe050, 0xc400000001, 0x0, 0x0, ...)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:705 +0x45
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0xb250860, 0x0, 0xc420188120, 0xc4200fe058, 0x1, 0xc420188110, 0xc4200fe050, 0xc400000001, 0x0, 0xc400000000, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000c0a0, 0xc42038a0f0, 0xc420385e78, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23f
github.com/jamiebaggott/vision.ProcessImage(0xc4202f2770, 0x6d, 0xc420080101, 0x0, 0x0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:99 +0x214
github.com/jamiebaggott/vision.ImageWorker()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:174 +0x10c
created by github.com/jamiebaggott/vision.init.1
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:38 +0x37c

goroutine 11 [syscall, locked to thread]:
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0xb250860, 0x0, 0xc420188260, 0xc4200fe0b8, 0x1, 0xc420188240, 0xc4200fe0b0, 0xc400000001, 0x0, 0x0, ...)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:705 +0x45
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0xb250860, 0x0, 0xc420188260, 0xc4200fe0b8, 0x1, 0xc420188240, 0xc4200fe0b0, 0xc400000001, 0x0, 0xc400000000, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000c0a0, 0xc42038a270, 0xc420389e78, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23f
github.com/jamiebaggott/vision.ProcessImage(0xc4202f25b0, 0x6d, 0xc42016e101, 0x417abd5, 0xc4200f6000)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:99 +0x214
github.com/jamiebaggott/vision.ImageWorker()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:174 +0x10c
created by github.com/jamiebaggott/vision.init.1
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:38 +0x37c

goroutine 13 [syscall, locked to thread]:
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0xb250860, 0x0, 0xc425d69680, 0xc4200fe130, 0x1, 0xc425d69670, 0xc4200fe128, 0xc400000001, 0x0, 0x0, ...)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:705 +0x45
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0xb250860, 0x0, 0xc425d69680, 0xc4200fe130, 0x1, 0xc425d69670, 0xc4200fe128, 0xc400000001, 0x0, 0xc400000000, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000c0a0, 0xc4200fd950, 0xc425cdfe78, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23f
github.com/jamiebaggott/vision.ProcessImage(0xc4202f2d20, 0x6e, 0xc420080601, 0x0, 0x0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:99 +0x214
github.com/jamiebaggott/vision.ImageWorker()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:174 +0x10c
created by github.com/jamiebaggott/vision.init.1
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:38 +0x37c

goroutine 14 [syscall, locked to thread]:
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0xb250860, 0x0, 0xc425d69700, 0xc4200fe180, 0x1, 0xc425d696f0, 0xc4200fe178, 0xc400000001, 0x0, 0x0, ...)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:705 +0x45
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0xb250860, 0x0, 0xc425d69700, 0xc4200fe180, 0x1, 0xc425d696f0, 0xc4200fe178, 0xc400000001, 0x0, 0xc400000000, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a
github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000c0a0, 0xc4200fd9e0, 0xc425d35e78, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
	/Users/Jamie/Documents/Go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23f
github.com/jamiebaggott/vision.ProcessImage(0xc4202f2930, 0x6c, 0xc42016e301, 0x0, 0x0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:99 +0x214
github.com/jamiebaggott/vision.ImageWorker()
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:174 +0x10c
created by github.com/jamiebaggott/vision.init.1
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:38 +0x37c

goroutine 43 [IO wait]:
internal/poll.runtime_pollWait(0x46addf0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc425cfc218, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc425cfc218, 0xc420154000, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc425cfc200, 0xc420154090, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc425cfc200, 0xc420154090, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc4200fe010, 0xc420154090, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc4200fe010, 0xc420154090, 0x24, 0x24, 0x0, 0x18)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc4200f8ee0)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 66 [sleep]:
time.Sleep(0x2aea5400)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
github.com/jamiebaggott/vision.GetRequest(0xc42005a000, 0x6b, 0x1, 0x2, 0x42d6a20, 0xc425ceebe0, 0xe, 0xc420154e40)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:23 +0x53e
github.com/jamiebaggott/vision.GetUsers(0xc420154e40, 0x30, 0xc42013c720, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:151 +0x4ce
github.com/jamiebaggott/vision.Run(0xc42013c720, 0x1b, 0xc420084aa0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:119 +0x10b
created by main.handler
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/main/hashtag.go:66 +0x4e1

goroutine 405 [chan receive, 1 minutes]:
github.com/jamiebaggott/vision.ProcessImages(0xc42013c720, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:81 +0x35b
created by github.com/jamiebaggott/vision.Run
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:118 +0xdc

goroutine 29 [sleep]:
time.Sleep(0x2aea5400)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
github.com/jamiebaggott/vision.GetRequest(0xc42005bd50, 0x6c, 0x1, 0x2, 0x42d6a20, 0xc425d9aa20, 0xe, 0xc4200181c0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:23 +0x53e
github.com/jamiebaggott/vision.GetUsers(0xc4200181c0, 0x32, 0xc420158860, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:151 +0x4ce
github.com/jamiebaggott/vision.Run(0xc420158860, 0x1b, 0xc4200846e0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:119 +0x10b
created by main.handler
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/main/hashtag.go:66 +0x4e1

goroutine 69 [IO wait]:
internal/poll.runtime_pollWait(0x46ada30, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc42025a118, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc42025a118, 0xc420244b00, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc42025a100, 0xc420244ba0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc42025a100, 0xc420244ba0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420286018, 0xc420244ba0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420286018, 0xc420244ba0, 0x24, 0x24, 0x0, 0x18)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc425e4b420)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 33 [IO wait]:
internal/poll.runtime_pollWait(0x46adbb0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc425e1c218, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc425e1c218, 0xc420244a00, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc425e1c200, 0xc420244ae0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc425e1c200, 0xc420244ae0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420144078, 0xc420244ae0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420144078, 0xc420244ae0, 0x24, 0x24, 0x0, 0x339cc)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc420146620)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 439 [IO wait]:
internal/poll.runtime_pollWait(0x46ad670, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc42025a298, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc42025a298, 0xc42008b000, 0x1000, 0x1000)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc42025a280, 0xc42008b000, 0x1000, 0x1000, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc42025a280, 0xc42008b000, 0x1000, 0x1000, 0x0, 0x8, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420286078, 0xc42008b000, 0x1000, 0x1000, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
crypto/tls.(*block).readFromUntil(0xc42038b710, 0xb4640e8, 0xc420286078, 0x5, 0xc420286078, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:488 +0x95
crypto/tls.(*Conn).readRecord(0xc420120380, 0x4376817, 0xc4201204a0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:590 +0xe0
crypto/tls.(*Conn).Read(0xc420120380, 0xc420342000, 0x1000, 0x1000, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:1134 +0x110
bufio.(*Reader).Read(0xc420328b40, 0xc4201e8e38, 0x9, 0x9, 0xc425d2bc70, 0x402b956, 0x4376820)
	/usr/local/Cellar/go/1.9/libexec/src/bufio/bufio.go:213 +0x30b
io.ReadAtLeast(0x44daa60, 0xc420328b40, 0xc4201e8e38, 0x9, 0x9, 0x9, 0xc425d2bcd0, 0xc425d2bcd0, 0x4007d52)
	/usr/local/Cellar/go/1.9/libexec/src/io/io.go:309 +0x86
io.ReadFull(0x44daa60, 0xc420328b40, 0xc4201e8e38, 0x9, 0x9, 0xc420113b00, 0xc425d2bd08, 0xc400000001)
	/usr/local/Cellar/go/1.9/libexec/src/io/io.go:327 +0x58
net/http.http2readFrameHeader(0xc4201e8e38, 0x9, 0x9, 0x44daa60, 0xc420328b40, 0x0, 0xc400000000, 0xc425d2bdf0, 0x4266459)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:1516 +0x7b
net/http.(*http2Framer).ReadFrame(0xc4201e8e00, 0xc4200fc180, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:1774 +0xa4
net/http.(*http2clientConnReadLoop).run(0xc425d2bfb0, 0x4376220, 0xc42033cfb0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7862 +0x92
net/http.(*http2ClientConn).readLoop(0xc42016eb60)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7788 +0x9d
created by net/http.(*http2Transport).newClientConn
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7053 +0x6b9

goroutine 67 [IO wait]:
internal/poll.runtime_pollWait(0x46adaf0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc420108798, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc420108798, 0xc420155200, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc420108780, 0xc420155200, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc420108780, 0xc420155200, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420286008, 0xc420155200, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420286008, 0xc420155200, 0x24, 0x24, 0x0, 0x18)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc425e4a1c0)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 56 [sleep]:
time.Sleep(0x2aea5400)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
github.com/jamiebaggott/vision.GetRequest(0xc420345ab0, 0x6c, 0x1, 0x2, 0x42d6a20, 0xc4200e1ae0, 0xe, 0xc425e04200)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:23 +0x53e
github.com/jamiebaggott/vision.GetUsers(0xc425e04200, 0x32, 0xc4201589a0, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:151 +0x4ce
github.com/jamiebaggott/vision.Run(0xc4201589a0, 0x1b, 0xc425dc08c0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:119 +0x10b
created by main.handler
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/main/hashtag.go:66 +0x4e1

goroutine 85 [IO wait]:
internal/poll.runtime_pollWait(0x46ad970, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc42025a598, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc42025a598, 0xc420155500, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc42025a580, 0xc420155530, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc42025a580, 0xc420155530, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420144098, 0xc420155530, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420144098, 0xc420155530, 0x24, 0x24, 0x0, 0x339cc)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc420147960)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 59 [sleep]:
time.Sleep(0x2aea5400)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/time.go:65 +0x130
github.com/jamiebaggott/vision.GetRequest(0xc425cf4000, 0x6d, 0x1, 0x2, 0x42d6a20, 0xc425d8a800, 0xe, 0xc4200185c0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:23 +0x53e
github.com/jamiebaggott/vision.GetUsers(0xc4200185c0, 0x33, 0xc42013cb40, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/request.go:151 +0x4ce
github.com/jamiebaggott/vision.Run(0xc42013cb40, 0x1b, 0xc425e205a0)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:119 +0x10b
created by main.handler
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/main/hashtag.go:66 +0x4e1

goroutine 446 [IO wait]:
internal/poll.runtime_pollWait(0x46add30, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc425e1c498, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc425e1c498, 0xc4200ae800, 0x400, 0x400)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc425e1c480, 0xc4200ae800, 0x400, 0x400, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc425e1c480, 0xc4200ae800, 0x400, 0x400, 0xc420420a00, 0x158f2dd4a2167001, 0xc42003f870)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc42000e048, 0xc4200ae800, 0x400, 0x400, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
crypto/tls.(*block).readFromUntil(0xc42024a090, 0xb4640e8, 0xc42000e048, 0x5, 0xc42000e048, 0xc42051c2c5)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:488 +0x95
crypto/tls.(*Conn).readRecord(0xc420130000, 0x4376817, 0xc420130120, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:590 +0xe0
crypto/tls.(*Conn).Read(0xc420130000, 0xc425d7d000, 0x1000, 0x1000, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/crypto/tls/conn.go:1134 +0x110
bufio.(*Reader).Read(0xc420328360, 0xc425decc78, 0x9, 0x9, 0xc42003fc70, 0x402b956, 0x4376820)
	/usr/local/Cellar/go/1.9/libexec/src/bufio/bufio.go:213 +0x30b
io.ReadAtLeast(0x44daa60, 0xc420328360, 0xc425decc78, 0x9, 0x9, 0x9, 0xc42003fcd0, 0xc42003fcd0, 0x4007d52)
	/usr/local/Cellar/go/1.9/libexec/src/io/io.go:309 +0x86
io.ReadFull(0x44daa60, 0xc420328360, 0xc425decc78, 0x9, 0x9, 0xc4203283c0, 0xc42003fd08, 0xc400000001)
	/usr/local/Cellar/go/1.9/libexec/src/io/io.go:327 +0x58
net/http.http2readFrameHeader(0xc425decc78, 0x9, 0x9, 0x44daa60, 0xc420328360, 0x0, 0xc400000000, 0xc42003fdf0, 0x4266459)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:1516 +0x7b
net/http.(*http2Framer).ReadFrame(0xc425decc40, 0xc42024ac00, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:1774 +0xa4
net/http.(*http2clientConnReadLoop).run(0xc42003ffb0, 0x4376220, 0xc4203397b0)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7862 +0x92
net/http.(*http2ClientConn).readLoop(0xc420080b60)
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7788 +0x9d
created by net/http.(*http2Transport).newClientConn
	/usr/local/Cellar/go/1.9/libexec/src/net/http/h2_bundle.go:7053 +0x6b9

goroutine 482 [chan receive]:
github.com/jamiebaggott/vision.ProcessImages(0xc42013cb40, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:81 +0x35b
created by github.com/jamiebaggott/vision.Run
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:118 +0xdc

goroutine 449 [chan receive]:
github.com/jamiebaggott/vision.ProcessImages(0xc420158860, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:81 +0x35b
created by github.com/jamiebaggott/vision.Run
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:118 +0xdc

goroutine 392 [IO wait]:
internal/poll.runtime_pollWait(0x46ad8b0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc42025a198, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc42025a198, 0xc425d8ce00, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc42025a180, 0xc425d8ce70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc42025a180, 0xc425d8ce70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420144008, 0xc425d8ce70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420144008, 0xc425d8ce70, 0x24, 0x24, 0x0, 0x339cc)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc420246e00)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 462 [chan receive]:
github.com/jamiebaggott/vision.ProcessImages(0xc4201589a0, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:81 +0x35b
created by github.com/jamiebaggott/vision.Run
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/database.go:118 +0xdc

goroutine 364 [IO wait]:
internal/poll.runtime_pollWait(0x46ad7f0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc425e1c118, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc425e1c118, 0xc42001c700, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc425e1c100, 0xc42001c7b0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc425e1c100, 0xc42001c7b0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc420286000, 0xc42001c7b0, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc420286000, 0xc42001c7b0, 0x24, 0x24, 0x0, 0xa9)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc425e4a2a0)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 483 [chan send]:
github.com/jamiebaggott/vision.ProcessImages.func1(0xc425d02960, 0xc4204f20c0, 0xc4200e1ba0, 0xc420345b20, 0xc42013cb40, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:62 +0x1a3
created by github.com/jamiebaggott/vision.ProcessImages
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:57 +0x327

goroutine 394 [IO wait]:
internal/poll.runtime_pollWait(0x46adc70, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc425cfd218, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc425cfd218, 0xc420245800, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc425cfd200, 0xc420245800, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc425cfd200, 0xc420245800, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc4200fe028, 0xc420245800, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc4200fe028, 0xc420245800, 0x24, 0x24, 0x0, 0x339cc)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc425dec000)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 385 [chan send]:
github.com/jamiebaggott/vision.ProcessImages.func1(0xc425d020f0, 0xc420119b00, 0xc425d049c0, 0xc4202f3ea0, 0xc42013c720, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:62 +0x1a3
created by github.com/jamiebaggott/vision.ProcessImages
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:57 +0x327

goroutine 461 [IO wait]:
internal/poll.runtime_pollWait(0x46ad4f0, 0x72, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/runtime/netpoll.go:173 +0x57
internal/poll.(*pollDesc).wait(0xc420108d98, 0x72, 0xffffffffffffff00, 0x44dd020, 0x44d9690)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:85 +0xae
internal/poll.(*pollDesc).waitRead(0xc420108d98, 0xc420244e00, 0x24, 0x24)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Read(0xc420108d80, 0xc420244e70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/internal/poll/fd_unix.go:125 +0x18a
net.(*netFD).Read(0xc420108d80, 0xc420244e70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/fd_unix.go:202 +0x52
net.(*conn).Read(0xc4200fe1a0, 0xc420244e70, 0x24, 0x24, 0x0, 0x0, 0x0)
	/usr/local/Cellar/go/1.9/libexec/src/net/net.go:176 +0x6d
gopkg.in/mgo%2ev2.fill(0x44e1700, 0xc4200fe1a0, 0xc420244e70, 0x24, 0x24, 0x0, 0x1e7)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:535 +0x53
gopkg.in/mgo%2ev2.(*mongoSocket).readLoop(0xc4201e8540)
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:551 +0x658
created by gopkg.in/mgo%2ev2.newSocket
	/Users/Jamie/Documents/Go/src/gopkg.in/mgo.v2/socket.go:194 +0x23f

goroutine 463 [chan send]:
github.com/jamiebaggott/vision.ProcessImages.func1(0xc425d6c2d0, 0xc42038ad80, 0xc425d83820, 0xc42005a150, 0xc4201589a0, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:62 +0x1a3
created by github.com/jamiebaggott/vision.ProcessImages
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:57 +0x327

goroutine 464 [chan send]:
github.com/jamiebaggott/vision.ProcessImages.func1(0xc425d6c3c0, 0xc42038ae40, 0xc425d83960, 0xc42005a1c0, 0xc420158860, 0x1b)
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:62 +0x1a3
created by github.com/jamiebaggott/vision.ProcessImages
	/Users/Jamie/Documents/Go/src/github.com/jamiebaggott/vision/image.go:57 +0x327
```"
12823,bazel error ,"ERROR: /Users/dile/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
	File ""/Users/dile/tensorflow/third_party/py/python_configure.bzl"", line 310
		_create_local_python_repository(repository_ctx)
	File ""/Users/dile/tensorflow/third_party/py/python_configure.bzl"", line 268, in _create_local_python_repository
		_get_python_bin(repository_ctx)
	File ""/Users/dile/tensorflow/third_party/py/python_configure.bzl"", line 166, in _get_python_bin
		_get_env_var(repository_ctx, _PYTHON_BIN_PATH, No..., ...)
	File ""/Users/dile/tensorflow/third_party/py/python_configure.bzl"", line 49, in _get_env_var
		_python_configure_fail((""'%s' environment variable is n...))
	File ""/Users/dile/tensorflow/third_party/py/python_configure.bzl"", line 37, in _python_configure_fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: 'PYTHON_BIN_PATH' environment variable is not set
 and referenced by '//third_party/py/numpy:headers'.
ERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted."
12822,TensorFlow import ,"Tensor flow report issue:

I am getting the following error  when importing tensorflow.

import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Deena Awny\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
"
12821,Wrong error message during compiling,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Amazon Linux 2017.03
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: release v1.3.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: CUDA 7.5/cuDNN 5.1.10
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: remove _patch_ command and try to compile

### Describe the problem
There's a wrong error message when you try to compile TensorFlow without `patch` command installed. It's not a big deal but could take a couple of hours to figure it out.

### Source code / logs
`
ERROR: /home/ec2-user/workplace/tensorflow/tensorflow/tools/pip_package/BUILD:100:1: no such package '@boringssl//': Traceback (most recent call last):
	File ""/home/ec2-user/workplace/tensorflow/tensorflow/workspace.bzl"", line 116
		_apply_patch(repo_ctx, repo_ctx.attr.patch_file)
	File ""/home/ec2-user/workplace/tensorflow/tensorflow/workspace.bzl"", line 107, in _apply_patch
		_execute_and_check_ret_code(repo_ctx, cmd)
	File ""/home/ec2-user/workplace/tensorflow/tensorflow/workspace.bzl"", line 91, in _execute_and_check_ret_code
		fail(""Non-zero return code({1}) when ...))
Non-zero return code(256) when executing 'patch -p1 -d /home/ec2-user/.cache/bazel/_bazel_ec2-user/4ee13f1db5bfc278f4537815cf99cd27/external/boringssl -i /home/ec2-user/workplace/tensorflow/third_party/boringssl/add_boringssl_s390x.patch':
Stdout:
Stderr: java.io.IOException: Cannot run program ""patch"" (in directory ""/home/ec2-user/.cache/bazel/_bazel_ec2-user/4ee13f1db5bfc278f4537815cf99cd27/external/boringssl""): error=2, No such file or directory and referenced by '//tensorflow/tools/pip_package:licenses'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
`"
12818,[feature request] Any way to control the order of send/recv (host to device data transfer) explicitly?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.2
- **Python version**:
3.5
- **Bazel version (if compiling from source)**:
0.5.2
- **CUDA/cuDNN version**:
8.0
- **GPU model and memory**:
GTX 1070

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Overlapping the host to device data transfer with GPU calculation is an important method to improve the performance of inference workload. But I found it very difficult to control the order of data transfers explicitly at this moment. I understand the send/recv operations are added implicitly when the original graph is split/optimized into sub-graphs. I tried to use control dependency + identity/assign operation (as suggested [here](https://github.com/tensorflow/tensorflow/issues/2848)) to hand control send/recv order. But it seems impossible to achieve this goal. 

For example, 
In the following code, we'd like to overlap the H2D memcpy of B->B_GPU with the matmul calculation.
```
import os
import tensorflow as tf
import numpy as np

from tensorflow.python.client import session
from tensorflow.python.framework import ops
from tensorflow.python.ops import variables

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

with ops.Graph().as_default():
  with tf.device('/cpu:0'):
    A = tf.placeholder(tf.float32, shape=(1000, 100), name=""A"")
    B = tf.placeholder(tf.float32, shape=(1000, 1000), name=""B"")
  with tf.device('/gpu:0'):
    C = variables.Variable(tf.random_normal([100,1000],stddev=0.1), name=""C"")
    A_GPU = tf.identity(A)
    with tf.control_dependencies([A_GPU]):
      B_GPU = tf.identity(B)
      D = tf.add(tf.matmul(A_GPU, C), B_GPU, name=""D"")

  random_A=np.random.rand(1000,100).astype(np.float32)
  random_B=np.random.rand(1000,1000).astype(np.float32)

  sess = session.Session()
  init = tf.global_variables_initializer()
  sess.run(init)

  for x in range(1,50):
    output = sess.run(D,feed_dict={A:random_A,B:random_B})
```

But it turned out that H2D memcpy A->A_GPU is not ensured to launch first. Actually among the 50 iterations, I noticed the order of transferring A->A_GPU and transferring B->B_GPU is randomly performed if we use multiple threads (because the two transfers are handled in different threads, and no-dependency could be built between them). 
If A->A_GPU launches first, the matmul op could be overlapped with B->B-GPU
![image](https://user-images.githubusercontent.com/8175586/30090954-3f28ef90-92a6-11e7-98ee-2a05e6a7463d.png)
otherwise, the matmul op has to wait until all H2D memcpys finished.
![image](https://user-images.githubusercontent.com/8175586/30091033-c6b2f988-92a6-11e7-8abb-f41feac3bbf1.png)

Only if we can control the order of send node explicitly, the overlap can be ensured.

Please let me know if I didn't use the identity operation correctly.

Thanks."
12817,"LMDB reader Error in Reading Data of multi-thread, queue based input pipeline","Hi,

I am now using LMDB reader to read and decode my custom data into tensorflow training pipeline, based on the example:
```
def read_my_file_format(filename_queue):
  reader = tf.SomeReader()
  key, record_string = reader.read(filename_queue)
  example, label = tf.some_decoder(record_string)
  processed_example = some_processing(example)
  return processed_example, label

def input_pipeline(filenames, batch_size, num_epochs=None):
  filename_queue = tf.train.string_input_producer(
      filenames, num_epochs=num_epochs, shuffle=True)
  example, label = read_my_file_format(filename_queue)
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch(
      [example, label], batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch
````
Here, I designed my own decoder and used LMDB reader here. But the problem is that, when the `num_epochs` is not 1, there will be ERROR:

```
Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```

Could you help check this?

Thanks
"
12816,The efficiency of sess.run,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12815,image classifier error,"Source : [tensorflow for poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#4)

```
Anujs-iMac:tensorflow-for-poets-2 anujchampjain$ python -m scripts.label_image     --graph=tf_files/retrained_graph.pb      --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg
2017-09-05 12:48:36.936100: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-05 12:48:36.936140: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-05 12:48:36.936144: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-05 12:48:36.936148: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""/Users/anujchampjain/anaconda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/anujchampjain/anaconda/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/anujchampjain/Desktop/Workspace/Tensorflow/tensorflow-for-poets-2/scripts/label_image.py"", line 120, in <module>
    input_operation = graph.get_operation_by_name(input_name);
  File ""/Users/anujchampjain/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2836, in get_operation_by_name
    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)
  File ""/Users/anujchampjain/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2708, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/Users/anujchampjain/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2768, in _as_graph_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'import/input' refers to an Operation not in the graph.""
```"
12813,How to add a user_ops to android .so file ?,"### Discription
I want to add a custom op for tensorflow and use it in android app. But I find it only tell us how to add an op in python . I put my own .cc file witch contains register code into the user_ops folder and bazel build again to generate a new .so file . But when I test this .so in android code, It says the custom op is not registered and crashed. How can I succeed to register a new op and build it into the android .so file? 

And I found that there are two folder named ""user_ops"" : tensorflow/tensorflow/user_ops ; and tensorflow/tensorflow/core/user_ops;  witch folder should I put my ops in ?  

Hope for your answer. Thank you so much!

------------------------

### System information
Tensorflow r1.2

### Source code / logs
![image](https://user-images.githubusercontent.com/10495849/30049205-4bb48d28-924c-11e7-828e-62e14ba2e9a6.png)

![image](https://user-images.githubusercontent.com/10495849/30049227-658ed708-924c-11e7-8a77-0fa3cb4a1105.png)

![image](https://user-images.githubusercontent.com/10495849/30049236-6cfa6b06-924c-11e7-8384-6de337db0807.png)




"
12812,tensorflow/tensorflow/examples/ios/benchmark/ .   Calculate % in Images.,"In this example How we can calculate the % of Prediction.
We are getting 1.0.978 water bottle 
1.0.988 bottle
1.0.078 mug
 type of result.

How we can convert in % value"
12811,The predict results using java and python is different,"My graph contains the following statements:
`tf.contrib.layers.batch_norm(incoming, is_training=is_training, scale=True, decay=0.99)`
` tf.contrib.layers.dropout(incoming, keep_prob=keep_prob, is_training=is_training)`

When the variable **is_training** is set to **True**, the saved model give the same result using Java and Python. The result is right.

But, when the variable **is_training** is set to **False**, the saved model give different result using Java and Python. Python give a right result. Java give a wrong result.

Why does this happen?
Tensorflow:1.2.0
OS: centos7
Java:Sun jdk 1.8.0.144
Python:3.4.5
"
12810,build libtensorflow-core.a error  ,"buf -lpthread -lm -lz
Undefined symbols for architecture x86_64:
""nsync::nsync_mu_init(nsync::nsync_mu_s_)"", referenced from:
tensorflow::mutex::mutex() in env.o
tensorflow::mutex::mutex() in random.o
""nsync::nsync_mu_lock(nsync::nsync_mu_s_)"", referenced from:
tensorflow::mutex::lock() in env.o
tensorflow::mutex::lock() in random.o
tensorflow::mutex::lock() in histogram.o
""nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)"", referenced from:
tensorflow::mutex::unlock() in env.o
tensorflow::mutex::unlock() in random.o
tensorflow::mutex::unlock() in histogram.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/9kacha/Desktop/TFFF/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1

    '[' 2 -ne 0 ']'
    echo 'armv7 compilation failed.'
    armv7 compilation failed.
    exit 1

how to fix this error ? "
12808,tensorflow performance issue for map_fn and gather,"I am trying to understand more about certain surprising results i see in implementing a tf graph .
The graph i am working with is just a forest (bunch of trees). This is just a plain forward inference graph , and nothing related to training. I am sharing the snippets for 2 implementation

code snippet 1: 

    with tf.name_scope(""main""):
       
        def get_tree_output(offset):
            loop_vars = (offset,)
            leaf_indice = tf.while_loop(cond,
                                        body,
                                        loop_vars,
                                        back_prop=False,
                                        parallel_iterations=1,
                                        name=""while_loop"")
            tree_score = tf.gather(score_tensor, leaf_indice, name=""tree-scores"")
            output = tf.add(tree_score, output)

        leaf_indices = tf.map_fn(get_tree_output,
                                 tree_offsets_tensor,
                                 dtype=INT_TYPE,
                                 parallel_iterations=n_trees,
                                 back_prop=False,
                                 name=""tree-scores"")

        tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")


code snippet 2:

    with tf.name_scope(""main""):
        tree_offsets_tensor = tf.constant(tree_offsets, dtype=INT_TYPE, name=""tree_offsets_tensor"")
        loop_vars = (tree_offsets_tensor,)
        leaf_indices = tf.while_loop(cond,
                                     body,
                                     loop_vars,
                                     back_prop=False,
                                     parallel_iterations=n_trees,
                                     name=""while_loop"")

        tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")



The rest of the code is exactly the same  : the constant tensors , variables, condition and body for the while loop. thread and parallelism was also the same in both case
code snippet2 :  takes about 500 micro sec to do inference 
code snippet 1 : take about  12 milli sec to do inference 

The difference is that in snippet 1 , I use `map_fn` to operate on `tree_offset_tensor`, where as in snippet 2 , I get rid of that `map_fn`, and just directly use that tensor, so as I understand in snippet1 `get_tree_output` method gets called with one element from `tree_offset_tensor`, we are  having multiple `while_loop` for each individual offset value, whereas in snippet 2 we just have one `while_loop` that just takes multiple offset values (basically the offset_tensor). 

I also tried another variation for snippet , instead of using the map_fn  I write a hand written for loop

code snippet 1 (variation for loop) :

    output = 0
    with tf.name_scope(""main""):
        for offset in tree_offsets:
            loop_vars = (offset,)  # offset here is a scalar 
            leaf_indice = tf.while_loop(cond,
                                        body,
                                        loop_vars,
                                        back_prop=False,
                                        parallel_iterations=1,
                                        name=""while_loop"")
            tree_score = tf.gather(score_tensor, leaf_indice, name=""tree-scores"")
            output = tf.add(tree_score, output)

        #leaf_indices = tf.map_fn(get_tree_output,
        #    tree_offsets_tensor, dtype=INT_TYPE,
        #    parallel_iterations=n_trees, back_prop=False,
        #    name=""tree-scores"")

        #tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        #output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")

This gives minor improvement :  9 millisec
The while condition does a bunch of gather operation , so if i use map_fn or the ""for loop"" the gather operates on a bunch of scalars instead of tensor of offset . Why is the code 20-40x slower , is the usage wrong or are there any caveats here ? Any help in understanding or optimizing this is appreciated"
12807,Speech-Recognitin issue,"Hi,

While running  ""python tensorflow/examples/speech_commands/train.py""

I am getting following issue.::
""ImportError: cannot import name audio_ops""

Please help. 
"
12805,"""High-Performance Models"" documentation page unclear on StagingArea not being a FIFOQueue","I feel that it would be good to modify the [High-Performance Models page](https://www.tensorflow.org/performance/performance_models ) to make it clear that the StagingArea class does not guarantee ordered delivery. 
This would mean changing the line:

> ... StagingArea is a queue-like operator similar to tf.FIFOQueue. The difference is that StagingArea offers simpler functionality and can be executed on both CPU and GPU in parallel with other stages.

to something like this:

> ... StagingArea is a queue-like operator similar to tf.FIFOQueue. The difference is that StagingArea does not guarantee FIFO ordering, but offers simpler functionality and can be executed on both CPU and GPU in parallel with other stages.


I just spent longer than I'd care to admit bug-hunting, when FIFO ordering was critical."
12804,Feature request - non-scalar Multinomial draws,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.12.6 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
For Multinomial distribution the _sample_n method does not support total_count to be a vector and throws a 
```
NotImplementedError(""Sample only supported for scalar number of draws."") 
```
[https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/distributions/multinomial.py#L236](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/distributions/multinomial.py#L236)
It would be a really useful feature to add. 


### Source code / logs
NA"
12803,tensorflow create myOwn_op.dll on windows,"Hi tf developers, I have one request regarding tensorflow's windows version. 

I have tensorflow+gpu successfully built on windows 10 with visual studio 2015, from the source code.
as a result, I get tensorflow.dll and tensorflow.lib.
I have CUDA8.0 and cudnn 5.0; with a gtx 1080 gpu equipped. 

however, my question is not about building and compiling tensorflow.
it is about creating tensorflow plugins. 
I followed the tutorial https://www.tensorflow.org/extend/adding_an_op to construct my own ""plug-in"".
and then I tried to compile a windows .dll; so windows would not export symbols automatically for me .
then I compile a static lib first and used your tools 
 /tensorflow/contrib/cmake/tools/create_def_file.py
to create a .def file for me and eventually used that to compile the .dll.

however, in my python code, when I tried to __correlation__ =  tf.load_op_library(correlation.dll) and I called      __correlation__.correlation()
with Correlation registered using REGISTER_OP(""Correlation"");
it still tells me
 AttributeError: module '7b088d8b906b36d3e50721b0adbaaa6a' has no attribute 'correlation'

I think this is just  a windows (or cl compiler) issue, maybe what REGISTER_OP(""Correlation"") did is just not picked up by the compiler,

so is there any thing I can do to make this happen on windows??
"
12799,problem with optimize_for_inference,"hi,
I'm using tensorflow 1.3.0 installed via pip with python 2.7.12 (Ubuntu 16.04, cuda 8, nvidia 1060)

when i try to optimize a custom model trained with tensorflow 1.3.0 with 

`python -m tensorflow.python.tools.optimize_for_inference --input saved_model.pb --output opt_model.pb --input_names=in --output_names=out`

i get the following error:

```
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/**********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py"", line 83, in main
    input_graph_def.ParseFromString(data)
  File ""/home/***********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py"", line 1069, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 633, in DecodeField
    if value._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 612, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py"", line 1105, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 743, in DecodeMap
    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py"", line 1095, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 799, in _SkipGroup
    new_pos = SkipField(buffer, pos, end, tag_bytes)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/google/protobuf/internal/decoder.py"", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.
```


i already did a ' export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python'
without doing it i got

```
Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/*********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/*********/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/*********/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference.py"", line 83, in main
    input_graph_def.ParseFromString(data)
google.protobuf.message.DecodeError: Error parsing message
```

I'm saving the model like:

```
builder = saved_model_builder.SavedModelBuilder(""model""))
builder.add_meta_graph_and_variables(sess,['serve'],signature_def_map= {""model"": tf.saved_model.signature_def_utils.predict_signature_def(inputs= {""in"" : X }, outputs= {""out"": pred_Y })})
builder.save()
```

i had the same issue with 1.2.1, after this i did the upgrade to 1.3.0 via pip install --upgrade and a retrain.

i have no issues loading and using the model with tensorflow-rust in my test application, but to add some context i got the error

```
Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef 
at  org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:392) at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:96)
```

when trying to load the model with tensorflow on android. first i thought this would be because of some unsupported ops, so i went the optimize_for_inference way. but i guess it looks like a protobuf issue
 
i would appreciate any hint to solve this issue.
with best regards

"
12797,UINT8 is used but only partly supported in the Java API.,"The datatype uint8 exists in the Java API (see DataType.UINT8) and is used by the LabelImage program. However, it does not seem to be fully supported. In particular, elemByteSize() in tensor_jni.cc does not have a case for TF_UINT8, which prevents uint8 tensors from being created or extracted from TensorFlow."
12796,About Cyclic Graph?,"Here is the code that confuses me:

```
import tensorflow as tf

a = tf.Variable(0)
update_a = tf.assign(a, a * 2 + 1)
update_b = tf.assign(a, a * 3 + 3)

s = tf.Session()
s.run(tf.global_variables_initializer())

_, _, x = s.run([update_b, update_a, a])
print(x)
```

It outputs different results (1, 3, 6 and 7, 7 is the most) at different runs.

I think it is caused by the cyclic graph. I don't know if tf has the constriction that the above graph is not allowed. But I have another more common example:

a = convnet_with_bn(img_a)
b = convnet_with_bn(img_b)

when the convnet is share, the bn' moving average operations is like the above code.

Can anybody tells me how I can deal with the above problem?
"
12795,"computed output size would be negative      [[Node: pool_3 = AvgPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 8, 8, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](mixed_10/join)]]  ","python train model and run bazel-bin/tensorflow/python/tools/optimize_for_inference  --input=/Users/andylin/Desktop/tess/retrained_graph.pb  --output=/Users/andylin/Desktop/tess/retrained_graph_optimized.pb  --input_names=Mul --output_names=final_result.
error:
computed output size would be negative      [[Node: pool_3 = AvgPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 8, 8, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](mixed_10/join)]]  "
12794,protobuf crashes at runtime when loading tensor lib.,"hardware: Huawei P7 Android 4.4.2

i tried ndk r12b , r10e , and api 9, api 14
all run into this error:

09-04 19:10:47.640 21660-21660/com.zhuxin.ecg.jijian A/libc: Fatal signal 6 (SIGABRT) at 0x0000549c (code=-6), thread 21660 (uxin.ecg.jijian)
09-04 19:10:47.740 162-162/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
09-04 19:10:47.740 162-162/? I/DEBUG: Build fingerprint: 'Huawei/P7-L00/hwp7:4.4.2/HuaweiP7-L00/C17B620:user/ota-rel-keys,release-keys'
09-04 19:10:47.740 162-162/? I/DEBUG: Revision: '0'
09-04 19:10:47.740 162-162/? I/DEBUG: pid: 21660, tid: 21660, name: uxin.ecg.jijian  >>> com.zhuxin.ecg.jijian <<<
09-04 19:10:47.740 162-162/? I/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
09-04 19:10:47.880 754-754/? W/View: requestLayout() improperly called by com.android.systemui.statusbar.phone.IconMerger{434090b0 V.E..... ......I. 0,0-270,72 #7f0a0069 app:id/notificationIcons} during layout: running second layout pass
...
09-04 19:10:48.360 162-162/? I/DEBUG:     #06  pc 0000274d  /system/bin/linker
09-04 19:10:48.360 162-162/? I/DEBUG:     #07  pc 00002823  /system/bin/linker
09-04 19:10:48.360 162-162/? I/DEBUG:     #08  pc 00002975  /system/bin/linker
09-04 19:10:48.360 162-162/? I/DEBUG:     #09  pc 000029e9  /system/bin/linker
09-04 19:10:48.360 162-162/? I/DEBUG:     #10  pc 00000f43  /system/bin/linker
09-04 19:10:48.360 162-162/? I/DEBUG:     #11  pc 00050ee3  /system/lib/libdvm.so (dvmLoadNativeCode(char const*, Object*, char**)+182)
09-04 19:10:48.360 162-162/? I/DEBUG:     #12  pc 00068885  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #13  pc 00027ea0  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #14  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)
09-04 19:10:48.360 162-162/? I/DEBUG:     #15  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)
09-04 19:10:48.360 162-162/? I/DEBUG:     #16  pc 00061595  /system/lib/libdvm.so (dvmCallMethodV(Thread*, Method const*, Object*, bool, JValue*, std::__va_list)+336)
09-04 19:10:48.360 162-162/? I/DEBUG:     #17  pc 000615b9  /system/lib/libdvm.so (dvmCallMethod(Thread*, Method const*, Object*, JValue*, ...)+20)
09-04 19:10:48.360 162-162/? I/DEBUG:     #18  pc 0006cd7d  /system/lib/libdvm.so (dvmInitClass+1020)
09-04 19:10:48.360 162-162/? I/DEBUG:     #19  pc 0006da87  /system/lib/libdvm.so (dvmResolveMethod+198)
09-04 19:10:48.360 162-162/? I/DEBUG:     #20  pc 000234f4  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #21  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)
09-04 19:10:48.360 162-162/? I/DEBUG:     #22  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)
09-04 19:10:48.360 162-162/? I/DEBUG:     #23  pc 00061879  /system/lib/libdvm.so (dvmInvokeMethod(Object*, Method const*, ArrayObject*, ArrayObject*, ClassObject*, bool)+392)
09-04 19:10:48.360 162-162/? I/DEBUG:     #24  pc 00069963  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #25  pc 00027ea0  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #26  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)
09-04 19:10:48.360 162-162/? I/DEBUG:     #27  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)
09-04 19:10:48.360 162-162/? I/DEBUG:     #28  pc 00061595  /system/lib/libdvm.so (dvmCallMethodV(Thread*, Method const*, Object*, bool, JValue*, std::__va_list)+336)
09-04 19:10:48.360 162-162/? I/DEBUG:     #29  pc 0004ac6b  /system/lib/libdvm.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #30  pc 0004ed47  /system/lib/libandroid_runtime.so
09-04 19:10:48.360 162-162/? I/DEBUG:     #31  pc 0004faef  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, char const*)+354)
09-04 19:10:48.360 162-162/? I/DEBUG: stack:
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e0  0006be74  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e4  81cfa290  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e8  beed5104  [stack]
# 09-04 19:10:48.360 162-162/? I/DEBUG:          beed50ec  812a0da8  /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so (std::unordered_map<std::string, google::protobuf::FieldDescriptorProto_Type, google::protobuf::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> > >::operator[](std::string&&)+48)
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f0  beed51a0  [stack]
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f4  81cfa290  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f8  81cfa290  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed50fc  20492111  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed5100  81cfa290  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed5104  00000001  
09-04 19:10:48.360 162-162/? I/DEBUG:          beed5108  00000015  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed510c  71a0b990  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5110  00000001  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5114  4007d9b5  /system/lib/libc.so (write+12)
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5118  4008e1d8  /system/lib/libc.so
09-04 19:10:48.370 162-162/? I/DEBUG:          beed511c  71a0b990  
09-04 19:10:48.370 162-162/? I/DEBUG:     #00  beed5120  00000006  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5124  00000016  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5128  0000549c  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed512c  400b1f0f  /system/bin/linker
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5130  400b1f0f  /system/bin/linker
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5134  4005628d  /system/lib/libc.so (pthread_kill+52)
09-04 19:10:48.370 162-162/? I/DEBUG:     #01  beed5138  00000006  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed513c  00000000  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5140  74a2f24c  
09-04 19:10:48.370 162-162/? I/DEBUG:          beed5144  400564a1  /system/lib/libc.so (raise+14)
09-04 19:10:48.370 162-162/? I/DEBUG:     #02  beed5148  beed5154  [stack]
09-04 19:10:48.370 162-162/? I/DEBUG:          beed514c  400551d7  /system/lib/libc.so
09-04 19:10:48.390 162-162/? I/DEBUG: memory near r1:
.....
09-04 19:10:49.350 754-754/? W/View: requestLayout() improperly called by com.android.systemui.statusbar.phone.IconMerger{434090b0 V.E..... ........ 0,0-270,72 #7f0a0069 app:id/notificationIcons} during second layout pass: posting in next frame
09-04 19:10:49.600 658-694/? W/InputDispatcher: channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)' ~ Consumer closed input channel or an error occurred.  events=0x9
09-04 19:10:49.600 658-694/? E/InputDispatcher: channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)' ~ Channel is unrecoverably broken and will be disposed!
09-04 19:10:49.710 362-466/? I/logserver: Object Path:/data/system/dropbox/, mask=0x00000080
09-04 19:10:49.710 362-466/? I/logserver: event->len=48, name=data_app_native_crash@1504523449716.txt.gz
09-04 19:10:49.710 362-466/? I/logserver: process_one_event, can not find this event(data_app_native_crash@1504523449716.txt.gz)
09-04 19:10:49.710 362-466/? I/logserver: clean_cur_cache:962, system(rm -r /data/log/logcache/3577632/* > /dev/null 2>&1)
09-04 19:10:49.710 658-1213/? W/InputDispatcher: Attempted to unregister already unregistered input channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)'
09-04 19:10:49.720 1095-1095/? I/HwLauncher: DynamicIcon onWindowVisibilityChanged 4 - com.android.calendar
"
12793,feature request: could tfdbg dump out debug data by specified step range?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.4(BuildVersion: 16E195) 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.3.0-rc1-1702-g512d3d086', '1.3.0')
- **Python version**: Python 2.7.13 (default, Apr  4 2017, 08:47:57)
- **Bazel version (if compiling from source)**: 0.5.4-homebrew(Build time: Fri Aug 25 16:55:29 2017 (1503680129))
- **CUDA/cuDNN version**: null
- **GPU model and memory**: null
- **Exact command to reproduce**: sess = dumping_wrapper.DumpingDebugWrapperSession(sess, session_root=""<dump_path>"", log_usage=False)

### feature request description
I find that DumpingDebugWrapperSession in [dumping_wrapper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/dumping_wrapper.py#L31) dumps debug data for each step, which makes the training very slow with hundreds (thousands?) of small files being generated. In fact, sometimes it is not necessary for us to need the information of all steps to debug.
So, could tfdbg dump out debug data by specified step range? Thanks.
"
12792,Feature request: let transform_graph use summarize_graph inputs/outputs guess as default flags,"`tensorflow/tools/graph_transforms/summarize_graph` is very useful for listing names of input and output nodes, however it seems like you still have to set `--inputs` and `--outputs` with `tensorflow/tools/graph_transforms/transform_graph` explicitly. Would it be possible to have `transform_graph` assume the same nodes from `summarize_graph` by default?"
12790,"tensorflow is moving too fast, could you release it a long term support version?","Hi
    Tensorflow  is moving too fast to use my future project , could you release it a long term support ã€stable  and full version?
"
12789,Zero accuracy if shuffle is False in TF Keras ImageDataGenerator,"If I use the TF Keras reimplementation (tensorflow.contrib.keras) and set the ImageDataGenerator's shuffle param to False, I get zero accuracy every time. Also this:

I've just used for the first time the ModelCheckpoint from function to save the best model (best_model = True) and wanted to test its performance. When the model was saved it said that the ""val_acc"" was at 83.3% before saving. I loaded the model and used the evaluate_generator on validation_generator but the result for ""val_acc"" was 0.639. I got confused and used it again and got 0.654 and then 0.647, 0.744 and so on. Questions are:

1. Am I loading the model correctly, and if not what did I miss? Why is the result so much different?
2. Why are the results between different evaluate_generator executions different (no retraining is happening, just shear execution of predict_generator multiple times in a row)?
important part of the code (ResNet50 fine-tuning):

```
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
              metrics=['accuracy'])
checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', monitor = ""val_acc"", verbose=1, save_best_only=True)
# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
    rescale = 1./ 255,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale=1. / 255)
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size = (img_height, img_width),
    batch_size = batch_size)
validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size = (img_height, img_width),
    batch_size = batch_size)
# fine-tune the model
model.fit_generator(
    train_generator,
    steps_per_epoch = math.ceil(train_samples/batch_size),
    epochs=100,
    workers = 120,
    validation_data=validation_generator,
    validation_steps=math.ceil(val_samples/batch_size),
    callbacks=[checkpointer])
model.load_weights(filepath='/tmp/weights.hdf5')
model.predict_generator(validation_generator, steps = math.ceil(val_samples/batch_size) )
>> 0.62
model.predict_generator(validation_generator, steps = math.ceil(val_samples/batch_size) )
>> 0.587
```
"
12788,"imagePickerController for iOS , not recognize ","Hello I have created pb and txt file for some specific images and want to test with same image. taking that image  from Gallery in iPhone Device and trying to recognise but its not working properly.

Can you please suggest.

How we can use tensor flow in Gallery Images. 
what size are required to use it."
12787,Sqeezenet pretrained model,"Where can I find the pretrained squeezenet model using CIFAR or image net data sets? (if possible on tensorflow)
I would like to use that to re train for some new classes for classification.
"
12784,Eager API symbols not exported in the CI libtensorflow.so,"The eager API symbols (i.e., `TFE_*`) do not seem to be exported in the `libtensorflow.so` built by the CI system. I tried the nightly builds and they're missing from them. I checked using `nm -D libtensorflow.so` as well as `readelf -Ws libtensorflow.so` and `objdump -TC libtensorflow.so`. I'm not sure where to look to fix this though as the Mac nightly build seems fine. @asimshankar @alextp "
12783,Behavior of SVM in tensorflow.contrib.learn.python.learn.estimators inconsistent between dense and sparse inputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Arch Linux
- **TensorFlow installed from (source or binary)**:
Binary (community arch linux repo)
- **TensorFlow version (use command below)**:
1.3.0-1
- **Python version**: 
3.6.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I'm trying to use the SVM in tensorflow.contrib.learn with sparse input. The minimal test with real dense valued column works. However when I try to reproduce the exact same classification problem but wrapped in sparse feature columns with sparse_columns_with_integerized_features and weighted_sparse_columns, it fails. It might be a misuse of the sparse columns from me but I've spent a good amount of time reading the doc and the source code without understanding why.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# TEST WITH REAL VALUED COLUMNS THAT WORKS
def input_fn():
  return {
      'example_id': constant_op.constant(['1', '2', '3']),
      'feature1': constant_op.constant([[0.0], [1.0], [3.0]]),
      'feature2': constant_op.constant([[1.0], [-1.2], [1.0]]),
  }, constant_op.constant([[1], [0], [1]])

feature1 = feature_column.real_valued_column('feature1')
feature2 = feature_column.real_valued_column('feature2')
svm_classifier = svm.SVM(feature_columns=[feature1, feature2],
                         example_id_column='example_id',
                         l1_regularization=0.0,
                         l2_regularization=0.0)
svm_classifier.fit(input_fn=input_fn, steps=30)
metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)
loss = metrics['loss']  # 0 as expected
accuracy = metrics['accuracy']  # 1 as expected

# TEST WITH SAME VALUES BUT SPARSE COLUMNS, FAILS
def input_fn():
  return {
      'example_id': constant_op.constant(['1', '2', '3']),
      'feature1_id': tf.SparseTensor(
                      indices=tf.constant([[0,0],[1,0],[2,0]], dtype=tf.int64),
                      values=tf.constant([0, 0, 0], dtype=tf.int64),
                      dense_shape=[3,1],
                      ),
      'feature2_id': tf.SparseTensor(
                      indices=tf.constant([[0,0],[1,0],[2,0]], dtype=tf.int64),
                      values=tf.constant([0, 0, 0], dtype=tf.int64),
                      dense_shape=[3,1],
                      ),
      'feature1_w': tf.SparseTensor(
                      indices=[[0,0],[1,0],[2,0]],
                      values=[0.0, 1.0, 3.0],
                      dense_shape=[3,1]
                      ),
      'feature2_w': tf.SparseTensor(
                      indices=[[0,0],[1,0],[2,0]],
                      values=[1.0, -1.2, 1.0],
                      dense_shape=[3,1]
                      )
      }, constant_op.constant([[1], [0], [1]])

feature1_id = feature_column.sparse_column_with_integerized_feature('feature1_id',
                                                                    bucket_size=2,)
feature2_id = feature_column.sparse_column_with_integerized_feature('feature2_id',
                                                                    bucket_size=2,)
feature1_w = feature_column.weighted_sparse_column(feature1_id, 'feature1_w')
feature2_w = feature_column.weighted_sparse_column(feature2_id, 'feature2_w')

svm_classifier = svm.SVM(feature_columns=[feature1_w, feature2_w],
                         example_id_column='example_id',
                         l1_regularization=0.0,
                         l2_regularization=0.0)
svm_classifier.fit(input_fn=input_fn, steps=30)
metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)
loss = metrics['loss'] # should be 0 but is 0.36363636
accuracy = metrics['accuracy'] # should be 1 but is 0.666667

# TEST WITH SPARSE COLUMNS IN ONE FEATURE COLUMN, SAME RESULTS
def input_fn():
  return {
      'example_id': constant_op.constant(['1', '2', '3']),
      'feature1_id': tf.SparseTensor(
                      indices=tf.constant([[0,0],[0,1],[1,0],[1,1],[2,0],[2,1]], dtype=tf.int64),
                      values=tf.constant([0, 1, 0, 1, 0, 1], dtype=tf.int64),
                      dense_shape=[3,2],
                      ),
      'feature1_w': tf.SparseTensor(
                      indices=[[0,0],[0,1],[1,0],[1,1],[2,0],[2,1]],
                      values=[0.0, 1.0, 1.0, -1.2, 3.0, 1.0],
                      dense_shape=[3,2],
                      )
      }, constant_op.constant([[1], [0], [1]])

feature1_id = feature_column.sparse_column_with_integerized_feature('feature1_id',
                                                                    bucket_size=2,)
feature1_w = feature_column.weighted_sparse_column(feature1_id, 'feature1_w')

svm_classifier = svm.SVM(feature_columns=[feature1_w],
                         example_id_column='example_id',
                         l1_regularization=0.0,
                         l2_regularization=0.0)
svm_classifier.fit(input_fn=input_fn, steps=30)
metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)
loss = metrics['loss']
accuracy = metrics['accuracy']

```"
12781,Compile from source on Skylake-X (Intel i9),"Hello,

Compiling tensorflow source on Skylake-X Intel i9 with ""--config=opt"" gives the following error in the snappy external module:

-----------------------

> ERROR: /home/armafire/.cache/bazel/_bazel_armafire/efbef35334c587b69e16a82829bb0e2d/external/snappy/BUILD:19:1: C++ compilation of rule '@snappy//:snappy' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
>   (cd /home/armafire/.cache/bazel/_bazel_armafire/efbef35334c587b69e16a82829bb0e2d/execroot/org_tensorflow && \
>   exec env - \
>     CUDA_TOOLKIT_PATH=/usr/local/cuda \
>     CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
>     GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/usr/bin/python \
>     PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
>     TF_CUDA_CLANG=0 \
>     TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
>     TF_CUDA_VERSION=8.0 \
>     TF_CUDNN_VERSION=6 \
>     TF_NEED_CUDA=1 \
>     TF_NEED_OPENCL=0 \
>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o' -fPIC -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-shift-negative-value -Wno-implicit-function-declaration -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/snappy/snappy.cc -o bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o)
> cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
> external/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':
> external/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
>      for (int i = 0; i < blocks_.size(); ++i) {
>                      ~~^~~~~~~~~~~~~~~~
> In file included from external/snappy/snappy-internal.h:34:0,
>                  from external/snappy/snappy.cc:30:
> external/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':
> external/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'
> external/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'
> external/snappy/snappy.cc:1460:78:   required from here
> external/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
>      if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {
>                       ~~~~~~~~~~~~^~~~~~~~~~~~~
> external/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'
>  #define PREDICT_TRUE(x) x
>                          ^
> /tmp/ccxBWytY.s: Assembler messages:
> **_/tmp/ccxBWytY.s:389: Error: no such instruction: `kmovq %rdx,%k3'
> /tmp/ccxBWytY.s:391: Error: no such instruction: `kshiftrq $32,%k3,%k2'
> /tmp/ccxBWytY.s:394: Error: no such instruction: `kmovq %k2,%rdx'
> /tmp/ccxBWytY.s:600: Error: no such instruction: `kmovq %rsi,%k1'
> /tmp/ccxBWytY.s:602: Error: no such instruction: `kshiftrq $32,%k1,%k0'
> /tmp/ccxBWytY.s:605: Error: no such instruction: `kmovq %k0,%rsi'_**
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 34.595s, Critical Path: 14.66s
> FAILED: Build did NOT complete successfully

-----------------------

The command I use is:

bazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures -j 64

If I change to --config=mkl, then it compiles fine. Therefore, it seems that the problem is ""--march=native"" (generated by ""--config=opt""), which forces the snappy module to generate AVX512, however, the generated assembly is not correct for Skylake-X Intel i9 (which supports AVX512). I tried ""--march=skylake-avx512"" and all other AVX512 options like ""-mavx512f"" and similar, and all result in the same error.

My goal is to compile tensorflow with Eigen AVX512. Any ideas how this can be done?"
12779,C++ tensorflow interface is taking too long to terminate and return the calculated graph output.,"Hi All,

I am loading a trained graph (from python) in c++ as explained in the below links,

1. https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183

2. https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f

I am using this tf feature in larger c++ application, the working is as follows, main function creates a process that will load the graph, calculate and return the output.  This process will be called every  ~0.1 seconds.  The interface works fine, however it takes rather long (~1 second) to terminate and return the value to main.  I have also timed duration from loading the graph until return, within the tf api, it takes ~0.015 seconds. 

I am unable to figure out the reason for this delay, I am not sure if it is due to c++ createprocess or tf sesion handling, can someone help me to address this issue. 

I have also posted the same question in  stackoverflow, which got down voted a lot, as I am unable to solve this problem I am posting it here too, sorry if it is not the correct platform.

I am using python 3.5 (Anaconda 4.2.0) on a Windows 7 system. 
tensorflow CPU version 1.2.0
bazel 0.5.3
VS 2015
 
### Describe the problem
C++ tensorflow interface is taking too long to terminate/return the calcualted graph output. 

### Source code / logs
https://github.com/snageshrao/tfCPPExample"
12776,nest.flatten() does not work with list,"
### System information
== cat /etc/issue ===============================================
Linux mews3153 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux mews3153 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/lib/libipp-intel:/opt/boost/lib/:/usr/lib64/:/mobileye/shared/boost-python2.7/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
/tmp/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem
The function *tensorflow.contrib.data.python.util.nest.flatten* fails to flatten a list. Given a flat list, it returns a nested list. This causes a problem in dataset_ops.py, when the datasource is created by zipping several datasources and then attempting to create an iterator.

### Source code / logs
#### Example 1:
from tensorflow.contrib.data.python.util import nest
nest.flatten([1,2,3,4])
> [[1, 2, 3, 4]]

nest.flatten((1,2,3,4)) # works with a tuple
> [1, 2, 3, 4]


#### Example 2:
import tensorflow as tf

dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))
dataset2 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4]))
dataset3 = tf.contrib.data.Dataset.zip([dataset1, dataset2])

iterator = dataset3.make_initializable_iterator()

sess = tf.InteractiveSession()
sess.run(iterator.initializer)
next1, next2 = iterator.get_next()

> running the above code:
Traceback (most recent call last):
  File ""/tmp/tds.py"", line 7, in <module>
    iterator = dataset3.make_initializable_iterator()
  File ""/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 464, in make_initializable_iterator
    return Iterator.from_dataset(self, shared_name)
  File ""/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 97, in from_dataset
    output_types=nest.flatten(dataset.output_types),
  File ""/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 1199, in output_types
    ds.output_types for ds in nest.flatten(self._datasets)
AttributeError: 'list' object has no attribute 'output_types'
"
12773,Halted-partial dataset download for speech_commands example renders example ~broken,"If a user attempts to run `tensorflow/examples/speech_commands/train.py` for the first time against a new `data_dir` location and then halts the execution of Python before the download can finish, subsequent attempts to run the script using the same data_dir specification (or lack of specification) will result in Python exiting in error with the gzip libraries complaining about incorrect CRC value.

This is because `AudioProcessor.maybe_download_and_extract_dataset(...)` in `tensorflow/examples/speech_commands/input_data.py` only downloads if the file doesn't already exist (sensibly) and assumes that `tarfile.open(...).extractall(...)` will complete error-free (less sensibly) - which it won't on the partially downloaded gzipped-tar asset. 

This probably merits a documentation inclusion as opposed to a code re-write since it's example code, but that decision is above my pay-grade.
"
12772,Using Sparse tensors to apply gradients in BackPropogation. ,"Hi All,

I am trying to use sparse tensors while applying gradients and i see below error.
 **Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(""Adam_1/update_conv1_1/weights/sub_2:0"", shape=(), dtype=float32)'**

I am assuming this to be a bug as the dtype int64 must have been hardcoded for sparse tensors. 
Is there any workaround for this issue? 

Here is what I am trying to do:
#################### this function multiplies gradients with prune weights so that gradient updation happens on pruned gradients.
```
def apply_prune_on_grads(grads_and_vars, sess, dict_n):
     print(""im inside pply_prune_on_grads"")
     i=0
     for k, v in dict_n.items():
         count=0
         for grad, var in grads_and_vars:
             if var.name == k:
                 op = (var.name).split(""/"")
                 if op[1] != 'biases:0':
                     v = tf.cast(tf.constant(v), tf.float32)
                     mask_tensor = tf.multiply(v,grad)
                     idx = tf.where(tf.not_equal(mask_tensor, 0))
                     sparse = tf.IndexedSlices(idx,tf.gather_nd(mask_tensor, idx))
                     grads_and_vars[count] = (sparse, var)
             count = count+1
         i=i+1
     return grads_and_vars

def prune_weights(weights_prune, sess,threshold = 0.01):
         print(""im inside prune weights"")
         sparse_weights = {}
         for v in weights_prune:
             value = sess.run(v) 
             under_threshold = abs(value) < threshold
             value[under_threshold] = 0
             sess.run(v.assign(value))
             sparse_weights[v.name] = np.logical_not(under_threshold)
         return sparse_weights
```

############################### The order in which i am calling the above functions in sess.run
```
 sparse_weights = prune_weights(variables_to_restore, sess)
 grads_and_vars = train_op.compute_gradients(loss, )
 grads_and_vars =  apply_prune_on_grads(grads_and_vars,sess,  sparse_weights)
 train_step = train_op.apply_gradients(grads_and_vars) 

```



"
12770,GPU support on Mac OS X? compile by oneself?,"I'm çº ç»“ to select ubuntu or Hackintosh as my PC.

Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.

Could anyone tell the reason?

Could I compile tensorflow with gpu[cuda] support by myself? Why or Why not?


"
12767,"tf_library syntax error: '+' operator applied to incompatible types (select of string, list)","Forgive me if I'm missing something obvious here (new to Bazel) but the tf_library Bazel rule seems to assume that `tfcompile_flags` is a string, while similar rules allow lists (cc_binary's copts for example). This is a gotcha that probably could be fixed easily.

# Example
```
tf_library(
  name = ""graph"",
  cpp_class = ""Graph"",
  graph = ""graph.pb"",
  config = ""graph.config.pb"",
  tfcompile_flags = [""--target_cpu='core-avx2'"", ""--xla_enable_fast_math=false""]
)
```
will crash upon build with `'+' operator applied to incompatible types (select of string, list)`

The solution is to manually concatenate options in the BUILD file: 
```
tf_library(
  name = ""graph"",
  cpp_class = ""Graph"",
  graph = ""graph.pb"",
  config = ""graph.config.pb"",
  tfcompile_flags = ""--target_cpu='core-avx2' --xla_enable_fast_math=false""
)
```"
12765,Three questions about ChiefSessionCreator?,"* There are two parameters of [ChiefSessionCreator][1]: `checkpoint_dir` and `checkpoint_filename_with_path`,
what difference between them?

* And ChiefSessionCreator has a parameter [scaffold][2] which has a parameter `saver`, does it mean ChiefSessionCreator uses scaffold.saver to save and restore variables?

* And scaffold also has a parameter `init_fn`, and it can be used like 
init_fn=[assign_from_checkpoint_fn][3], assign_from_checkpoint_fn will new a saver with the parameter `var_list` of assign_from_checkpoint_fn. So here comes another question, ChiefSessionCreator will use scaffold.saver to restore variables or init_fn?

To answer the three questions above, you may need read the source code, but I am not capable to do it, so I need your help. Thanks.

  [1]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/ChiefSessionCreator
  [2]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/Scaffold
  [3]: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/assign_from_checkpoint_fn"
12764,the side &deep model   is not good compare with deep model and wide model ,"these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:

XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep
Training data is downloaded to /tmp/tmpFB4dsd
Test data is downloaded to /tmp/tmpomj5Pi
2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpWei9wK
accuracy: 0.850071
accuracy_baseline: 0.763774
auc: 0.894038
auc_precision_recall: 0.743199
average_loss: 0.393638
global_step: 2000
label/mean: 0.236226
loss: 39.3179
prediction/mean: 0.242167


XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide
Training data is downloaded to /tmp/tmpFJdWft
Test data is downloaded to /tmp/tmpjB5nm7
2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpuPHsDx
accuracy: 0.835391
accuracy_baseline: 0.763774
auc: 0.882763
auc_precision_recall: 0.694257
average_loss: 0.352975
global_step: 2000
label/mean: 0.236226
loss: 35.2563
prediction/mean: 0.240918


XXT@apptruexxnet:~$ python wide_n_deep_tutorial.py
Training data is downloaded to /tmp/tmpDdWc_T
Test data is downloaded to /tmp/tmpFF0PZJ
2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on
 your machine and could speed up CPU computations.
2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo
ur machine and could speed up CPU computations.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.
model directory = /tmp/tmpq2M7SE
accuracy: 0.820834
accuracy_baseline: 0.763774
auc: 0.850518
auc_precision_recall: 0.676198
average_loss: 0.424271
global_step: 2000
label/mean: 0.236226
loss: 42.3776
prediction/mean: 0.256489

I don't know why looks likes this, so someone can help me? thank you."
12761,Allow to build tensorflow as a bazel external dependency.,"It would be nice to be able to just add a @org_tensorflow in bazel and be able to build it as a dependency.  

At the best of my knowledge the syntaxnet dockerfile is the best example on having tensorflow as a submodule.
https://github.com/tensorflow/models/blob/c259259299db3b486ccdfd6cdec44b884623053a/syntaxnet/Dockerfile#L63

But still building it is not straightforward and probably does not work if you activate CUDA/XLA/MLK because those depends on other bazel subprojects.

Are there any plans to support this use case or I am missing something in the best practices to build tensorflow as a subpackage ? 
"
12760,typo in config.py (line 688),"Currently is:

```
cudnn_path_from_ldconfig = re.search('.*libcudnn.so .* => (.*)',
```

should be:

```
cudnn_path_from_ldconfig = re.search('.*libcudnn.so.* => (.*)',
```

(note the extra space in the current version)
"
12758,[Feature request]: Hyper Parameters optimizer,"Hi,

Do you plan to add a code in order to optimize automatically the hyper-parameters of deep learning algorithm ? (number of hidden layers, value of the learning rate, type of down-sampling etc).

Best,
"
12757,[Feature Request]: Resize netwoks on the fly,"Hi,

Do you plan to change the present paradigm about tensorflow neural network that make the networks size to be static after using run ?

Is it plan to be able to resize on the fly the network shape (number of kernels, type of activation function, type of down sampling) during the training for instance.

Thanks.
"
12755,build_default_serving_input_fn() uses a name that is not a valid variable scope. ,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 LTS 
- **TensorFlow installed from (source or binary)**: SOURCE (pip install tensorflow) 
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: N/A 
- **CUDA/cuDNN version**: N/A 
- **GPU model and memory**: N/A 
- **Exact command to reproduce**:


### Describe the problem

See this short example:

$ python -c ""import tensorflow as tf 
f = {'feature': tf.placeholder(name='feature', shape=[32], dtype=tf.float32) } 
serving_input = tf.contrib.learn.utils.input_fn_utils.build_default_serving_input_fn(f)
serving_input()""

Traceback (most recent call last):
  File ""<string>"", line 4, in <module>
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/input_fn_utils.py"", line 112, in input_fn
    name=t.name)
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1548, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2094, in _placeholder
    name=name)
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 374, in apply_op
    with g.as_default(), ops.name_scope(name) as scope:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 4522, in name_scope
    with g.as_default(), g.name_scope(n) as scope:
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3172, in name_scope
    raise ValueError(""'%s' is not a valid scope name"" % name)
ValueError: 'feature:0' is not a valid scope name

The issue is that in input_fn_utils.py here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/utils/input_fn_utils.py#L112

The attribute ""t.name"" is passed to array_ops.placeholder(), and t.name will always have a "":N"" suffix, and "":"" is not a valid variable scope name, so this function fals. 

### Source code / logs

See above. "
12754,"Issue with tf.py_func, tf.Tensor.set_shape and tf.Queues","This works fine:
```
import tensorflow as tf
x = tf.py_func(lambda: 42, [], tf.int64)
x = reshape(x, [1])
sess = tf.Session()
y = tf.train.shuffle_batch([x], 32, 64, 32)
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)
sess.run(y)
```

But simply setting the shape with `tf.Tensor.set_shape` instead of reshaping does not work:

```
import tensorflow as tf
x = tf.py_func(lambda: 42, [], tf.int64)
x.set_shape([1])
sess = tf.Session()
y = tf.train.shuffle_batch([x], 32, 64, 32)
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)  # Fails here.
sess.run(y)

2017-09-01 20:46:12.585027: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Shape mismatch in tuple component 0. Expected [1], got []
2017-09-01 20:46:12.585264: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Shape mismatch in tuple component 0. Expected [1], got []
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Shape mismatch in tuple component 0. Expected [1], got []
	 [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](shuffle_batch/random_shuffle_queue, PyFunc)]]
```

Even though this works fine:

```
import tensorflow as tf
x = tf.py_func(lambda: 42, [], tf.int64)
x.set_shape([1])
sess.run(x)
```"
12753,tf.image.resize_bilinear has nearest neighbor gradients when downscaling,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
v1.2.0-5-g435cdfc 1.2.1

- **Python version**: 
3.5.2

### Describe the problem
When using `tf.image.resize_bilinear` to downscale, the gradients of the output w.r.t. the input is the same as what we get from nearest neighbors. For instance, using the code below you can confirm that the gradients are being sparsely backpropagated to specific pixels of the input image.

### Source code / logs
```
import numpy as np
import tensorflow as tf


if __name__ == '__main__':
    session = tf.Session()

    x = tf.placeholder(tf.float32, shape=[None, 16, 16, 3])
    x_low = tf.image.resize_bilinear(x, (4, 4))
    loss = tf.reduce_sum(x_low)
    loss_grad = tf.gradients(loss, x)

    grad = session.run(loss_grad, {x: np.ones([1, 16, 16, 3], dtype=np.float32)})[0]
    grad = grad[0, ...].transpose(2, 0, 1)
    print(grad)  # should not be sparse, but it is

    session.close()
```

The output is:
```
[[[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]

 [[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]

 [[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]
```"
12751, bazel build tensorflow/python/tools:optimize_for_inference failed,"$ bazel build tensorflow/python/tools:optimize_for_inference
ERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.
INFO: Elapsed time: 0.875s"
12750,Android-Tensorflow model loading issue with SavedModelBundle.load(),"Loading the model in Android give below error:

FATAL EXCEPTION: main
                  Process: tensorflow.lgsi.com.posapplication, PID: 516
                  java.lang.RuntimeException: Unable to start activity ComponentInfo{tensorflow.lgsi.com.posapplication/tensorflow.lgsi.com.posapplication.MainActivity}: java.lang.U**nsupportedOperationException**: **Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you**
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2727)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2788)
                      at android.app.ActivityThread.-wrap12(ActivityThread.java)
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1504)
                      at android.os.Handler.dispatchMessage(Handler.java:102)
                      at android.os.Looper.loop(Looper.java:154)
                      at android.app.ActivityThread.main(ActivityThread.java:6248)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:872)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:762)
                   Caused by: java.lang.UnsupportedOperationException: Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you
                      at org.tensorflow.SavedModelBundle.load(Native Method)
                      at org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:38)
                      at tensorflow.com.posapplication.tagger.PosTagger.<init>(PosTagger.java:23)
                      at tensorflow.lgsi.com.posapplication.tagger.PosTagger.getInsPosTagger(PosTagger.java:30)
                      at tensorflow.com.posapplication.MainActivity.onCreate(MainActivity.java:56)
                      at android.app.Activity.performCreate(Activity.java:6757)
                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1119)
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2680)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2788)Â 
                      at android.app.ActivityThread.-wrap12(ActivityThread.java)Â 
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1504)Â 
                      at android.os.Handler.dispatchMessage(Handler.java:102)Â 
                      at android.os.Looper.loop(Looper.java:154)Â 
                      at android.app.ActivityThread.main(ActivityThread.java:6248)Â 
                      at java.lang.reflect.Method.invoke(Native Method)Â 
                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:872)Â 
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:762)Â 
I/art: Starting

**Model is saved in Python by using below API:**
builder = tf.saved_model.builder.SavedModelBuilder(r'./tmp/model')
builder.add_meta_graph_and_variables(session, [tf.saved_model.tag_constants.SERVING])
builder.save(True) 

**Model is loading in Andoid using below api:**
inferenceInterface = new TensorFlowInferenceInterface(context.getAssets(), MODEL_FILE);"
12749,Android- Tensorflow model loading issue,"I have saved model using tf.train.Saver.save() and then freeze the graph bu using below method:
def freezeModel(modelName):
    input_graph_path = modelName + '.pb'
    checkpoint_path = './' + modelName + '.ckpt'
    input_saver_def_path = """"
    input_binary = True
    output_node_names = ""x_indices,x_values,x_dense_shape,unary_scores,transition_params,train_op""
    restore_op_name = ""save/restore_all""
    filename_tensor_name = ""save/Const:0""
    output_frozen_graph_name = 'frozen_' + modelName + '.pb'
   
    clear_devices = True

    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,input_binary, checkpoint_path, output_node_names,restore_op_name, filename_tensor_name,output_frozen_graph_name, clear_devices, """")
    #freeze_graph.freeze_graph(input_saved_model_dir='Model/', saved_model_tags='serve')
    return


Then loading the modelName.pb file in Android using inference api as below:
TensorFlowInferenceInterface inferenceInterface = new **TensorFlowInferenceInterface**(context.getAssets(), MODEL_FILE);

I am getting **below error**:
Caused by: **java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32**
; NodeDef: Less_1 = Less[T=DT_INT64](ToInt64_2, ToInt64_3); Op<name=Less; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_INT32]>
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:439)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:98)


 **Here is the code using to create the graph:**
x_indices_t= tf.placeholder(tf.int64, shape=(None, 3), name=""x_indices"")
x_values_t= tf.placeholder(tf.float32, shape=(None), name=""x_values"")
x_dense_shape_t= tf.placeholder(tf.int64, shape=(3), name=""x_dense_shape"")#[len(X_train), max_word_count, feature_count]
shape_unary_score_t = tf.placeholder(dtype=tf.int64, shape=(None, None, None), name=""shape_unary_score"")#[num_examples_t, max_word_count, num_tags]
shape_t = tf.shape(shape_unary_score_t)

x_t = tf.SparseTensor(indices=x_indices, values=x_values, dense_shape=x_dense_shape)


To resolve above error i changed the dtype of indices and shapes to int32 then tf.SparseTensor() is givving below error:
File ""CRF_POS_Trainer_Tensor - Training.py"", line 323, in <module>
    x_t = tf.SparseTensor(indices=x_indices_t, values=x_values_t, dense_shape=x_dense_shape_t)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py"", line 119, in _init_
    indices, name=""indices"", dtype=dtypes.int64)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 676, in convert_to_tensor
    as_ref=False)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 741, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 614, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""x_indices:0"", shape=(?, 3), dtype=int32)'
        

So how to i resolve issue to create SparseTensor with int32 data.

**Tensorflow Version: 1.3.0**"
12748,ImportError: DLL load failed @ VS2012 / PTVS 2.1.1 / Windows 7,"## System information
-OS Platform: Windows 7
-TensorFlow version: tensorflow-gpu==1.3.0
-CUDA/cuDNN version: 8.0/6.0
-GPU model and memory: GeForce GTX 750
-IDE: Visual Studio 2012/PTVS 2.1.1/virtualenv==15.1.0

## Problem
I installed tensorflow-gpu in ""virtualenv"".
Under the following environment, it works well.
    @Command prompt: activate ""virtualenv"" > start python > import tensorflow : 
However. it does not work well under the following environment.
    @VS2012: Python Environments=""virtualenv"" > I try to run ""main.py"" which has only 1 line; ""import tensorflow"": Then, the error mentioned below occurs!
**How can I use ""tensorflow-gpu"" @VS2012 ?**

## Log
Traceback (most recent call last):
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: ì§€ì •ëœ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<debug input>"", line 1, in <module>
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: ì§€ì •ëœ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\WORK_online3\pyToolbox_Packages\virtualenv\venv35-64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



"
12747,How to build a full C++ API libtensorflow_cc.so for Android?,"### Describe the problem
When I use 
```
bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures
```
to compile the full C++ API libtensorflow_cc.so for Android, but It can not work.


### Source code / logs
```
C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command 
  (exec env - \
    PATH=/home/mlin2/.cargo/bin:/home/mlin2/bin:/home/mlin2/anaconda2/bin:/usr/lib64/qt-3.3/bin:/home/mlin2/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/go/bin:/opt/gopath/bin:/home/mlin2/software/android-ndk-r12b/:/home/mlin2/.fzf/bin:/home/mlin2/.local/bin:/home/mlin2/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/mlin2/anaconda2/bin/python \
    PYTHON_LIB_PATH=/home/mlin2/anaconda2/lib/python2.7/site-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d '-frandom-seed=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o' -iquote external/highwayhash -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o).
In file included from external/highwayhash/highwayhash/arch_specific.h:23:0,
                 from external/highwayhash/highwayhash/sip_hash.h:23,
                 from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, State*)':
external/highwayhash/highwayhash/compiler_specific.h:50:30: error: expected initializer before 'alignas'
 #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11
                              ^
external/highwayhash/highwayhash/state_helpers.h:49:41: note: in expansion of macro 'HH_ALIGNAS'
   char final_packet[State::kPacketSize] HH_ALIGNAS(32) = {0};
                                         ^
In file included from external/highwayhash/highwayhash/sip_hash.h:25:0,
                 from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/state_helpers.h:64:10: error: 'final_packet' was not declared in this scope
   memcpy(final_packet, remaining_bytes, remaining_size - remainder_mod4);
          ^
external/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::UpdateState(const char*, highwayhash::HH_U64, State*)':
external/highwayhash/highwayhash/state_helpers.h:76:76: error: there are no arguments to 'static_assert' that depend on a template parameter, so a declaration of 'static_assert' must be available [-fpermissive]
   static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
                                                                            ^
external/highwayhash/highwayhash/state_helpers.h:76:76: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:0:
external/highwayhash/highwayhash/sip_hash.h: At global scope:
external/highwayhash/highwayhash/sip_hash.h:33:9: error: expected nested-name-specifier before 'Key'
   using Key = HH_U64[2];
         ^
external/highwayhash/highwayhash/sip_hash.h:36:42: error: 'Key' does not name a type
   explicit HH_INLINE SipHashStateT(const Key& key) {
                                          ^
external/highwayhash/highwayhash/sip_hash.h: In constructor 'highwayhash::SipHashStateT<kUpdateIters, kFinalizeIters>::SipHashStateT(const int&)':
external/highwayhash/highwayhash/sip_hash.h:37:39: error: invalid types 'const int[int]' for array subscript
     v0 = 0x736f6d6570736575ull ^ key[0];
                                       ^
external/highwayhash/highwayhash/sip_hash.h:38:39: error: invalid types 'const int[int]' for array subscript
     v1 = 0x646f72616e646f6dull ^ key[1];
                                       ^
external/highwayhash/highwayhash/sip_hash.h:39:39: error: invalid types 'const int[int]' for array subscript
     v2 = 0x6c7967656e657261ull ^ key[0];
                                       ^
external/highwayhash/highwayhash/sip_hash.h:40:39: error: invalid types 'const int[int]' for array subscript
     v3 = 0x7465646279746573ull ^ key[1];
                                       ^
external/highwayhash/highwayhash/sip_hash.h: At global scope:
external/highwayhash/highwayhash/sip_hash.h:104:7: error: expected nested-name-specifier before 'SipHashState'
 using SipHashState = SipHashStateT<2, 4>;
       ^
external/highwayhash/highwayhash/sip_hash.h:105:7: error: expected nested-name-specifier before 'SipHash13State'
 using SipHash13State = SipHashStateT<1, 3>;
       ^
external/highwayhash/highwayhash/sip_hash.h:110:29: error: 'SipHashState' was not declared in this scope
 HH_INLINE void PaddedUpdate<SipHashState>(const HH_U64 size,
                             ^
external/highwayhash/highwayhash/sip_hash.h:113:43: error: 'SipHashState' has not been declared
                                           SipHashState* state) {
                                           ^
external/highwayhash/highwayhash/sip_hash.h:110:16: error: template-id 'PaddedUpdate<<expression error> >' for 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, int*)' does not match any template declaration
 HH_INLINE void PaddedUpdate<SipHashState>(const HH_U64 size,
                ^
external/highwayhash/highwayhash/sip_hash.h:122:29: error: 'SipHash13State' was not declared in this scope
 HH_INLINE void PaddedUpdate<SipHash13State>(const HH_U64 size,
                             ^
external/highwayhash/highwayhash/sip_hash.h:125:45: error: 'SipHash13State' has not been declared
                                             SipHash13State* state) {
                                             ^
external/highwayhash/highwayhash/sip_hash.h:122:16: error: template-id 'PaddedUpdate<<expression error> >' for 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, int*)' does not match any template declaration
 HH_INLINE void PaddedUpdate<SipHash13State>(const HH_U64 size,
                ^
external/highwayhash/highwayhash/sip_hash.h:146:39: error: 'SipHashState' does not name a type
 static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,
                                       ^
external/highwayhash/highwayhash/sip_hash.h:146:56: error: expected unqualified-id before '&' token
 static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,
                                                        ^
external/highwayhash/highwayhash/sip_hash.h:146:56: error: expected ')' before '&' token
external/highwayhash/highwayhash/sip_hash.h:146:58: error: expected initializer before 'key'
 static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,
                                                          ^
external/highwayhash/highwayhash/sip_hash.h:152:41: error: 'SipHash13State' does not name a type
 static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,
                                         ^
external/highwayhash/highwayhash/sip_hash.h:152:60: error: expected unqualified-id before '&' token
 static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,
                                                            ^
external/highwayhash/highwayhash/sip_hash.h:152:60: error: expected ')' before '&' token
external/highwayhash/highwayhash/sip_hash.h:152:62: error: expected initializer before 'key'
 static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,
                                                              ^
external/highwayhash/highwayhash/sip_hash.cc:18:20: error: 'highwayhash::SipHash' has not been declared
 using highwayhash::SipHash;
                    ^
external/highwayhash/highwayhash/sip_hash.cc:19:20: error: 'highwayhash::SipHash13' has not been declared
 using highwayhash::SipHash13;
                    ^
external/highwayhash/highwayhash/sip_hash.cc:20:7: error: expected nested-name-specifier before 'Key'
 using Key = highwayhash::SipHashState::Key;
       ^
external/highwayhash/highwayhash/sip_hash.cc:21:7: error: expected nested-name-specifier before 'Key13'
 using Key13 = highwayhash::SipHash13State::Key;
       ^
external/highwayhash/highwayhash/sip_hash.cc: In function 'highwayhash::HH_U64 SipHashC(const HH_U64*, const char*, highwayhash::HH_U64)':
external/highwayhash/highwayhash/sip_hash.cc:26:42: error: ISO C++ forbids declaration of 'type name' with no type [-fpermissive]
   return SipHash(*reinterpret_cast<const Key*>(key), bytes, size);
                                          ^
external/highwayhash/highwayhash/sip_hash.cc:26:42: error: expected '>' before 'Key'
external/highwayhash/highwayhash/sip_hash.cc:26:42: error: expected '(' before 'Key'
external/highwayhash/highwayhash/sip_hash.cc:26:42: error: 'Key' was not declared in this scope
external/highwayhash/highwayhash/sip_hash.cc:26:46: error: expected primary-expression before '>' token
   return SipHash(*reinterpret_cast<const Key*>(key), bytes, size);
                                              ^
external/highwayhash/highwayhash/sip_hash.cc: In function 'highwayhash::HH_U64 SipHash13C(const HH_U64*, const char*, highwayhash::HH_U64)':
external/highwayhash/highwayhash/sip_hash.cc:30:44: error: ISO C++ forbids declaration of 'type name' with no type [-fpermissive]
   return SipHash13(*reinterpret_cast<const Key13*>(key), bytes, size);
                                            ^
external/highwayhash/highwayhash/sip_hash.cc:30:44: error: expected '>' before 'Key13'
external/highwayhash/highwayhash/sip_hash.cc:30:44: error: expected '(' before 'Key13'
external/highwayhash/highwayhash/sip_hash.cc:30:44: error: 'Key13' was not declared in this scope
external/highwayhash/highwayhash/sip_hash.cc:30:50: error: expected primary-expression before '>' token
   return SipHash13(*reinterpret_cast<const Key13*>(key), bytes, size);
                                                  ^
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 1.812s, Critical Path: 0.14s
```"
12746,Feature Request: Loading weights for layers defined in tf.layers api,"Let's say I define a layer using the `tf.layers` API as shown below:

```
conv1 = tf.layers.conv2d(input_img, filters=32, 
                                     kernel_size=(3,3), 
                                     padding='same', 
                                     name='Conv1')
```

Now I can build a whole network defining such layers. Can you please introduce another functionality for the `tf.layers` api so that for each layer we can set the weights in a single line like this:

`conv1.set_weights(weights)` or something like this `conv1.set_params(param_values)`

This would be very very useful."
12745,"Distributed tensorflow - Stuck at ""CreateSession still waiting for response from worker""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.0 && 1.3
- **Python version**: 
2.7
- **CUDA/cuDNN version**:
no

### Describe the problem
Run the code on k8s with the spec `1 ps + 20 workers`. there are some workers print the log all the time.
```
I tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12
I tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17
```
I met two situations:
1. worker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.
2. all of other works wait for the worker 12 and 17.

I could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug? 
Some have also encountered this problem. see [Stackoverflow](https://stackoverflow.com/search?q=CreateSession+still+waiting+for+response+from+worker).
### Source code / logs
```
import datetime
import json
import numpy as np
import os
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_integer(""max_epochs"", 10000000, ""Number of steps to run trainer."")
flags.DEFINE_string(""checkpoint_path"", ""./checkpoint/"",
                    ""The checkpoint directory"")
flags.DEFINE_string(""output_path"", ""./tensorboard/"",
                    ""indicates training output"")
flags.DEFINE_integer(""checkpoint_period"", 1,
                     ""Number of epochs to save checkpoint."")
flags.DEFINE_float(""learning_rate"", 0.01, ""Initial learning rate."")
FLAGS = flags.FLAGS


def main():
    # Create train data
    train_X = np.linspace(-1, 1, 100)
    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10
    learning_rate = FLAGS.learning_rate
    start_training_time = datetime.datetime.now()

    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    # Exampmle: {""cluster"": {""ps"": [""127.0.0.1:3001""], ""worker"": [""127.0.0.1:3002"", ""127.0.0.1:3003""]}, ""task"": {""index"": 0, ""type"": ""worker""}}
    env = json.loads(os.environ.get(""TF_CONFIG"", ""{}""))
    task_data = env.get(""task"", None)
    cluster_spec = env[""cluster""]
    task_type = task_data[""type""]
    task_index = task_data[""index""]

    cluster = tf.train.ClusterSpec(cluster_spec)
    server = tf.train.Server(cluster,
                             job_name=task_type,
                             task_index=task_index)

    if task_type == ""ps"":
        server.join()
    elif task_type == ""worker"":
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:{}/task:{}"".format(task_type, task_index),
                cluster=cluster)):

            # Define the model
            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])
            keys = tf.identity(keys_placeholder)
            X = tf.placeholder(""float"", shape=[None, 1])
            Y = tf.placeholder(""float"", shape=[None, 1])
            w = tf.Variable(0.0, name=""weight"")
            b = tf.Variable(0.0, name=""bias"")
            global_step = tf.Variable(0, name=""global_step"", trainable=False)
            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))
            train_op = optimizer.minimize(loss, global_step=global_step)
            predict_op = tf.multiply(X, w) + b
            tf.summary.scalar(""loss"", loss)
            summary_op = tf.summary.merge_all()
            init_op = tf.global_variables_initializer()
            saver = tf.train.Saver()
            #saver = tf.train.Saver(sharded=True)


            sv = tf.train.Supervisor(is_chief=(task_index == 0),
                                     logdir=FLAGS.checkpoint_path,
                                     init_op=init_op,
                                     #summary_op=summary_op,
                                     summary_op=None,
                                     saver=saver,
                                     global_step=global_step,
                                     save_model_secs=60)

            try:
                with sv.managed_session(server.target) as sess:
                    print(""Save tensorboard files into: {}"".format(FLAGS.output_path))
                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)

                    print(""Run training with epoch number: {}"".format(
                        FLAGS.max_epochs))
                    for i in range(FLAGS.max_epochs):
                        for (x, y) in zip(train_X, train_Y):
                            x = np.array([[x]])
                            y = np.array([[y]])
                            sess.run(train_op, feed_dict={X: x, Y: y})

                        if i % FLAGS.checkpoint_period == 0:
                            x = np.array([[train_X[0]]])
                            y = np.array([[train_Y[0]]])
                            summary_value, loss_value, step = sess.run(
                                [summary_op, loss, global_step],
                                feed_dict={X: x,
                                           Y: y})
                            print(""Epoch: {}, loss: {}"".format(i, loss_value))
                            if task_index == 0:
                                writer.add_summary(summary_value, step)

                    writer.close()

                    end_training_time = datetime.datetime.now()
                    print(""[{}] End of distributed training."".format(
                        end_training_time - start_training_time))


            except Exception as e:
                print(e)


if __name__ == ""__main__"":
    main()
```
"
12744,a,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12743,Gradients with TensorArray fail after scatter of 0 size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see below

### Describe the problem
The following fails:
```
import tensorflow as tf

x_orig = tf.zeros([3], tf.float32)

ta = tf.TensorArray(tf.float32, size=1, element_shape=[3])
ta = ta.write(0, x_orig)
ta = ta.scatter(tf.range(0, 0), ta.gather(tf.range(0, 0)))
x = ta.read(0)
g, = tf.gradients(tf.reduce_sum(x), [x_orig])

sess = tf.Session()
print sess.run(g)
```
with this error:
```
tensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
         [[Node: gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, range, gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]
```
The above code works if `ta = ta.scatter(tf.range(0, 0), ta.gather(tf.range(0, 0)))`, which gathers a 0x3 tensor and then scatters it into zero indices in `ta`, is removed.

This seems to happen because `_TensorArrayScatterGrad` does not (and cannot?) copy over the `element_size` from the Python `TensorArray` object that `.scatter` was called on.

I submitted #12742 as a fix, which works for the above snippet.."
12740,[Feature Request]: fold batch norm into convolution weights of graph transform tool ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: 8.0, 6.0
- **GPU model and memory**: GTX1080
- **Exact command to reproduce**:

bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  strip_unused_nodes(type=float, shape=""1,299,299,3"")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms'

After diving into the generated pb graph, I found that the current fold batch norm ops seems to only fold batch norm into two ops: mul and add, which is not exactly as the readme demonstrated: fold the batch norm into the weights of convolution. If this is the case, it would be better to further merge the mul and add into conv weights."
12739,TF 1.3 keras TimeDistributed wrapper issue - rnn() got an unexpected keyword argument 'input_length',"When I use TimeDistributed  wrapper from keras I'm getting unexpected keyword argument 'input_length'

# **System Info:**

**Windows 10**
**TF 1.3.0**
**Python 3.5**

**Code :**

```python
import json

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.contrib.keras import layers
from tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed
from tensorflow.python.estimator.inputs import numpy_io

MAX_NB_WORDS = 200000
MAX_SEQUENCE_LENGTH = 25
EMBEDDING_DIM = 300
VALIDATION_SPLIT = 0.1
TEST_SPLIT = 0.1
RNG_SEED = 13371447
NB_EPOCHS = 25
DROPOUT = 0.1
BATCH_SIZE = 32

tf.logging.set_verbosity(tf.logging.INFO)

Q1_TRAINING_DATA_FILE = 'gen/q1_train.npy'
Q2_TRAINING_DATA_FILE = 'gen/q2_train.npy'
LABEL_TRAINING_DATA_FILE = 'gen/label_train.npy'
NB_WORDS_DATA_FILE = 'gen/nb_words.json'
with open(NB_WORDS_DATA_FILE, 'r') as f:
    nb_words = json.load(f)['nb_words']


def model_fn(features, labels, mode, params):
    input_data = features['x']

    Q1_Data = input_data[:, 0]
    Q2_Data = input_data[:, 1]

    q1 = layers.Embedding(nb_words + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(Q1_Data)
    q1 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q1)
    q1 = layers.Lambda(lambda x: tf.reduce_max(x, axis=1, keep_dims=False))(q1)

    q2 = layers.Embedding(nb_words + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(Q2_Data)
    q2 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q2)
    q2 = layers.Lambda(lambda x: tf.reduce_max(x, axis=1, keep_dims=False))(q2)

    merged = layers.concatenate([q1, q2])
    # merged = layers.Flatten()(merged)
    merged = layers.Dense(200, activation='relu')(merged)
    merged = tf.layers.dropout(merged, rate=DROPOUT)
    merged = layers.Dense(200, activation='relu')(merged)
    merged = tf.layers.dropout(merged, rate=DROPOUT)
    merged = layers.Dense(200, activation='relu')(merged)
    merged = tf.layers.dropout(merged, rate=DROPOUT)
    merged = layers.Dense(200, activation='relu')(merged)
    merged = tf.layers.dropout(merged, rate=DROPOUT)

    predictions = layers.Dense(1)(merged)

    predictions = tf.reshape(predictions, [-1])

    train_op = None
    eval_metric_ops = None

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions={""duplicate"": predictions})

    loss = tf.losses.sigmoid_cross_entropy(labels, predictions)
    optimizer = tf.train.AdamOptimizer()
    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())

    eval_metric_ops = {
        ""accuracy"": tf.metrics.accuracy(labels, predictions)
    }

    print(predictions)

    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)


print('\n')
print('Loading Numpy inputs')
q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))
q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))
labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))

X = np.stack((q1_data, q2_data), axis=1)
y = labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)

print('\n')
print('Preparing Numpy Input_Fn for both train and test')
train_input_fn = numpy_io.numpy_input_fn(x={'x': X_train}, y=y_train, shuffle=True, batch_size=BATCH_SIZE,
                                         num_epochs=None)

test_input_fn = numpy_io.numpy_input_fn(x={'x': X_test}, y=y_test, shuffle=False, batch_size=BATCH_SIZE, num_epochs=1)

nn = tf.estimator.Estimator(model_fn=model_fn, params=None, model_dir='build/')

print('\n')
print('Training...............')
nn.train(input_fn=train_input_fn, steps=100)

print('\n')
print('Training Complete, Evaluating............')
ev = nn.evaluate(input_fn=test_input_fn)
print(""Loss: %s"" % ev[""loss""])
print(""Accuracy: %s"" % ev[""accuracy""])

```

**Exception:**
Traceback (most recent call last):
  File ""D:/PlayGround/Git/Coding2Fun/Blog/DeepLearning/Quora-NLP/model.py"", line 103, in <module>
    nn.train(input_fn=train_input_fn, steps=100)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 241, in train
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 630, in _train_model
    model_fn_lib.ModeKeys.TRAIN)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 615, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""D:/PlayGround/Git/Coding2Fun/Blog/DeepLearning/Quora-NLP/model.py"", line 38, in model_fn
    q1 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q1)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\engine\topology.py"", line 396, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 450, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Programs\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\contrib\keras\python\keras\layers\wrappers.py"", line 208, in call
    unroll=False)
**TypeError: rnn() got an unexpected keyword argument 'input_length'**"
12736,Tensorflow 1.3 with Python 3.6.2 under Windows 10 64 Bit OS has issue when run tensorflow/tensorflow/examples/image_retraining/label_image.py,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 X64 Enterprise Edition
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: Anaconda 4.4.0 Python 3.6.2
- **Bazel version (if compiling from source)**: no
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**:
(tensorflow13) C:\Users\James\Tensorflow\model-retrain\tensorflow-for-poets-2\scripts>python .\label_image.py --image c:\Users\James\Tensorflow\sample_img\Panda001.jpg --graph c:\Users\James\Tensorflow\model-retrain\tensorflow-for-poets-2\scripts\retrained_graph.pb --labels C:\Users\James\Tensorflow\model-retrain\tensorflow-for-poets-2\scripts\retrained_labels.txt

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Error Log:
2017-09-01 09:27:46.902115: I C:\tf_jenkins\home\workspace\nightly-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Traceback (most recent call last):
  File "".\label_image.py"", line 120, in <module>
    input_operation = graph.get_operation_by_name(input_name);
  File ""C:\Users\James\AppData\Local\conda\conda\envs\tensorflow13\lib\site-packages\tensorflow\python\framework\ops.py"", line 3225, in get_operation_by_name
    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)
  File ""C:\Users\James\AppData\Local\conda\conda\envs\tensorflow13\lib\site-packages\tensorflow\python\framework\ops.py"", line 3097, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""C:\Users\James\AppData\Local\conda\conda\envs\tensorflow13\lib\site-packages\tensorflow\python\framework\ops.py"", line 3157, in _as_graph_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'import/input' refers to an Operation not in the graph.""

"
12735,Magic name directories prevent to embed the mkl library,"For some reasons the build_pip_package script sues the magic directory `solib_k8`. 
That directory does not exists if you compile mkl on a vm machine, where the libraries are added under `_solib_local`. 

```
fabriziomilo@instance-2:~/ml-models/tensorflow$ ag solib_k8
tensorflow/tools/pip_package/build_pip_package.sh
94:      mkdir ""${TMPDIR}/_solib_k8""
96:                     bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \
97:        ""${TMPDIR}/_solib_k8""
110:      if [ -d bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl ]; then
111:        mkdir ""${TMPDIR}/_solib_k8""
113:          bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \
114:          ""${TMPDIR}/_solib_k8""
127:      if [ -d bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl ]; then
128:        mkdir ""${TMPDIR}/_solib_k8""
130:                            bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \
131:          ""${TMPDIR}/_solib_k8""

tensorflow/tools/pip_package/setup.py
159:matches += ['../' + x for x in find_files('*', '_solib_k8') if '.py' not in x]
fabriziomilo@instance-2:~/ml-models/tensorflow$ find ./bazel-bin/ -iname \*mkl\*.so
./bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so
./bazel-bin/tensorflow/tools/pip_package/simple_console.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so
./bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so
./bazel-bin/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so```"
12733,Incompatible Shapes during Validation for TensorFlow's seq2seq module,"I'm using TensorFlow's seq2seq module. During validation, my decoder will occasionally produce output sequences with different lengths than the target sequences; this causes the following error when calculating the loss using `tf.nn.sigmoid_cross_entropy_with_logits` (batch major, not time major):

`InvalidArgumentError (see above for traceback): Incompatible shapes: [128,22,4] vs. [128,26,4]`

What is the best practice for dealing with this problem?

I checked how the NMT tutorial solved the problem. As far as I can tell, they use a TrainingHelper during validation, which forces the decoder to unroll the same number of steps as the target sequence, but this seems like cheating - they're estimating how the model will perform during inference, but the decoder is receiving additional information (the target sequence length) that it won't have at inference time. I opened an [issue](https://github.com/tensorflow/nmt/issues/73) to clarify, but I haven't heard back.

I also [posted](https://stackoverflow.com/questions/45944131/incompatible-shapes-during-validation-for-tensorflows-seq2seq-module) on StackOverflow, but from earlier experience, I doubt I'll receive a response.

My problem isn't specific to platform or TensorFlow version, but here's that information regardless:

OS: macOS Sierra version 10.12.6
TensorFlow installed from source
TensorFlow version: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
"
12732,Has anyone tried to visualize this program with tensorboard? ,"I was trying to visualize the vectors with thensorboard, but it did not work, it was loading for over 30 minutes, what is wrong? "
12731,Reproducing results in tensorflow,"Hi All,

I set the graph-level seed and op-level seed to be 0, and run the same script for 10 times. However, for a different run, 5 out of 10 times, the results are exactly the same (weights in the model and the training loss). For the other 5 times, the results are completely different for each run apart from the initialization steps.

Anyone has any idea of what is going on here?

Thanks!"
12730,Op type not registered 'CudnnRNNParamsSize' in binary,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 64bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: Cude - 8.0, cuDNN - 6.0
- **GPU model and memory**: M1000M
- **Exact command to reproduce**: 

### Describe the problem
The tutorial RNN model fails due to: ""Op type not registered 'CudnnRNNParamsSize' in binary""

### Source code / logs
https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py

```
b'unknown' 1.3.0
2017-08-31 16:25:32.021526: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-31 16:25:32.021822: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-31 16:25:32.959216: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:955] Found device 0 with properties: 
name: Quadro M1000M
major: 5 minor: 0 memoryClockRate (GHz) 1.0715
pciBusID 0000:01:00.0
Total memory: 2.00GiB
Free memory: 1.65GiB
2017-08-31 16:25:32.959478: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:976] DMA: 0 
2017-08-31 16:25:32.959642: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:986] 0:   Y 
2017-08-31 16:25:32.959848: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0)
Traceback (most recent call last):
  File ""C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py"", line 526, in <module>
    tf.app.run()
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py"", line 470, in main
    m = PTBModel(is_training=True, config=config, input_=train_input)
  File ""C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py"", line 135, in __init__
    output, state = self._build_rnn_graph(inputs, config, is_training)
  File ""C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py"", line 174, in _build_rnn_graph
    return self._build_rnn_graph_cudnn(inputs, config, is_training)
  File ""C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py"", line 186, in _build_rnn_graph_cudnn
    params_size_t = self._cell.params_size()
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 597, in params_size
    direction=self._direction)[0]
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\contrib\cudnn_rnn\ops\gen_cudnn_rnn_ops.py"", line 283, in cudnn_rnn_params_size
    name=name)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2632, in create_op
    set_shapes_for_outputs(ret)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1911, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 595, in call_cpp_shape_fn
    require_shape_fn)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 654, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""C:\Programs\Anaconda3\lib\contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""C:\Programs\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CudnnRNNParamsSize' in binary running on ULTRALISK. Make sure the Op and Kernel are registered in the binary running in this process.
```
"
12729,NO documentation for Linux SUSE SLES 12,"Hello Team,

I struggled to get the clear documentation for Linux SUSE SLES12. Now a days one single rpm usually contains the entire install able binaries.

Could you please add this as new feature?

Regards,
Naveen"
12728,Run Retrained inception model,"I have trained tensorflow inception v3 model with new data sets. (on ubuntu)
I got output_graph.pb and output_labels.txt files in tmp folder.
When I tried to run as below 

python label_image.py --image /home/ubuntu/140924_HDR3DSC_0092_ISO00050_SS1250.JPG --graph /tmp/output_graph.pb --labels /tmp/output_labels.txt

I got below errors

  File ""label_image.py"", line 121, in <module>
    input_operation = graph.get_operation_by_name(input_name);
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2836, in get_operation_by_name
    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2708, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2768, in _as_graph_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'import/input' refers to an Operation not in the graph.""

Please suggest me what to do to resolve this.
Please suggest me if anything wrong from my end."
12727,Is tf.one_hot() w/ GPU not working under windows10?,"I'm trying to run tensorflow in Windows10 environment.
When I use `tf.one_hot` function with GPU then it occur error.
Below is test code, and it's working find in Ubuntu.

### Test code
```python
from sklearn.datasets.samples_generator import make_regression
import tensorflow as tf

X, y, coef = make_regression(n_samples=100, n_features=2, noise=0.1, coef=True)
# To make all X values positive integer
X = list(map(lambda xx: list(map(lambda x: abs(int(x*10)), xx)), X))

tf.reset_default_graph()
def build_model(x):
    xs = tf.split(x, [1,1], 1)
    h1s = []
    for i in range(2):
        with tf.variable_scope('h1_%d'%i):
            # Seems below line is the problem..
            xs_temp = tf.reshape(tf.one_hot(xs[i], depth=30, on_value=1.0, off_value=0.0), [-1, 30])
            weights = tf.get_variable('weight', [30, 2], initializer=tf.contrib.layers.xavier_initializer())
            biases = tf.get_variable('biases', [2], initializer=tf.constant_initializer(0.0))
            xs_temp = tf.nn.relu(tf.matmul(xs_temp, weights) + biases)
            h1s.append(xs_temp)
    h1 = tf.concat(h1s, 1)
    with tf.variable_scope('h2'):
        weights = tf.get_variable('weight', [4, 1], initializer=tf.contrib.layers.xavier_initializer())
        biases = tf.get_variable('biases', [1], initializer=tf.constant_initializer(0.0))
        y = tf.matmul(h1, weights) + biases
    return y

with tf.device('/gpu:0'):
    X_ph =  tf.placeholder(shape=[None,2],dtype=tf.int32)
    y_ph =  tf.placeholder(shape=None,dtype=tf.float32)
    output = build_model(X_ph)
loss = tf.losses.mean_squared_error(y_ph, output)
updateModel = tf.train.AdamOptimizer(1e-2).minimize(loss)

init = tf.global_variables_initializer()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:
    sess.run(init)
    for _ in range(100):
        sess.run(updateModel, feed_dict={X_ph: X, y_ph: y})
```

### Error 
```
InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h1_1/one_hot': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: h1_1/one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, axis=-1, _device=""/device:GPU:0""](split:1, h1_1/one_hot/depth, h1_1/one_hot/on_value, h1_1/one_hot/off_value)]]
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Titan XP (12GB)
- **Exact command to reproduce**:
"
12726,AssertionError: Cannot find .runfiles directory for bazel-bin/tensorflow/python/tools/freeze_graph,"After I install the tensorflow from source, i'm trying to use freeze_graph tool, and it appear this error. 
My OS is MacOS 10.12.5"
12725,error bazel building quantize_graph with cuda,"this is for tf 1.3 and bazel 0.53 with cuda 8 and cudnn 6 on windows 10 64 and python 3.6

$ bazel build tensorflow/tools/quantization:quantize_graph  --verbose_failures
____Loading complete.  Analyzing...
____Found 1 target...
____Building...
____[0 / 10] Linking tensorflow/python/gen_state_ops_py_wrappers_cc.exe [for host]
____From Linking tensorflow/python/gen_state_ops_py_wrappers_cc.exe [for host]:
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/pthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
   Creating library bazel-out/host/bin/tensorflow/python/gen_state_ops_py_wrappers_cc.lib and object bazel-out/host/bin/tensorflow/python/gen_state_ops_py_wrappers_cc.exp
____From Compiling tensorflow/core/kernels/batch_norm_op_gpu.cu.cc:
cl : Command line warning D9002 : ignoring unknown option '-x'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=relaxed-constexpr'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=ftz=true'
cl : Command line warning D9002 : ignoring unknown option '-msse3'
cl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed
cl : Command line warning D9027 : source file 'cuda' ignored
ERROR: C:/users/user/downloads/tensorflow/tensorflow/core/kernels/BUILD:2703:1: C++ compilation of rule '//tensorflow/core/kernels:depthwise_conv_op_gpu' failed (Exit 2): cl.exe failed: error executing command
    SET CUDA_COMPUTE_CAPABILITIES=6.1
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.14393.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.14393.0\um\x64;
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Anaconda3\;C:\users\user\downloads\;C:\tools\msys64\usr\local\bin;C:\tools\msys64\usr\bin;C:\tools\msys64\usr\bin;C:\tools\msys64\opt\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\tools\msys64\usr\bin\site_perl;C:\tools\msys64\usr\bin\vendor_perl;C:\tools\msys64\usr\bin\core_perl;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\user\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=8.0
    SET TF_CUDNN_VERSION=6
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL=0
    SET TMP=C:\Users\user\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /nologo /I. /Ibazel-out/msvc_x64-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/msvc_x64-py3-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_sycl /Iexternal/protobuf /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf /Iexternal/gif_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/msvc_x64-py3-opt/genfiles/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/msvc_x64-py3-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/msvc_x64-py3-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/msvc_x64-py3-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/snappy /Ibazel-out/msvc_x64-py3-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda /Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/protobuf/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf/src /Iexternal/gif_archive/lib /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include /showIncludes /MT /O2 /c tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc /Fobazel-out/msvc_x64-py3-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.o -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true -DGOOGLE_CUDA=1 -msse3 /DLANG_CXX11 /D__VERSION__=""MSVC"" /DPLATFORM_WINDOWS /DTF_COMPILE_LIBRARY /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc.
.\tensorflow/core/util/cuda_kernel_helper.h(359): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(360): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(361): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(362): error C3861: 'atomicAdd': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(365): error C3861: 'atomicMax': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(366): error C3861: 'atomicMax': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'max': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(393): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(399): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(445): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(462): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(498): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(507): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(516): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__float_as_int': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(526): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(529): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(537): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(540): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(548): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(557): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(566): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__float_as_int': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(576): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(579): error C3861: '__int_as_float': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__double_as_longlong': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(587): error C3861: 'atomicCAS': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(590): error C3861: '__longlong_as_double': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(617): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(619): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(620): error C3861: '__shfl': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(621): error C3861: '__shfl': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(622): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(637): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(639): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(640): error C3861: '__shfl_up': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(641): error C3861: '__shfl_up': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(642): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(657): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(659): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(660): error C3861: '__shfl_down': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(661): error C3861: '__shfl_down': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(662): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(677): error C2065: 'warpSize': undeclared identifier
.\tensorflow/core/util/cuda_kernel_helper.h(679): error C2059: syntax error: 'volatile'
.\tensorflow/core/util/cuda_kernel_helper.h(680): error C3861: '__shfl_xor': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(681): error C3861: '__shfl_xor': identifier not found
.\tensorflow/core/util/cuda_kernel_helper.h(682): error C2059: syntax error: 'volatile'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): error C2182: '__launch_bounds__': illegal use of type 'void'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(71): error C2061: syntax error: identifier 'DepthwiseConv2dGPUKernelNHWC'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(161): error C2143: syntax error: missing ';' before '{'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(161): error C2447: '{': missing function header (old-style formal list?)
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(295): error C2182: '__launch_bounds__': illegal use of type 'void'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(295): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(296): error C2061: syntax error: identifier 'DepthwiseConv2dGPUKernelNCHW'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(431): error C2143: syntax error: missing ';' before '{'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(431): error C2447: '{': missing function header (old-style formal list?)
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(759): warning C4068: unknown pragma
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(711): error C2182: '__launch_bounds__': illegal use of type 'void'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(711): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(712): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropInputGPUKernelNHWC'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(779): error C2143: syntax error: missing ';' before '{'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(779): error C2447: '{': missing function header (old-style formal list?)
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(916): error C2182: '__launch_bounds__': illegal use of type 'void'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(916): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(917): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropFilterGPUKernelNHWC'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1024): error C2143: syntax error: missing ';' before '{'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1024): error C2447: '{': missing function header (old-style formal list?)
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1170): error C2182: '__launch_bounds__': illegal use of type 'void'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1170): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1171): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropFilterGPUKernelNCHW'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1284): error C2143: syntax error: missing ';' before '{'
tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1284): error C2447: '{': missing function header (old-style formal list?)
cl : Command line warning D9002 : ignoring unknown option '-x'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=relaxed-constexpr'
cl : Command line warning D9002 : ignoring unknown option '-nvcc_options=ftz=true'
cl : Command line warning D9002 : ignoring unknown option '-msse3'
cl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed
cl : Command line warning D9027 : source file 'cuda' ignored
Target //tensorflow/tools/quantization:quantize_graph failed to build
____Elapsed time: 8.033s, Critical Path: 4.14s
"
12724,Flatten all gradients in an MLP to a tensor of rank 1 (i.e. 1D array),"Suppose the following Keras model:

```
model = Sequential()
model.add(Dense(512, activation='sigmoid', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))
```
Obviously we can calculate the gradients by:

`grads = K.gradients(loss, params)
` 

which just calls:

`tf.gradients(loss, variables, colocate_gradients_with_ops=True)
`

This returns a list of tensors containing:

1) a tensor with 512x784 elements (input to hidden connections)
2) a tensor with the biases of the 512 units in the hidden layer
3) a tensor with 10x512 elements (hidden to output connections)
4) a tensor with the biases of the 10 output units

I would like to ask if there's a simple way to ""flatten"" `grads` to a single tensor of rank 1 (i.e. 1D array) with `(512x784)+512+(10x512)+10 `elements, without looping over the layers and corresponding biases.

Thanks"
12723,uploads tensorflow as follow error:,"uploads tensorflow as follow error:

ImportError: numpy.core.multiarray failed to import   Failed to load the native TensorFlow runtime.  See https://www.tensorflow.org/install/install_sources#common_installation_problems 


Thank you for your help!
"
12722,from tensorflow.python.ops.gen_audio_ops import *,"I am trying to follow the steps as per the readme file.
I get an import error.
After i inspected the repo i found that ""gen_audio_ops"" is missing under tensorflow/python/ops package.

Following is an image which shows the import 
![image](https://user-images.githubusercontent.com/17523473/29909246-9f1e1b0e-8e42-11e7-8f89-d601281d38c0.png)


Can you please help to get me the said package."
12721,dataset with tf.pyfunc error,"The new IO API is awesome, but I found this error in my project.
![image](https://user-images.githubusercontent.com/25046619/29908076-feb68952-8e51-11e7-8bd2-feed44d143cb.png)
 I tried to get some index from dataset, then make a batch index, 
given the batch index, I got the features from memory which used tf.py_func API, but I found this error.
Invalid argument: Shapes of all inputs must match: values[0].shape = [4096] != values[3].shape = [4096,41]"
12718,how to find distance from camera to object?,"hello.
i am a beginner.
how to find distance(Meter) from camera to object with tensorflow?
Thanks."
12717,The installation instructions not updated for latest Mac OS X. ,Please update instructions; ML engineers dont have time to waste spending on configurations.
12715,"C++ gradients: reduce_min, reduce_max","Anyone already working on adding these two operators to the C++ gradients? Otherwise, I'll sign up for it.

/cc @bpiel @suharshs "
12712,sharing variables but matrices are transposed even though src & dst tensors appear to have same shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
binary (pip install tensorflow-gpu==1.2.1)

- **TensorFlow version (use command below)**:
v1.2.0-5-g435cdfc, 1.2.1

- **Python version**: 
Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)
Type ""copyright"", ""credits"" or ""license"" for more information.
IPython 5.3.0 -- An enhanced Interactive Python.

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
Cuda V8.0.61, CuDNN 5.1

- **GPU model and memory**:
GeForce GTX 1080, 8GB

### Describe the problem
At first I thought I was doing something wrong (initially posted on [stackoverflow](https://stackoverflow.com/questions/45969089/trying-to-share-tensorflow-variables-between-autoencoder-and-decoder-but-weight)), but now I think this is a bug. Full text from SO pasted below.

I want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.

To be specific the error is 

> Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).

But in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:

    print('dense from {} to {}'.format(t, post_z_flat_dim))
    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)

which gives the output
> autoencoder: dense from Tensor(""z/Merge:0"", shape=(?, 128), dtype=float32) to 65536

> decoder: dense from Tensor(""z_1:0"", shape=(?, 128), dtype=float32) to 65536

The only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])

Relevant code is below followed by the output.

```python
class Model:
    def __init__(self,
                 usage, # Usage enum
                 reuse=None, # instance of another model to reuse sess, graph and variables from
                 ):

        if reuse == None:
            config = tf.ConfigProto()
            config.gpu_options.allow_growth=True    
            self.graph = tf.Graph()
            self.sess = tf.Session(graph=self.graph, config=config)
            root_scope = self.name
        else:
            self.graph = reuse.graph
            self.sess = reuse.sess
            root_scope = reuse.name

        with self.graph.as_default():
            with tf.variable_scope(root_scope, reuse=(reuse != None)):
                
                if usage >= Usage.autoencoder:
                    self.x = tf.placeholder(tf.float32, (None,) + tuple(img_shape), name='x')
                    t = self.x        
                    print('> x', t.shape, t.name)

                    # ENCODER
                    for i, filter_depth in enumerate(filters):
                        name = 'encoder{}'.format(i)
                        with tf.variable_scope(name):         
                            t = tf.layers.conv2d(...)                
                            if usage >= Usage.trainer: t = tf.layers.dropout(inputs=t, rate=self.dropout_conv_amt_T, training=True)
                            print('> conv', t.shape, t.name)
                            
                 
                # Z    
                if usage >= Usage.autoencoder:
                    t = tf.contrib.layers.flatten(t)
                    print('> pre z flat', t.shape, t.name)

                    if usage >= Usage.trainer:
                        t = tf.layers.dropout(inputs=t, rate=self.dropout_fc_amt_T, training=True)
                    
                    def vae_z(t):
                        self.mu = tf.layers.dense(inputs=t, units=z_dim, activation=None)
                        self.log_sigma = tf.layers.dense(inputs=t, units=z_dim, activation=None)                        
                        epsilon = tf.random_normal(tf.shape(self.log_sigma), name='epsilon')
                        t = self.mu + epsilon * tf.exp(self.log_sigma)
                        return t
                    
                    def nonvae_z(t):
                        return tf.layers.dense(inputs=t, units=z_dim, activation=None)
                    
                    t = tf.cond(self.use_vae_T, lambda: vae_z(t), lambda: nonvae_z(t), name='z')
                    self.z = t
                else:
                    t = tf.placeholder(tf.float32, [None, z_dim], name='z')
                    self.z = t

                print('> z', t.shape, t.name)
                
                # TODO is there a better way to calculate desired flat shape post z?
                post_z_img_dim = self.img_shape[0] // (2**len(filters))
                post_z_img_shape = [-1, post_z_img_dim, post_z_img_dim, filters[-1]]
                post_z_flat_dim = filters[-1] * post_z_img_dim * post_z_img_dim
                print('dense from {} to {}'.format(t, post_z_flat_dim))
                t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)
                print('> post z flat', t.shape, t.name)
                
                t = tf.reshape(t, shape=tf.constant(post_z_img_shape))
                print('> post z img', t.shape, t.name)


                # DECODER   

```

Output when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.

    autoencoder = Model(usage=Usage.autoencoder, reuse=None)
    --------------------------------------------------------------------------------
    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
    > x (?, 256, 256, 3) x:0
    > conv (?, 128, 128, 64) encoder0/conv/Relu:0
    > conv (?, 64, 64, 128) encoder1/conv/Relu:0
    > conv (?, 32, 32, 128) encoder2/conv/Relu:0
    > conv (?, 16, 16, 256) encoder3/conv/Relu:0
    > pre z flat (?, 65536) Flatten/Reshape:0
    > z (?, 128) z/Merge:0
    dense from Tensor(""z/Merge:0"", shape=(?, 128), dtype=float32) to 65536
    > post z flat (?, 65536) dense/Relu:0
    > post z img (?, 16, 16, 256) Reshape:0
    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0
    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0
    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0
    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0
    > y (?, 256, 256, 3) final/conv/Sigmoid:0


    
    decoder1 = Model(usage=Usage.decoder, reuse=None)
    --------------------------------------------------------------------------------
    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
    > z (?, 128) z:0
    dense from Tensor(""z:0"", shape=(?, 128), dtype=float32) to 65536
    > post z flat (?, 65536) dense/Relu:0
    > post z img (?, 16, 16, 256) Reshape:0
    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0
    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0
    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0
    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0
    > y (?, 256, 256, 3) final/conv/Sigmoid:0
    


    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)
    --------------------------------------------------------------------------------
    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3
    > z (?, 128) z_1:0
    dense from Tensor(""z_1:0"", shape=(?, 128), dtype=float32) to 65536
    Traceback (most recent call last):
    
      File ""<ipython-input-4-6f2915008bb4>"", line 1, in <module>
        decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)
    
      File ""/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py"", line 152, in __init__
        t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py"", line 218, in dense
        return layer.apply(inputs)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 320, in apply
        return self.__call__(inputs, **kwargs)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 286, in __call__
        self.build(input_shapes[0])
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py"", line 123, in build
        trainable=True)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1049, in get_variable
        use_resource=use_resource, custom_getter=custom_getter)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 948, in get_variable
        use_resource=use_resource, custom_getter=custom_getter)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 349, in get_variable
        validate_shape=validate_shape, use_resource=use_resource)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 275, in variable_getter
        variable_getter=functools.partial(getter, **kwargs))
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 228, in _add_variable
        trainable=trainable and self.trainable)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 341, in _true_getter
        use_resource=use_resource)
    
      File ""/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 658, in _get_single_variable
        found_var.get_shape()))
    
    ValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).




"
12707,Input function tutorial doesn't deliver reported performance.,"Running the example explained [here](https://www.tensorflow.org/get_started/input_fn), results in a reported loss 100 times bigger than what's reported in the tutotrial. It also doesn't converge, and it seems to be going up and down.

The expected performance is:

```
INFO:tensorflow:Step 1: loss = 483.179
INFO:tensorflow:Step 101: loss = 81.2072
INFO:tensorflow:Step 201: loss = 72.4354
...
INFO:tensorflow:Step 1801: loss = 33.4454
INFO:tensorflow:Step 1901: loss = 32.3397
INFO:tensorflow:Step 2001: loss = 32.0053
INFO:tensorflow:Step 4801: loss = 27.2791
INFO:tensorflow:Step 4901: loss = 27.2251
INFO:tensorflow:Saving checkpoints for 5000 into /tmp/boston_model/model.ckpt.
INFO:tensorflow:Loss for final step: 27.1674.
```

Whereas, here is what I get when I run it:

```
$ python2 boston.py 
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/boston_model', '_save_summary_steps': 100}
INFO:tensorflow:Create CheckpointSaverHook.
2017-08-30 16:38:36.378259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-30 16:38:36.378318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-30 16:38:36.378321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-30 16:38:36.378325: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-30 16:38:36.378328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-5000
INFO:tensorflow:Saving checkpoints for 5001 into /tmp/boston_model/model.ckpt.
INFO:tensorflow:loss = 3897.4, step = 5001
INFO:tensorflow:global_step/sec: 476.014
INFO:tensorflow:loss = 2826.95, step = 5101 (0.210 sec)
INFO:tensorflow:global_step/sec: 485.788
INFO:tensorflow:loss = 4138.43, step = 5201 (0.206 sec)
INFO:tensorflow:global_step/sec: 475.238
INFO:tensorflow:loss = 3182.46, step = 5301 (0.210 sec)
INFO:tensorflow:global_step/sec: 477.947
INFO:tensorflow:loss = 4233.69, step = 5401 (0.209 sec)
INFO:tensorflow:global_step/sec: 484.752
INFO:tensorflow:loss = 3593.6, step = 5501 (0.206 sec)
INFO:tensorflow:global_step/sec: 487.786
INFO:tensorflow:loss = 3316.72, step = 5601 (0.205 sec)
INFO:tensorflow:global_step/sec: 493.849
INFO:tensorflow:loss = 3041.69, step = 5701 (0.203 sec)
INFO:tensorflow:global_step/sec: 462.593
INFO:tensorflow:loss = 2753.05, step = 5801 (0.216 sec)
INFO:tensorflow:global_step/sec: 468.178
INFO:tensorflow:loss = 4609.25, step = 5901 (0.214 sec)
INFO:tensorflow:global_step/sec: 505.747
INFO:tensorflow:loss = 5840.77, step = 6001 (0.198 sec)
INFO:tensorflow:global_step/sec: 484.518
INFO:tensorflow:loss = 4149.44, step = 6101 (0.206 sec)
INFO:tensorflow:global_step/sec: 516.622
INFO:tensorflow:loss = 2485.26, step = 6201 (0.193 sec)
INFO:tensorflow:global_step/sec: 480.998
INFO:tensorflow:loss = 3849.6, step = 6301 (0.208 sec)
INFO:tensorflow:global_step/sec: 480.866
INFO:tensorflow:loss = 3684.53, step = 6401 (0.208 sec)
INFO:tensorflow:global_step/sec: 468.132
INFO:tensorflow:loss = 2943.32, step = 6501 (0.214 sec)
INFO:tensorflow:global_step/sec: 463.973
INFO:tensorflow:loss = 3135.92, step = 6601 (0.216 sec)
INFO:tensorflow:global_step/sec: 464.991
INFO:tensorflow:loss = 4463.47, step = 6701 (0.215 sec)
INFO:tensorflow:global_step/sec: 386.855
INFO:tensorflow:loss = 3488.36, step = 6801 (0.259 sec)
INFO:tensorflow:global_step/sec: 402.492
INFO:tensorflow:loss = 3472.3, step = 6901 (0.248 sec)
INFO:tensorflow:global_step/sec: 468.325
INFO:tensorflow:loss = 6648.4, step = 7001 (0.213 sec)
INFO:tensorflow:global_step/sec: 482.791
INFO:tensorflow:loss = 5161.27, step = 7101 (0.207 sec)
INFO:tensorflow:global_step/sec: 468.145
INFO:tensorflow:loss = 6147.99, step = 7201 (0.214 sec)
INFO:tensorflow:global_step/sec: 465.968
INFO:tensorflow:loss = 5066.78, step = 7301 (0.215 sec)
INFO:tensorflow:global_step/sec: 464.296
INFO:tensorflow:loss = 3810.97, step = 7401 (0.215 sec)
INFO:tensorflow:global_step/sec: 464.02
INFO:tensorflow:loss = 3235.41, step = 7501 (0.215 sec)
INFO:tensorflow:global_step/sec: 465.755
INFO:tensorflow:loss = 4455.11, step = 7601 (0.215 sec)
INFO:tensorflow:global_step/sec: 445.682
INFO:tensorflow:loss = 2335.67, step = 7701 (0.224 sec)
INFO:tensorflow:global_step/sec: 446.977
INFO:tensorflow:loss = 4264.51, step = 7801 (0.224 sec)
INFO:tensorflow:global_step/sec: 473.768
INFO:tensorflow:loss = 2708.95, step = 7901 (0.211 sec)
INFO:tensorflow:global_step/sec: 470.603
INFO:tensorflow:loss = 4107.13, step = 8001 (0.212 sec)
INFO:tensorflow:global_step/sec: 494.138
INFO:tensorflow:loss = 2911.78, step = 8101 (0.202 sec)
INFO:tensorflow:global_step/sec: 504.535
INFO:tensorflow:loss = 7470.54, step = 8201 (0.198 sec)
INFO:tensorflow:global_step/sec: 516.137
INFO:tensorflow:loss = 2581.56, step = 8301 (0.195 sec)
INFO:tensorflow:global_step/sec: 503.267
INFO:tensorflow:loss = 4270.98, step = 8401 (0.198 sec)
INFO:tensorflow:global_step/sec: 528.762
INFO:tensorflow:loss = 4186.45, step = 8501 (0.189 sec)
INFO:tensorflow:global_step/sec: 509.902
INFO:tensorflow:loss = 5767.94, step = 8601 (0.196 sec)
INFO:tensorflow:global_step/sec: 510.832
INFO:tensorflow:loss = 3960.38, step = 8701 (0.196 sec)
INFO:tensorflow:global_step/sec: 525.036
INFO:tensorflow:loss = 4677.09, step = 8801 (0.190 sec)
INFO:tensorflow:global_step/sec: 522.431
INFO:tensorflow:loss = 1912.38, step = 8901 (0.191 sec)
INFO:tensorflow:global_step/sec: 521.464
INFO:tensorflow:loss = 2646.11, step = 9001 (0.192 sec)
INFO:tensorflow:global_step/sec: 523.779
INFO:tensorflow:loss = 2803.72, step = 9101 (0.191 sec)
INFO:tensorflow:global_step/sec: 517.767
INFO:tensorflow:loss = 2995.35, step = 9201 (0.193 sec)
INFO:tensorflow:global_step/sec: 467.628
INFO:tensorflow:loss = 3767.14, step = 9301 (0.214 sec)
INFO:tensorflow:global_step/sec: 463.383
INFO:tensorflow:loss = 3825.25, step = 9401 (0.215 sec)
INFO:tensorflow:global_step/sec: 554.173
INFO:tensorflow:loss = 3352.59, step = 9501 (0.180 sec)
INFO:tensorflow:global_step/sec: 528.989
INFO:tensorflow:loss = 4678.92, step = 9601 (0.189 sec)
INFO:tensorflow:global_step/sec: 490.677
INFO:tensorflow:loss = 2191.4, step = 9701 (0.204 sec)
INFO:tensorflow:global_step/sec: 463.207
INFO:tensorflow:loss = 5998.51, step = 9801 (0.216 sec)
INFO:tensorflow:global_step/sec: 461.608
INFO:tensorflow:loss = 3440.55, step = 9901 (0.217 sec)
INFO:tensorflow:Saving checkpoints for 10000 into /tmp/boston_model/model.ckpt.
INFO:tensorflow:Loss for final step: 4595.88.
INFO:tensorflow:Starting evaluation at 2017-08-30-14:38:47
INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000
INFO:tensorflow:Finished evaluation at 2017-08-30-14:38:47
INFO:tensorflow:Saving dict for global step 10000: average_loss = 16.7064, global_step = 10000, loss = 1670.64
Loss: 1670.643555
INFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000
Predictions: [array([ 34.28430557], dtype=float32), array([ 18.48677063], dtype=float32), array([ 24.49580383], dtype=float32), array([ 34.83223343], dtype=float32), array([ 15.90157318], dtype=float32), array([ 20.3986187], dtype=float32)]
```

Environment:

== cat /etc/issue ===============================================
Linux adrin-leni.ancud.de 4.12.7-1-ARCH #1 SMP PREEMPT Sun Aug 13 08:17:09 CEST 2017 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux adrin-leni.ancud.de 4.12.7-1-ARCH #1 SMP PREEMPT Sun Aug 13 08:17:09 CEST 2017 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
"
12706,ERROR:tensorflow:Only one valid folder of images found at tf_files2/images - multiple classes are needed for classification.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12704,Memory leak when reusing variables with slim,"### System information:
- Windows 7 x64
- Python 3.5.2 |Anaconda 4.2.0 (64-bit)
- Tensorflow 1.3.0 installed via pip


### Problem
I want to use TF-Slim models to classify images in a server. For this, I would like to load the network only once and reuse the variables. I adjusted the tutorial given here: https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb

### Code to reproduce
```
import tensorflow as tf
from tensorflow.contrib import slim

import urllib.request as urllib
from nets import inception
from preprocessing import inception_preprocessing


image_size = inception.inception_v1.default_image_size

initialized = False
graph = tf.Graph()

init_fn = None

def predict():
    global initialized
    global init_fn
    with graph.as_default():
        url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'
        image_string = urllib.urlopen(url).read()
        image = tf.image.decode_jpeg(image_string, channels=3)
        processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)
        processed_images  = tf.expand_dims(processed_image, 0)
        with slim.arg_scope(inception.inception_v1_arg_scope()):
            logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False, reuse=initialized)
        probabilities = tf.nn.softmax(logits)
        if not initialized:
            init_fn = slim.assign_from_checkpoint_fn(""tmp/checkpoints/inception_v1.ckpt"", slim.get_model_variables(""InceptionV1""))
        with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8)) as sess:
            init_fn(sess)
            np_probabilities = sess.run(probabilities)
        initialized = True
    return np_probabilities.tolist()

for _ in range(20):
    predict()
```


### Memory Usage monitored with Process Explorer
![memory](https://user-images.githubusercontent.com/20746434/29866915-33ed6506-8d7a-11e7-8dd4-1fc3e1b2f1d7.png)
This is the memory usage when calling the predict function 20 times. As you can see, it keeps increasing.


Am I doing it wrong, or is it a bug?"
12701,Feature Request: callback argument for tf.contrib.data.Dataset.ignore_errors() to enable error logging,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: GTX 1080, 8GB
- **Exact command to reproduce**: -

### Describe the problem
The [`tf.contrib.data.Dataset.ignore_errors()`](https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/contrib/data/Dataset#ignore_errors) function is extremely useful when processing data that has not been fully cleaned beforehand (e.g., with streaming input), but I find the ""silently ignoring all errors"" a bit too strict.
It would be very useful, for example, to know which files raise an error when processing the dataset, while keeping the exception-masking feature of the function.

IMO, the most flexible way to obtain this would be to include an optional `callback` argument to the function that gets called passing as arguments the value in the dataset that raised an error.
This way custom logging can be done for each erroneous data sample and dataset inspection becomes much simpler.

Would this be very hard to implement?
"
12699,libtensorflow-core.a contains duplicate symbol CreateGPUTracerEv,"Running `build_all_ios.sh` produces a `libtensorflow-core.a` that contains a symbol twice. When linking on iOS the error is

```
duplicate symbol __ZN10tensorflow15CreateGPUTracerEv in:
    <PROJDIR>/.../libtensorflow-core.a(gpu_tracer.o)
ld: 1 duplicate symbol for architecture x86_64
```

I checked with `nm` just to be sure

```
$ nm libtensorflow-core.a | grep CreateGPUTracerEv
---------------- T __ZN10tensorflow15CreateGPUTracerEv
---------------- T __ZN10tensorflow15CreateGPUTracerEv
```

Branch `master`
MacOS Sierra 10.12.6 (16G29)
Xcode Version 8.3.3 (8E3004b)"
12698,Issue loading label file on Android example,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Android 8.0
- **TensorFlow installed from (source or binary)**:
Android Binary
- **TensorFlow version (use command below)**:
- **Python version**: 
NA
- **Bazel version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
NA


Whenever I try to build and launch the TF Classify demo app provided, the app crashes with the following error:

`08-29 21:03:01.791 9614-9614/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main
                                                                   Process: org.tensorflow.demo, PID: 9614
                                                                   java.lang.RuntimeException: Problem reading label file!
                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:100)
                                                                       at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:107)
                                                                       at org.tensorflow.demo.CameraActivity$2.onPreviewSizeChosen(CameraActivity.java:324)
                                                                       at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:407)
                                                                       at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:414)
                                                                       at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)
                                                                       at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)
                                                                       at android.view.TextureView.getHardwareLayer(TextureView.java:390)
                                                                       at android.view.TextureView.draw(TextureView.java:339)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)
                                                                       at android.view.View.draw(View.java:18847)
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)
                                                                       at android.view.View.draw(View.java:18847)
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)
                                                                       at android.view.View.draw(View.java:19122)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)
                                                                       at android.view.View.draw(View.java:18847)
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)
                                                                       at android.view.View.draw(View.java:18847)
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)
                                                                       at android.view.View.draw(View.java:18847)
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)
                                                                       at android.view.View.draw(View.java:19122)
                                                                       at com.android.internal.policy.DecorView.draw(DecorView.java:785)
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)
                                                                       at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:643)
                                                                       at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:649)
                                                                       at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:757)
                                                                       at android.view.ViewRootImpl.draw(ViewRootImpl.java:2980)
                                                                       at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2794)
                                                                       at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2347)
                                                                       at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1386)
                                                                       at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6733)
                                                                       at android.view.Choreographer$CallbackRecord.run(Choreographer.java:911)
                                                                       at android.view.Choreographer.doCallbacks(Choreographer.java:723)
                                                                       at android.view.Choreographer.doFrame(Choreographer.java:658)
                                                                       at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:897)
                                                                       at android.os.Handler.handleCallback(Handler.java:789)
                                                                       at android.os.Handler.dispatchMessage(Handler.java:98)
                                                                       at android.os.Looper.loop(Looper.java:164)
                                                                       at android.app.ActivityThread.main(ActivityThread.java:6541)
                                                                       at java.lang.reflect.Method.invoke(Native Method)
                                                                       at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
                                                                    Caused by: java.io.FileNotFoundException: imagenet_comp_graph_label_strings.txt
                                                                       at android.content.res.AssetManager.openAsset(Native Method)
                                                                       at android.content.res.AssetManager.open(AssetManager.java:374)
                                                                       at android.content.res.AssetManager.open(AssetManager.java:348)
                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:93)
                                                                       at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:107)Â 
                                                                       at org.tensorflow.demo.CameraActivity$2.onPreviewSizeChosen(CameraActivity.java:324)Â 
                                                                       at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:407)Â 
                                                                       at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:414)Â 
                                                                       at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)Â 
                                                                       at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)Â 
                                                                       at android.view.TextureView.getHardwareLayer(TextureView.java:390)Â 
                                                                       at android.view.TextureView.draw(TextureView.java:339)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)Â 
                                                                       at android.view.View.draw(View.java:18847)Â 
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)Â 
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)Â 
                                                                       at android.view.View.draw(View.java:18847)Â 
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)Â 
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)Â 
                                                                       at android.view.View.draw(View.java:19122)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)Â 
                                                                       at android.view.View.draw(View.java:18847)Â 
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)Â 
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)Â 
                                                                       at android.view.View.draw(View.java:18847)Â 
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)Â 
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)Â 
                                                                       at android.view.View.draw(View.java:18847)Â 
                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)Â 
                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)Â 
                                                                       at android.view.View.draw(View.java:19122)Â 
                                                                       at com.android.internal.policy.DecorView.draw(DecorView.java:785)Â 
                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)Â 
                                                                       at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:643)Â 
                                                                       at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:649)Â 
                                                                       at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:757)Â 
                                                                       at android.view.ViewRootImpl.draw(ViewRootImpl.java:2980)Â 
                                                                       at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2794)Â 
                                                                       at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2347)Â 
                                                                       at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1386)Â 
                                                                       at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6733)Â 
                                                                       at android.view.Choreographer$CallbackRecord.run(Choreographer.java:911)Â 
                                                                       at android.view.Choreographer.doCallbacks(Choreographer.java:723)Â 
                                                                       at android.view.Choreographer.doFrame(Choreographer.java:658)Â 
                                                                       at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:897)Â 
                                                                       at android.os.Handler.handleCallback(Handler.java:789)Â 
                                                                       at android.os.Handler.dispatchMessage(Handler.java:98)Â 
                                                                       at android.os.Looper.loop(Looper.java:164)Â 
                                                                       at android.app.ActivityThread.main(ActivityThread.java:6541)Â 
                                                                       at java.lang.reflect.Method.invoke(Native Method)Â 
                                                                       at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)Â 
                                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)Â 
`
The app has storage permissions enabled and was built on windows from the makefile, not Bazel."
12697,During handling of the above exception. Another exception occured,"![qq 20170829141338](https://user-images.githubusercontent.com/13164077/29851834-e8c4b112-8d68-11e7-971e-f29455d764d5.png)
![qq 20170829141241](https://user-images.githubusercontent.com/13164077/29851833-e8c00a36-8d68-11e7-913b-82aafd793e75.png)
Does anyone know what is the cause of this?
My system environmentï¼šUbuntu_16.04_X86_64; nvidia dirver version:v384.66 nvcc_version:v8.0 cuDNN:6.0 .
I dont know why.
Thank you before"
12696,Training model on Android ,"I want to training my model on the Android mobile phones by using Tensorflow. How to train the model and save the model on Android mobile phone? I can not find corresponding functions on Java API of TensorFlow.

Is it possible to find a way to implement such functions on android mobile phone? maybe tensorflow on Android with Python bindings? Or other Deep learning frameworkï¼Ÿ"
12695,Bundling tensorflow app to desktop,"I have got a tensorflow based app which recognizes the objects state. I use opencv and tensorflow to get this done. The final application has to be a desktop app. How do I bundle all dependencies and export it for desktop?

**PS:** I have asked the same question in stackoverflow with no response for my question hence Im asking this here hoping for a positive reply"
12694,queue skips first few elements under multiple enqueue threads,"### Possible problems
I want to use `tf.slice_input_producer` to produce a list of filenames, and then use multiple threads to load data and feed it to a `tf.FIFOQueue`. It seems first few elements have been skipped unexpectedly.

It only happens when there exist multiple enqueue threads.

I have searched the web, and only find one similar question on `stackoverflow.com` with zero answer.
https://stackoverflow.com/questions/44725917/tensorflow-train-batch-skip-3-examples

Sorry for creating this issue. Maybe I missed something. However, I really feel deeply puzzled about this and tried a lot to solve this.

### Minimum reproducible example
```python
import tensorflow as tf

a, = tf.train.slice_input_producer([tf.range(100)], shuffle=False)
q = tf.FIFOQueue(32, [tf.int32], shapes=[[]])
tf.train.queue_runner.add_queue_runner(
    tf.train.queue_runner.QueueRunner(q, [q.enqueue([a])] * 8)
)
with tf.Session() as sess:
    tf.train.start_queue_runners(sess=sess)
    for _ in range(100):
        print(sess.run(q.dequeue()))
```

### Expected output
Some reordering may take place, but roughly, the programs count from 0 to 99.

### Actual output
```
7
8
9
...
96
97
98
99
0
1
2
3
4
5
6
```

### System information
My TensorFlow is installed from `pip install tensorflow-gpu --user` the day before yesterday.
* System: ArchLinux x86_64
* CUDA: V8.0.61
* Python: 3.6.2
* GPU: GTX 860M (2G memory)
* CPU: i7-4710MQ (4 cores 8 threads)
```
== cat /etc/issue ===============================================
Linux archlinux-sun 4.12.8-2-ARCH #1 SMP PREEMPT Fri Aug 18 14:08:02 UTC 2017 x86_64 GNU/Linux
LSB_VERSION=1.4

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 7.1.1 20170630
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux archlinux-sun 4.12.8-2-ARCH #1 SMP PREEMPT Fri Aug 18 14:08:02 UTC 2017 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.2)
tensorflow (1.3.0)
tensorflow-gpu (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/lib/nvidia:/usr/lib32/nvidia:/usr/lib:/usr/lib32:/usr/lib:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 30 08:42:01 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 860M    Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   53C    P0    N/A /  N/A |      5MiB /  2002MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      4172    G   /usr/lib/xorg-server/Xorg                        4MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
```"
12693,TensorBoard executed stuck,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 1703
- **TensorFlow installed from (source or binary)**: install binary with GPU version by pip based on Python 3.5.2
- **TensorFlow version (use command below)**:('1.3.0')
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:none
- **CUDA/cuDNN version**:cuda v8.0/cuDNN v6,0
- **GPU model and memory**:GeForce GTX 1080 Ti 11GB

### Describe the problem
I am trying to open TensorBoard after running mnist_with_summaries.py source code from the TensorBoard tutorials (https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py). However, 
After I run tensorboard --logdir=/tmp/tensorflow/mnist in cmd, it stuck and did not showing anything else.  I try to use tensorboard --logdir=/tmp/tensorflow/mnist --debug and have the same result. 
![capture](https://user-images.githubusercontent.com/16062793/29848367-ae4c35b0-8ce5-11e7-8e9e-0ee44058fda3.PNG)

"
12692,Speech Commands Example URL broken,"Found here:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands#speech-commands-example

http://tensorflow.org/tutorials/audio_recognition is broken"
12690,Try compile for raspbery pi and got graph.pb.h missing,"tensorflow/core/framework/graph.pb.h: No such file or directory
 #include ""tensorflow/core/framework/graph.pb.h"
12689,multi-GPU training too slow when L2 regularizer enabled ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 home edition
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.3.0
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
Cuda 8.0 / cudnn 6.0
- **GPU model and memory**:
Nvidia 1080 GTX 8GB x 2
- **Exact command to reproduce**:

### Describe the problem
I try to train a face recognition classifier using inception-resnet-v1 model with 2 GPUs. On single GPU the training proceeds just fine, with a processing capacity around 220 images/sec, but when I train with 2 GPUs I only observe marginal benefit (capacity increases to around 280 images/sec). After some profiling I found out that the poor performance was somehow due to the introduced L2 regularizer. When the regularizer is enabled, the computation of the gradient becomes unexpectedly slow, resulting in the slowdown of the entire training cycle. As a comparison, if the regularizer is disabled, I could obtain a processing capacity around 350 images/sec, which although not perfect, is more or less satisfactory. The boost in the performance in the latter scenario cannot be attributed to the reduced computation complexity from the removal of the regularizer. This is evidenced by a reference experiment with exactly the same parameter except for on a single GPU, see below. 

I cannot figure out an explanation for this. It took me several days to narrow down the problem to, seemingly,  the introduction of L2 regularizer and the computation of gradient.

In my program I used standard tf.slim layers together with a downloaded inception-resnet-v1 model script.  The main part of the code is the following:

```python
def main(args):
    num_gpus = args.num_gpus
    batch_size_per_gpu = args.batch_size_per_gpu
    batch_size = batch_size_per_gpu * num_gpus

    num_classes = 10000
    synthetic_images = 0.01 * np.random.randn(batch_size, 160, 160, 3)
    synthetic_labels = np.random.randint(0, 10000, batch_size)

    with tf.Graph().as_default(), tf.device('/cpu:0'):
        # the synthetic array is converted to a tf.Variable
        images = tf.Variable(tf.convert_to_tensor(synthetic_images, dtype=tf.float32), trainable=False)
        labels = tf.Variable(tf.convert_to_tensor(synthetic_labels, dtype=tf.int32), trainable=False)

        image_batches = tf.split(images, num_or_size_splits=num_gpus)
        label_batches = tf.split(labels, num_or_size_splits=num_gpus)

        total_loss = [None] * num_gpus
        grads = [None] * num_gpus

        opt = tf.train.RMSPropOptimizer(0.01, epsilon=0.01)

        reuse_variables = False
        for i in range(num_gpus):
            with tf.device(""/gpu:{0}"".format(i)), tf.name_scope(""tower{0}"".format(i)) as scope:
                with slim.arg_scope([slim.variable], device='/cpu:0'):
                    print(""Building graph for tower{0}..."".format(i))

                    total_loss[i] = tower_loss(images=image_batches[i],
                                               labels=label_batches[i],
                                               num_classes=num_classes,
                                               keep_probability=1,
                                               phase_train=True,
                                               embedding_size=512,
                                               weight_decay=args.weight_decay,
                                               reuse=reuse_variables)

                    grads[i] = opt.compute_gradients(total_loss[i])
                    reuse_variables = True

        # then open a session and calculate the tower gradients
        with tf.Session() as sess:
            sess.run(init)
            print(""Warming up..."")
            for i in range(10):
                _ = sess.run(grads)

            print(""Benchmarking..."")
            for i in range(num_iterations):
                _ = sess.run(grads)
```
where `tower_loss()` is defined as follows:

``` python   
def tower_loss(images, labels, num_classes, keep_probability, phase_train, embedding_size=128, weight_decay=0.0,
               reuse=None):
    batch_norm_params = {
        'decay': 0.995,
        'epsilon': 0.001,
        'fused': True
    }

    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
                        weights_regularizer=slim.l2_regularizer(weight_decay),
                        normalizer_fn=slim.batch_norm,
                        normalizer_params=batch_norm_params
                        ):
        embeddings = inception_resnet_v1(images, is_training=phase_train,
                                         dropout_keep_prob=keep_probability,
                                         bottleneck_layer_size=embedding_size,
                                         reuse=reuse)

    logits = slim.fully_connected(embeddings,
                                  num_classes,
                                  activation_fn=None,
                                  reuse=reuse,
                                  weights_regularizer=slim.l2_regularizer(weight_decay),
                                  scope=""fc"")
    entropy_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))
    if weight_decay == 0:
        print(""L2 regularizer disabled."")
        return entropy_loss
    else:
        print(""Regularization losses added to total loss."")
        return tf.add_n([entropy_loss] + tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
```

The whole script (contains codes to generate synthetic data) can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/issue.py). The inception-resnet-v1 model script can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/inception_resnet_modified.py).

To reproduce the problem, just run 
`python issue.py --num_gpus 2  --weight_decay 0
`
The output would be something like
```
Duration: 7.1 secs; 358.1 images/sec or 0.71 sec/batch
Duration: 7.5 secs; 343.1 images/sec or 0.75 sec/batch
Duration: 7.2 secs; 354.9 images/sec or 0.72 sec/batch
Duration: 7.1 secs; 358.8 images/sec or 0.71 sec/batch
Duration: 7.2 secs; 354.9 images/sec or 0.72 sec/batch
Total duration: 36.2sec        Performance: 353.9 images/sec
```
In contrast, with the presence of L2 regularizer, running
`python issue.py --num_gpus 2  --weight_decay 0.0001
`
one would obtain
```
Duration: 9.0 secs; 284.9 images/sec or 0.90 sec/batch
Duration: 9.1 secs; 279.9 images/sec or 0.91 sec/batch
Duration: 8.7 secs; 292.8 images/sec or 0.87 sec/batch
Duration: 9.3 secs; 276.7 images/sec or 0.93 sec/batch
Duration: 8.5 secs; 302.4 images/sec or 0.85 sec/batch
Total duration: 44.6 sec        Performance: 287.0 images/sec
```

As a reference, with single GPU, the difference of processing capacity between with and without regularizer is negligeable, at least in this demo. To check it, run
`python issue.py --num_gpus 1  --weight_decay 0
`
gives
```
Duration: 5.6 secs; 228.1 images/sec or 0.56 sec/batch
Duration: 5.6 secs; 228.7 images/sec or 0.56 sec/batch
Duration: 5.7 secs; 225.8 images/sec or 0.57 sec/batch
Duration: 5.6 secs; 230.3 images/sec or 0.56 sec/batch
Duration: 5.6 secs; 226.8 images/sec or 0.56 sec/batch
Total duration: 28.1 sec        Performance: 227.9 images/sec
```
and
`python issue.py --num_gpus 1  --weight_decay 0.0001
`
gives
```
Duration: 5.6 secs; 227.5 images/sec or 0.56 sec/batch
Duration: 5.7 secs; 226.3 images/sec or 0.57 sec/batch
Duration: 5.6 secs; 229.4 images/sec or 0.56 sec/batch
Duration: 5.6 secs; 227.5 images/sec or 0.56 sec/batch
Duration: 5.7 secs; 224.3 images/sec or 0.57 sec/batch
Total duration: 28.2 sec        Performance: 227.0 images/sec
```

"
12686,Feature Request: C++ gradient for SoftmaxCrossEntropyWithLogits,"Implement the gradient for SoftmaxCrossEntropyWithLogits in c++ so that it is available for TF_AddGradients.

This is the python code that I believe would need to be ported:
https://github.com/tensorflow/tensorflow/blob/4b2fb49fd7578afe7e289936f347af581b5bdab1/tensorflow/python/ops/nn_grad.py#L409
"
12683,XLA leads to core dump,"### System information

[output of tf_env_collect.sh](http://paste.ubuntu.com/25424565/)

#### Tensorflow

Tensorflow compiled from the source v1.3.0(9e76bf3)

with cuda, with xla, without mpi, without mkl

#### OS
CentOS 7

out put of `uname -a`:

Linux zhanghao 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

#### python
Python 2.7.13 |Intel Corporation| (default, Apr 27 2017, 15:33:46)

[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux2

#### Bezel
Build label: 0.5.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jun 27 13:27:03 2017 (1498570023)
Build timestamp: 1498570023
Build timestamp as int: 1498570023

#### GPU
CUDA 8.0 cuDNN 6.0.21
GPU: GeForce GTX 950M

### Describe the problem
core dump when use xla with gpu,

BTW, if use cpu only, xla won't lead to core dump

### Source code
This is code to reproduce the bug
```
import tensorflow as tf
import sys
D = 2
A = tf.random_normal(shape=[D, D, 2], dtype=tf.float32,name=""A"")
B = tf.random_normal(shape=[D, D, 2], dtype=tf.float32, name=""B"")
E = tf.ones(shape=[D], dtype=tf.float32, name=""EBA"")
H = tf.reshape(tf.constant([[0.25,0,0,0],[0,-0.25,0.5,0],[0,0.5,-0.25,0],[0,0,0,0.25]],
                           dtype=tf.float32),[2,2,2,2],name=""Hamiltonian"")
EA = tf.multiply(A,tf.reshape(E,[D,1,1]))
AB = tf.tensordot(EA,B,[[1],[0]],name=""AB"")
S, U, V = tf.svd(tf.reshape(AB,[2*D,2*D]))
UU = tf.transpose(tf.multiply(tf.reshape(U[:,:D],[D,2,D]),tf.reshape(E,[D,1,1])),[0,2,1],name=""nA"")
data = UU / tf.reduce_max(UU)
config = tf.ConfigProto()
if len(sys.argv)>1:
    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
print sess.run(data)
```
save as MPS.py and run `python MPS.py` and get:
```
2017-08-29 20:51:54.856662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-29 20:51:54.857232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 950M
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
2017-08-29 20:51:54.857292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-08-29 20:51:54.857301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-08-29 20:51:54.857312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0)
[[[ 0.64684284 -0.48666194]
  [-0.56722689 -0.19181968]]

 [[ 0.34154066  0.77098727]
  [ 1.         -0.0881796 ]]]
```
and then run `python MPS.py -`, and get:
```
2017-08-29 20:52:18.127327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-29 20:52:18.127719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 950M
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
2017-08-29 20:52:18.127780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-08-29 20:52:18.127789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-08-29 20:52:18.127799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0)
2017-08-29 20:52:18.319392: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-08-29 20:52:18.319435: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Executor present with 1 visible devices
2017-08-29 20:52:18.319739: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-08-29 20:52:18.320615: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x7f59008c0350 executing computations on platform CUDA. Devices:
2017-08-29 20:52:18.320640: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0
2017-08-29 20:52:18.474432: F tensorflow/compiler/xla/util.cc:183] Check failed: p1.size() == p2.size() (3 vs. 0)
[1]    14077 abort (core dumped)  python MPS.py -
```"
12681,Feature request: dilated pooling,"Hi,

I posted initially in keras-users, but @fchollet suggested that this would need to be implemented first at TensorFlow level

https://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508

Similarly to dilated convolution, dilated pooling uses a dilated kernel. For example, dilated kernel with pool_size=(3,2), dilation_rate=(2,4) (* = non-zero elements: this is where values are sampled to compute max pooling)
        
        * 0 0 0 *
        0 0 0 0 0
        * 0 0 0 *
        0 0 0 0 0
        * 0 0 0 *

This was proposed in 

Li H, Zhao R, Wang X. Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification. arXiv preprint arXiv:14124526. 2014.

and it's used by DeepCell

Van Valen et al. (2016), PLoS Comput Biol, ""Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments""â€‹ http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177

DeepCell is state of the art for cell segmentation, but it's implemented with Keras 1, and ad-hoc code based on a bit outdated Theano.

https://github.com/CovertLab/DeepCell

To the best of my knowledge, there's a lack of testing of whether dilation in pooling improves segmentation. I have written one in Keras, by shifting the kernel and slicing the image, then running regular 2D pooling on each slice, and reassembling the results (code at the end of post, but probably not very interesting). However, this approach is probably quite inefficient. 

The question would be whether any TF developer would be willing to extend the current tf.contrib.keras.backend.pool2d

```
pool2d(
    x,
    pool_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    pool_mode='max'
)

```

with a `dilation_rate` argument, and the functionality at that level. From there, @fchollet is happy to modify the Keras API to include this option.


Code for Keras implementation of dilated 2D pooling

```
    def _pooling_function(self, inputs, pool_size, strides,
                          padding, data_format, dilation_rate):

        # inputs for tests
        inputs = K.variable(np.reshape(range(1,4*3*5*8+1), (4, 3, 5, 8)))########
        inputs = K.variable(np.reshape(range(1,1*1*5*8+1), (1, 1, 5, 8)))########
        
        if data_format == 'channels_first': # (batch,chan,row,col)
            nbatch = K.get_variable_shape(inputs)[0]
            #nchan = K.get_variable_shape(inputs)[1]
            nrows = K.get_variable_shape(inputs)[2]
            ncols = K.get_variable_shape(inputs)[3]
        elif data_format == 'channels_last': # (batch,row,col,chan)
            nbatch = K.get_variable_shape(inputs)[0]
            #nchan = K.get_variable_shape(inputs)[1]
            nrows = K.get_variable_shape(inputs)[2]
            ncols = K.get_variable_shape(inputs)[3]
        else:
            raise ValueError('Expected data format to be channels_first or channels_last')

        # number of blocks to split the input into. Each dilation (row or 
        # column) goes into a separate block
        nblocks = dilation_rate
        
        # size of each block we are going to split the input images in
        block_sz = (int(np.ceil(nrows / dilation_rate[0])), 
                    int(np.ceil(ncols / dilation_rate[1])))

        # pad inputs so that they can be split into equal blocks
        padded_size = np.multiply(block_sz, nblocks)
        padding_len = ((0, padded_size[0] - nrows), (0, padded_size[1] - ncols))
        inputs = K.spatial_2d_padding(inputs, padding=padding_len, data_format=data_format)
 
        # split the inputs into blocks
        split_inputs = []
        for row_offset in range(nblocks[0]):
            for col_offset in range(nblocks[1]):
                if data_format == 'channels_first': # (batch,chan,row,col)
                    split_inputs = split_inputs + [inputs[:, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]]]
                elif data_format == 'channels_last': # (batch,row,col,chan)
                    split_inputs = split_inputs + [inputs[:, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :]]
        split_inputs = K.concatenate(split_inputs, axis=0)                        
    
        # pool each block without dilation
        split_inputs = K.pool2d(split_inputs, pool_size, strides=(1,1),
                                padding='same', data_format=data_format,
                                pool_mode='max')

        # reassemble blocks
        output = np.zeros(shape=list(inputs.shape.eval()), dtype=inputs.dtype)
        for idx in range(nbatch*nblocks[0]*nblocks[1]):
            multi_index = np.unravel_index(idx, dims=(nblocks[0], nblocks[1], nbatch))
            row_offset = multi_index[0]
            col_offset = multi_index[1]
            batch = multi_index[2]
            if data_format == 'channels_first': # (batch,chan,row,col)
                output[batch, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]] = split_inputs[idx, :, :, :].eval()
            elif data_format == 'channels_last': # (batch,row,col,chan)
                output[batch, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :] = split_inputs[idx, :, :, :].eval()
        output = K.variable(output)
        
        # remove padding
        if padding == 'valid':
            
            if data_format == 'channels_first': # (batch,chan,row,col)
                output = output[:, :, (pool_size[0]-1)*dilation_rate[0]:nrows, 
                                (pool_size[1]-1)*dilation_rate[1]:ncols]
            elif data_format == 'channels_last': # (batch,row,col,chan)
                output = output[:, (pool_size[0]-1)*dilation_rate[0]:nrows, 
                                (pool_size[1]-1)*dilation_rate[1]:ncols, :]
                    
        elif padding == 'same':
        
            if data_format == 'channels_first': # (batch,chan,row,col)
                output = output[:, :, 0:nrows, 0:ncols]
            elif data_format == 'channels_last': # (batch,row,col,chan)
                output = output[:, 0:nrows, 0:ncols, :]
                
        else:
            
            raise NotImplementedError

        # return tensor
        return output

```"
12679,object_detection eval,When evaluating the model? tensorboard values is NaN why?
12676,Tensorflow building error,"I am trying to build tensorflow with this statement ""bazel build -c opt //tensorflow/examples/android:tensorflow_demo"" 

the WORKSPACE is 
`android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,    
    #build_tools_version = ""25.0.2"",    
	build_tools_version = ""26.0.1"",
    path = ""C:/Users/ST/AppData/Local/Android/Sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""C:/Users/ST/Downloads/Tensorflow_Compile/android-ndk-r12b"",    
    api_level=14)
`

but I got an error message:
""ERROR: C:/msys64/tmp/_bazel_st/_ztysx-6/external/protobuf_archive/BUILD:93:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 3).

This application has requested the Runtime to terminate it in an unusual way.
Please contact the application's support team for more information.
Cannot create temporary file in C:\WINDOWS\: Permission denied
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 5.465s, Critical Path: 0.63s""

I am using Windows 10, bazel 0.5.4, python 2.7. Is there anyway to fix this problem?"
12674,"Same code, runs fine in one machine, ValueError in another. ","I am running a modified version of the LFADS code available at

https://github.com/tensorflow/models/tree/master/lfads

I run the exact same code in two machines, in one it works fine, in another it throws a ValueError. Here are the two tf_env.txt

## MY MACHINE (CODE WORKS)
== cat /etc/issue ===============================================
Darwin Daniels-MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Daniels-MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-gpu (1.1.0)
tensorflow-tensorboard (0.1.2)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = unknown
tf.COMPILER_VERSION = unknown
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

## THE CLUSTER MACHINE (THROWS THE VALUE ERROR)
== cat /etc/issue ===============================================
Linux holmes 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7.3 (Maipo)""
VERSION_ID=""7.3""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.3
REDHAT_SUPPORT_PRODUCT_VERSION=""7.3""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 6.1.0
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux holmes 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
numpydoc (0.6.0)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /cm/shared/apps/slurm/16.05.8/lib64/slurm:/cm/shared/apps/slurm/16.05.8/lib64:/cm/local/apps/gcc/6.1.0/lib:/cm/local/apps/gcc/6.1.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./collect.sh: line 105: nvidia-smi: command not found


### SOURCE CODE/ LOGS

Here is the log for the error:

Traceback (most recent call last):
.....
  File ""./lfads.py"", line 1014, in _optimization
    grads = tf.gradients(self.cost, tvars)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 562, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 378, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 566, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (50, ?) and (1,) are not compatible


I have diffed the files involved, to double check that they are the same. Can someone point me to what can possibly be happening?
"
12673,Java load python trained mode by SavedModelBundle predict wrong result," 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
14.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.3/1.2.1
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

 

### Describe the problem
I trained a simple model using python and save it with SavedModelBuilder. Then I loaded it using java. it's loaded successfully but predict wrong result.

### Source code / logs
My python code is in
https://gist.github.com/fancyerii/af10e8cc35ffda4c983c6e9cf074b317
and java code https://gist.github.com/fancyerii/c1096f9f8c9bd9705da1b1c7d6ca2138
"
12671,Bundling tensorflow app to desktop,I have got a tensorflow based app which recognizes the objects state. I use opencv and tensorflow to get this done. The final application has to be a desktop app. How do I bundle all dependencies and export it for desktop?
12667,Distributed Training Randomly Stops During the Training Process,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**:  Tesla K80, 12G


### Describe the problem
In my distributed training program, there are one server and two workers, which all run in separately nvidia-docker container. At the beginning, the cluster works just fine, but running normally after several hours, the two workers just stop.

My training process:
1. I create three nvidia-docker containers, one for parameter server, two for workers
2. In every container, I run the `train_replica` function below after defining all necessary parts such as cluster_spec, inference function, data batch and so on.
3. It works correctly at the beginning
4. It stops several hours later


### Source code / logs
My trainer function:
```{python}
def train_replica(cluster_spec,
                  get_data_batch,
                  inference_fn,
                  get_init_fn,
                  get_learning_rate,
                  get_optimizer,
                  get_train_variables,
                  replica_param,
                  train_param,
                  ):
    job_name = replica_param['job_name']
    task_index = replica_param['task_index']
    sync_replicas = train_param['sync_replicas']
    log_dir = train_param['log_dir']
    assert job_name in ['ps', 'worker']
    server = tf.train.Server(cluster_spec, job_name=job_name,
                             task_index=task_index, config=get_ps_session_config())
    if job_name == 'ps':
        server.join()
    else:
        is_chief = (task_index == 0)
        device_setter = tf.train.replica_device_setter(cluster=cluster_spec)
        with tf.Graph().as_default():
            with tf.device(device_setter):
                global_step = create_global_step()
                learning_rate = get_learning_rate(global_step)
                data_batch = get_data_batch()
                _ = inference_fn(data_batch)
                total_loss, task_loss = get_losses()
                optimizer = get_optimizer(learning_rate)
                if sync_replicas:
                    optimizer = tf.train.SyncReplicasOptimizer(
                        opt=optimizer,
                        replicas_to_aggregate=cluster_spec.num_tasks('worker'),
                        total_num_replicas=cluster_spec.num_tasks('worker'),
                        name='sync_replica_optimizer'
                    )
                train_op = slim.learning.create_train_op(
                    total_loss=total_loss,
                    optimizer=optimizer,
                    global_step=global_step,
                    variables_to_train=get_train_variables(),
                    clip_gradient_norm=train_param['clip_norm'],
                    gradient_multipliers=train_param['gradient_multipliers'],
                )
                init_fn = get_init_fn() if get_init_fn is not None else None
                scaffold = tf.train.Scaffold(
                    init_op=tf.global_variables_initializer())
                scaffold._init_fn = init_fn
                hooks = [tf.train.StopAtStepHook(train_param['train_steps'])]
                if sync_replicas is True:
                    hooks.append(optimizer.make_session_run_hook(is_chief))
                chief_only_hooks = [tf.train.LoggingTensorHook([total_loss, task_loss], 100)]
                step_ind = 0
                with tf.train.MonitoredTrainingSession(
                        master=server.target,
                        is_chief=is_chief,
                        checkpoint_dir=log_dir,
                        scaffold=scaffold,
                        hooks=hooks,
                        chief_only_hooks=chief_only_hooks,
                        config=get_worker_session_config(task_index)) as session:
                    while not session.should_stop():
                        session.run(train_op)
                        step_ind += 1
                        if step_ind % 1000 == 0:
                            tf.logging.debug('Training Step At {s}'.format(s=step_ind))
```
"
12663,Improve documentation of tf.gfile.GFile vs tf.gfile.FastGFile,"is not clear from the docs what is the difference between the two. 
I assumed that one has thread locking (.GFile) and the other does not, but both say the same thing in the docs:
- [https://www.tensorflow.org/api_docs/python/tf/gfile/GFile](https://www.tensorflow.org/api_docs/python/tf/gfile/GFile)
- [https://www.tensorflow.org/api_docs/python/tf/gfile/FastGFile](https://www.tensorflow.org/api_docs/python/tf/gfile/FastGFile)"
12659,tf.maximum does not return nan when inputs contain nan,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Tesla K40m, 11439MiB
- **Exact command to reproduce**: python main.py

### Describe the problem
`tf.maximum(a, b)` should return `nan` when `a` or `b` contain `nan`. However, it does not at some cases.

### Source code / logs
main.py
```python
import tensorflow as tf
import numpy as np

a = tf.placeholder(dtype=tf.float32)
max_a = tf.maximum(a, 1.)
with tf.Session():
    print max_a.eval(feed_dict={a: np.nan})
```
The output is:
```
1.0
```"
12657,Undefined op in speech_commands example,"I am using tensorflow 1.3.0 built from source.

I run the newly released speech_commands example (its in tensorflow/examples/speech_commands), following all the stuffs and all good. However, after I used the freeze.py script and generated the pb graph, I am unable to load it back. The code I used to load pb graph is:

```
def load_graph(frozen_graph_filename):
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    with tf.Graph().as_default() as graph:
        tf.import_graph_def(
            graph_def, 
            input_map=None, 
            return_elements=None, 
            name=""imported"", 
            op_dict=None, 
            producer_op_list=None
        )
    return graph
graph = load_graph('./test_loading_graph_back.pb')
```

Error:
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 285, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named DecodeWav in defined operations."
12656,DNN Classifier producing Blue Screen of Death every time,"Hi,  

When running inputs 9 10 and 11 from the below notebook, I get a blue screen of death every time?  

https://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb

I'm using: 

Python ver 3.5.3
Tensorflow 1.2.1
Running on a windows 7 pro machine

Any tips on how to resolve this?

"
12653,Device string in eager API differs from main ops API,"In the Python API device strings are normalized such that `/cpu:0` becomes `/device:CPU:0`. This happens in the `device.py` and more specifically, [here](https://github.com/tensorflow/tensorflow/blob/668db64a5d612d5f96b5d87772ce6ff6531fc035/tensorflow/python/framework/device.py#L192). However, the eager execution API throws an error if provided `/device:CPU:0` for the device. It only works if I set it to `CPU:0`. Why does that inconsistency exist? @asimshankar @alextp 

P.S. I try to tag people that I believe are relevant based on previous issues/discussions. Please let me know if that's annoying and would prefer me not tagging anyone. :)"
12650,TF build fails for simulator architectures on master,"When building TF for iOS from `master` with

    ./build_all_ios.sh

The build fails with

```
clang++ -x c++ -M -std=c++11 -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal ../../internal/*.c ../../testing/*.c ../../platform/posix/src/clock_gettime.c ../../platform/c++11/src/nsync_semaphore_mutex.cc ../../platform/posix/src/per_thread_waiter.c ../../platform/c++11/src/yield.cc ../../platform/c++11/src/time_rep_timespec.cc ../../platform/c++11/src/nsync_panic.cc \
		  ../../platform/c++11/src/start_thread.cc > dependfile
clang++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal -O -arch i386 -fno-exceptions -stdlib=libc++ -fembed-bitcode  -mios-simulator-version-min=8.0 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk -fPIC -x c++ -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/common.c
clang: error: no such sysroot directory: '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk' [-Werror,-Wmissing-sysroot]
make: *** [common.o] Error 1
```

For device architectures the build uses `iPhoneOS10.3.sdk`, which is there and correct, but not for the sim architecture (see missing SDK). If I manually symlink the current `iPhoneSimulator.sdk` as `iPhoneSimulator10.0.sdk` I get another error

```
clang++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11 -I../../platform/gcc_no_tls -I../../platform/macos -I../../platform/posix -pthread -I../../public -I../../internal -O -arch i386 -fno-exceptions -stdlib=libc++ -fembed-bitcode  -mios-simulator-version-min=8.0 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk -fPIC -x c++ -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/common.c
In file included from ../../internal/common.c:18:
../../platform/c++11/platform.h:19:10: fatal error: 'string.h' file not found
#include <string.h>
         ^
1 error generated.
make: *** [common.o] Error 1
```

MacOS Sierra 10.12.6 (16G29)
Xcode Version 8.3.3 (8E3004b)
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin"
12649,Android buffer overflow exception when running only a certain model above a certain image resolution?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android Lollipop
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.2
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 4.5
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: Running app on android studio

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have met with a very peculiar problem that seem to show that a certain model, Inception V3, which I got from the TF-slim library, seem to consume more memory than usual and cause a bufferoverflow problem like in here:

```
08-28 21:23:40.121 1094-1094/com.example.android.androidevaluateimagenet D/AndroidRuntime: Shutting down VM
08-28 21:23:40.122 1094-1094/com.example.android.androidevaluateimagenet E/AndroidRuntime: FATAL EXCEPTION: main
                                                                                           Process: com.example.android.androidevaluateimagenet, PID: 1094
                                                                                           java.nio.BufferOverflowException
                                                                                               at java.nio.FloatBuffer.put(FloatBuffer.java:444)
                                                                                               at org.tensorflow.Tensor.writeTo(Tensor.java:390)
                                                                                               at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:338)
                                                                                               at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fetch(TensorFlowInferenceInterface.java:301)
                                                                                               at com.example.android.androidevaluateimagenet.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:149)
                                                                                               at com.example.android.androidevaluateimagenet.MainActivity.getInferenceTime(MainActivity.java:267)
                                                                                               at com.example.android.androidevaluateimagenet.MainActivity$2.onClick(MainActivity.java:345)
                                                                                               at android.view.View.performClick(View.java:4763)
                                                                                               at android.view.View$PerformClick.run(View.java:19821)
                                                                                               at android.os.Handler.handleCallback(Handler.java:739)
                                                                                               at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                                               at android.os.Looper.loop(Looper.java:135)
                                                                                               at android.app.ActivityThread.main(ActivityThread.java:5272)
                                                                                               at java.lang.reflect.Method.invoke(Native Method)
                                                                                               at java.lang.reflect.Method.invoke(Method.java:372)
                                                                                               at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:909)
                                                                                               at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:704)
```

Specifically, if you try an image resolution higher than 360x360 in the model, it crashes with the error. The interesting thing is that this doesn't happen for a significantly larger model, the Inception Resnet V2, although it supposedly consumes a lot more memory. With that larger inception resnet v2 model, I can use a resolution of over 400 with no issues. I have rebooted my device, switched it and run it on another identical device and another brand of device, but still there's this problem. I can't exactly locate the issue, but specifically here is what I have traced in the error:


Specific error stack trace:

**TensorFlowImageClassifier.java:**

        inferenceInterface.fetch(outputName, outputs);

**TensorFlowInferenceInterace.java:**

    public void fetch(String var1, float[] var2) {
        this.fetch(var1, FloatBuffer.wrap(var2));
    }

**Tensor.java:**

    public void writeTo(FloatBuffer var1) {
        if(this.dtype != DataType.FLOAT) {
            throw incompatibleBuffer(var1, this.dtype);
        } else {
            ByteBuffer var2 = this.buffer();
            var1.put(var2.asFloatBuffer());
        }
    }

**FloatBuffer.java:**

    public FloatBuffer put(FloatBuffer src) {
        if (src == this)
            throw new IllegalArgumentException();
        int n = src.remaining();
        if (n > remaining())
            throw new BufferOverflowException();
        for (int i = 0; i < n; i++)
            put(src.get());
        return this;
    }

I am not sure why this happens, as everything else works. In fact, if I try a resolution below 360, the Inception V3 model works perfectly fine. Note that I got the checkpoint model from TF-slim and froze it. I believe the method I used to freeze it works well, since there is no problem for all other models except Inception V3. So I can only conclude the problem lies within the layers. But I am not exactly sure how I can find out which layer is causing the problem, or even if it is because of the layers, I'm not sure how to fix it. I have included the layers of Inception V3 and Inception Resnet V2 in order here:

Inception V3 Layers: https://gist.github.com/kwotsin/82016b003057cdcffec1bc9d1ea1e02b
Inception Resnet V2 Layers: https://gist.github.com/kwotsin/893e11fe171af426091b89645d6a86d3

If it is really the problem within the model, then I thought it could be because of a faulty implementation of a certain operation that is causing overly huge memory consumed.

As an alternative fix, is there a way to check and raise the limit of the buffer size for the app to run successfully?"
12648,Variable in Dataset map function is not stably initialized,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macos
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.3
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:0.5.3-homebrew
- **CUDA/cuDNN version**:NO
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
unable to init variable in map, flatmap, fillter function of Dataset API

### Source code / logs
```python
import tensorflow as  tf
from  tensorflow.contrib.data import Dataset

dataset = Dataset.range(100)

def mapf(v):
    temp = tf.Variable(
            tf.random_uniform(
                    [1],
                    minval=0,
                    maxval=10,
                    dtype=tf.int32,
            ),
            trainable=False,
            collections=[tf.GraphKeys.LOCAL_VARIABLES],
            name=""my_var""
    )

    with tf.control_dependencies([tf.variables_initializer([temp])]):
        temp = temp + 1

    return temp


dataset = dataset.map(mapf)
dataset = dataset.batch(1)

iterator = dataset.make_one_shot_iterator()

next_element = iterator.get_next()
local_init = tf.local_variables_initializer()

with tf.Session() as sess:
    sess.run(local_init)
    try:
        while True:
            print(sess.run([next_element]))
    except tf.errors.OutOfRangeError as e:
        print(""ending"")
```
output is not stable:
```
2017-08-28 18:09:31.817313: W tensorflow/core/framework/op_kernel.cc:1192] Failed precondition: Attempting to use uninitialized value my_var
         [[Node: my_var/read = Identity[T=DT_INT32, _class=[""loc:@my_var""]](my_var)]]
Traceback (most recent call last):
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/Users/liyanan/anaconda3/lib/python3.6/contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value my_var
         [[Node: my_var/read = Identity[T=DT_INT32, _class=[""loc:@my_var""]](my_var)]]
         [[Node: IteratorGetNext_2 = IteratorGetNext[output_shapes=[[?,1]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""dataset.py"", line 90, in <module>
    print(sess.run([next_element]))
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/Users/liyanan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value my_var
         [[Node: my_var/read = Identity[T=DT_INT32, _class=[""loc:@my_var""]](my_var)]]
         [[Node: IteratorGetNext_2 = IteratorGetNext[output_shapes=[[?,1]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator)]]


```

OR 

```
[array([[2]], dtype=int32)]
[array([[9]], dtype=int32)]
[array([[1]], dtype=int32)]
[array([[5]], dtype=int32)]
[array([[9]], dtype=int32)]
[array([[9]], dtype=int32)]
.....
ending
```"
12647,tensorflow1.3 bazel-bin/tensorflow/examples/label_image/label_image failed to run.,"step 1:python tensorflow/examples/image_retraining/retrain.py    --architecture=inception_v3    --output_graph=inception_v3.pb  ....  
step2: bazel build tensorflow/examples/label_image
step3: bazel-bin/tensorflow/examples/label_image/label_image
then get the following error:
tensorflow/examples/label_image/main.cc:349] Running model failed: Not found: FeedInputs: unable to find feed output input.
Any step wrong? Could you give me some hint?"
12646,building tensorflow from source / general issue,"Dear Tensorflow team,

I was facing a problem described here:
https://stackoverflow.com/questions/45859371/error-building-tensorflow-on-centos-7

In short words: In the documentation there is no any single word about bazel version need for specific version of Tensorflow. In particular, I could not build a Python package with the newest version of Bazel. Experimentally I found out that that the master version (r1.3) can be built successfully with Bazel not newer than 0.5.1. (with very old versions it also doesn't work). 
So I am kindly asking to mention a Bazel version (as well as other dependencies, such as gcc, etc) to have successful build for the next releases.

I spent a lot of time to find a solution. Probably it will be useful experience for others.

Cheers,
Sergey"
12644,compile tensorlfow c++ object class as /clr project to make the dll  fail !!,"------------------------

### System information
- **windows10**:
- **install tensorflow**: from source by vs2015 
- **TensorFlow version**: r1.2
-**CMake**:  3.8.2 build

### Error List
1. 
>#error instruction:  <condition_variable> is not supported when compiling with /clr or /clr:pure.	CppWrapper	d:\Developer\Microsoft Visual Studio 14.0\VC\include\condition_variable	17	
2. 
>#error instruction:  <mutex> is not supported when compiling with /clr or /clr:pure.	CppWrapper	d:\Developer\Microsoft Visual Studio 14.0\VC\include\mutex	8	
3.
> #error instruction:  <thread> is not supported when compiling with /clr or /clr:pure.	CppWrapper	d:\Developer\Microsoft Visual Studio 14.0\VC\include\thread	8	
4. 
 >e:\tensor\tensorflow\tensorflow\contrib\cmake\build\external\eigen_archive\eigen\src/Core/ArithmeticSequence.h(205): fatal error C1001: An internal error has occurred in the compiler.
"
12641,Improve all-in-memory file copy architecture (Python at least),"Current file copy (at least via Python `tf.gfile.Copy` ([gfile.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py#L22)â†’ [file_io.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L371) â†’ [file_io.i](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.i#L113))) involves copying the source contents into memory, and then writing memory to the destination. For scenarios like https://github.com/tensorflow/tensorflow/issues/12630 which is working with an 11GB asset, this is unacceptable design.

`file_system.h`'s `WritableFile` is not stubbed to allow anything like a streaming, though its `RandomAccessFile` is. (not entirely, entirely, true - i suppose `WriteableFile.Append(const StringPiece& data)` could be employed in a streamable fashion -ish.)

To cull the Python low hanging fruit, at least, please implement [file_io.i](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.i#L113) using a regular streaming design instead of the above described current design."
12640,Is there any  class similar to tf.contrib.learn.monitors.ValidationMonitor in TF-Slim for evalution in logging?,"I want to evaluate my model using validation data when training and show the evalution result in logging,which is similar to tf.contrib.learn.monitors.ValidationMonitor,and i can customize my metrics to evaluate.Is there any one in TF-Slim?"
12639,There are sudden change in validation line,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **Os platform version**: Win10
- **install From conda:
- **tensorflow version** :tensorflow 1.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 5.0
- **GPU model and memory**: GTX1080ti



### Describe the problem
[I resume weights model from local. I find there are sudden change in validation line, but my train line don't this problem. I think maybe there are something wrong with tensorflow source code.]
below is train history graph
(https://github.com/xiaoerlaigeid/face-and-Pedestrian-detection-/blob/master/Figure_1.png)

### Source code / logs
`		if RESUME:
			print('Restoring previously trained model at %s' % MODEL_SAVE_PATH)
			saver.restore(sess, MODEL_SAVE_PATH)

			# Restore previous loss history
			with open('loss_history.p', 'rb') as f:
				loss_history = pickle.load(f)
		else:
			print('Training model from scratch')
			# Variable initialization
			sess.run(tf.global_variables_initializer())

			# For book-keeping, keep track of training and validation loss over epochs, like such:
			# [(train_acc_epoch1, valid_acc_epoch1), (train_acc_epoch2, valid_acc_epoch2), ...]
			loss_history = []`"
12635,"Training doesn't start when using batch_normalization,","I am following the article.
https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization

When I use the below code, the training doesn't start.

`update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)`

Any help will be greatly appreciated."
12632,tf.image.resize_images differs from PIL/scipy.misc.imresize,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux localhost.localdomain 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""                                                              
VERSION_ID=""7""                                                                  
CENTOS_MANTISBT_PROJECT_VERSION=""7""                                             
REDHAT_SUPPORT_PRODUCT_VERSION=""7""     

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.1-0-gb4957ff
tf.COMPILER_VERSION = v1.2.1-0-gb4957ff
Sanity check: array([1], dtype=int32)

- **Python version**: 
2.7

- **Bazel version (if compiling from source)**:
0.4.5

- **CUDA/cuDNN version**:
CUDA8.0/CUDNN6.0

- **GPU model and memory**:
GeForce GTX 1080  8113MiB

- **Exact command to reproduce**:
See Source code

### Describe the problem
As the title states, tf.image.resize_images returns different values compared to imresize in scipy or PIL. This is important because we expect the same behaviour for migrating code originally written using scipy.misc.imresize or PIL.

### Source code / logs
```python
import numpy as np
import tensorflow as tf
from scipy.misc import imresize

image = (255 * np.random.rand(127, 127, 3)).astype(np.uint8)
resize_size = [255, 255]

image_resized = tf.image.resize_images(image, resize_size, method=tf.image.ResizeMethod.BILINEAR)
with tf.Session() as sess:
    image_resized_tf = sess.run(image_resized)

image_resized_np = imresize(image, resize_size, interp='bilinear')

diff = image_resized_np.astype(np.float32) - image_resized_tf.astype(np.float32)
print('resized image diff: {}'.format(np.mean(np.abs(diff))))
# resized image diff: 31.6155033112
```
"
12630,It seems that tf.gfile.Copy can not support larget hdfs file,"It seems that when using ``tf.gfile.Copy(""hdfs://default/some_hdfs_file"",""./test"")``, if `some_hdfs_file` are large file such as 11GB in my case, it will report Exception.  Change to a 3GB file works fine.

I'm using the latest version of tensorflow.


```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 384, in copy
    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)
  File ""/home/x/anaconda2/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://default/tmp/x/x/dmcluster_predict_data_v3/20170817/023645/part-00000
```"
12628,tf.layers.conv2d does not accept higher dimension tensor,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 x64
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: r1.3
- **Python version**:  3.5 amd 64
- **Bazel version (if compiling from source)**: Used binary
- **CUDA/cuDNN version**: CUDA 8.0 + CuDNN 6.0
- **GPU model and memory**:  GeForce GTX 1070 8.00GiB

I am trying to feed a ```tf.layers.conv2d``` with tensor shape [batch_size, size_i_want_to_separate, width, height, channels] but I received ```ValueError: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [1, 4, 640, 377, 3]```. I think it is OK to feed layer with high dimension as long as last three dimension is image.

If it is impossible to add this feature, may I try to merge first two dimensions together and feed it to conv2d layer. Then separate it to origin form but maintain the order? How to do it?   
"
12621,Calculating marginals in CRF,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12619,batch normalization,"These days i have meet some problem about BN layers, code is here, i want run my net on mnist
dataset, it worked when i am training, how when i verify on valiation data or test date,  when i change the state 'is_training'. what is wrong when i am verifing and how can i save mean and val in training state? 

``import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)


#define some weights
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.01)
    return tf.Variable(initial)

def bias_variable(shape):
	initial = tf.constant(0.01, shape=shape)
	return tf.Variable(initial)

def conv2d(input, in_features, out_features, kernel_size, with_bias=False):
	W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])
	conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')
	if with_bias:
		return conv + bias_variable([ out_features ])
	return conv

def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):
    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)
    current = tf.nn.relu(current)
    current = conv2d(current, in_features, out_features, kernel_size)
    current = tf.nn.dropout(current, keep_prob)
    return current

def block(input, layers, in_features, growth, is_training, keep_prob):
    current = input
    features = in_features
    for idx in xrange(layers):
        tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)
        current = tf.concat([current, tmp],3)
        features += growth
    return current, features

def avg_pool(input, s):
    return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')


#define graph

layers =  12
print 'create graph ...'

x = tf.placeholder(tf.float32, [None, 784])
y_label = tf.placeholder(tf.float32, [None, 10])
lr = tf.placeholder(tf.float32)
keep_prob = tf.placeholder(tf.float32)
is_training = tf.placeholder(tf.bool, shape=[])

current = tf.reshape(x, [ -1, 28, 28, 1 ])
current = conv2d(current, 1, 16, 3)

current, features = block(current, layers, 16, 12, is_training, keep_prob)
current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)
current = avg_pool(current, 2)  #14x14
current, features = block(current, layers, features, 12, is_training, keep_prob)
current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)
current = avg_pool(current, 2)#7x7
current, features = block(current, layers, features, 12, is_training, keep_prob)

current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)
current = tf.nn.relu(current)
current = avg_pool(current, 7)
final_dim = features
current = tf.reshape(current, [-1, final_dim])
Wfc = weight_variable([final_dim, 10])      #set classifiers
bfc = bias_variable([10])
y_predict = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_predict))
l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])
weight_decay = 1e-4
#update moving_mean and moving_variance
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy + l2 * weight_decay)

correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # caculate the right numbers

def mytrain():
    print 'train ...'
    saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for epoch in xrange(1, 10):    #train
            if epoch < 150:
                l = 0.5
            elif epoch <200:
                l = 0.1
            else:
                l = 0.01
            print 'epoch: ',epoch
            batch_x, batch_y = mnist.train.next_batch(500)
            _,acc,loss = sess.run([train_step,accuracy,cross_entropy],
                                     feed_dict={x: batch_x, y_label: batch_y, lr:l,is_training: True, keep_prob: 0.8})
            print 'train acc : ',acc,"" loss: "",loss
            #val
            batch_x_val = mnist.validation.images
            batch_y_val = mnist.validation.labels
            acc,loss = sess.run([accuracy,cross_entropy],feed_dict={x: batch_x_val, y_label: batch_y_val, is_training: False, keep_prob: 0.8})
            print 'val acc : ',acc,' loss: ',loss
        saver.save(sess, 'temp/densenet.ckpt')

def mytest():
    print 'test ...'
    saver = tf.train.Saver()
    with tf.Session() as sess:
        saver.restore(sess, './temp/densenet.ckpt')
        x_val = mnist.validation.images
        y_val = mnist.validation.labels
        val_results = sess.run(accuracy,
                                 feed_dict={x: x_val, y_label: y_val, is_training: True, keep_prob: 1.})
        print 'val acc: ', val_results
        right = 0
        for i in range(100):
            x_test, y_test = mnist.validation.next_batch(500)
            test_results = sess.run(accuracy,
                                    feed_dict={x: x_test, y_label: y_test, is_training: False, keep_prob: 1.})
            #right = right + test_results
        print 'test:  acc: ', test_results

if __name__ == '__main__':
    mytrain()
    mytest()

"
12618,Android Demo - SpeechActivity - Fatal Exception,"Samsung Galaxy J1 - Android ver. 5.1.1

08-26 11:56:55.226: I/TensorFlowInferenceInterface(14094): Model load took 41ms, TensorFlow version: 1.3.0
08-26 11:56:55.226: I/TensorFlowInferenceInterface(14094): Successfully loaded model from 'file:///android_asset/conv_actions_frozen.pb'
08-26 11:56:55.231: D/AndroidRuntime(14094): Shutting down VM
08-26 11:56:55.231: E/AndroidRuntime(14094): **FATAL EXCEPTION**: main
08-26 11:56:55.231: E/AndroidRuntime(14094): Process: org.tensorflow.demo, PID: 14094
08-26 11:56:55.231: E/AndroidRuntime(14094): java.lang.NoSuchMethodError: **No virtual method requestPermissions**([Ljava/lang/String;I)V in class Lorg/tensorflow/demo/SpeechActivity; or its super classes (declaration of 'org.tensorflow.demo.SpeechActivity' appears in /data/app/org.tensorflow.demo-1/base.apk)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at org.tensorflow.demo.SpeechActivity.requestMicrophonePermission(SpeechActivity.java:158)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:153)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.Activity.performCreate(Activity.java:6609)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1134)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3086)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3243)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.ActivityThread.access$1000(ActivityThread.java:218)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1718)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.os.Handler.dispatchMessage(Handler.java:102)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.os.Looper.loop(Looper.java:145)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at android.app.ActivityThread.main(ActivityThread.java:6917)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at java.lang.reflect.Method.invoke(Native Method)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at java.lang.reflect.Method.invoke(Method.java:372)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1404)
08-26 11:56:55.231: E/AndroidRuntime(14094): 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1199)

"
12613,tensorflow installation error,"I have installed tensorflow and keras. I have used virtualenviroment given in tensorflow website. It is showing the below error when I try to import keras with tensorflow as backend


 CPU_COUNT = psutil.cpu_count()
AttributeError: 'module' object has no attribute 'cpu_count'

tensorflow version = Version: 1.3.0

keras = Version: 2.0.7

dask = Version: 0.15.0

pandas = Version: 0.20.3

For detailed error, attaching the image
![untitled](https://user-images.githubusercontent.com/18217467/29738791-2213765e-8a4b-11e7-8305-b3fa516097f1.png)
"
12612,Eager execution with string tensors.,"@alextp String tensors seem to cause lots of errors for me, when using the eager execution API, that do not occur when using normal ops and executing them using a session.

The way I'm creating string tensors is by allocating a `TF_Tensor` with the right data type, shape, and size (where I use `TF_StringEncodedSize` to get the byte sizes of each element). Then, I encode the strings using `TF_StringEncode` and write them in the tensor buffer along with the offset table, as described [here](https://github.com/tensorflow/tensorflow/blob/752dcb61ef7a8fd6555909dc37c1f2a2e5792227/tensorflow/c/c_api.h#L211). Afterwards, I call `TFE_NewTensorHandle` to get an eager tensor handle and I delete the `TF_Tensor` object. This works fine for normal op construction and execution using sessions, but it fails for eager execution. Often I get a `check failed: IsAligned()` error from `tensor.cc`, but other times the whole application crashes, such as in the example that follows:

```scala
val t = Tensor.fill(STRING, Shape())(""test"")  // The generated tensor is good. 
                                              // I can access its elements and use it 
                                              // with session execution.
val tt = tfe.stack(Seq(t, t))  // I can no longer access the tensor elements correctly 
                               // and when I try to slice it I get either errors (crashing) 
                               // or empty strings.
tt.unstack(2)                  // Crashes with the error shown below.
```

This is run using my Scala API but I hope that the error I get may help resolve it. In the error stacktrace I get this:

```
Stack: [0x0000700011571000,0x0000700011671000],  sp=0x000070001166cbf0,  free space=1006k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libsystem_platform.dylib+0x15da]  _platform_memmove$VARIANT$Nehalem+0x1da
C  [libc++.1.dylib+0x3af29]  _ZNSt3__112basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEC2ERKS5_+0x8d
C  [libtensorflow.so+0x502fdb]  _ZNSt3__110__function6__funcIZN5Eigen8internal14TensorExecutorIKNS2_14TensorAssignOpINS2_9TensorMapINS2_6TensorINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEELi1ELi1ElEELi16ENS2_11MakePointerEEEKNS2_15TensorSlicingOpIKNS2_6DSizesIlLi1EEESK_KNS6_INS7_IKSD_Li1ELi1ElEELi16ESF_EEEEEENS2_16ThreadPoolDeviceELb0EE3runERSS_RKST_EUlllE_NSB_ISY_EEFvllEEclEOlS12_+0x5b
C  [libtensorflow.so+0x1aa30d]  _ZNK5Eigen16ThreadPoolDevice11parallelForElRKNS_12TensorOpCostENSt3__18functionIFllEEENS5_IFvllEEE+0xbd
C  [libtensorflow.so+0x502ebf]  _ZN5Eigen8internal14TensorExecutorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorINSt3__112basic_stringIcNS5_11char_traitsIcEENS5_9allocatorIcEEEELi1ELi1ElEELi16ENS_11MakePointerEEEKNS_15TensorSlicingOpIKNS_6DSizesIlLi1EEESI_KNS3_INS4_IKSB_Li1ELi1ElEELi16ESD_EEEEEENS_16ThreadPoolDeviceELb0EE3runERSQ_RKSR_+0x11f
C  [libtensorflow.so+0x5a06f6]  _ZN10tensorflow22HandleStridedSliceCaseIN5Eigen16ThreadPoolDeviceENSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEELi1EEEvPNS_15OpKernelContextERKNS_3gtl10ArraySliceIxEESG_SG_RKNS_11TensorShapeEbPNS_6TensorE+0x136
C  [libtensorflow.so+0x58b082]  _ZN10tensorflow14StridedSliceOpIN5Eigen16ThreadPoolDeviceENSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEEE7ComputeEPNS_15OpKernelContextE+0x5f2
C  [libtensorflow.so+0x1c60b9b]  _ZN10tensorflow16ThreadPoolDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x13b
C  [libtensorflow.so+0x8585]  _ZN10tensorflow15KernelAndDevice3RunEPNSt3__16vectorINS_6TensorENS1_9allocatorIS3_EEEES7_+0x285
C  [libtensorflow.so+0x47ce]  TFE_Execute+0x36e
```

**EDIT:** The following error seems to actually come up with all tensor data types, for various ops (such as the `""Unpack""` op).

Another time I also got this error:

```
C  [libtensorflow.so+0x20f0]  TFE_TensorHandleDeviceName+0x0
```"
12611,tf.gfile.FastGFile can't read french path,"tf.gfile.FastGFile(r'F:\train\Rubus_pedatifolius_GenÃ©v\4000.jpg', 'rb').read() failed,
but, tf.gfile.FastGFile(r'C:\Users\d\Desktop\4000.jpg', 'rb').read() successed!
so, is it  a bug or  tf.gfile.FastGFile can't read french path?"
12610,tf.estimator.inputs.numpy_input_fn does not accept dict as labels.,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 x64
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: r1.3
- **Python version**:  3.5 amd 64
- **Bazel version (if compiling from source)**: Used binary
- **CUDA/cuDNN version**: CUDA 8.0 + CuDNN 6.0
- **GPU model and memory**:  GeForce GTX 1070 8.00GiB


According to the ```tf.estimator.Estimator``` [document](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator), estimator accept dict as labels input. However, when I creating a ```tf.estimator.inputs.numpy_input_fn``` as:
```python
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": samples.astype(np.float32)},
    y={'go_action': np.full(samples.shape[0], 1), 'operation': ops.astype(np.float32)},
    batch_size=100,
    num_epochs=None,
    shuffle=True)
``` 

then feed it to ```estimator``` like:
```python
action_estimator.train(
    input_fn=train_input_fn,
    steps=20000,
    hooks=[logging_hook])

```

it throw me an error:
```
Traceback (most recent call last):
  File ""G:/Python/onmyoji-hacker/primary/cnnopnet.py"", line 120, in <module>
    hooks=[logging_hook])
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 241, in train
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 628, in _train_model
    input_fn, model_fn_lib.ModeKeys.TRAIN)
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 499, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 585, in _call_input_fn
    return input_fn(**kwargs)
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\inputs\numpy_io.py"", line 109, in input_fn
    if len(set(v.shape[0] for v in ordered_dict_x.values())) != 1:
  File ""C:\Python\Python35\lib\site-packages\tensorflow\python\estimator\inputs\numpy_io.py"", line 109, in <genexpr>
    if len(set(v.shape[0] for v in ordered_dict_x.values())) != 1:
AttributeError: 'dict' object has no attribute 'shape'

```

"
12608,gather_nd bounds checking not working,"When using `gather_nd`, sometimes out-of-bounds `indices` lead to errors (bounds checking -- the expected behavior) and sometimes it seems to just read zeros. I expect it reading just other memory from the GPU, but I've never observed anything _other_ than zeros so I'm not sure. When I run on the CPU the bounds seem to be appropriately checked i.e. I get the errors desired. Here's some example code:

```python
import tensorflow as tf
sess = tf.Session()
print(sess.run(tf.gather_nd(tf.zeros([5,5,5]) + 1, [[6,6,6]])))
print(sess.run(tf.gather_nd(tf.zeros([1,5]) + 1, [-50000000000000000])))
print(sess.run(tf.gather_nd(tf.reshape(tf.range(5*5*5), [5,5,5]), [[6,6,6]])))
```

The first two print statements execute successfully, which is a bug: the indices are clearly out of range, and the arrays are clearly all 1's; but instead it returns an appropriately shaped array of 0's. (The +1 is not necessary to trigger the bug, but highlights that it's drawing from incorrect memory). The third line, for some reason, has the bounds checking operate correctly, and says that -- yes -- the index [6,6,6] is not in the bounds. It appears to something based on what the previous op is, _maybe_? Where some ops, such as `stack`, allow me to go outside the bounds, while others such as `reshape` don't. Here's an example interactive session to show the output I get.

```python
Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> sess = tf.Session()
2017-08-25 17:10:43.788433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 17:10:43.788455: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 17:10:43.788463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 17:10:43.788466: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 17:10:43.788470: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 17:10:43.919384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-25 17:10:43.919779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.645
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.59GiB
2017-08-25 17:10:43.919795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-08-25 17:10:43.919801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-08-25 17:10:43.919814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
>>> sess.run(tf.gather_nd(tf.zeros([5,5,5]) + 1, [[6,6,6]]))
array([ 0.], dtype=float32)
>>> sess.run(tf.gather_nd(tf.zeros([1,5]) + 1, [-50000000000000000]))
array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)
>>> sess.run(tf.gather_nd(tf.reshape(tf.range(5*5*5), [5,5,5]), [[6,6,6]]))
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]

Caused by op 'GatherNd_2', defined at:
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1338, in gather_nd
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]
```

Version info:
Linux Mint 4.4.0-53-generic x86_64
Python version 3.5.2
CUDA version release 8.0, V8.0.61
cuDNN version 6.0.21
Tensorflow version v1.3.0-rc2-20-g0787eee 1.3.0
nvidia drivers version 375

"
12605,Impossible to change variable inside checkpoint,"I have a checkpoint file and I want to change (increase) shapes of some Variables from this checkpoint.
It seems to be impossible using tf.assign 
Tried to use `tf.assign()`:

    `for var in tf.global_variables():
         if var.name == ""22-convolutional/biases:0"":
             assign = tf.assign(var, a, validate_shape=False)
             sess.run(assign)`

And then, when I am trying to execute 

`sess.run(tf.global_variables_initializer())`

I have an error

`Assign requires shapes of both tensors to match. lhs shape= [1,1,1024,60] rhs shape= [1,1,1024,55]
[[Node: 22-convolutional/kernel/Adam_1/Assign = Assign[T=DT_FLOAT, 
_class=[""loc:@22-convolutional/kernel""], use_locking=true, validate_shape=true, 
_device=""/job:localhost/replica:0/task:0/cpu:0""](22-convolutional/kernel/Adam_1, zeros_51)]]`

So, is it really impossible in TF? Could it be a feature request?"
12604,`export_meta_graph` fails if graph has no variables,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8
- **GPU model and memory**:
1060
- **Exact command to reproduce**:
```
import tensorflow as tf

x = tf.placeholder(""float32"")
y = x + x
tf.train.export_meta_graph(""test.meta"")
tf.reset_default_graph()
g = tf.train.import_meta_graph(""test.meta"")
```
yields

**INFO:tensorflow:Saver not created because there are no variables in the graph to restore**

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The export and import meta_graph features fail if a graph has no variables.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

x = tf.placeholder(""float32"")
y = x + x
tf.train.export_meta_graph(""test.meta"")
tf.reset_default_graph()
g = tf.train.import_meta_graph(""test.meta"")
```"
12601,Produce mask when padding batches,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: binary via pip
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See source code below

### Describe the problem
Currently, the Dataset API (as well as the older queue-based API) allows for batches to be padded, but there is no way to determine which values were inserted for padding in the resulting output tensors unless care is taken to pick a padding value that will never occur in the original dataset. It would be useful if this could be indicated via a separate mask tensor that indicates this rather than having the user compute it manually (if it's even possible for their particular data). E.g., this is useful for sequential tagging (like part-of-speech tagging), where each example is a multi-word sentence and each label is actually a sequence of labels. Computing the correct losses requires knowledge of which values were just included for padding.

Currently, I do this using Fuel, which, for a given batch, gives me one array for the input data, one for the label, and then one mask each for the input and the label. The masks are binary and of the same shape as the input/label array, respectively. I then feed these into TensorFlow via the feed_dict mechanism.

Here's an example from the programmer's guide:
```
dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=[None])

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]
print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],
                               #      [5, 5, 5, 5, 5, 0, 0],
                               #      [6, 6, 6, 6, 6, 6, 0],
                               #      [7, 7, 7, 7, 7, 7, 7]]
```
With a mask, this might look like:
```
dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=[None], mask=True)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==> ([[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]],
                               #      [[0, 0, 0], [1, 0, 0], [1, 1, 0], [1, 1, 1]])
print(sess.run(next_element))  # ==> ([[4, 4, 4, 4, 0, 0, 0],
                               #       [5, 5, 5, 5, 5, 0, 0],
                               #       [6, 6, 6, 6, 6, 6, 0],
                               #       [7, 7, 7, 7, 7, 7, 7]],
                               #      [[1, 1, 1, 1, 0, 0, 0],
                               #       [1, 1, 1, 1, 1, 0, 0],
                               #       [1, 1, 1, 1, 1, 1, 0],
                               #       [1, 1, 1, 1, 1, 1, 1]])
```

(Obviously, this particular example is simple to compute manually, but imagine the case where there is no obvious padding value to pick that wouldn't occur in the data.)"
12599,accept,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12598,data dependent variable initialization in tf 1.3.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux
- **TensorFlow installed from (source or binary)**: `pip install tensorflow-gpu`
- **TensorFlow version (use command below)**: `v1.3.0-rc2-20-g0787eee 1.3.0`
- **Python version**: `Python 3.6.2`
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: `cuda 8.0.61`/`cudnn6 6.0.21`
- **GPU model and memory**: GeForce GTX 980M 8GB
- **Exact command to reproduce**:

### Describe the problem
After updating to tf 1.3 my usual routines for data dependent initialization of variables are broken. They take massive amounts of time and memory. I have written a minimal example to reproduce the problem (see below). In fact, the example suggests that the time increases exponentially with the number of layers, i.e. it takes takes roughly 6, 12 and 24 seconds to initialize 10, 11 and 12 layers, respectively. On a different machine running on tf 1.2.1 the same code initializes 100 layers in less than a second yet it produces the same, correct values. Similiarly, the (host) memory usage explodes. I should also mention that the issue appears when defining the graph, i.e. before any session is opened, and not during execution of the initialization operation.

I would be glad if someone can point out a better way of doing the kind of data dependent initialization shown in the example (using two passes is somewhat annoying) but the issue is that the update made this method completely unusable. [Traceback from keyboard interrupt](https://github.com/tensorflow/tensorflow/files/1251882/traceback.txt) suggest that the recently introduced [`_build_initializer_expr`](https://github.com/tensorflow/tensorflow/blob/ebc421daf2c812fdfc3007294741c6c07f4957c3/tensorflow/python/ops/variables.py#L763) might be involved.

### Source code / logs

    import time
    import tensorflow as tf

    def layer(x, name, init):
        with tf.variable_scope(name, reuse = not init):
            initializer = x
            b = tf.get_variable('b', dtype=tf.float32, initializer = initializer)
            # without next if we get error
            # 'Attempting to use uninitialized variable.'
            # for layer 1
            if init:
                return x + b.initialized_value()
            else:
                return x + b


    if __name__ == ""__main__"":
        n_layers = 10

        print(""tensorflow {} {}"".format(tf.GIT_VERSION, tf.VERSION))
        print(""n_layers {}"".format(n_layers))

        def _pass(x, init):
            h = x
            for i in range(n_layers):
                h = layer(h, ""layer_{}"".format(i), init)
            return h
        
        # first pass for initialization
        x = tf.constant([1.0])
        t = time.time()
        h = _pass(x, True)
        print(""init pass {:.4}"".format(time.time() - t))

        # second pass as usual
        t = time.time()
        h = _pass(x, False)
        print(""next pass {:.4}"".format(time.time() - t))

        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            print(""final value {}"".format(h.eval()))


This is basically the data dependent initialization method as described in [OpenAI's weight norm code](https://github.com/openai/weightnorm) stripped down to the bare minimum where the problem occurs. This weight normalization code as well as the [PixelCNN++ code](https://github.com/openai/pixel-cnn) are also affected by this.

Output on 1.3.0:

    tensorflow v1.3.0-rc2-20-g0787eee 1.3.0
    n_layers 10
    init pass 5.934
    next pass 0.003919
    final value [ 1024.]

Output on 1.2.1 (different machine):

    tensorflow 1.2.1
    n_layers 10
    init pass 0.1085
    next pass 0.00644
    final value [ 1024.]




"
12597,TF-Slim can't exclude variables when import data from checkpoint files,"I exclude variables that don't exist in the checkpoint file by using the he func 

          variables_to_restore = slim.get_variables_to_restore(exclude=exclude_set) 

these variables come from slim.train.MomentumOptimizer() which include  '../momentum:0' in name.But the program seems to still try to find them in the checkpoint.This really confusing me. Anyone can help me?


The link of stackoverflow is:
https://stackoverflow.com/questions/45877948/tf-slim-cant-exclude-variables-when-import-data-from-checkpoint-files"
12596,Feature request: QueueRunner for C++ API that initializes from queues and ops and start with ClientSession,"Right now there is an implementation of QueueRunner in ""tensorflow/cc/training/queue_runner.h"". It's created from a QueueRunnerDef that is initialized with string names rather than queues and ops objects. QueueRunner's current implementation is also started by using the ""Session"" low level class rather than the newer ""ClientSession"" class."
12595,[Slim]No cross validation in inception?,"
I run the code for fine tune inception in slim. but I find that only train split is used when training. so is there no  cross validation in inception?  how should I stop training(just at the max_steps)? what's validation split for? (evaluate the trained model?)  "
12593,Android Demo - Bug - Exception in TF Detect activity,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
At build.gradle (Module: android)
def nativeBuildSystem = 'none'
At CameraActivity.java:
useCamera2API = true;
Suggested by @andrewharp at #12431
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows version: 10 Pro
Android version: 6.0.1
Demo version: commit 18e4590
- **TensorFlow installed from (source or binary)**:
x
- **TensorFlow version (use command below)**:
x
- **Python version**:
x 
- **Bazel version (if compiling from source)**:
x
- **CUDA/cuDNN version**:
x
- **GPU model and memory**:
x
- **Exact command to reproduce**:
x

### Describe the problem
Running the activity TF Detect after installing the project with Android Studio results in an IllegalArgumentException being thrown.

The problem has been noticed before with other configurations at issue #12431 and pull request #10771 and tested recently by @ArtsiomCh  using CMake and Ubuntu.

### Source code / logs
```
08-25 11:46:10.596 9185-9185/? I/art: Late-enabling -Xcheck:jni
08-25 11:46:10.636 9185-9185/? D/TidaProvider: TidaProvider()
08-25 11:46:10.722 9185-9185/org.tensorflow.demo D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.DetectorActivity@df7138f
08-25 11:46:10.749 9185-9185/org.tensorflow.demo I/CameraManagerGlobal: Connecting to camera service
08-25 11:46:10.784 9185-9185/org.tensorflow.demo I/tensorflow: CameraActivity: Camera API lv2?: true
08-25 11:46:10.788 9185-9185/org.tensorflow.demo D/tensorflow: CameraActivity: onStart org.tensorflow.demo.DetectorActivity@df7138f
08-25 11:46:10.788 9185-9185/org.tensorflow.demo D/tensorflow: CameraActivity: onResume org.tensorflow.demo.DetectorActivity@df7138f
08-25 11:46:10.796 9185-9217/org.tensorflow.demo D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true
08-25 11:46:10.801 9185-9185/org.tensorflow.demo D/ActivityThreadInjector: clearCachedDrawables.
08-25 11:46:10.840 9185-9217/org.tensorflow.demo I/Adreno: QUALCOMM build                   : a7823f5, I59a6815413
                                                           Build Date                       : 09/23/16
                                                           OpenGL ES Shader Compiler Version: XE031.07.00.00
                                                           Local Branch                     : mybranch22028469
                                                           Remote Branch                    : quic/LA.BR.1.3.3_rb2.26
                                                           Remote Branch                    : NONE
                                                           Reconstruct Branch               : NOTHING
08-25 11:46:10.846 9185-9217/org.tensorflow.demo I/OpenGLRenderer: Initialized EGL, version 1.4
08-25 11:46:10.858 9185-9185/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480
08-25 11:46:10.858 9185-9185/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Valid preview sizes: [1280x960, 1280x720, 864x480, 640x640, 800x480, 720x480, 640x480, 480x640]
08-25 11:46:10.859 9185-9185/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [832x468, 768x432, 576x432, 640x360, 480x360, 480x320, 384x288, 352x288, 320x240, 240x320, 240x160, 176x144]
08-25 11:46:10.859 9185-9185/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Exact size match found.
08-25 11:46:10.861 9185-9185/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: ???
08-25 11:46:10.861 9185-9185/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: person
... Etc.
08-25 11:46:10.863 9185-9185/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: hair drier
08-25 11:46:10.863 9185-9185/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: toothbrush
08-25 11:46:10.864 9185-9185/org.tensorflow.demo I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
08-25 11:46:10.864 9185-9185/org.tensorflow.demo E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
08-25 11:46:10.864 9185-9185/org.tensorflow.demo I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
08-25 11:46:10.896 9185-9185/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
08-25 11:46:12.133 9185-9185/org.tensorflow.demo I/TensorFlowInferenceInterface: Model load took 1213ms, TensorFlow version: 1.2.0
08-25 11:46:12.136 9185-9185/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'
08-25 11:46:12.137 9185-9185/org.tensorflow.demo I/tensorflow: DetectorActivity: Sensor orientation: 90, Screen orientation: 0
08-25 11:46:12.137 9185-9185/org.tensorflow.demo I/tensorflow: DetectorActivity: Initializing at size 640x480
08-25 11:46:12.139 9185-9185/org.tensorflow.demo W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.
08-25 11:46:12.148 9185-9185/org.tensorflow.demo I/CameraManager: Using legacy camera HAL.
08-25 11:46:12.280 9185-9216/org.tensorflow.demo I/tensorflow: CameraConnectionFragment: Opening camera preview: 640x480
08-25 11:46:12.285 9185-9216/org.tensorflow.demo I/CameraDeviceState: Legacy camera service transitioning to state CONFIGURING
08-25 11:46:12.286 9185-9217/org.tensorflow.demo E/HAL: hw_get_module_by_class: module name gralloc
08-25 11:46:12.286 9185-9290/org.tensorflow.demo I/RequestThread-0: Configure outputs: 2 surfaces configured.
08-25 11:46:12.286 9185-9217/org.tensorflow.demo E/HAL: hw_get_module_by_class: module name gralloc
08-25 11:46:12.286 9185-9290/org.tensorflow.demo D/Camera: app passed NULL surface
08-25 11:46:12.297 9185-9185/org.tensorflow.demo I/Choreographer: Skipped 85 frames!  The application may be doing too much work on its main thread.
08-25 11:46:12.309 9185-9216/org.tensorflow.demo I/CameraDeviceState: Legacy camera service transitioning to state IDLE
08-25 11:46:12.313 9185-9216/org.tensorflow.demo I/RequestQueue: Repeating capture request set.
08-25 11:46:12.314 9185-9290/org.tensorflow.demo W/LegacyRequestMapper: convertRequestMetadata - control.awbRegions setting is not supported, ignoring value
08-25 11:46:12.314 9185-9290/org.tensorflow.demo W/LegacyRequestMapper: Only received metering rectangles with weight 0.
08-25 11:46:12.314 9185-9290/org.tensorflow.demo W/LegacyRequestMapper: Only received metering rectangles with weight 0.
08-25 11:46:12.493 9185-9291/org.tensorflow.demo I/CameraDeviceState: Legacy camera service transitioning to state CAPTURING
08-25 11:46:12.515 9185-9216/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 0 at size 307200
08-25 11:46:12.516 9185-9216/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 1 at size 153599
08-25 11:46:12.516 9185-9216/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 2 at size 153599
08-25 11:46:12.517 9185-9216/org.tensorflow.demo E/tensorflow: ObjectTracker: libtensorflow_demo.so not found, tracking unavailable
08-25 11:46:12.517 9185-9216/org.tensorflow.demo I/tensorflow: MultiBoxTracker: Initializing ObjectTracker: 640x480
08-25 11:46:12.517 9185-9216/org.tensorflow.demo E/tensorflow: ObjectTracker: Native object tracking support not found. See tensorflow/examples/android/README.md for details.
08-25 11:46:12.525 9185-9216/org.tensorflow.demo E/tensorflow: MultiBoxTracker: Object tracking support not found. See tensorflow/examples/android/README.md for details.
08-25 11:46:12.544 9185-9216/org.tensorflow.demo E/art: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420ToARGB8888(byte[], byte[], byte[], int[], int, int, int, int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888___3B_3B_3B_3IIIIIIZ)
08-25 11:46:12.545 9185-9216/org.tensorflow.demo W/tensorflow: ImageUtils: Native YUV -> RGB implementation not found, falling back to Java implementation
08-25 11:46:12.717 9185-9217/org.tensorflow.demo V/RenderScript: 0x558f6c1290 Launching thread(s), CPUs 6
08-25 11:46:13.060 9185-9215/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[image_tensor], outputs:[detection_boxes, detection_scores, detection_classes, num_detections]
                                                                                 
                                                                                 --------- beginning of crash
08-25 11:46:13.060 9185-9215/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference
                                                                   Process: org.tensorflow.demo, PID: 9185
                                                                   java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                     device='CPU'; T in [DT_FLOAT]
                                                                     device='CPU'; T in [DT_INT32]
                                                                     device='GPU'; T in [DT_STRING]
                                                                     device='GPU'; T in [DT_BOOL]
                                                                     device='GPU'; T in [DT_INT32]
                                                                     device='GPU'; T in [DT_FLOAT]
                                                                   
                                                                   	 [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater)]]
                                                                       at org.tensorflow.Session.run(Native Method)
                                                                       at org.tensorflow.Session.access$100(Session.java:48)
                                                                       at org.tensorflow.Session$Runner.runHelper(Session.java:295)
                                                                       at org.tensorflow.Session$Runner.run(Session.java:245)
                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:142)
                                                                       at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:158)
                                                                       at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:340)
                                                                       at android.os.Handler.handleCallback(Handler.java:742)
                                                                       at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                       at android.os.Looper.loop(Looper.java:154)
                                                                       at android.os.HandlerThread.run(HandlerThread.java:61)
08-25 11:46:13.061 9185-9215/org.tensorflow.demo E/MQSEventManagerDelegate: failed to get MQSService.
08-25 11:46:13.080 9185-9185/org.tensorflow.demo I/RequestQueue: Repeating capture request cancelled.
08-25 11:46:13.194 9185-9327/org.tensorflow.demo E/BufferQueueProducer: [SurfaceTexture-1-9185-1] cancelBuffer: BufferQueue has been abandoned
```"
12592,Tensorflow internally creates operators,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I made custom distributed inception code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
TensorFlow installed from (source or binary): Unmodified source with RDMA Verbs enabled
TensorFlow version (use command below): 1.3.0-rc1
Python version: 2.7.12
Bazel version (if compiling from source): 0.5.1
CUDA/cuDNN version: 8.0/5.1.5
GPU model and memory: NVIDIA TITAN Xp PCIe 12GB (4 per node)

I tried to run distributed inception, and I removed some nodes by breaking control dependencies in the graph.  However, tensorflow creates operators such as ""inception_v3/mixed_17x17x1280a/branch7x7x3/Conv_2/BatchNorm/AssignMovingAvg_1_S21149"" internally and called it. Why this kind of operators are created and is there any way to prevent it? "
12591,Error: tensorflow.python.framework.errors_impl.InternalError: Unable to get element from the feed as bytes.,"I have met this error:

> Traceback (most recent call last):
  File ""demo_pb.py"", line 25, in <module>
    result_dict = news_demo.newsAggreg({image_path})
  File ""/home/rszj/liutao/news_aggreg/news_demo_pb.py"", line 32, in newsAggreg
    predict = news_predict.run(images_path)
  File ""/home/rszj/liutao/news_aggreg/news_predict_pb.py"", line 201, in run
    predict = sess.run(pred, feed_dict={x: imgs, is_train:False})
  File ""/home/rszj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/rszj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/rszj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/rszj/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Unable to get element from the feed as bytes.

Background: 
    Firstly, I have a simple cnn program, and I build a model.ckpt files. 
    Secondly, I make be ckpt converting to pb.
    Thridly, I used ckpt model doing predict and pb model doing predict.
    In simple cnn program, ckpt and pb all do normal predict by using `predict = sess.run(pred, feed_dict={x: imgs, is_train:False})` .
   
   However, when I using upper method to cnn-rnn program, I could have a normal predict result using ckpt model file, but it had a `tensorflow.python.framework.errors_impl.InternalError: Unable to get element from the feed as bytes.` problem using pb.
   I can't solve this problem, I need help!
  Please!
  Thanks!"
12590,Cannot build TF 1.3/1.2 for iOS,"I've built TF 1.1 before without problems as follows

    git clone --branch r1.1 https://github.com/tensorflow/tensorflow.git
    ./<makefile_dir>/build_all_ios.sh

If I change the branch to `r1.3` it stops with a segmentation fault. Also seems to happen with `r1.2`.

```
...
gcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -mios-simulator-version-min=8.0 -arch x86_64 -fembed-bitcode -D__thread= -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.3.sdk -MT <REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_X86_64/tensorflow/core/kernels/reduction_ops_common.o -MMD -MP -MF <REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/dep/ios_X86_64//tensorflow/core/kernels/reduction_ops_common.Td -I. -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/downloads/ -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/kernels/reduction_ops_common.cc -o <REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_X86_64/tensorflow/core/kernels/reduction_ops_common.o
clang: error: unable to execute command: Segmentation fault: 11
clang: error: clang frontend command failed due to signal (use -v to see invocation)
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
clang: note: diagnostic msg: PLEASE submit a bug report to http://developer.apple.com/bugreporter/ and include the crash backtrace, preprocessed source, and associated run script.
clang: note: diagnostic msg: 
********************

PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:
Preprocessed source(s) and associated run script(s) are located at:
clang: note: diagnostic msg: /var/folders/xy/txmq57f90mq40d8ltjxhkqgm0000gn/T/reduction_ops_sum-5e29c4.cpp
clang: note: diagnostic msg: /var/folders/xy/txmq57f90mq40d8ltjxhkqgm0000gn/T/reduction_ops_sum-5e29c4.sh
clang: note: diagnostic msg: Crash backtrace is located in
clang: note: diagnostic msg: /Users/era/Library/Logs/DiagnosticReports/clang_<YYYY-MM-DD-HHMMSS>_<hostname>.crash
clang: note: diagnostic msg: (choose the .crash file that corresponds to your crash)
clang: note: diagnostic msg: 

********************
make: *** [<REDACTED>/tfbuild/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_X86_64/tensorflow/core/kernels/reduction_ops_sum.o] Error 254
make: *** Waiting for unfinished jobs....
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
```

System MacOS Sierra 10.12.6 (16G29)

It also seems to fail with other error messages in different compile runs (on different files of course) like `clang: error: unable to execute command: Illegal instruction: 4` or malloc/free errors."
12589,ValueError: graph_def is invalid at node u'a/Assign': Input tensor 'a:0' Cannot convert a tensor of type string to an input of type string_ref.,"I want to change the input directory for validation using `input_map` in tf.train.import_meta_graph. Here is the demo code

For training:
```
from tensorflow.core.framework import variable_pb2
from tensorflow.python.framework import ops
from tensorflow.python.ops import variables
from tensorflow.python.framework.ops import register_proto_function

register_proto_function(
    ops.GraphKeys.LOCAL_VARIABLES,
    proto_type=variable_pb2.VariableDef,
    to_proto=variables.Variable.to_proto,
    from_proto=variables.Variable.from_proto)
a = tf.train.match_filenames_once('data/wxtrain/*', name='a')
with tf.Session() as ss:
    ss.run(tf.group(tf.local_variables_initializer(),tf.global_variables_initializer()))
    saver = tf.train.Saver(tf.local_variables())
    saver.save(ss, 'log/model.ckpt')
    print(ss.run(a, feed_dict={a:'1.'}))
```
For testing:
```
from tensorflow.core.framework import variable_pb2
from tensorflow.python.framework import ops
from tensorflow.python.ops import variables
from tensorflow.python.framework.ops import register_proto_function

register_proto_function(
    ops.GraphKeys.LOCAL_VARIABLES,
    proto_type=variable_pb2.VariableDef,
    to_proto=variables.Variable.to_proto,
    from_proto=variables.Variable.from_proto)
newa = tf.train.match_filenames_once('data/wxtest/*', name='newa')
latest = tf.train.latest_checkpoint('log/')
saver = tf.train.import_meta_graph(latest+'.meta', 
                                   input_map={'a:0':newa},
                                   import_scope='import')
with tf.Session() as ss:
    ss.run(tf.group(tf.local_variables_initializer(),tf.global_variables_initializer()))
    saver.restore(ss, latest)
    print(""resume "", latest)
    print(ss.run(newa, feed_dict={newa:'2.'}))
```
However, I encounter the following errors:

> ValueError: graph_def is invalid at node u'a/Assign': Input tensor 'a:0' Cannot convert a tensor of type string to an input of type string_ref.

I use tensorflow 1.2. It seems to be a bug."
12588,Error with tf.losses.sparse_softmax_cross_entropy: weights?,"I use the function tf.losses.sparse_softmax_cross_entropy for my unbalanced dataset but i get error when using it. My question is if my labels in one batch have shape (8,4096) , my prediction or logits have shape (8,4096,14) and I have 14 classes. What should the shape of my weights?

Another question: should i have one specific weight for each class or a specific weight for each sample?
Thanks"
12587,[building tensorflow with bazel]Error:Constants.h:429:2: error: #error The preprocessor symbol 'Success' is defined,"- Environment information:

Ubuntu version: 14.04
bazel version: 0.5.1
tensorflow version:  r 1.1
cuda version: 8.0.44
cudnn version: 5.1.5
command: bazel build tensorflow/examples/PreProcess_modify

- Describe the problem:

tensorflow/examples/PreProcess_modify/BUILD:10:1: C++ compilation of rule '//tensorflow/examples/PreProcess_modify:chartest' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 130 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:360:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4,
                 from ./tensorflow/core/framework/tensor.h:19,
                 from ./tensorflow/cc/framework/ops.h:21,
                 from ./tensorflow/cc/ops/const_op.h:19,
                 from ./tensorflow/examples/PreProcess_modify/Tensorflow/TFClassifier.h:29,
                 from ./tensorflow/examples/PreProcess_modify/include/GameRecognize.h:14,
                 from ./tensorflow/examples/PreProcess_modify/GameRecog/CharRecognize.h:23,
                 from tensorflow/examples/PreProcess_modify/Projects/Src/chartest.cpp:13:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Constants.h:429:2: error: #error The preprocessor symbol 'Success' is defined, possibly by the X11 header file X.h
 #error The preprocessor symbol 'Success' is defined, possibly by the X11 header file X.h
  ^
In file included from /usr/include/X11/Xlib.h:44:0,
                 from ./tensorflow/examples/PreProcess_modify/Os/TqcOs.h:21,
                 from tensorflow/examples/PreProcess_modify/Projects/Src/chartest.cpp:11:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Constants.h:436:3: error: expected identifier before numeric constant
   Success = 0,        
   ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Constants.h:436:3: error: expected '}' before numeric constant
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Constants.h:436:3: error: expected unqualified-id before numeric constant
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:360:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4,
                 from ./tensorflow/core/framework/tensor.h:19,
                 from ./tensorflow/cc/framework/ops.h:21,
                 from ./tensorflow/cc/ops/const_op.h:19,
                 from ./tensorflow/examples/PreProcess_modify/Tensorflow/TFClassifier.h:29,
                 from ./tensorflow/examples/PreProcess_modify/include/GameRecognize.h:14,
                 from ./tensorflow/examples/PreProcess_modify/GameRecog/CharRecognize.h:23,
                 from tensorflow/examples/PreProcess_modify/Projects/Src/chartest.cpp:13:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Constants.h:549:1: error: expected declaration before '}' token
 } // end namespace Eigen
 ^
INFO: Elapsed time: 678.166s, Critical Path: 5.46s"
12585,Tensorflow estimator with shared network,"I am building a tensorflow model with new [estimator]( https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) high level api. My model looks like ![this]( https://i.stack.imgur.com/gxxo0.png).

In fact, the model is more complex than that due to the model is used to simulate game operation. Classification is responsible for decide whether it is good time for action. Then the regression will give the details about the action. It contains a combination of CNN and RNN. 

However, due to the complexity and memory consumption, it is impossible to train and run classification and regression as two network simultaneously. Also, when I create my estimator like:

```python
    # Create the Estimator
    mnist_classifier = tf.estimator.Estimator(
        model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model"")

```
I can only provide one model function for the estimator. Is is possibile to train and run two estimator together? "
12583,"BUG: indices[0] = [0,-1] is out of bounds for _OneHotColumn","### Describe the problem

For `sparse_column_with_keys`, out-of-vocabulary feature values is -1 by default. However, -1 is invalid index when sparse_tensor is converted to tensor in `one_hot_column`.


### Source code / logs

code:

```python
    ids = fc.sparse_column_with_keys(""ids"", [""marlo"", ""omar"", ""stringer""])
    weighted_ids = fc.weighted_sparse_column(ids, ""weights"")
    one_hot = fc.one_hot_column(weighted_ids)
```

logs:

```python
  File ""/Users/facai/Workshop/sina/Prometheus/prometheus/python/feature/column_test.py"", line 373, in _feature_column_from_tensor_dict
    return layers.input_from_feature_columns(tensor_dicts, feature_columns)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 212, in input_from_feature_columns
    default_name='input_from_feature_columns')
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 143, in _input_from_feature_columns
    output_rank=output_rank))
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py"", line 936, in _to_dnn_input_layer
    return sparse_ops.sparse_tensor_to_dense(weighted_column)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py"", line 845, in sparse_tensor_to_dense
    name=name)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py"", line 710, in sparse_to_dense
    name=name)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_sparse_ops.py"", line 1094, in _sparse_to_dense
    validate_indices=validate_indices, name=name)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): indices[0] = [0,-1] is out of bounds: need 0 <= index < [1,4]
         [[Node: input_from_feature_columns/col_tag_weighted_by_col_weight_one_hot/SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT64, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_from_feature_columns/col_tag_weighted_by_col_weight_one_hot/SparseMerge/SparseReorder, input_from_feature_columns/col_tag_weighted_by_col_weight_one_hot/SparseMerge/Identity, input_from_feature_columns/col_tag_weighted_by_col_weight_one_hot/SparseMerge/SparseReorder:1, input_from_feature_columns/col_tag_weighted_by_col_weight_one_hot/SparseToDense/default_value)]]
```
"
12581, rnn.MultiRNNCell problems and solution,"ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x11d32cbd0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/gru_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([GRUCell(...)] * num_layers), change to: MultiRNNCell([GRUCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)

the origin code:
from tensorflow.contrib import rnn
        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""inputs"")
        keep_prob = tf.placeholder(dtype=tf.float32, name=""keep_prob"")
        cell = rnn.GRUCell(10)
       cell = rnn.DropoutWrapper(cell=cell, input_keep_prob=keep_prob)
       cell = rnn.MultiRNNCell([cell for _ in range(5)], state_is_tuple=True)

      outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=look_up, dtype=tf.float32)

solution:
      inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""inputs"")
      keep_prob = tf.placeholder(dtype=tf.float32, name=""keep_prob"")
      cell = rnn.MultiRNNCell([rnn.DropoutWrapper(rnn.GRUCell(10), input_keep_prob=keep_prob) for _ in range(5)] , state_is_tuple=True)
"
12579,[building tensorflow with bazel ] Error:C++ compilation of rule '@boringssl//:crypto' failed.,"  **Environment:
    GCC 4.9.1
    glibc :2.11.3
    bazel:0.4.0/0.4.5/0.5.3(these versions have been tried)
    OS: SUSE 
 It seems that boringssl can't work with command `bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package`. Here is the  log:**

  WARNING: /hdata/users/rll/tensorflow/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /hdata/users/rll/tensorflow/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /var/lib/hive/.cache/bazel/_bazel_hive/e5053b6fc588ac2d9981b522e9f221e1/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).
In file included from /usr/include/fcntl.h:38:0,
                 from external/boringssl/src/crypto/bio/socket_helper.c:21:
/usr/include/sys/stat.h:372:56: error: array type has incomplete element type
 extern int futimens (int __fd, __const struct timespec __times[2]) __THROW;
                                                        ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 47.092s, Critical Path: 17.01s
"
12575,Usage of bit_casted_tensor() function with C++ api?(implementation on IOS),"### Any tutorial on this bit_casted_tensor function?
I am building an app on IOS based on ios-tensorflow model. The model generated a tensor with type float. However, for my model, a unit8 format is required. So I want to transfer the tensor's format.

I found this, `bit_casted_tensor()`, maybe useful for directly transformation. But I when tried to use it in the code like the following,
`   tensorflow::TTypes<tensorflow::uint8>::Tensor a_new_tensor = bit_casted_tensor(image_tensor);` the issue `use of undeclared identifier 'bit_casted_tensor' ` always showed up.

According to reference, bit_casted_tensor function should use like this:

`TTypes< T, NDIMS >::ConstTensor bit_casted_tensor() const `

> Return the tensor data to an _Eigen::Tensor_ with the same size but a bitwise cast to the specified _dtype T_.
> 
> Using a bitcast is useful for move and copy operations. NOTE: this is the same as tensor() except a bitcast is allowed. 
[TF Reference Link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor#classtensorflow_1_1_tensor_1afced940422a1e726d9487cb3cb039630)

I am not sure if I use it correctly? Or it is due to the limited support of tensorflow function on IOS platform? 

## Keen for any kind of help !!"
12571,tf.subtract doesn't work for uint8 and uint16 images (such as PNG),"It seems that tf.subtract doesn't support uint8 and unit16 image.
Could someone please add a PR to enable the uint16 and int16 support for subtraction.

Thanks,"
12570,"No gradient for `cdf`, `sample` and other functions for several distributions in `tf.distributions`","------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8.0 cuDNN 6
- **GPU model and memory**: nVIDIA K2100M, 2G
- **Exact command to reproduce**: See below
dist_par = tf.Variable(1.0)
dist = tf.distributions.Beta(dist_par,1.0)
print(tf.gradients(dist.cdf(0.5), dist_par))

>> [None]

The output says there is no gradient.

### Describe the problem
For several distributions such as `Beta` and `Gamma`, there is no gradient of their functions such as `cdf`, `log_cdf`, `sample` with respect to the parameters of these distributions. While gradients are provided for these functions of distributions such as `Normal` and `Laplace`.
I think theoretically the gradients should exist. And they are useful when people build a model in which samples drawn from these distributions as prior distributions are marginalized while the parameters of these distributions are optimized. It would be nice if they can be implemented
Thanks!

### Source code / logs
See above
"
12569,missing Documentation of the method AttentionWrapper.zero_state(...),"Hello , 


I have noticed that the method AttentionWrapper.zero_state( batch_size,dtype) does not have any description of its functionality in the  documentation website , below is a reference link : 
https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper

I really hope that this gets fixed , I have spent a couple of days trying to debug a code that I have written until I realized that I was misusing the method . 


thank you "
12568,'module' object has no attribute 'sparse_column_with_vocabulary_file',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Running on Cloud ML Engine
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  2.7


### Describe the problem
I am trying to use tf.contrib.layers.sparse_column_with_vocabulary_file() and I am getting an error that it does not exist. I recognize that it is not showing up when I search for it in the API, but it is showing up in the source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py#L669) -- I even specified the r1.3 branch and it was still showing up.  Has this been removed and I am just looking in the wrong place? It seems like this may be a bug and the function should exist in 1.3?

If it was deprecated is it because there are workarounds when trying to generate a feature column from a sparse tensor of words? I can create the hash table with:

```
table = tf.contrib.lookup.index_table_from_file(vocabulary_file = vocab_file)
table.lookup(word)
``` 
but since I am trying to add this in the feature columns, I need something to read in the sparse tensor of words in ie. 
`words = tf.contrib.layers.???` or `words = tf.feature_column.???`

Or (Assuming deprecation) is the suggested implementation just to do all of these transformations in the `input_fn():` and just pass a sparse_column_with_integerized_feature() directly. Personally this feels awkard to perform half of the transformation in the input function but without sparse_column_with_vocabulary_file it feels like there is no other choice:

With this function it should be easy to go from:
""This is a sentence"" --> tf.string_split() yields [""This"", ""is"", ""a"", ""sentence""] (sparse tensor of strings) within the input_fn --> and then tf.contrib.sparse_column_with_vocabulary_file (sparse tensor of ids) --> tf.contrib.layers.embedding_column() which yields the embedding from a sparse tensor of id's.
"
12567,Tensorflow should not depend on tensorboard,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: irrelevant
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux
- **TensorFlow installed from (source or binary)**: binary (the issue is about pip dependencies)
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.5
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: irrelevant
- **Exact command to reproduce**: pip install tensorflow

### Describe the problem
I wish to install tensorflow without having to install tensorboard.

Currently there is a circular dependency between tensorflow and tensorboard.  However, tensorflow runs perfectly well without tensorboard, therefore the dependency should be removed. Additionally having a simpler dependency structure would facilitate packaging for NixOS.

### Source code / logs
irrelevant"
12560,"after ops_to_register.h changed, IOS camera example still return No OpKernel support 'Less' op","### System information
Run on MacOS 10.12
Xcode 8.3.3
Python 3.5
tensorflow 1.2.1 installed with anaconda 
bazel 0.5.2-homebrew

### Main Problem: 
Although I changed ops_to_register.h file and recompile the static library, IOS camera example code still returned No OpKernel support 'Less' op error. **The same library works fine** in another project [JieHe's ios-tensorflow object detection project](https://github.com/JieHe96/iOS_Tensorflow_ObjectDetection_Example) when I replace the model load the model. I can't tell why I got the issues with official tensorflow ios code.

### Describe the problem
I trained my model based on the ssd_mobilenet network. T hen the model was optimized for usage on ios. Freeze it with `export_inference_graph.py`, optimize it with` optimize_for_inference.py`, and then binary reduced the size with `bazel build -c tensorflow/tools/graph_transforms:transform_graph`

In that, the model was well-generated as desired. 


**For the static library:**
And then I try to import the model into my app. So I print all the ops and put the file under tensorflow/core/framework. 

>   bazel build tensorflow/python/tools:print_selective_registration_header 
  bazel-bin/tensorflow/python/tools/print_selective_registration_header \
    --graphs=path/to/graph.pb > ops_to_register.h

In the Makefile, first delete the line ""-D__ANDROID_TYPES_SLIM__ "" under ""# Settings for iOS."" for all ""$(IOS_ARCH)"". And run

```
tensorflow/contrib/makefile/download_dependencies.sh
tensorflow/contrib/makefile/compile_ios_protobuf.sh 
tensorflow/contrib/makefile/compile_ios_tensorflow.sh ""-O3  -DANDROID_TYPES=ANDROID_TYPES_FULL -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION""
```
After I generated the static tensorflow library, I `pod install` the podfile in camera example as required. Modified the input/output tensor and try to build the app. However, the same error with ""No kernel registed XXX"" still occurs .

I also tried my model and the same library on [JieHe's ios-tensorflow code](https://github.com/JieHe96/iOS_Tensorflow_ObjectDetection_Example), it can run smoothly. So I assumed] my static library was well generated to including all the ops I need. So I don't really know how to solve it with the example code?

My error log: 

> 2017-08-24 17:21:18.679927: F /Users/yingjie/tensorflow-master/tensorflow/examples/ios/App-test/camera/CameraExampleViewController.mm:730] Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'Less' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]"
12557,`indicator_column` raises a TypeError when `weighted_categorical_column` is used as its input.,"### Describe the problem

`indicator_column` raises a TypeError when `weighted_categorical_column` is used as its input.

I have fixed the bug, and the PR is coming later.


### Source code / logs

```python
    indicator_tensor = _transform_features(features, [indicator])[indicator]
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py"", line 377, in _transform_features
    outputs[column] = builder.get(column)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py"", line 1533, in get
    transformed = column._transform_feature(self)  # pylint: disable=protected-access
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py"", line 2476, in _transform_feature
    vocab_size=self._variable_shape[-1])
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/sparse_ops.py"", line 1140, in sparse_merge
    type(vocab_size))
TypeError: vocab_size has to be a Tensor or Python int. Found <class 'tensorflow.python.framework.tensor_shape.Dimension'>
```
"
12556,MonitoredTrainingSession can't add summary and checkpoint hook,"I trying to add summary and checkpoint to my distributed tensorflow training with custom data 

<pre>summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)
checkpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)
with tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:</pre>

with above I can't run the sess with error of below
<pre>2017-08-24 19:38:31.250556: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions
2017-08-24 19:38:31.250633: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions
      [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=""/job:worker/replica:0/task:0/gpu:0""]()]]
2017-08-24 19:38:31.251688: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions
2017-08-24 19:38:31.251734: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions
      [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=""/job:worker/replica:0/task:0/gpu:0""]()]]
2017-08-24 19:38:31.253478: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,3] has negative dimensions
2017-08-24 19:38:31.253522: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,3] has negative dimensions
      [[Node: labels = Placeholder[dtype=DT_INT32, shape=[?,3], _device=""/job:worker/replica:0/task:0/gpu:0""]()]]</pre>

how ever if I run without the summary and checkpoint hook it just work fine....
I want to specify which timing and which directory to view my summary and save the checkpoint.
Does anyone have an idea how to solve this problem?
"
12555,"[[Node: pool_3 = AvgPoolT=DT_FLOAT, data_format=""NHWC"", ksize=[1, 8, 8, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""]]","iOS App - I have changes All the file name and Size , but still getting below error.

Source Code URL:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios/camera

Changes Code : const int wanted_input_width = 229;
const int wanted_input_height = 229;
const int wanted_input_channels = 3;
const float input_mean = 128.0f;
const float input_std = 128.0f;
const std::string input_layer_name = ""Mul"";
const std::string output_layer_name = ""final_result"";

computed output size would be negative
[[Node: pool_3 = AvgPoolT=DT_FLOAT, data_format=""NHWC"", ksize=[1, 8, 8, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""]]

Please suggest what we are doing wrong .
In build pb and txt are working fine (imagenet_comp_graph_label_strings.txt, tensorflow_inception_graph.pb)

but created new pb and .txt is not working(rounded_graph.pb and retrained_labels.txt).
Note: I also rename the pb and txt file ,"
12553,Object detection evaluation warning,"Hi,
Here is my system details:

- macOS10
- no GPU
- python3.6
- tensorflow1.3
- installed tf from source

I'm running the evaluation of object detection, but every time I run into this warning:

```
WARNING:root:The following classes have no ground truth examples: 0
/Users/Mohamad/Projects/Python/tensorflow/models/object_detection/utils/metrics.py:145: RuntimeWarning: invalid value encountered in true_divide
  num_images_correctly_detected_per_class / num_gt_imgs_per_class)
```
I want to know how to solve this warning? Is it something serious?

Thanks."
12546,Simple release note typo.,"Simple typo on 1.3.0 release note.
![2017-08-24 14 57 12](https://user-images.githubusercontent.com/6247953/29651709-f8d65474-88dc-11e7-9a73-b68f7db09474.png)
"
12545,tfdbg doesn't work when using start_queue_runners,"Today, i tried to use tfdbg to debug my tensorflow model, but i found that the run end CLI didn't show up when using start_queue_runners. here is my code.
```python
tf.global_variables_initializer().run()

tf.local_variables_initializer().run()

coord = tf.train.Coordinator()

threads = tf.train.start_queue_runners(sess=sess, coord=coord)

GLOBAL_STEP = 2

debug_sess = tf_debug.LocalCLIDebugWrapperSession(sess=sess)

try:
     while not coord.should_stop():
           model.train_n_iteration(sess=debug_sess, n=300, file_writer=file_writer)
           model.save_deeplabv3_ckpt(ckpt_path='ckpt/deeplabv3.ckpt', global_step=GLOBAL_STEP)
           GLOBAL_STEP += 1
except tf.errors.OutOfRangeError:
      print('num epoch exceeds the limits')
   
except KeyboardInterrupt:
      print('key board interrupt')
    
finally:
     coord.request_stop()
     coord.join(threads=threads)
```
"
12543,Error in installation guide for windows -- should use cuDNN v6.0 with tensorflow v1.3.0,"In the installation guide for windows https://www.tensorflow.org/install/install_windows, where it says 'Requirements to run TensorFlow with GPU support' and 'cuDNN v5.1'.

Actually using cnDNN v5.1 will not work. Instead cnDNN v6.0 works.
This error would cause great waste of time for newbees. Please update the doc. Thanks."
12541,Feature Request - Dynamic Dispatch of optimized functions for deployment builds,"P
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.12.16 
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
N/A
- **Python version**: 
N/A
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem

Hello. Thanks for TF.

Recently OpenCV has integrated dynamic dispatch allowing OpenCV library to leverage optimal code paths for supported hardware acceleration (sse, fma, avx, avx2 etc), from a single compiled binary. This is super useful for deployment builds when you are unsure what CPU family/features will potentially run your binary. 

I understand TF's execution model and potentially XLA may add complications, not even to say GPU support - but for CPU ops, or CPU only deployments I imagine something equivalent would be well received.

While we are speaking of deployment builds, are folks just shipping non-optimized-for-client-hardware binaries, or compiling different builds for different hardware features for end products?

Thank you. 
"
12539,libtensorflow_cc.so linker issues with r 1.3 on Mac OS X Undefined symbols for architecture,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, but this appears to be a linker issue.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X 10.12.6

- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Git tagged at 1.3 and master at 593dc8e5d65f4db93e8f5fced772abb3531a9752 
- **Python version**: 
2.7 (OS Default).
- **Bazel version (if compiling from source)**:
 0.5.3-homebrew
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
bazel clean
./configure stating N to every option for CPU only build.
bazel build //tensorflow:libtensorflow_cc.so with no optimizations  (testing deployment)

### Describe the problem

The above commands builds a libtensorflow_cc.so from for TF 1.3

Linking my built libtensorflow_cc.so to a C++ / Obj-C App which has successfully linked against a TF 1.2 build of libtensorflow_cc.so, results in the following linker errors in the log section of this bug report.

No code changes were made on my end between a working 1.2 libtensorflow_cc.so and a new 1.3 libtensorflow_cc.so - Just replacing the binary so file, building clean and re-compiling.

Thank you.

### Source code / logs
```Ld /Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Products/Release/Synopsis.framework/Versions/A/Synopsis normal x86_64
    cd /Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/Synopsis-Framework/Synopsis
    export MACOSX_DEPLOYMENT_TARGET=10.11
    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang++ -arch x86_64 -dynamiclib -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk -L/Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Products/Release -L/Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/Synopsis-Framework/Synopsis/Synopsis/Tensorflow/lib -F/Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Products/Release -F/Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/Synopsis-Framework/Synopsis/Synopsis/OpenCV -filelist /Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Intermediates/Synopsis-Framework.build/Release/Synopsis-macOS.build/Objects-normal/x86_64/Synopsis.LinkFileList -install_name @rpath/Synopsis.framework/Versions/A/Synopsis -Xlinker -rpath -Xlinker @executable_path/../Frameworks -Xlinker -rpath -Xlinker @loader_path/Frameworks -mmacosx-version-min=10.11 -Xlinker -object_path_lto -Xlinker /Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Intermediates/Synopsis-Framework.build/Release/Synopsis-macOS.build/Objects-normal/x86_64/Synopsis_lto.o -fobjc-arc -fobjc-link-runtime -stdlib=libc++ -lblas -framework Accelerate -lz -framework Cocoa -framework CoreMedia -framework CoreMediaIO -framework CoreVideo -framework OpenCL -framework opencv2 /Users/vade/Documents/Repositories/Synopsis/Synopsis/Synopsis/Synopsis-Framework/Synopsis/Synopsis/Tensorflow/lib/libtensorflow_cc.so -single_module -compatibility_version 1 -current_version 1 -Xlinker -dependency_info -Xlinker /Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Intermediates/Synopsis-Framework.build/Release/Synopsis-macOS.build/Objects-normal/x86_64/Synopsis_dependency_info.dat -o /Users/vade/Library/Developer/Xcode/DerivedData/Synopsis-cdbnfomidhkpiodcmhdwciqcvshg/Build/Products/Release/Synopsis.framework/Versions/A/Synopsis

Undefined symbols for architecture x86_64:
  ""tensorflow::TensorShape::SlowCopyFrom(tensorflow::TensorShape const&)"", referenced from:
      tensorflow::Tensor::Tensor(tensorflow::Tensor const&) in TensorflowFeatureModule.o
  ""tensorflow::TensorShape::DestructorOutOfLine()"", referenced from:
      -[TensorflowFeatureModule initWithQualityHint:] in TensorflowFeatureModule.o
      tensorflow::Tensor::Tensor(tensorflow::Tensor const&) in TensorflowFeatureModule.o
  ""tensorflow::TensorShape::TensorShape(tensorflow::gtl::ArraySlice<long long>)"", referenced from:
      -[TensorflowFeatureModule initWithQualityHint:] in TensorflowFeatureModule.o
  ""tensorflow::TensorShape::dim_size(int) const"", referenced from:
      Eigen::DSizes<long, 4> tensorflow::TensorShape::AsEigenDSizesWithPadding<4>() const in TensorflowFeatureModule.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```"
12538,FPGA Implementation on TensorFlow,"Hi all,

Recently, I read some papers about implementing Intel Xeon Phi on Tensorflow. Therefore, I want to make TensorFlow support FPGA board. But there is no information in Google and other forum/community.

For my understanding, we should implement the FPGA supporting in a class of TensorFlow which distributes tasks to different devices (like GPU/CPU). Is possible that implementing the FPGA supporting in that class? Or is there any good suggestions to make the progress to support FPGA on TensorFlow? 

I am looking forward any help or advice!

Thanks,
Kevin"
12537,Printing lines without logging prefix using tf.Print(),"Right now, all messages printed via `tf.Print()` are prefixed by:

```
<timestamp>: I tensorflow/core/kernels/logging_ops.cc:79]
```

It would be useful to have a way to print user-friendly output from within the graph. Therefore, suggest adding a parameter for the logging format to `tf.Print()`, or at least adding a flag that disables the logging prefix."
12536,Eager execution API strided slice problem.,"@alextp there seems to be a problem with eagerly executing strided slice ops. The problem seems to be in the shape inference component and is non-deterministic. If you add the following code in the `eager/c_api_test.cc` file and run the test multiple times, you'll notice that sometimes it succeeds and sometimes it fails, randomly. More specifically, I get errors related to the strides input tensor, but I think this may have to do with how the memory is managed for eager tensors:

```cc
TFE_TensorHandle* TestBeginTensorHandle() {
  int64_t dims[] = {2};
  int data[] = {1, 0};
  TF_Tensor* t = TF_AllocateTensor(
      TF_INT32, &dims[0], sizeof(dims) / sizeof(int64_t), sizeof(data));
  memcpy(TF_TensorData(t), &data[0], TF_TensorByteSize(t));
  TFE_TensorHandle* th = TFE_NewTensorHandle(t);
  TF_DeleteTensor(t);
  return th;
}

TFE_TensorHandle* TestEndTensorHandle() {
  int64_t dims[] = {2};
  int data[] = {2, 1};
  TF_Tensor* t = TF_AllocateTensor(
      TF_INT32, &dims[0], sizeof(dims) / sizeof(int64_t), sizeof(data));
  memcpy(TF_TensorData(t), &data[0], TF_TensorByteSize(t));
  TFE_TensorHandle* th = TFE_NewTensorHandle(t);
  TF_DeleteTensor(t);
  return th;
}

TFE_TensorHandle* TestStridesTensorHandle() {
  int64_t dims[] = {2};
  int data[] = {1};
  TF_Tensor* t = TF_AllocateTensor(
      TF_INT32, &dims[0], sizeof(dims) / sizeof(int64_t), sizeof(data));
  memcpy(TF_TensorData(t), &data[0], TF_TensorByteSize(t));
  TFE_TensorHandle* th = TFE_NewTensorHandle(t);
  TF_DeleteTensor(t);
  return th;
}

TEST(CAPI, ExecuteStridedSlice) {
  TF_Status* status = TF_NewStatus();
  TF_SessionOptions* opts = TF_NewSessionOptions();
  TFE_Context* ctx = TFE_NewContext(opts, status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TF_DeleteSessionOptions(opts);

  TFE_TensorHandle* a = TestMatrixTensorHandle();
  TFE_TensorHandle* begin = TestBeginTensorHandle();
  TFE_TensorHandle* end = TestEndTensorHandle();
  TFE_TensorHandle* strides = TestStridesTensorHandle();
  TFE_Op* op = TFE_NewOp(ctx, ""StridedSlice"", status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_OpAddInput(op, a, status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_OpAddInput(op, begin, status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_OpAddInput(op, end, status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_OpAddInput(op, strides, status);
  CHECK_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_OpSetAttrType(op, ""T"", TFE_TensorHandleDataType(a));
  TFE_OpSetAttrType(op, ""Index"", TFE_TensorHandleDataType(begin));
  TFE_OpSetAttrInt(op, ""begin_mask"", 0);
  TFE_OpSetAttrInt(op, ""end_mask"", 0);
  TFE_OpSetAttrInt(op, ""ellipsis_mask"", 0);
  TFE_OpSetAttrInt(op, ""new_axis_mask"", 0);
  TFE_OpSetAttrInt(op, ""shrink_axis_mask"", 3);

  TFE_TensorHandle* retvals[2] = {nullptr};
  int num_retvals = 2;  // Should be reduced to 1 by the TFE_Execute call.
  TFE_Execute(op, &retvals[0], &num_retvals, status);
  EXPECT_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  TFE_DeleteOp(op);
  TFE_DeleteTensorHandle(a);
  TFE_DeleteTensorHandle(begin);
  TFE_DeleteTensorHandle(end);
  TFE_DeleteTensorHandle(strides);
  TFE_DeleteContext(ctx, status);
  ASSERT_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  ASSERT_EQ(1, num_retvals);

  TF_Tensor* t = TFE_TensorHandleResolve(retvals[0], status);
  TFE_DeleteTensorHandle(retvals[0]);
  ASSERT_EQ(TF_OK, TF_GetCode(status)) << TF_Message(status);
  float result[1] = {0};
  EXPECT_EQ(sizeof(result), TF_TensorByteSize(t));
  memcpy(&result[0], TF_TensorData(t), TF_TensorByteSize(t));
  TF_DeleteTensor(t);
  EXPECT_EQ(3, result[0]);
  TF_DeleteStatus(status);
}
```"
12534,[CMake] fatal error: 'nsync_time_init.h' file not found on mac,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Darwin T-X.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
- **TensorFlow installed from (source or binary)**:
Source.
- **TensorFlow version (use command below)**:
Not applicable. Happens during build.
- **Python version**: 
Not applicable. Happens during build.
- **Bazel version (if compiling from source)**:
Not applicable. Compiling directory tensorflow/contrib/cmake using CMake 3.7.2 and clang Apple LLVM version 8.1.0 (clang-802.0.42).
- **CUDA/cuDNN version**:
Not applicable.
- **GPU model and memory**:
Not applicable.
- **Exact command to reproduce**:
cmake --build /Users/kasper/Development/tensorflow_pr/tensorflow/contrib/cmake --target all -- -j 8

### Describe the problem
This is a bug related to the CMake build of TensorFlow which fails on my system due to an error with the nsync build. The nsync build seems to not include some headers (see below). I have tested with TensorFlow v1.0.0, v1.1.0, v1.2.0 and the current master branch. I have not been able to investigate the nsync error because a standalone nsync build (i.e. clone->cmake->make) fails at an earlier step (`/Users/kasper/Development/nsync/platform/macos/platform.h:35:13: error: typedef redefinition with different types ('int' vs 'enum clockid_t') typedef int clockid_t;`). Unfortunately, I have not been able to submit an issue to https://github.com/google/nsync and resolve this problem because issues are disabled for this repository.

### Source code / logs
[ 46%] Building CXX object CMakeFiles/nsync.dir/platform/c++11/src/time_rep_timespec.cc.o
no
/Users/kasper/Development/TensorFlowImageFilter/SuperBuild/cmake-build-debug/TensorFlow-build/nsync/src/nsync/platform/c++11/src/time_rep_timespec.cc:19:10: fatal error: 'nsync_time_init.h' file not found
#include ""nsync_time_init.h"""
12532,Undefined symbol 'fixed_address_empty_string' : new tensorflow op with protobuf ,"I would like to create a new operation that can communicate to an
external python process. At the momemnt, I created a new operation
that sends to a python process ""hello world"" with `protobuf`.

In this tiny example, I'm sending a string. In the future I would like
to send more complex data, like Eigen matrices, that's why I chose
`protobuf`. (and for possible 'easy integration into tensorflow).

**msg.proto** :
```
package prototest;

message Foo {
  required string bar = 1;
}
```

- `protoc msg.proto --cpp_out=. --python_out=.`
- generates : `msg.pb.cc  msg.pb.h  msg_pb2.py`

**hello_world.cc** :

```cpp
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/tensor_shape.h""
#include ""tensorflow/core/platform/default/logging.h""
#include ""tensorflow/core/framework/shape_inference.h""

// to send serialized data through UPD socket
#include <sys/socket.h>
#include <arpa/inet.h>

// generated header file from protoc
#include ""msg.pb.h""

namespace tensorflow{
    namespace shape_inference{

        Status HelloWorldShape(InferenceContext* c){
            std::cout << ""shape_infernce is done"" << std::endl;
            return Status::OK();
        }
        REGISTER_OP(""HelloWorld"")
            .SetShapeFn(HelloWorldShape)
            .Doc(R""doc(HelloWorld operation)doc"");
    } // end namespace shape_inference

    class HelloWorldOp : public OpKernel {
    public :
        // constructor
        explicit HelloWorldOp(OpKernelConstruction* context) : OpKernel(context) {
            std::cout << ""HelloWorldOp constructor"" << std::endl;
        }

        void Compute(OpKernelContext* context) override {
            std::cout << ""Start Compute method"" << std::endl;
            //-----------------------------------------------------------------
            // send something to a Python process with protobuf
            struct sockaddr_in addr;
            addr.sin_family = AF_INET;
            inet_aton(""127.0.0.1"", &addr.sin_addr);
            addr.sin_port = htons(5555);

            // initialise a foo and set some properties
            GOOGLE_PROTOBUF_VERIFY_VERSION;

            prototest::Foo foo;
            foo.set_bar(""Hello World"");

            // serialise to string, this one is obvious ; )
            std::string buf;
            foo.SerializeToString(&buf);

            int sock = socket(PF_INET, SOCK_DGRAM, 0);
            sendto(sock, buf.data(), buf.size(), 0, (struct sockaddr *)&addr, sizeof(addr));
            //------------------------------------------------------------------
            std::cout << ""Compute method is done"" << std::endl;
        }
    };
    REGISTER_KERNEL_BUILDER(Name(""HelloWorld"").Device(DEVICE_CPU), HelloWorldOp);
} // end namespace tensorflow
```

To compile and run my code, I use a test scrip found at [#10950](http://github.com/tensorflow/tensorflow/issues/10950)

**compiler_and_run.py** :

```python
#!/usr/bin/env python3.5

# Demo from https://github.com/tensorflow/tensorflow/issues/10950

from __future__ import print_function
import os
import sys
import tensorflow as tf


my_dir = os.path.dirname(os.path.abspath(__file__))
so_filename = ""lib_hello_world.so""
cc_filename = ""hello_world.cc""


def compile():
    # Fix for undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE.
    # https://github.com/tensorflow/tensorflow/issues/1419
    from google.protobuf.pyext import _message as msg
    lib = msg.__file__
    ld_flags = [
        ""-Xlinker"", ""-rpath"", ""-Xlinker"", os.path.dirname(lib),
        ""-L"", os.path.dirname(lib), ""-l"", "":"" + os.path.basename(lib)]
    common_opts = [""-shared"", ""-O2"", ""-std=c++11""]
    if sys.platform == ""darwin"":
        common_opts += [""-undefined"", ""dynamic_lookup""]
    common_opts += [""-I"", tf.sysconfig.get_include()]
    common_opts += [""-fPIC""]
    common_opts += [""-D_GLIBCXX_USE_CXX11_ABI=0""]  # might be obsolete in the future
    opts = common_opts + [cc_filename, ""-o"", so_filename]
    opts += ld_flags
    cmd_bin = ""g++""
    cmd_args = [cmd_bin] + opts
    from subprocess import Popen, PIPE, STDOUT, CalledProcessError
    print(""compile call: %s"" % "" "".join(cmd_args))
    proc = Popen(cmd_args, stdout=PIPE, stderr=STDOUT)
    stdout, stderr = proc.communicate()
    assert stderr is None  # should only have stdout
    if proc.returncode != 0:
      print(""compile failed: %s"" % cmd_bin)
      print(""Original stdout/stderr:"")
      print(stdout)
      raise CalledProcessError(returncode=proc.returncode, cmd=cmd_args)
    assert os.path.exists(so_filename)


def main():
    print(""TensorFlow version:"", tf.GIT_VERSION, tf.VERSION)
    os.chdir(my_dir)
    compile()
    mod = tf.load_op_library(""%s/%s"" % (my_dir, so_filename))


if __name__ == ""__main__"":
    main()
```


This returns :

```txt
TensorFlow version: v1.2.0-rc2-21-g12f033d 1.2.0
compile call: g++ -shared -O2 -std=c++11 -I /usr/local/lib/python3.5/dist-packages/tensorflow/include -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 hello_world.cc -o lib_hello_world.so -Xlinker -rpath -Xlinker /usr/local/lib/python3.5/dist-packages/protobuf-3.2.0-py3.5-linux-x86_64.egg/google/protobuf/pyext -L /usr/local/lib/python3.5/dist-packages/protobuf-3.2.0-py3.5-linux-x86_64.egg/google/protobuf/pyext -l :_message.cpython-35m-x86_64-linux-gnu.so
Traceback (most recent call last):
  File ""./compile_and_test.py"", line 55, in <module>
    main()
  File ""./compile_and_test.py"", line 51, in main
    mod = tf.load_op_library(""%s/%s"" % (my_dir, so_filename))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
    None, None, error_msg, error_code)
tensorflow.python.framework.errors_impl.NotFoundError: /src/ext_hello_world/lib_hello_world.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE
```

Compilation seems to work (no fatal error). But `tf.load_op_library()``fails due to undefined library in shared lib (*.so).

This undefined symbol seems to come from protobuf.

I installed `protoc` by myself (see Note about it).


In `tf.sysconfig.get_include()` there is a  `google/protobuf` folder with header files from built tensorflow.

So I don't know which header files are used during compilation :
 - header files from include files of tensorflow ?
 - header files of hand-installed protobuf ?

Or this undefined symbol is not due to this fact ? 

- How can I resolve this undefined symbol error in the shared library ?
- May I have to install `protoc` starting from `tensorflow/includes/google/protobuf` ? (and not from scratch)


I observed that in [#10950](https://github.com/tensorflow/tensorflow/issues/10950) : 
```python
from google.protobuf.pyext import _message as msg
lib = msg.__file__
```
returns : `/u/zeyer/.local/lib/python2.7/site-packages/google/protobuf/pyext/_message.so`

In my case it's : `/usr/local/lib/python3.5/dist-packages/protobuf-3.2.0-py3.5-linux-x86_64.egg/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so`
This file about protobuf seems to be completely different ... 

 

**Note about protoc install** :
`protoc` (protobuf compiler) was not installed.
I identified the version of protobuf used in tensorflow : v3.2.0 !

After, I followed protobuf installation instruction (C++ and Python implementation).

    ```bash
    cd /opt/
    # clone protobuf repo
    git clone https://github.com/google/protobuf.git
    cd protobuf

    # change to the right branch
    # list tags
    git tag -l
    git checkout tags/v3.2.0

    # install protobuf
    apt-get install autoconf automake libtool curl make g++ unzip
    ./autogen.sh
    ./configure
    make
    make check
    make install
    ldconfig

    # protoc version
    protoc --version
    >>> libprotoc 3.2.0

    # print linker and compiler files
    pkg-config --cflags --libs protobuf
    >>> -pthread -I/usr/local/include -L/usr/local/lib -lprotobuf -pthread -lpthread

    #some useful env variables
    PB_INC=$(pkg-config --cflags protobuf)
    PB_LINK=$(pkg-config --libs protobuf)
    TF_INC=$(python3.5 -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
    TF_LIBS=$(find $TF_INC/../ -name ""*.so"")

    # here, I used python3.5
    cd python
    export LD_LIBRARY_PATH=../src/.libs
    python3.5 setup.py build --cpp_implementation
    python3.5 setup.py test --cpp_implementation
    python3.5 setup.py install --cpp_implementation
    ```
### System information

- docker : Docker version 1.12.6, build 78d1802
- image : tensorflow/tensorflow:1.2.0-devel-gpu-py3
- based on : ubuntu 16.04 (4.4.0-78-generic)
- tensorflow build from source
- tensorflow version : 1.2.0
- python version : Python 3.5.2
- bazel version :
    ```
    Build label: 0.4.5
    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Thu Mar 16 12:19:38 2017 (1489666778)
    Build timestamp: 1489666778
    Build timestamp as int: 1489666778
    ```
- gcc -v :
    ```
    Using built-in specs.
    COLLECT_GCC=gcc
    COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper
    Target: x86_64-linux-gnu
    Configured with: ../src/configure -v --with-pkgversion='Ubuntu 5.4.0-6ubuntu1~16.04.4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
    Thread model: posix
    gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)
    ```
Information from https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh :

```txt
== cat /etc/issue ===============================================
Linux 2b98f5ebc987 4.4.0-78-generic #99-Ubuntu SMP Thu Apr 27 15:29:09 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes
== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux 2b98f5ebc987 4.4.0-78-generic #99-Ubuntu SMP Thu Apr 27 15:29:09 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow (1.2.0)

== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.2.0
tf.GIT_VERSION = v1.2.0-rc2-21-g12f033d
tf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d
Sanity check: array([1], dtype=int32)
== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
Wed Aug 23 17:24:13 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 370.28                 Driver Version: 370.28                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce 940MX       Off  | 0000:01:00.0     Off |                  N/A |
| N/A   54C    P0    N/A /  N/A |    277MiB /  2002MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
```"
12530,Is there any method to dump the IR and machine code of XLA compiler?,"Is there any method to dump the IR of tensorflow xla `from the compilers front end to the backend`, as detailed as better!
Such as the HLO IR to LLO IR and the machine code generated by xla backend? I wonder the details of TensorFlow XLA compiler, the official website isn't detailed..."
12528,Data Augmentation of uint16 images (such as PNG),"How we can do data augmentation (rotation, flip,...) of uint16 images?

Here is a piece of my code:

import os
import tensorflow as tf

filQ=['ex4497_fbp_13.png','ex4497_fbp_12.png']
filQQ=tf.train.string_input_producer(filQ)

reader =tf.WholeFileReader()
key, value = reader.read(filQQ)

myimg = tf.image.decode_png(value, dtype=tf.uint16) 
myimg1 = tf.image.rot90(myimg,k=1,name=None)

I get an error that:

TypeError: Value passed to parameter 'tensor' has DataType uint16 not in list of allowed values: uint8, int8, int32, int64, bool, float16, float32, float64, complex64, complex128, string

So basically unit16 doesn't exist!
unit8 is not a good choice because the pixel values in my image are NOT in the range [0 255],
and we can only use data type uint8, uint16 for PNG in tensorflow!!


So again the question becomes:
**How we can do data augmentation (rotation, flip,...) of uint16 images?**


"
12526,"This is regarding the installation of tensorflow in windows.For version 1.3,you have said cuDNN 5.1 for CUDA 8.0 is required.I wasted 2 days trying to overcome the errors but later I found that cuDNN 6 is required for TensorFlow v1.3","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12525,bug in DataFeeder constructor  ,"
### System information
- centos 7.0
- python 3.4
- tensorflow 1.2.1


### Describe the problem
When I call tensorflow.contrib.learn.DNNRegressor.fit(x_train_dict , y_train,steps=1000)  , x_train_dict is dict  and y_train is array , the program throws the following exception: 
```
File ""/home/star/yuce.ddxq.mobi/zhuge/management/commands/forecast_product_sale.py"", line 148, in tflearn_dnn_train2
    regressor.fit(x_train_dict, y_train, steps=10000, batch_size=10)
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py"", line 289, in new_func
    return func(*args, **kwargs)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 439, in fit
    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1340, in fit
    epochs=None)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 137, in _get_input_fn
    epochs=epochs)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 152, in setup_train_data_feeder
    x, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)
  File ""/usr/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py"", line 326, in __init__
    dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if x_is_dict else check_array(y, y.dtype)
AttributeError: 'numpy.ndarray' object has no attribute 'items'
```
and the related code is:  
```
   x_is_dict, y_is_dict = isinstance(x, dict), y is not None and isinstance(
        y, dict)
    if isinstance(y, list):
      y = np.array(y)

    self._x = dict([(k, check_array(v, v.dtype)) for k, v in list(x.items())
                   ]) if x_is_dict else check_array(x, x.dtype)
    self._y = None if y is None else \
      dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if x_is_dict else check_array(y, y.dtype)

```
the last line of the code seems wrong , it should use the y_is_dict instead of x_is_dict  ? 
I change the code to : 
```
dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if y_is_dict else check_array(y, y.dtype)
```
and then  it works .


"
12524,nsync is broken on Windows in Bazel build,"http://ci.tensorflow.org/job/tf-master-win-bzl/1459/console
```
11:36:56 ERROR: C:/tmp/_bazel_system/424zmya1/external/nsync/BUILD:357:13: Configurable attribute ""copts"" doesn't match this configuration (would a default condition help?).
11:36:56 Conditions checked:
11:36:56  @nsync//:android_arm
11:36:56  @nsync//:android_arm64
11:36:56  @nsync//:android_armeabi
11:36:56  @nsync//:android_x86_32
11:36:56  @nsync//:android_x86_64
11:36:56  @nsync//:clang_macos_x86_64
11:36:56  @nsync//:gcc_linux_aarch64
11:36:56  @nsync//:gcc_linux_ppc64
11:36:56  @nsync//:gcc_linux_x86_64_1
11:36:56  @nsync//:gcc_linux_x86_64_2
11:36:56  @nsync//:ios_x86_64
11:36:56 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted
```
 nsync is introduced in b48cfaea2aea3707a33e60c10385a87e37101b95

Fortunately, nsync already builds on Windows with CMake, I've sent a PR to add Bazel support.
https://github.com/google/nsync/pull/1

/cc @m3bm3b Could you please merge the PR, or is there any other way you prefer me to contribute to this repo?
"
12523,[feature request] Need QuantizedFusedBatchNorm ,There is no QuantizedFusedBatchNorm operator. The graph transform tools treats FusedBatchNorm operator with fold_old_batch_morm. But not all bns are ready to be folded. This will leave many isolated un-quantized fbn ops in the graph.
12522,Stacking CNN with LSTM,"I am trying to stack CNN before LSTM, however, I am experience a little problem. 
My LSTM + CTC works fine. However, I want to pass extracted feature from CNN to LSTM instead of whole image. 

The code is here: https://gist.github.com/kjanjua26/b756b6aae2277423c1f94b435a82f808

I error I am facing is: 

`File ""trainer2.py"", line 182, in <module>
    train()
  File ""trainer2.py"", line 75, in train
    logits, inputs, targets, seq_len,W, b = model.get_train_model()
  File ""/home2/kamranjanjua/tf_cnnlstm/tlstm9Aug/model.py"", line 97, in get_train_model
    outputs, _ = tf.nn.bidirectional_dynamic_rnn(forwardH1,backwardH1,x,seq_len,dtype=tf.float32)
  File ""/home/kamranjanjua/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 652, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/home/kamranjanjua/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 845, in dynamic_rnn
    dtype=dtype)
  File ""/home/kamranjanjua/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 919, in _dynamic_rnn_loop
    ""Input size (depth of inputs) must be accessible via shape inference,""
ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.`

Any help in this matter would be appreciated. I am kind of stuck here. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 0.12 (Using this on purpose since my older code works in 0.12 and I didn't update it for the new version.
- **Python version**: 2.7
- **GPU model and memory**: TitanX, 12 GB
"
12521,Install instructions should ask for libcudnn 6.0,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.04
- **TensorFlow installed from (source or binary)**:
binary (pip3)
- **TensorFlow version (use command below)**:
v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 
Python 3.5.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
6.0
- **GPU model and memory**:
GTX 1080 8GB
- **Exact command to reproduce**:
NA

### Describe the problem
The install instructions at https://www.tensorflow.org/install/install_linux tell users to install libcudnn 5.1. 
I followed these and ended up with the error on 

    import tensorflow

    ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory

So I installed libcudnn 6.0 instead and tensorflow is now working. 
My request is for the install instructions to be updated to reflect the move to cudnn 6.0

### Source code / logs
NA"
12519,Bug on the gradients graph computation - C++ API,"The gradient computation in the C++ API works as follow.
Let's say that we have the following graph:
```
                Tanh
                  |
         Assign MatMul
         /    \ /    \
      Const   Var   Const
```
Here our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.

Then the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.

In our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.

The PR https://github.com/tensorflow/tensorflow/pull/12397 updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).

As @skye pointed it out as a comment in the PR, there is other cases where there is still a problem. I am working on it."
12518,core dump when running tensorflow,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
    yes.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
   Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 1070 8GB gpu memory
- **Exact command to reproduce**:
 

### Describe the problem
https://github.com/ilovin/lstm_ctc_ocr/tree/master
python lstm_ocr.py
when I run it, core dumps.
I use gdb and got this message:

(gdb) file python
Reading symbols from python...(no debugging symbols found)...done.
(gdb) run lstm_ocr.py 
Starting program: /home/lili/tf1.2.1-py3/bin/python lstm_ocr.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff3a36700 (LWP 350)]
[New Thread 0x7ffff3235700 (LWP 351)]
[New Thread 0x7ffff0a34700 (LWP 352)]
[New Thread 0x7fffee233700 (LWP 353)]
[New Thread 0x7fffe9a32700 (LWP 354)]
[New Thread 0x7fffe9231700 (LWP 355)]
[New Thread 0x7fffe6a30700 (LWP 356)]
[Thread 0x7ffff3235700 (LWP 351) exited]
[Thread 0x7fffe6a30700 (LWP 356) exited]
[Thread 0x7fffe9231700 (LWP 355) exited]
[Thread 0x7fffe9a32700 (LWP 354) exited]
[Thread 0x7fffee233700 (LWP 353) exited]
[Thread 0x7ffff0a34700 (LWP 352) exited]
[Thread 0x7ffff3a36700 (LWP 350) exited]
[New Thread 0x7fffe6a30700 (LWP 364)]
[New Thread 0x7fffe9231700 (LWP 365)]
[New Thread 0x7fffe9a32700 (LWP 366)]
[New Thread 0x7fffee233700 (LWP 367)]
[New Thread 0x7fffa1d72700 (LWP 368)]
[New Thread 0x7fffa1571700 (LWP 369)]
[New Thread 0x7fffa0d70700 (LWP 371)]
[New Thread 0x7fff9bfff700 (LWP 372)]
loading train data, please wait--------------------- get image:  128000
loading validation data, please wait--------------------- get image:  500
2017-08-23 15:42:02.794111: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-23 15:42:02.794131: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-23 15:42:02.794135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-23 15:42:02.794161: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-23 15:42:02.794166: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
[New Thread 0x7fffaa462700 (LWP 374)]
[New Thread 0x7fffa9c61700 (LWP 375)]
[New Thread 0x7fffa9460700 (LWP 376)]
[New Thread 0x7fff9a13e700 (LWP 377)]
[New Thread 0x7fff9993d700 (LWP 378)]
[New Thread 0x7fff9913c700 (LWP 379)]
[New Thread 0x7fff9893b700 (LWP 380)]
[New Thread 0x7fff7ffff700 (LWP 381)]

Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
__GI___pthread_mutex_lock (mutex=0x3028) at ../nptl/pthread_mutex_lock.c:67
67      ../nptl/pthread_mutex_lock.c: no such file or directory.


### Source code / logs

"
12517,building from source for iOS crashes because of nsync compilation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.5
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: sh build_all_ios.sh

### Describe the problem
I am building tensorflow from source for iOS. it crashes when executing

`TARGET_NSYNC_LIB=tensorflow/contrib/makefile/compile_nsync.sh -t ios`

First, it looks for a specific version of the iPhoneSimulator (rather than the latest). When I change my symlink to point to the correct version, it crashes saying that <string.h> cannot be found

Note: the previous call to `HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh` executes correctly

### Source code / logs
"
12515,Inconsistent use of naming `Backprop`,"### Describe the problem
Tensorflow code has two types of naming `Backprop` and `BackProp`. How about to rename all `BackProp` to `Backprop` for consistency?

### Source code / logs
- `Backprop` in code : [control_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L909), [op_level_cost_estimator.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/op_level_cost_estimator.cc#L672)"
12514,incorrect gradient of reduce_prod(tf.complex*),"### Describe the problem
Tensorflow computes the wrong result for the following gradient:
```python
import tensorflow as tf
x = tf.Variable(1.0)
E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))
sess = tf.Session()
sess.run(tf.variables_initializer([x]))
sess.run(tf.gradients(E,x))
```
Tensorflow returns 10.0
The correct result is -6 since:
```
E = real((x+2i*x)^2) = real((1+2i)^2) * x^2 = real(1+4i-4) * x^2 = -3*x^2
dE/dx = -6*x = -6 for x=1
```
Below is mathematically equivalent code for E, for which Tensorflow returns the correct result of -6.0:
```python
E = tf.real( tf.complex(x,2*x) * tf.complex(x,2*x) )
E = tf.real(tf.exp(tf.reduce_sum(tf.log(tf.complex( [x,x], [2*x,2*x] )))))
```

### System information
Linux distribution = Arch Linux (up to date)
TensorFlow was installed from the Arch Linux package python-tensorflow
I'm using an x86_64 CPU. I'm not using my GPU.
numpy (1.13.1)
protobuf (3.3.2)
tensorflow (1.3.0)
python (3.6.2)"
12512,NaNs during training with `tf.contrib.rnn.LayerNormBasicLSTMCell`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**:  3.5
- **CUDA/cuDNN version**: CUDA 8.0 and CuDNN 6 (although I can replicate without a GPU)
- **GPU model and memory**: Nvidia K80 (from Amazon P2 instance)
- **Exact command to reproduce**:

### Describe the problem

When training a model using `tf.contrib.rnn.LayerNormBasicLSTMCell`, sometimes my weights go to `nan`, even though the training data looks perfectly innocent.

~~I have **not** seen this with Tensorflow 1.2.1, which leads me to suspect that there's been a regression somewhere, but I could've just been luckier (ðŸ€) with TF 1.2~~ (nevermind -- I have reproduced this with TF 1.2.1)

### Source code / logs

I've created two examples of this in https://github.com/alanhdu/tensorflow-12512 (clone the repo, enter a folder, and run `test.py` or build and run the `Dockerfile`. The key line(s) there are:

```python
print([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])
sess.run(train_step, feed_dict)
print([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])
```

The first print statement prints all `True`s, which is good -- but after the training step, suddenly some of the weights have `nan`s in them (and hence there's one `False` in the second print statement)."
12510,Support layer wise batch normalization parameter in tensorflow/contrib estimators,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   yes
- **OS Platform and Distribution:    Linux Ubuntu 14.04)
- **TensorFlow installed from (source or binary)**: installed from source
- **TensorFlow version (use command below)**:   1.2.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Tensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Following is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:

diff
--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py
index cb15ef2..a3d9c01 100644
--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py
+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py
@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):
       params.get(""input_layer_min_slice_size"") or 64 << 20)
   num_ps_replicas = config.num_ps_replicas if config else 0
   embedding_lr_multipliers = params.get(""embedding_lr_multipliers"", {})
+  layer_norm_func = params.get(""layer_norm_func"")
+  layer_norm_params = params.get(""layer_norm_params"", {})
+
+  layer_norm_params[""mode""] = mode
 
   features = _get_feature_dict(features)
   parent_scope = ""dnn""
@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):
             net,
             num_hidden_units,
             activation_fn=activation_fn,
+           normalizer_fn=layer_norm_func,
+           normalizer_params=layer_norm_params,
             variables_collections=[parent_scope],
             scope=hidden_layer_scope)
         if dropout is not None and mode == model_fn.ModeKeys.TRAIN:
@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):
                weight_column_name=None,
                optimizer=None,
                activation_fn=nn.relu,
+              layer_norm_func=None,
+              layer_norm_params=None,
                dropout=None,
                gradient_clip_norm=None,
                enable_centered_bias=False,
@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):
             ""optimizer"": optimizer,
             ""activation_fn"": activation_fn,
             ""dropout"": dropout,
+           ""layer_norm_func"": layer_norm_func,
+           ""layer_norm_params"": layer_norm_params,
             ""gradient_clip_norm"": gradient_clip_norm,
             ""embedding_lr_multipliers"": embedding_lr_multipliers,
             ""input_layer_min_slice_size"": input_layer_min_slice_size,


"
12508,DNNClassifier estimator cannot be exported,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **tensorflow/tensorflow:latest container**
- **ubuntu linux**
- **installed from pip**
- **TensorFlow version 'v1.2.0-5-g435cdfc', '1.2.1'**:
- **Python 2.7**: 
- **Bazel version (if compiling from source)**:

### Exact command to reproduce
```python
classifier = DNNClassifier(feature_columns=feature_columns,
                         hidden_units=[10, 20, 10],
                         n_classes=3,
                         model_dir=model_path)

classifier.export_savedmodel(MODEL_PATH, script.serving_input_receiver_fn)
```
### Describe the problem
Trying to export the model DNNClassifier throws the exception:
```bash
Exception during training: A default input_alternative must be provided.
 Traceback (most recent call last):
  File ""algo.py"", line 78, in train
    nn.export_savedmodel(MODEL_PATH, script.serving_input_receiver_fn, default_output_alternative_key=None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1280, in export_savedmodel
    actual_default_output_alternative_key)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 259, in build_all_signature_defs
    raise ValueError('A default input_alternative must be provided.')
```
The problem happens because DNNClassifier constructor creates a head with name `None`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/dnn.py#L365

"
12505,Building Error Solved,
12501,EditDistance crashes under C++ environment,"### System information
- **Windows 10**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.3-rc2**:
- **Python version 3.5**:
- **VisualStudio 2017**:

### Describe the problem
I have created a C++ example with EditDistance and linked a debug version of tensorflow.dll which is created from tensorflow source.  The compiled EditDistance example crashed at **session.Run( outputs, &output_tensor );**. I have tried in this example also in the tensorflow environment it crashes also. It seems the EditDistance operator doesn't work under C++. I have searched another EditDistance examples but it seems nobody has tested it outside the python environment.

### Source code / logs
```
#include <iostream>
#include <vector>
#include <string>
#include <unordered_set>

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/lib/gtl/edit_distance.h""
#include ""tensorflow/core/util/sparse/sparse_tensor.h""
#include ""tensorflow/core/protobuf/config.pb.h""

using namespace std;
```

    int main( int argc, char *argv[] )
```
{
           using namespace tensorflow;
           using namespace tensorflow::ops;
           using namespace tensorflow::gtl;
           Scope root = Scope::NewRootScope();
```

           std::vector<Tensor> output_tensor;
           ClientSession session( root );


           Tensor hypho2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 4 ), 3 } ) );
           Tensor hypho2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 4 )} ) );

           makeIndex( {""bear""}, hypho2_ix );
           makeChar ( {""bear""}, hypho2_vals );

           Tensor truth2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 5 ), 3 } ) );
           Tensor truth2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 5 )} ) );

           makeIndex( { ""beers"" }, truth2_ix );
           makeChar ( { ""beers"" }, truth2_vals );

           // Declaration of edit distance
           auto address_dist = EditDistance( root
                                   , hypho2_ix
                                   , hypho2_vals
                                   , {3,1,1}// {static_cast<int64_t>(1),3}//test_address_shape
                                   , truth2_ix
                                   , truth2_vals
                                   , {3,1,1} //ref_address_shape
                                   , EditDistance::Normalize(false) );

                                      const std::vector<Output> outputs = {address_dist};
           session.Run( outputs, &output_tensor );
     }   
        void makeIndex( const std::vector<string>& rsoStringVector, tensorflow::Tensor& roIndexTensor )
        {
           auto ix_t = roIndexTensor.matrix<int64_t>();
           std::size_t stCounter = 0;
           for( std::size_t stX = 0; stX < rsoStringVector.size() ; stX++ )
           {
              const std::string& rsString = rsoStringVector[ stX ];
              for( std::size_t stY = 0; stY < rsString.size(); stY++ )
              {
                 ix_t( stCounter, 0 ) = stX;
                 ix_t( stCounter, 1 ) = 0;
                 ix_t( stCounter, 2 ) = stY;
                 stCounter++;
              }
           }
        }

        void makeChar( const std::vector<std::string>& rsoStringVector, tensorflow::Tensor& roCharTensor )
        {
           auto vals_t = roCharTensor.vec<std::string>();

           int64_t i64Index = 0;
           for( std::size_t stX = 0; stX < rsoStringVector.size(); stX++ )
           {
              const std::string& rsString = rsoStringVector[ stX ];
              for( std::size_t stY = 0; stY < rsString.size(); stY++ )
              {
                 vals_t( i64Index++ ) = ( rsString[ stY ] );
              }
           }
        }
"
12500,Tensorflow Debugger crashes on tab complete,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary (pip wheel)
- **TensorFlow version (use command below)**: 1.3
- **Python version**:  3.5
- **GPU model and memory**: (CPU only)

### Describe the problem
Sometimes when using `tfdbg`, tab-completing on `pt` will crash the debugger (unfortunately, I haven't figured out how to reproduce this consistently yet) and my terminal will look like:

![screenshot from 2017-08-22 14-44-11](https://user-images.githubusercontent.com/1914111/29582837-2a0cd4d8-874c-11e7-899d-8430575ce1ba.png)

the terminal will be unresponsive, and scrolling + regular mouse selection will be disabled (although `SHIFT` + mouse selection will work).

### Source code / logs

Reconstructing the traceback:

```
Traceback (most recent call last):
File ""GenerativeLSTM.py"", line 151, in <module>
    m.train(klabels, config={""epochs"": 30, ""batch_size"": 1})
File ""GenerativeLSTM.py"", line 140, in train
    [next_batch, (train_step, merged)], feed_dict)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 532, in run
    run_end_resp = self.on_run_end(run_end_req)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 338, in on_run_end
    self._run_start_response = self._launch_cli()
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 429, in _launch_cli
        title_color=self._title_color)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 502, in run_ui
    exit_token = self._ui_loop()
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 578, in _ui_loop
    tab_completed = self._tab_complete(command)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 1507, in _tab_complete
    self._display_candidates(candidates)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 1554, in _display_candidates
    pad, _, _ = self._display_lines(candidates_output, 0)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 1130, in _display_lines
    pad = self._screen_new_output_pad(rows, cols)
File ""/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 978, in _screen_new_output_pad
    return curses.newpad(rows, cols)
_curses.error: curses function returned NULL
```

As I said, I unfortunately have not figured out how to reproduce this regularly -- this happens every once in a while when I'm working on the LSTM from #12465. I'll try to create a minimal reproducible test case later, but I figured I'd file this ticket first."
12498,Cannot open https://www.tensorflow.org in Safari,"As subject says, always error msg saying ""Safari cannot open the page, because it cannot open make the secure connection with the server"". Has anybody encountered the problem and can you share your experience how to fix ?
Opening the website from Firefox is OK.
System: Mac 10.12.6."
12496,SSL certificate for tensorflow.org expired,The SSL certificate for https://tensorflow.org (*not* https://www.tensorflow.org) expired on June 29.
12492,Feature request: manual parallel,"when I run my code in that way:

    for i in range(num):
        with tf.control_dependencies(None):
            output[i] = tf.identity(deepnn(x))

TF will run the deepnn(x) one by one.
Would it be possible to parallel it manually like this:

    #pragma omp for"
12491,AttributeError: 'int' object attribute '__doc__' is read-only,"In the file `tensorflow/compiler/tests/adagrad_test_poplar.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py"", line 191`, I have an error when trying to run unit tests.

Python is unhappy with trying to assign to __doc__.

This is on OS/X, with a virtualenv containing all of the modules required for the head of master on 22nd August 2017.

I have had to add `autograd` and `enum`, and as a consequence of the OS/X built-in numpy, I have switched to building tensorflow in a virtualenv.

"
12489,"error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions"," bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Linux Ubuntu 16.04
Bazel version (if compiling from source):0.5.2
CUDA/cuDNN version:9.0/7.0
GPU model and memory:GTX 965m

ERROR: /home/zhouzl/tensorflow-master/tensorflow/contrib/seq2seq/BUILD:51:1: error while parsing .d file: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.d (No such file or directory).
In file included from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/common_functions.h:50:0,
                 from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:115,
                 from <command-line>:0:
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""
                        ^
/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token """"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead."""" is not valid in preprocessor expressions
 #define __CUDACC_VER__ ""__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.""

"
12486,Feature request: stop requiring the same dtype for inputs in tf.shape_n,"As for Tensorflow 1.3, [tf.shape_n](https://www.tensorflow.org/api_docs/python/tf/shape_n) takes a list of tensors as input to produce a list of shapes as output. However, it produces an error if tensors of different types are provided. As far as I can tell, whether tensors are the same type or not is completely irrelevant to the behavior of this function, making it an arbitrary constraint that limits its functionality for no particular reason.

Would it be possible to remove such restriction if there's no good reason to keep it?
"
12482,Failure to build on OS/X,"I have the following error when building on OS/X:

```
./tensorflow/core/platform/default/mutex.h:25:10: fatal error: 'nsync_cv.h' file not found
#include ""nsync_cv.h""
         ^
1 error generated.
```

Earlier on I get the following warning:
```
WARNING: /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/core/BUILD:1632:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/core/BUILD:1632:1.
```
"
12480,Use Tensorflow in conjunction with PyTorch/Theano,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 0.12.1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: 5.1
- **GPU model and memory**: 1080-Ti
- **Exact command to reproduce**: None

### Describe the problem
I'm trying to use Tensorflow in conjunction with PyTorch (I built the model in Tensorflow to generate vector representations and PyTorch trains on top of those). However, the problem is that PyTorch runs out of memory because TF will replicate the model in ALL available CUDA devices. In this case CUDA_VISIBLE_DEVICES is not helpful, and I tried GPU device tf.device(""/gpu:0"") but Tensorflow still fills up all GPUs' memory.

Is there some way to actually limit Tensorflow's GPU usage to one and free up the other for other DL libraries like PyTorch or Theano?"
12478,Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms),"INFO:tensorflow:Creating bottleneck at /tmp/bottleneck\dandelion\10043234166_e6dd915111_n.jpg_inception_v3.txt
2017-08-22 12:36:45.710892: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\framework\op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().
2017-08-22 12:36:46.351813: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2017-08-22 12:36:46.351943: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2017-08-22 12:36:46.354865: E c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\stream_executor\cuda\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2017-08-22 12:36:46.356828: F c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\kernels\conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)"
12475,Feature request: sparse_tensor_dense_matmul optimization on GPU,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-0-g2784b1c', '1.3.0-rc2')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: 8.0 / 5.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X
- **Exact command to reproduce**:

I would like to optimize sparse_tensor_dense_matmul operation on GPU to process sparse input completely on GPU. Now code like this:
```
import tensorflow as tf

with tf.device('/gpu:0'):
    st = tf.SparseTensor(
        tf.constant([[0, 0], [1, 1]], dtype=tf.int64),
        tf.constant([1.2, 3.4], dtype=tf.float32),
        tf.constant([2, 2], dtype=tf.int64)
    ) 
    v = tf.Variable([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)
    st = tf.sparse_tensor_dense_matmul(st, v)
    st = tf.reduce_min(st)
    optimizer = tf.train.AdamOptimizer()
    trainer = optimizer.minimize(st)

with tf.Session() as sess:
    print(sess.run(trainer))
```
Fails with error:
```
Traceback (most recent call last):
  File ""test_tf3.py"", line 18, in <module>
    print(sess.run(trainer))
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2, _device=""/device:GPU:0""](Const, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_1, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_2)]]
```
It looks like it requires ""int64 strided slice"" to be executed on GPU. So maybe it just needs to enable int64 strided slice on GPU."
12474,CUDA 9RC + cuDNN7,"Things have moved forward.  I strongly suggest building from head with CUDA 9 and cuDNN 7.  All of the necessary codes **should** be in the TF 1.4 tag but given we are still working on new features for FP16, I would build from head if that is of interest.  I do not like to share anything I have not personally tested as I know how frustrating trying to get things to compile can be.


**Everything below this line is OUT DATED as of 19-OCT**

This is an **unofficial** and **very** not supported patch to make it possible to compile TensorFlow with CUDA9RC and cuDNN 7 or CUDA8 + cuDNN 7.

During testing on V100 (Volta) and ResNet-50 FP32  using CUDA 9RC + cuDNN 7 was significantly faster than CUDA 8 + cuDNN 6, which was not a surprise.  I am about to test on P100s.  I am sharing this patch informally so those that are interested can play with cuDNN 7 as well as CUDA 9RC before we have the official release.  As we have more interesting code, e.g FP16 models, I will share it in this issue.   I expect NVIDIA will start to submit official cuDNN 7 patches very soon.  

**Note:**  This patch may work on more recent versions of TensorFlow but it will likely bit rot so keep that in mind.  Apply the cuDNN 7 patch and then fast-forwarding the branch might be the best approach.  My git skills are not strong so do what you think is best.  

1. Download the patches
- [0001-CUDA-9.0-and-cuDNN-7.0-support.patch](https://storage.googleapis.com/tf-performance/public/cuda9rc_patch/0001-CUDA-9.0-and-cuDNN-7.0-support.patch)
- [eigen.f3a22f35b044.cuda9.diff](https://storage.googleapis.com/tf-performance/public/cuda9rc_patch/eigen.f3a22f35b044.cuda9.diff)

2. Clone the tensorflow repo
```https://github.com/tensorflow/tensorflow.git```

3. Checkout the revision that the TensorFlow patch can apply to:
```git checkout db596594b5653b43fcb558a4753b39904bb62cbd~```

4. Apply the TensorFlow patch:
```git apply ~/Downloads/0001-CUDA-9.0-and-cuDNN-7.0-support.patch```

5. Run ./configure. When it asks for the CUDA version, put 9.0 (or 8). When it asks for the cudnn version, put 7.0.1 (Entering '7' worked fine for me). Make sure you put the paths to the right Cuda and cudnn versions and have your ldconfig or LD_LIBARY_PATH set to point to the CUDA 9 folder.
```./configure```

6. Attempt to build TensorFlow, so that Eigen is downloaded. **This build will fail if building for CUDA9RC but will succeed for CUDA8**
 ```bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package```

7. Apply the Eigen patch:
```bash
    cd -P bazel-out/../../../external/eigen_archive
    patch -p1 < ~/Downloads/eigen.f3a22f35b044.cuda9.diff
```

8. Build TensorFlow successfully
```bash
    cd -
    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

I have run this process myself on Ubuntu 14.04 with Python 2.7.

Thank for NVIDIA for the early patch and @reedwm who created most of these instructions.  

If you are using Python 2.7 and gcc 4.8+ here is a [.whl](https://storage.googleapis.com/tf-performance/tf_binary/gcc4_8/tensorflow-1.4.HEAD.b9ac2d7eb.AUG_1.CUDA9rc-cp27-none-linux_x86_64.whl) where I followed the instructions above and [one with CUDA 8 and cuDNN 7](https://storage.googleapis.com/tf-performance/tf_binary/gcc4_8/tensorflow-1.4.HEAD.b9ac2d7eb.AUG_1.CUDA8_cuDNN7-cp27-none-linux_x86_64.whl) which I have yet to test.  Stress again, this was created by me and not the TensorFlow build team.  My/our goal is to engage with anyone that wants to try this out and try to have a little fun.  :-)
"
12473,[bug] gradients of scatter_nd_add return None ,"The gradient of scatter_nd_add  always return None.  If the gradient is not implemented, it should raise an exception. 

```python

import tensorflow as tf
import numpy as np 
import matplotlib.pyplot as plt
rng = np.random



x = tf.Variable(np.random.random((2, 4)).astype('float32'))
indice = tf.Variable(np.random.randint(0, 4, size=(2, 4, )), dtype=tf.int32)




x_val = np.random.random((2, 4))
indice_val = np.random.randint(0, 4, size=(2, 4, ))
val_val = np.random.random((2, 4))


# tf Graph Input
X = tf.placeholder(""float"")
Y = tf.placeholder(""float"")


# Set model weights
W = tf.Variable(np.random.random((2, 4)).astype('float32'), name=""weight"")
b = tf.Variable(np.random.random((2, )).astype('float32'), name=""bias"")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

y = tf.scatter_nd_add(x, indice, pred)


grad = tf.gradients(y, [W,b])

```"
12472,Feature request: EASGD,"EASGD is a very useful algorithm for distributed asynchronous training, which based on an elastic force which links the parameters of workers with a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. 

In practice I find it usually outperforms ordinary ASGD.  I have been working on it  for some days and I can make a pull request if you are interested in it.

## The link address of the paper
[Deep learning with Elastic Averaging SGD](https://arxiv.org/pdf/1412.6651.pdf)

"
12468,external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory,"## System information
After run the tf_env_collect.sh in my terminal, i get this infomation:

== cat /etc/issue ===============================================
Linux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.4)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc1-1204-g084d29e
tf.COMPILER_VERSION = v1.3.0-rc1-1204-g084d29e
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/include:/usr/local/cuda-8.0/include:$LD_LIBRARY_PATH
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Aug 22 09:32:48 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K620         Off  | 0000:01:00.0      On |                  N/A |
| 34%   40C    P0     2W /  30W |    291MiB /  1999MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1037    G   /usr/lib/xorg/Xorg                             142MiB |
|    0      1867    G   compiz                                          58MiB |
|    0      2340    G   ...el-token=36B9BD8BE2E6AD02534077E8E73C38D9    89MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7

Tensorflow version:('v1.3.0-rc1-1204-g084d29e', '1.3.0')

##  Describe the problem

 when i  use ""bazel build //tensorflow/examples/android:tensorflow_demo""  in terminal i get the error:

ERROR: /home/saners/tensorflow/tensorflow/core/kernels/BUILD:4581:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 77 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,
                 from ./tensorflow/core/kernels/bias_op_gpu.h:21,
                 from tensorflow/core/kernels/bias_op.cc:30:
external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory
 #include <cuda_runtime.h>
                          ^
compilation terminated.
Target //tensorflow/examples/android:tensorflow_demo failed to build

my tensorflow has installed normally,but i don't know how to solve this problem.
Anyone can help me ? "
12467,Error in estimator.py ,"Hi, 
I am running the tutorial code A Guide to TF Layers: Building a Convolutional Neural Network on API r.1.3
https://www.tensorflow.org/tutorials/layers

My code is here. 
https://gist.github.com/Po-Hsuan-Huang/91e31d59fd3aa07f40272b75fe2a924d

The error shows:
```
runfile('/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py', wdir='/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST')
Extracting MNIST-data/train-images-idx3-ubyte.gz
Extracting MNIST-data/train-labels-idx1-ubyte.gz
Extracting MNIST-data/t10k-images-idx3-ubyte.gz
Extracting MNIST-data/t10k-labels-idx1-ubyte.gz
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/mnist_convnet_model', '_save_summary_steps': 100}
Traceback (most recent call last):

  File ""<ipython-input-1-c9b70e26f791>"", line 1, in <module>
    runfile('/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py', wdir='/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST')

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py"", line 94, in execfile
    builtins.execfile(filename, *where)

  File ""/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py"", line 129, in <module>
    main(None)

  File ""/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py"", line 117, in main
    hooks=[logging_hook])

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 241, in train
    loss = self._train_model(input_fn=input_fn, hooks=hooks)

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 630, in _train_model
    model_fn_lib.ModeKeys.TRAIN)

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 615, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)

  File ""/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py"", line 24, in cnn_model_fn
    input_layer = tf.reshape(features, [-1, 28, 28, 1])

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2619, in reshape
    name=name)

  File ""/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 493, in apply_op
    raise err

TypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'x': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(100, 784) dtype=float32>}. Consider casting elements to a supported type.
```

I traced down a little bit, and found the function `estimator._call_input_fn()`  does not use parameter 'mode' at all, thus not able to create a tuple comprises features and labels. Is it the tutorial that needs to be modified, or there is some problem with this function. I don't understand why `mode ` is unused here. 

Thanks ! Sorry if this should not be posted here.

```
def _call_input_fn(self, input_fn, mode):
    """"""Calls the input function.

    Args:
      input_fn: The input function.
      mode: ModeKeys

    Returns:
      Either features or (features, labels) where features and labels are:
        features - `Tensor` or dictionary of string feature name to `Tensor`.
        labels - `Tensor` or dictionary of `Tensor` with labels.

    Raises:
      ValueError: if input_fn takes invalid arguments.
    """"""
    del mode  # unused
    input_fn_args = util.fn_args(input_fn)
    kwargs = {}
    if 'params' in input_fn_args:
      kwargs['params'] = self.params
    if 'config' in input_fn_args:
      kwargs['config'] = self.config
    with ops.device('/cpu:0'):
      return input_fn(**kwargs)

```"
12466,tensorflow contrib modules break when running in a pyc only environment,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Amazon Linux 2016.09
- **TensorFlow installed from (source or binary)**:
binary, no gpu, virtualenv method
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
python -c ""from tensorflow.contrib.layers import fully_connected""

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When building a python install for a disk space constrained environment, it's fairly common to first compile all .py files to .pyc files, and then only install the .pyc files, to save a few megabytes of space. When attempting this for tensorflow, it became unable to import it's contrib modules (traceback below). From some testing I did, this appears to be due to differing behavior in this snippet from the `get_path_to_datafile` function in `python/platform/resource_loader.py`: `data_files_path = _os.path.dirname(_inspect.getfile(_sys._getframe(1)))`

when the .py files are present, this snippet produces an absolute path. When only the .pyc files are present, it produces a relative path, which is then concatenated with another path, as the `get_path_to_datafile` function is called twice.

### Source code / logs
```
File ""./tensorflow/contrib/__init__.py"", line 27, in <module>
File ""./tensorflow/contrib/cudnn_rnn/__init__.py"", line 28, in <module>
File ""./tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py"", line 34, in <module>
File ""./tensorflow/contrib/util/loader.py"", line 55, in load_op_library
File ""./tensorflow/python/framework/load_library.py"", line 64, in load_op_library
NotFoundError: ./tensorflow/contrib/util/./tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory
```"
12465,Tensorflow Debugger eats disk space with RNNs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**:  3.5

### Describe the problem

I have an RNN that I'm trying to debug using `LocalCLIDebugWrapperSession` which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via `tf.nn.dynamic_rnn`, `tfdbg` uses a lot of disk space (>5 GB sometimes). When I accidentally collected gradient infromation via `tf.contrib.layers.optimize_loss(..., summaries=[""gradients"", ...])`, this ballooned even more to >70 GB (which promptly crashed my laptop).

From inspecting the `tfdbg` dump in `/tmp/`, it looks like this is because tfdbg` dumps out information for each time step. Especially with `tf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.

In some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem."
12461,README link to Linux GPU Python 3.5 nightly build is broken ,"I figured you wouldn't need the form information. The link:

https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl

My response:

```
HTTP ERROR 404

Problem accessing /view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl. Reason:

    Not Found
Powered by Jetty://
```
"
12460,TensorFlow Android 1.2 - Adds READ_PHONE_STATE permission,
12456,TF 1.3 Install error,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  3.6

### Describe the problem
Error when installing with pip

pip3 install tensorflow-gpu

This error appears :

  ```
File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 357, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 314, in read
    data = self._fp.read(amt)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py"", line 63, in read
    self._close()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py"", line 50, in _close
    self.__callback(self.__buf.getvalue())
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/controller.py"", line 275, in cache_response
    self.serializer.dumps(request, response, body=body),
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py"", line 55, in dumps
    ""body"": _b64_encode_bytes(body),
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py"", line 12, in _b64_encode_bytes
    return base64.b64encode(b).decode(""ascii"")
  File ""/home/ubuntu/anaconda3/lib/python3.6/base64.py"", line 58, in b64encode
    encoded = binascii.b2a_base64(s, newline=False)
```"
12454,Misleading error message on type mismatch,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6
```python
import tensorflow as tf
x = tf.get_variable('asdfds', shape=[10], dtype=tf.int32)
x*0.5
```
Prints:
```
Traceback (most recent call last):               
  File ""a.py"", line 9, in <module>               
    x*0.5                                        
TypeError: unsupported operand type(s) for *: 'Variable' and 'float' 
```
Variable with matching dtype can multiply with float, the problem here is int32 cannot multiply with float.
Since usually the dtype of some tensor is not directly written in code, this misleading message can cause confusions."
12453,Adaptive optmizers do not work well with multi-head networks when some labels are missing,"Use case: we have a multi-task network with many outputs. Each example in the dataset has only subset of the labels; non-existing loss components are masked out. A single optimizer is used, because the presence of particular labels in the example stream is statically unknown.

Problem: masked loss components give zero gradient estimates for the corresponding variables. That breaks adaptive optimizers (Momentum, Adam, ...) because steps are taken and slots are corrupted with the incoming zero gradient estimates. The desired behaviour is: do nothing for variables (and corresponding slots) that were effectively unused in the forward pass (like it is done for the embeddings using the IndexedSlices trick).
Related problem: tf.global_norm, tf.clip_by_global_norm are also affected by these stray zero gradient estimates.

Minimal example/demonstration:
```python
In [14]: feature = tf.constant([1.0])
In [15]: output = feature * 42.0
In [16]: loss = tf.squared_difference(output, [0.0])
In [17]: mask = tf.constant([False])
In [18]: masked_loss = tf.boolean_mask(loss, mask)
In [19]: masked_loss_grad = tf.gradients(masked_loss, feature)
In [20]: masked_loss_grad
Out[20]: [<tf.Tensor 'gradients/mul_3_grad/Reshape:0' shape=(1,) dtype=float32>]
In [21]: masked_loss_grad[0].eval(session=session)
Out[21]: array([ 0.], dtype=float32)
```
This is technically correct -- the gradient is indeed zero -- but if `feature` was a Variable, we don't really want to apply this gradient in an adaptive optimizer, because the training example didn't have useful information about the loss function.

Feature request: better way to deal with that (probably a special value for the gradients that would signify ""nonexisting"" gradients; e.g. tensorflow uses `None`, when this information is available statically).

For now here is a simple and ugly workaround:
```python
def _is_all_zeros(grad):
    all_zeros = tf.equal(tf.count_nonzero(grad), 0)
    return all_zeros

class HackedMomentumOptimizer(tf.train.MomentumOptimizer):
    def __init__(self, *args, **kwargs):
        super(HackedMomentumOptimizer, self).__init__(*args, **kwargs)

    def _apply_dense(self, grad, var):
        all_zeros = _is_all_zeros(grad)
        return tf.cond(all_zeros,
                       lambda: tf.no_op(),
                       lambda: super(HackedMomentumOptimizer, self).
		       _apply_dense(grad, var))

    def _resource_apply_dense(self, grad, var):
        all_zeros = _is_all_zeros(grad)
        return tf.cond(all_zeros,
                       lambda: tf.no_op(),
                       lambda: super(HackedMomentumOptimizer, self).
                       _resource_apply_dense(grad, var))

    def _apply_sparse(self, grad, var):
        all_zeros = _is_all_zeros(grad)
        return tf.cond(all_zeros,
                       lambda: tf.no_op(),
                       lambda: super(HackedMomentumOptimizer, self).
                       _apply_sparse(grad, var))

    def _resource_apply_sparse(self, grad, var, indices):
        all_zeros = _is_all_zeros(grad)
        return tf.cond(all_zeros,
                       lambda: tf.no_op(),
                       lambda: super(HackedMomentumOptimizer, self).
                       _resource_apply_sparse(grad, var, indices))


def _clip_gradients(gradients_variables, clip_norm=20.):
    gradients, variables = six.moves.zip(*gradients_variables)

    def _replace_nonexisting_grad(grad):
        all_zeros = _is_all_zeros(grad)
        return tf.cond(all_zeros,
                       lambda: tf.zeros([], dtype=tf.float32),
                       lambda: grad)
    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]
    fixed_global_norm = tf.global_norm(gradients_filtered)
    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm,
	                                            use_norm=fixed_global_norm)
    tf.summary.scalar('global_norm', global_norm)
    return six.moves.zip(gradients, variables)
```"
12451,The tensorflow wasn't compiled to use AVX2 on Windows,"I instaled the python 3 and the tensorflow from pip.
But when I run the program I receive the warnings:
![image](https://user-images.githubusercontent.com/29586672/29518621-76586372-8650-11e7-8a86-36e4fce5923d.png)


How to hidden or fix the warnings on Windows? Thank you so much!"
12450,Freezing Model drops Output Accuracy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.2 and v1.3 (Same Problem repeats on both)
- **Python version**: 
3.5
- **CUDA/cuDNN version**:
Cuda compilation tools, release 8.0, V8.0.44
cuDNN Version 5.1.10
- **GPU model and memory**:
GeForce GTX 1060 - 6GB

I have a image segmentation network designed to classify roads and obstacles.  I want to freeze the model and serve it as a API. So I used the default TensorFlow tool for freezing the model. After freezing, the output given by the network are completely off and inaccurate.

Here is one sample. 

**The Input Image**

![00016](https://user-images.githubusercontent.com/13032916/29518128-d016402e-8695-11e7-95e5-1fdac3e5b0f6.png)

**Output when tested using ckpt files**
![16_actual](https://user-images.githubusercontent.com/13032916/29518247-3623ea56-8696-11e7-94e0-6eabe13ab066.png)

**Output after Freezing the Model**
![16](https://user-images.githubusercontent.com/13032916/29518258-4087803e-8696-11e7-9a56-4c5d45d7fda1.png)

I have tried to freeze using different versions of tensorflow, but that has not helped. Since the network is performing as excepted when tested against checkpoint, the issue, I think is in the Freeze Models Script. The network uses Batch_normalisation. Could this be the reason for this drop because I saw a couple of issues linked to that of similar nature? How can I avoid that?
"
12449,std::system_error after restoring model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, used saving/restoring example [here](https://www.tensorflow.org/programmers_guide/saved_model)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Red Hat 4.4.7-1
- **TensorFlow installed from (source or binary)**: Anaconda
- **TensorFlow version (use command below)**: 1.2, CPU-only
- **Python version**: 3.5
- **Exact command to reproduce**: [See example here](https://www.tensorflow.org/programmers_guide/saved_model). Error occurs on line `saver.restore(sess, ""/tmp/model.ckpt"")`

### Describe the problem
When restoring the model using the simple example listed above, the program terminates with the error: 
```
terminate called after throwing and instance of 'std::system_error'
what(): Resource temporarily unavailable
```
Given such a simple example it can't be a memory issue, and there seems to be no other information about this error message online (eg. stackoverflow). Could it be a bug?"
12446,tensorflow cmake compile problem on vs2015,"------------------------

### System information
- **windows10**:
- **install tensorflow**: from source by vs2015 
- **TensorFlow version**: r1.2
- **Python**: Anaconda 4.2.0, python3.5
-**SWIG**:  swigwin3.0.10
-**CMake**:  3.8.2
- **Command line to compile**:
`D:\Developer\Microsoft Visual Studio 14.0\VC\bin\amd64\vcvars64.bat`
`cd tensorflow\tensorflow\contrib\cmake`
` mkdir build`
`cd build`
`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^`
`More? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^`
`More? -DPYTHON_EXECUTABLE=D:/Developer/Anaconda3/python.exe ^`
`More? -DPYTHON_LIBRARIES=D:/Developer/Anaconda3/libs/python35.lib ^`
`More? -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX`
### Describe the problem
after generate the configure files, run the command as bellow:
`MSBuild /M:10 /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`   
I have used this command to compile the tensorflow c++ example project and some c++ libs successfully about a month ago.But now, I tried many times never success.

#### Error log
But this time it didn't work, and return the compile error :  
`""C:\Users\Administrator\Desktop\tensorflow\tensorflow\contrib\cmake\build\tf_tutorials_example_trainer.vcxproj""
       (default target) (1) ->
       ""C:\Users\Administrator\Desktop\tensorflow\tensorflow\contrib\cmake\build\tf_core_framework.vcxproj"" (default ta
       rget) (6) ->
       (CustomBuild target) ->
         C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd
       .exe"" exited with code 1. [C:\Users\Administrator\Desktop\tensorflow\tensorflow\contrib\cmake\build\tf_core_fram
       ework.vcxproj]
    277 Warning(s)
    1 Error(s)`
"
12445,proto files compiled with go: inconsistent package names: tensorflow.grpc tensorflow,"when I compile proto files with go, I got the error:
`protoc-gen-go: error:inconsistent package names: tensorflow.grpc tensorflow
--go_out: protoc-gen-go: Plugin failed with status code 1.`

my command:
`protoc -I=./tensorflow_serving  --go_out=plugins=grpc:. ./tensorflow/core/protobuf/*.proto`
tensorflow version: 1.3.0

I found that in the tensorflow/core/protobuf/master_service.proto and tensorflow/core/protobuf/worker_service.proto go package  declarations is 

> package tensorflow.grpc;

and other proto files in protobuf is

> package tensorflow.grpc;

go do not support different package names in one packageã€‚
"
12444,The sample code on PROGRAMMER'S GUIDE has a problem,"Hello,

There is a problem on the PROGRAMMER'S GUIDE.
https://www.tensorflow.org/programmers_guide/variables#sharing_variables

In the second code snippet of the chapter ""Sharing variables""
```Python
input1 = tf.random_normal([1,10,10,32])
input2 = tf.random_normal([1,20,20,32])
x = conv_relu(input1, kernel_shape=[5, 5, 1, 32], bias_shape=[32])
x = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails.
```

### Problem
The shape doesn't match.

### Solution
Change the third line to
```Python
x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])
```

And the code after this also have the problem."
12442,Installation failure on Windows 10,"Hi,

i'm having trouble installing tensorflow. It already worked and evereything was fine when i changed cuDNN to 6.0 for something else.
But now, even after installing everything from the drivers of the gpu, to CUDA and cudnn5.1 new, tensorflow installation is failing. I am always getting this error, but every solution i found in other issues with similar errors is not working.
All the pathes are set correct. Anyone has an idea?

`>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_
module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_
module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line
24, in <module>
    from tensorflow.python import *
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py""
, line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tenso
rflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-pac
kages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-pac
kages\tensorflow\python\__init__.py"", line 49, in <module>                ed
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-pac
kages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>       e
    raise ImportError(msg)                                                oved
ImportError: Traceback (most recent call last):
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-pac
kages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_i
mport_helper
    return importlib.import_module(mname)
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\importlikages\tensorflow\python\pywrap_tenso
b\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import          kages\tensorflow\python\pywrap_tenso
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlock
ed                                                                        kages\tensorflow\python\pywrap_tenso
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\rkoch\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help`"
12441,Installation problem while building tensorflow(GPU) ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12440,How to Retrain Inception v3 without a reshape layer?,"Hi all,

I was trying the pretrained Inception v3 model available in tensorflow.When i was looking into the tensorgraph of the retrained model i found that while retrianing it uses a reshape layer followed by a full connected layer.

Is there any method to retrain the model without using the reshape layer?
Please suggest me any possible solution if possible.

Thank and Regards"
12439,Train Multibox object detector included in the TF Detect Android demo,Can anyone guide me how to train our own multibox_model.pb included in the tensorflow android example??
12437,Tensorflow GPU import causes exeptions [Resolved],"There is a problem occures when I import tensorflow.

> 
> ""C:\Program Files\Python35\python.exe"" D:/NetDisks/DropBox/Dropbox/Develop/Python/TensorFlow/GPUTest.py
> Traceback (most recent call last):
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
>     return importlib.import_module(mname)
>   File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
>   File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
>   File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
>   File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
>   File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
>   File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
>   File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
>   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
> ImportError: DLL load failed: ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
>     return importlib.import_module('_pywrap_tensorflow_internal')
>   File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
> ImportError: No module named '_pywrap_tensorflow_internal'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""D:/NetDisks/DropBox/Dropbox/Develop/Python/TensorFlow/GPUTest.py"", line 1, in <module>
>     import tensorflow as tf
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
>     return importlib.import_module(mname)
>   File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
>   File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
>   File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
>   File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
>   File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
>   File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
>   File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
>   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
> ImportError: DLL load failed: ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
>     return importlib.import_module('_pywrap_tensorflow_internal')
>   File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
> ImportError: No module named '_pywrap_tensorflow_internal'
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/install_sources#common_installation_problems
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
> 
> Process finished with exit code 1
> 
> 

Windows 7 x64 SP1

CPU: Intel Core I7 990x
Motherboard: Asus P6T SE
GPU: GTX 1080 TI

tensorflow have been installed with native PIP, command: 

> pip install tensorflow-gpu

Python 3.5.2

CUDA 8.0 Toolkit

CUDNN 5.1

The code is:
`import tensorflow as tf`

Path:

> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin;
> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\libnvvp;
> C:\Program Files\Python35\Scripts\;
> C:\Program Files\Python35\;
> C:\Program Files\Microsoft MPI\Bin\;
> C:\Windows\;
> C:\Windows\System32;
> C:\Program Files\dotnet\;
> C:\Program Files\Anaconda3;
> C:\Program Files\Anaconda3\Scripts;
> C:\Program Files\Anaconda3\Library\bin;
> C:\Program Files\Anaconda2;
> C:\Program Files\Anaconda2\Scripts;
> C:\Program Files\Anaconda2\Library\bin;
> C:\Program Files (x86)\Anaconda3;
> C:\Program Files (x86)\Anaconda3\Scripts;
> C:\Program Files (x86)\Anaconda3\Library\bin;
> C:\Program Files (x86)\Anaconda2;
> C:\Program Files (x86)\Anaconda2\Scripts;
> C:\Program Files (x86)\Anaconda2\Library\bin;
> C:\Program Files\NVIDIA Corporation\cuda\bin;
> C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;
> C:\Program Files\NVIDIA Corporation\cuda\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin;
> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0;
> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64;
> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64;

CUDA_HOME

> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0

CUDA_PATH

> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0

CUDA_PATH_V8_0

> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0


MSVC++ 2015 x64 is installed

cudnn64_5.dll is located at:
> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin


Also if i install a regular tensorflow, it works OK.

How do I deal with/fix this?

"
12436,zeros_like doesn't fully respect the optimize argument,"[The definition of `zeros_like`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1463) is:

```python
def zeros_like(tensor, dtype=None, name=None, optimize=True):
  with ops.name_scope(name, ""zeros_like"", [tensor]) as name:
    tensor = ops.convert_to_tensor(tensor, name=""tensor"")

    if tensor.shape.is_fully_defined():
      # We can produce a zeros tensor independent of the value of 'tensor',
      # since the shape is known statically.
      return zeros(tensor.shape, dtype=dtype or tensor.dtype, name=name)

    if dtype is not None and dtype != tensor.dtype:
      return zeros(
          shape_internal(tensor, optimize=optimize), dtype=dtype, name=name)
    else:
      return gen_array_ops._zeros_like(tensor, name=name)
```

We can see that if the shape of `tensor` is already known, the `optimize` parameter is ignored, which is inconsistent with the [documented behavior](https://www.tensorflow.org/api_docs/python/tf/zeros_like)."
12434,OpenMP/OpenACC support for tensorflow,Is there any effort in porting the Tensorflow using OpenMP or OpenACC?
12431,[android demo] UnsatisfiedLinkError exception on app start if build with nativeBuildSystem = 'none',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
app build at Android Studio 2.3.3 on Linux Ubuntu 16.04
app tested on Meizu M2 Note (Android 5.1, API 22) 
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Master branch from GitHub
- **Python version**: 
2.7 (not used in that case)
- **Bazel version (if compiling from source)**:
0.5.3 (not used in that case) 
- **CUDA/cuDNN version**:
none
- **GPU model and memory**:
none
- **Exact command to reproduce**:
Android Studio/Run (build.gradle)

### Describe the problem
With `nativeBuildSystem = 'none'` app exit with UnsatisfiedLinkError exception (see log below).
Quick investigation shows call of `env.ImageUtils.convertYUV420SPToARGB8888(Native Method)` native method from `CameraActivity.onPreviewFrame(CameraActivity.java:113)`. 
`convertYUV420SPToARGB8888` suppose to be implemented in `libtensorflow_demo.so` wich is not created in case of `nativeBuildSystem = 'none'`. (With `nativeBuildSystem = 'cmake'` `libtensorflow_demo.so` created and `convertYUV420SPToARGB8888` works fine)
Quick fix may be obtained by including `libtensorflow_demo.so` into JCenter and download it like the TensorFlow Inference Interface package.
Another solution would be Java implementation of `convertYUV420SPToARGB8888`, the same as it been done for `convertYUV420ToARGB8888` in commit #003deb8 at PR #10771 ""Refactor and implementation of the camera API 1, it fixes #8736 #10771"".
I start working with that Java implementation of `convertYUV420SPToARGB8888` but I don't think I have enough skills to make it. At the moment I create Java wrapper for native convertYUV420SPToARGB8888 and make all native methods `private` to isolate the rest of the Java code from occasional use them:
[https://github.com/ArtsiomCh/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java](url)
[https://github.com/ArtsiomCh/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/env/ImageUtils.java](url)
Would be great if @osdamv (creator of Java implementation of `convertYUV420ToARGB8888`) can help in porting Java implementation of `convertYUV420ToARGB8888` for `convertYUV420SPToARGB8888`

### Source code / logs
08-20 13:28:38.212 19058-19058/? I/TensorFlowInferenceInterface: Model load took 1303ms, TensorFlow version: 1.2.0
08-20 13:28:38.217 19058-19058/? I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/tensorflow_inception_graph.pb'
08-20 13:28:38.219 19058-19058/? I/TensorFlowImageClassifier: Read 1001 labels, output layer size is 1008
08-20 13:28:38.219 19058-19058/? I/tensorflow: ClassifierActivity: Sensor orientation: 90, Screen orientation: 0
08-20 13:28:38.219 19058-19058/? I/tensorflow: ClassifierActivity: Initializing at size 640x480
08-20 13:28:38.223 19058-19058/? W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.
08-20 13:28:38.225 19058-19058/? E/art: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)
08-20 13:28:38.226 19058-19058/? E/AndroidRuntime: FATAL EXCEPTION: main
                                                   Process: org.tensorflow.demo, PID: 19058
                                                   java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)
                                                       at org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(Native Method)
                                                       at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:113)
                                                       at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1288)
                                                       at android.os.Handler.dispatchMessage(Handler.java:111)
                                                       at android.os.Looper.loop(Looper.java:194)
                                                       at android.app.ActivityThread.main(ActivityThread.java:5877)
                                                       at java.lang.reflect.Method.invoke(Native Method)
                                                       at java.lang.reflect.Method.invoke(Method.java:372)
                                                       at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1020)
                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:815)
08-20 13:28:38.265 19058-19058/? I/Process: Sending signal. PID: 19058 SIG: 9"
12430,external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory,"## System information
After run the tf_env_collect.sh in my terminal, i get this infomation:

== cat /etc/issue ===============================================
Linux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.4)

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Aug 21 17:31:46 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K620         Off  | 0000:01:00.0      On |                  N/A |
| 34%   41C    P8     1W /  30W |    307MiB /  1999MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1014    G   /usr/lib/xorg/Xorg                             134MiB |
|    0      1723    G   compiz                                          82MiB |
|    0      2368    G   /proc/self/exe                                  88MiB |
+-----------------------------------------------------------------------------+
== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7

Tensorflow version:('v1.3.0-rc1-1204-g084d29e', '1.3.0')

##  Describe the problem

1. 1
when i use  ""sudo bazel build //tensorflow/examples/android:tensorflow_demo""   to get .apk 
I meet the error:
ERROR: /home/saners/tensorflow/tensorflow/core/kernels/BUILD:4581:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 77 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,
                 from ./tensorflow/core/kernels/bias_op_gpu.h:21,
                 from tensorflow/core/kernels/bias_op.cc:30:
external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory
 #include <cuda_runtime.h>
                          ^
compilation terminated.
Target //tensorflow/examples/android:tensorflow_demo failed to build

1.2

 when i use ""bazel build //tensorflow/examples/android:tensorflow_demo"" in the terminal
the error is changed : 
tensorflow/core/kernels/lrn_op.cc:34:31: fatal error: cuda/include/cuda.h: No such file or directory

1.3
""import tensorflow""  and  ""import tensorflow as tf""  in python  is ok but 
why  it has some problems after run the _tf_env_collect.sh_ for example ''ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory
""
and how to fix the error   ""LD_LIBRARY_PATH is unset"" and ""DYLD_LIBRARY_PATH is unset""

my tensorflow has installed,but i don't know how to solve this problem.
Anyone can help me ? I'am very tired to deal with the error but no method works.
THANK YOU VERY MUCH!"
12428,Train our own dataset,"Can you guide me to train our own data-set for TFdetect example??? I read lots of guidelines regarding this, but those are confusing me So can anyone give me well ordered set of instructions to complete this.."
12427,Error when retraining with retrain.py,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: original one
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.10
- **TensorFlow installed from (source or binary)**: `sudo pip3 install tensorflow`
- **TensorFlow version (use command below)**: latest, `sudo pip3 install tensorflow` before retraining, `v1.0.0-65-g4763edf-dirty 1.0.1`
- **Python version**: Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
- **Bazel version (if compiling from source)**: not used
- **CUDA/cuDNN version**: not used
- **GPU model and memory**: not used
- **Exact command to reproduce**:

I know it's Ubuntu 16.10, but mb the problem in something different (that you may already know)?

### Describe the problem
Error when retraining mobilenet

### Source code / logs

```
~$ python ~/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py \
>     --image_dir ~/Desktop/train_tf/images/ \
>     --learning_rate=0.0005 \
>     --testing_percentage=20 \
>     --validation_percentage=20 \
>     --train_batch_size=32 \
>     --validation_batch_size=-1 \
>     --flip_left_right True \
>     --random_scale=30 \
>     --random_brightness=30 \
>     --eval_step_interval=100 \
>     --how_many_training_steps=10000 \
>     --architecture mobilenet_1.0_128_quantized
INFO:tensorflow:Looking for images in 'road'
INFO:tensorflow:Looking for images in 'car'
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]
Traceback (most recent call last):
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/home/osboxes/anaconda3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1058, in main
    distorted_image_tensor, resized_image_tensor, bottleneck_tensor)
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 599, in get_random_distorted_bottlenecks
    {resized_input_tensor: distorted_image_data})
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]

Caused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 984, in main
    create_model_graph(model_info))
  File ""/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 282, in create_model_graph
    model_info['resized_input_tensor_name'],
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 288, in import_graph_def
    op_def=op_def)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[""SAME"", ""VALID""]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)
	 [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/cpu:0""](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]

```
"
12426,tf.losses.log_losses calculation may be incorrect,"https://github.com/tensorflow/tensorflow/blob/a0bbfa6afec617697576210fb22f3ea9c3a57b61/tensorflow/python/ops/losses/losses_impl.py#L409

here we are only considering one extreme that the value of prediction can be ""zero"", missing out probably that it can be ""one"" also.

If the variables predictions value is 1 then this would return a log loss of 
-log(1+1e-7) = -4.3429446044209946e-08
wherein if such occurs multiple times than it can indicate a lower loss.

I suggest clipping the value between [ 0.0000001 , 0.9999999 ] "
12425,Feature: TF on mix environnment CPU-GPU,"When using TF-GPU, either switches to GPU depending on GPU availability.

When we have a mixed environnment,
CPU +GPU, it would be better to distribute across CPU and GPU available,
specify the core available to TF for full usage (keeping one for centralized processing).









"
12423,can i used it by C#?,"can i used it by C#?
What time can provide C # versionï¼Ÿ
Why isn't the official c # demo?
thanks"
12422,"[Bug] bazel test kernel_tests:scatter_ops_test on master branch, got ImportError: No module named autograd.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
master
- **Python version**:
2.7 
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
8.0/5.1.10
- **GPU model and memory**:
M40
- **Exact command to reproduce**:
bazel test -c opt --verbose_failures //tensorflow/python/kernel_tests:scatter_ops_test


### Describe the problem
I have clone the source and run kernel test with the  above command, got following error.
```
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1                                     
-----------------------------------------------------------------------------   
Traceback (most recent call last):                                              
  File ""/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/scatter_ops_test.py"", line 23, in <module>
    from tensorflow.python.framework import constant_op                         
  File ""/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py"", line 48, in <module>
    from tensorflow.python.eager import execute                                 
  File ""/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 21, in <module>
    from autograd import core as ag_core                                        
ImportError: No module named autograd
```

Then, I check out  branch v1.3, it is ok.

"
12420,ValueError: No attr named '_XlaCompile' and AttributeError: 'NoneType' object has no attribute 'back_prop' with tf.while_loop and templates,"### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.2, 1.3
- **Python version**: 3.4
- **CUDA/cuDNN version**: 5, 6
- **GPU model and memory**: GTX 1080 Ti

### Describe the problem

When I run `tf.train.Optimizer().minimize(loss)` I receive the error message below. The code is a bit unwieldy, so I just provide the overall structure of my code:

```
x = { ... }
def some_template(..):
  bar(x[..])
def foo(..):
  return foobar(x[..])
template = tf.make_template(""my_template"", some_template)
def loop_body(inputs, ..):
  for i in range(x[..]):
    net = foo(inputs)
    net = template(net)
  return inputs, ..
net, *_ = tf.while_loop(cond, loop_body, vars)
```

The functions and templates that are called in the loop body access some Python variables from the outer scope (read-only, i.e. without side-effects), and they are, of course, expected to be constant at run-time. The graph construction seems to work perfectly fine. Only when I construct the first minimization operation, it fails at the first graph node that makes use of a variable (`Model/ModuleB_0/b_46/` in this case). One thing that might be unusual about my graph is that it uses an earlier part of the graph as target for the outputs, so the loss is defined as a function of `output_node` and `tf.stop_gradient(earlier_node)`. What might maybe also be interesting is that the `frame_name` below contains the same path concatenated twice `Model/ModuleB_0/while/Model/ModuleB_0/while/`. I also have some `trainable=True` variables which are not in the subgraph of the training op, but they are not in the `var_list` argument for the training op, so that should be fine. Is this a bug or am I doing something wrong?

### Source code / logs
```
Traceback (most recent call last):
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.
y"", line 343, in _MaybeCompile
    xla_compile = op.get_attr(""_XlaCompile"")
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"",
ine 1705, in get_attr
    str(self._node_def))
ValueError: No attr named '_XlaCompile' in name: ""Model/ModuleB_0/while/Level_4/BottomRight/Conv2D
ayer_2/BiasAdd/Enter""
op: ""Enter""
input: ""Model/ModuleB_0/b_46/read""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""frame_name""
  value {
    s: ""Model/ModuleB_0/while/Model/ModuleB_0/while/""
  }
}
attr {
  key: ""is_constant""
  value {
    b: true
  }
}
attr {
  key: ""parallel_iterations""
  value {
    i: 10
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 98, in <module>
    tf.app.run()
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", li
ne 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 61, in main
    model = Model(tf.flags.FLAGS.__flags)
  File ""/media/other_dir/me/Code/model/model.py"", line 193, in __init__
    scope=""Model/ModuleA_%i"" % i)
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p
y"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p
y"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p
y"", line 542, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p
y"", line 348, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p
y"", line 542, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_gra
d.py"", line 208, in _EnterGrad
    if not grad_ctxt.back_prop:
AttributeError: 'NoneType' object has no attribute 'back_prop'

```"
12419,Non-fused batch norm with NCHW is mush slower than with NHWC,"### Describe the problem
I noticed that in my environment, non-fused batch norm with ""NCHW"" format  run significantly slower 
### Environment info
```bash
Environment

GPU: 8x NVIDIAÂ® TeslaÂ® M40/P40
OS: Ubuntu 16.04 LTS with tests run via Docker
CUDA / cuDNN: 8.0 / 5.1
Tensorflow Version: 1.2.1
Docker Images: docker pull tensorflow/tensorflow:1.2.1-gpu
DataSet: Synthetic images
```
### Source code / logs
Here is my source code
```python
import time 
import tensorflow as tf
import pdb 

data_format = ""NCHW""
fused=False
batch_size=512
nclass = 1001
num_steps = 100 

with tf.device(""/gpu:0""):
    images = tf.truncated_normal(
            shape=[batch_size, 227, 227, 3], 
            dtype=tf.float32,
            stddev=1e-1,
            name=""fake_images"")
    images = tf.contrib.framework.local_variable(images, name='images')
with tf.device(""/cpu:0""):
    labels = tf.random_uniform(
            [batch_size],
            dtype=tf.int32,
            minval=1,
            maxval=nclass,
            name=""fake_labels"")
    labels = tf.contrib.framework.local_variable(labels, name='labels')
    labels -= 1

with tf.device(""/gpu:0""):
    images *=  1. / 256
    images = tf.subtract(images, 0.5)
    images = tf.multiply(images, 2.0)
    if data_format == ""NCHW"":
        images = tf.transpose(images, [0, 3, 1, 2])
    logits = tf.contrib.layers.conv2d(images, 64, [11, 11], [4, 4],
                                      padding=""VALID"", biases_initializer=None,
                                      data_format=data_format)
    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)
    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)

    logits = tf.contrib.layers.conv2d(logits, 192, [5, 5],
                                      biases_initializer=None,
                                      data_format=data_format)
    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)
    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)

    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],
                                      biases_initializer=None,
                                      data_format=data_format)
    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)
    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],
                                      biases_initializer=None,
                                      data_format=data_format)
    logits = tf.contrib.layers.conv2d(logits, 256, [3, 3],
                                      biases_initializer=None,
                                      data_format=data_format)
    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)

    logits = tf.contrib.layers.flatten(logits)
    logits = tf.contrib.layers.fully_connected(logits, 4096)
    logits = tf.nn.dropout(logits, 0.5)
    logits = tf.contrib.layers.fully_connected(logits, 4096)
    logits = tf.nn.dropout(logits, 0.5)
    logits = tf.contrib.layers.fully_connected(logits, nclass, activation_fn=None)

    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=logits, labels=labels, name='xentropy')
    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')

    opt = tf.train.GradientDescentOptimizer(0.005)
    training_op = opt.minimize(loss)
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    train_op = tf.group(*([training_op] + update_ops))

    step_train_times = []
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        for local_step in xrange(num_steps):
            batch_start = time.time()
            sess.run(train_op)
            step_train_times.append(time.time() - batch_start)
            images_per_sec = batch_size / step_train_times[-1]
            print ""local step %d, %.2f images/sec"" % (
                    local_step+1, images_per_sec)

```

And my tested results:

 data format |  fused/non-fused batch norm  |  images/sec
------------ | ------------- | --------------
NHWC | fused |  1085.5 
NHWC | non-fused |  969.1
NCHW | fused |  1315.6
NCHW | non-fused | **301.1**

I test benchmarks with model resnet50 on P40 and get similar results

 data format |  fused/non-fused batch norm  |  images/sec
------------ | ------------- | --------------
NHWC | fused |  65.7 
NHWC | non-fused |  51.0
NCHW | fused |   84.4
NCHW | non-fused | **7.5**


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I found issue https://github.com/tensorflow/tensorflow/issues/7551 similar with my problem but  with opposite result"
12418,"Feature Request: Add gradient for ""BiasAdd"" op in C++ API",Add BiasAdd gradient implementation for C++ API so it can be added to graph through AddSymbolicGradients function.
12417,Can't use reshape in a while_loop   InvalidArgumentError ,"tf.reshape( x ,[batch_size,-1])

Batch_size is a global variable. This instruction within the body of a while loop  produce the error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Reshape' has inputs from different frames. The input 'while/strided_slice' is in frame 'while/while/'. The input 'Reshape/shape' is in frame ''.

It's very layered, there are many other similar instructions in the code, reshape that involve global variables, but are compiled correctly.

I'm using tensorflow 1.1 on python3 installed by Anaconda."
12416,Outdated Documentation (ImportError: libcudnn.so.6),"### Problem Description
The binary version for GPU provided [here](https://www.tensorflow.org/install/install_linux#python_36) on installation page of Ubuntu expects cuDNN v6. However, the documentation mentions the version of cuDNN required as 5.1.

Kindly rectify the error in documentation.

### System information
- Linux Ubuntu 16.04
- TensorFlow installed from binary (GPU Version)
- Tensorflow version v1.3.0-rc2-20-g0787eee 1.3.0"
12414,tf Dataset / Iterator console flood when using CUDA builds,"### System information
- **Have I written custom code **: Yes
- **OS Platform and Distribution **: Manjaro linux, Arch Linux repo
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version **: 1.3.0
- **Python version**: 3.6.2
- **Bazel version **:
- **CUDA/cuDNN version**: cuda 8.0.61, cudnn 7.0.1 & cudnn6 6.0.21
- **GPU model and memory**: Nvidia 1080 GTX 8GB
- **Exact command to reproduce**:

### The problem

When using a tensorflow wheel built with cuda support, my app prints the following warning message at the end of a training epoch:

`2017-08-19 14:01:18.214060: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,132], [?], [?,?], [?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]`

The code trains a seq2seq model, and I assume the message gets printed somewhere downstream of seq2seq.dynamic_decode. The message still gets printed even when the NN cells are not wrapped with a tf.contrib.rnn.DeviceWrapper with device field indicating a GPU, only works fine on non-cuda builds.

All of this happens while the code is protected with the try/except statements:
```
        for epoch in range(num_epochs):
            session.run(iterator.initializer)
            while True:
                try:
                    session.run([operation])
                except tf.errors.OutOfRangeError:
                    break
```

Now the only cheeky thing is that I am using the binaries from Arch Linux repositories, but these are far from being dodgy.
[python-tensorflow](https://www.archlinux.org/packages/community/x86_64/python-tensorflow/)
[python-tensorflow-cuda](https://www.archlinux.org/packages/community/x86_64/python-tensorflow-cuda/)
[The build script](https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/tensorflow)

This problem was in tensorflow 1.2 and persists in tensorflow 1.3.
Also tested on a laptop without dedicated gpu, but same OS and packages, works fine.
"
12413,FIFOQueue '_1_input_producer' is closed.,"I'm using an `input_pipeline` in a `tf.managed_session()`, and encountered the following error message:

```
Traceback (most recent call last):
  File ""/home/darth/GitHub Projects/gru_svm/model/gru_svm.py"", line 206, in <module>
    main()
  File ""/home/darth/GitHub Projects/gru_svm/model/gru_svm.py"", line 193, in main
    coord.join(threads)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/lib/python3/dist-packages/six.py"", line 686, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run
    enqueue_callable()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1063, in _single_operation_run
    target_list_as_strings, status, None)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.CancelledError: FIFOQueue '_1_input_producer' is closed.
	 [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer, input_producer/limit_epochs)]]
```

Here's the session block I wrote:

```
sv = tf.train.Supervisor(logdir=CHECKPOINT_PATH, summary_op=None)

    with sv.managed_session() as sess:

        sess.run(init_op)

        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)

        if checkpoint and checkpoint.model_checkpoint_path:
            saver.restore(sess, checkpoint.model_checkpoint_path)

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord, sess=sess)

        try:
            step = 0
            while not coord.should_stop():
                example_batch, label_batch = sess.run([examples, labels])

                feed_dict = {x_input: example_batch, y_input: label_batch, state: current_state,
                             learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}

                summary, _, epoch_loss, next_state = sess.run([merged, optimizer, cost, states], feed_dict=feed_dict)

                accuracy_ = sess.run(accuracy, feed_dict=feed_dict)

                current_state = next_state

                if step % 100 == 0:
                    print('step [{}] loss : {}, accuracy : {}'.format(step, epoch_loss, accuracy_))
                    writer.add_summary(summary, step)
                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)

                step += 1
        except tf.errors.OutOfRangeError:
            print('EOF -- training done at step {}'.format(step))
        finally:
            writer.close()
            coord.request_stop()

        coord.join(threads)

        saver = tf.train.Saver()
        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)
```

Thank you in advance for your help!
"
12410,Distributed Tensorflow: data feeding in the beginning,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.31
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

### Describe the problem
I am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.

I have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)

### Source code / logs

Exception thrown:
Caused by op u'ReaderReadV2', defined at:
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py"", line 180, in <module>
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py"", line 157, in manager
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py"", line 61, in worker
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py"", line 174, in main
    process()
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py"", line 169, in process
    serializer.dunits processed is zero
ump_stream(func(split_index, iterator), outfile)
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py"", line 2408, in pipeline_func
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py"", line 2408, in pipeline_func
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py"", line 2408, in pipeline_func
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py"", line 345, in func
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py"", line 793, in func
  File ""/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py"", line 240, in _mapfn
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py"", line 105, in map_fun
    ((x, y_), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py"", line 56, in read_csv_examples
    sg_key, sg_csv = sg_reader.read(sg_queue)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 193, in read
    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 422, in _reader_read_v2
    queue_handle=queue_handle, name=name)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2359, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1242, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:worker/replica:0/task:1/cpu:0""](sg_reader, sg_queue)]]
	 [[Node: embedding_lookup/DynamicPartition_S751 = _Recv [client_terminated=false, recv_device=""/job:ps/replica:0/task:6/cpu:0"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=6148508285306453080, tensor_name=""edge_726_embedding_lookup/DynamicPartition"", tensor_type=DT_INT32, _device=""/job:ps/replica:0/task:6/cpu:0""]()]]
"
12409,about fps,"In android device, if I want to know the fps in the detection activity, what should I do in the DetectorActivity.java?
"
12406,compilation error ,"On Ubuntu 16.04 with gcc5

./configure
You have bazel 0.5.3 installed.
Please specify the location of python. [Default is /usr/bin/python]:
Found possible Python library paths:
/usr/local/lib/python2.7/dist-packages
/usr/lib/python2.7/dist-packages
Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with MKL support? [y/N]
No MKL support will be enabled for TensorFlow
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n]
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N]
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
No XLA support will be enabled for TensorFlow
Do you wish to build TensorFlow with VERBS support? [y/N]
No VERBS support will be enabled for TensorFlow
Do you wish to build TensorFlow with OpenCL support? [y/N]
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Do you want to use clang as CUDA compiler? [y/N]
nvcc will be used as CUDA compiler
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8
Please specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Invalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8
Please specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Invalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1
Please specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""6.1""]:
Do you wish to build TensorFlow with MPI support? [y/N]
MPI support will not be enabled for TensorFlow
Configuration finished

BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)
tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50: required from here
tensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'
cudnnStatus_t retval = ::__name(args...); 
^
tensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'
__macro(cudnnSetRNNDescriptor)"
12405,tf.boolean_mask doesn't check array bounds.,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: archlinux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 8.0/6.0

```python
import numpy as np
import tensorflow as tf
m = tf.placeholder(tf.bool, shape=[None, 1], name='m')
b = tf.placeholder(tf.float32, shape=[1, None], name='b')
output = tf.boolean_mask(b, m)
mask = [[True],[True],[True],[True],[True]]
y = np.random.rand(1, 4)

with tf.Session() as sess:
    print(y)
    print(sess.run(output, feed_dict={m: mask, b:y}))
```

The code runs, while I expect an index out-of-bound error.

In fact, even when mask is a 4x1 boolean, I still expect an error (since b is 1x4). But that's somewhat arguable."
12403,InvalidArgument error with tf.scan(),"### System information
- No custom code
- Error encountered on Windows 10 and CentOS 6.7
- binary
- TF 1.1.0 and 1.2.1 respectively
- 3.6
- CUDA 8.0, CUDNN 5.1
- GeForce 940MX 2GB (ran out of memory in execution), Titan X 12 GB

### Describe the problem
I encountered this error when using `tf.scan` on an MultiRNNCell.

> InvalidArgumentError (see above for traceback): Max scatter index must be <= array size (141 vs. 141)

### Source code / logs
If you look here: **[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/tensor_array_ops.cc#L1072](url)**

``` 
OP_REQUIRES(
          ctx, max_index < array_size,
          errors::InvalidArgument(""Max scatter index must be <= array size ("",
                                  max_index, "" vs. "", array_size, "")""));
    }
````

It seems that the condition (<) is not consistent with the error message (<=).
"
12402,DataLossError. Checksum does not match when using multiple TFRecordDataset via tf.case,"See below. Please let me know what I can do to provide more information for you.  I am working on pulling out the offending code into a standalone py file to replicate the bug elsewhere, but it might take a few days.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: arch linux (LTS kernel 4.9.44-1)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3 (master branch as of commit `566d167c`)
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: 0.5.2-2
- **CUDA/cuDNN version**: 8.0.61-2/6.0.21-2
- **GPU model and memory**: 1080 GTX Ti
- **Exact command to reproduce**:

I don't have exact source for you yet of a trimmed down example. I will see if I can put one together. Generally, the steps are

- Generate a few `TFRecord` files
- Instantiate a few `TFRecordDataset` from the files.
- Perform necessary pre-processing on `TFRecordDataset` entries for each dataset, then on the dataset itself (`.cache`, `.repeat`, `.shuffle`, `.batch`, etc) For example, we might have datasets `train_dataset`, `validate_dataset`, and `test_dataset`.
- Initialize each dataset (`.make_initializable_iterator`)
- Train a model using an input tensor `x` of

```
train_phase = tf.placeholder(dtype=tf.int32, shape=())
x = tf.case([(tf.equal(train_phase, 1), lambda: train_dataset),
             (tf.equal(train_phase, 2), lambda: validate_dataset),
             (tf.equal(train_phase,3), lambda: test_dataset)],
         default=lambda: train_dataset)
# ... construct model_op using tensor x as input ...
# ... call the initializer ...

# train for a bunch
for i in ...
     sess.run(main_op, feed_dict={train_phase: 1})
# validate
for i in ...
     sess.run(main_op, feed_dict={train_phase: 2})
# test
for i in ...
     sess.run(main_op, feed_dict={train_phase: 3})
```

### Describe the problem
I am using a single tensor `x` to represent my input values for all three phases of my model training (training, validation, and testing). To alternate between them without duplicating entire graphs, I set the tensor to be conditional on the value of `train_phase` (again, train, validate, or test) by using `tf.case`. 

If the above steps are followed, my system will eventually (non-deterministically) crash during the training/validating/testing of the model. This is independent of the dataset used (I am using well-scrubbed versions of NYUv2, CIFAR-10, KITTI, etc.) and does not occur in particular records. The error is always long the lines of

```
DataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904
	 [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](input_pipeline/Iterator_1)]]
	 [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_69_add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
 Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904
	 [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](input_pipeline/Iterator_1)]]
	 [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_69_add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
``` 

It can occur during any phase (that is, on any branch of the `tf.case`). It occurs with multiple versions of CUDA 8 and TF (at least since TFRecordDataset came out). It occurs with single or multiple GPUS. It occurs with different Linux kernel versions (multiple variants of 4.10, 4.11, 4.12, including the 4.9 TLS branch). 

However, it does **NOT** occur if I don't use `tf.case` (that is, I use a single TFRecordDataset tensor as the input, i.e., only do training and skip validate/test).

I would assume this is a complication with the CUDA drivers, as I occasionally (though not always) get kernel panics at the same time. I will also occasionally (though not always) get errors like `malloc(): smallbin double linked list corrupted`.

I should also add that I've already tested for hardware issues and seem to have ruled all of them out (memory failures, HD/SSD cables, motherboard, PSU spikes, etc.). 


### Source code / logs
The full trace/output is

```
Caught unexpected exception during training: Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904
	 [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](input_pipeline/Iterator_1)]]
	 [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_69_add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]

Caused by op 'input_pipeline/IteratorGetNext_1', defined at:
  File ""train.py"", line 266, in <module>
    datasets = hem.get_datasets(args)
  File ""/mnt/research/projects/autoencoders/hem/util/data.py"", line 64, in get_datasets
    x = iterator.get_next()
  File ""/usr/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 311, in get_next
    name=name))
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 698, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3046, in create_op
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1604, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904
	 [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](input_pipeline/Iterator_1)]]
	 [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_69_add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
 Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904
	 [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](input_pipeline/Iterator_1)]]
	 [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_69_add_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]
```
"
12401,`import tensorflow.contrib.layers` takes very long time,"I've used Cprofile to test my code, and I find that `import tensorflow.contrib.layers` takes very long time. Here is the quick screenshot of the result:

```
14495011 function calls (14451561 primitive calls) in 6.324 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   1023/1    0.058    0.000    6.328    6.328 {built-in method builtins.exec}
        1    0.000    0.000    6.328    6.328 test_import.py:10(<module>)
   1066/1    0.006    0.000    6.328    6.328 <frozen importlib._bootstrap>:966(_find_and_load)
   1066/1    0.005    0.000    6.328    6.328 <frozen importlib._bootstrap>:939(_find_and_load_unlocked)
   1360/1    0.001    0.000    6.328    6.328 <frozen importlib._bootstrap>:214(_call_with_frames_removed)
    759/1    0.002    0.000    6.328    6.328 {built-in method builtins.__import__}
    881/2    0.004    0.000    6.328    3.164 <frozen importlib._bootstrap>:659(_load_unlocked)
    833/2    0.003    0.000    6.328    3.164 <frozen importlib._bootstrap_external>:667(exec_module)
 6575/625    0.005    0.000    6.102    0.010 <frozen importlib._bootstrap>:996(_handle_fromlist)
... 
```
As seen above, it took more than 6 secs to just do `import tensorflow.contrib.layers`, and I wonder if this is expected. Also, I am pretty interested in why it takes so long time to do importing there.

**P.S** The profiling test runs on MacOS Sierra 10.12.5 with Tensorflow v1.2 and Python 3.5."
12396,tf Dataset with tf.py_func doesn't work as the tutorial says,"### System information
-  Ubuntu 16.04
- TF 1.3 (released version)
- CUDA 8.0
- CUDNN 6.0
- Python 3.5

### Describe the problem
Dataset with tf.py_func() doesn't work as the dataset tutorial says in (https://www.tensorflow.org/programmers_guide/datasets)


### Source code / logs
minimum code to reproduce the problem.
```python
import tensorflow as tf
from tensorflow.contrib.data.python.ops.dataset_ops import Dataset
import numpy as np
import glob
import sys
import os

def _read_py_function(filename):
    filename = filename.decode(sys.getdefaultencoding())
    data = np.load(filename)
    # print(data.shape)
    label = np.random.randint(5)
    # print(label)
    return data.astype(np.float32), np.cast[np.float32](label)


sample_size = 5
seq_len = 3
num_samples = 7


data_dir = 'test_toy_data/'

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

for i in range(num_samples):
    np.save('%s/toy_%d.npy' % (data_dir, i),
            np.random.rand(seq_len, sample_size))


tr_filenames = glob.glob('%s/toy*.npy' % (data_dir))

tr_dataset = Dataset.from_tensor_slices((tr_filenames))
tr_dataset = tr_dataset.map(
    lambda filename: tf.py_func(_read_py_function,
                                [filename],
                                [tf.float32, tf.float32]))
tr_dataset = tr_dataset.shuffle(buffer_size=20)
tr_dataset = tr_dataset.batch(2)


iterator = tr_dataset.make_initializable_iterator()
next_element = iterator.get_next()

sess = tf.Session()

for _ in range(1):
    # training
    sess.run(iterator.initializer)

    while True:
        try:
            feats, labs = sess.run(next_element)
            print(feats.shape)
            print(labs)
        except tf.errors.OutOfRangeError:
            break
```"
12395,Does StreamExecutor support OpenCL?,"I only see TENSORFLOW_STREAM_EXECUTOR_MACHINE_MANAGER_PREFER_OPENCL option in machine_manager.cc. However I don't see where SE links to OpenCL headers and libs. So I wonder of SE supports OpenCL?

Thanks,
RLE"
12394,[bug] CUDA messes up after improperly closing Tensorflow session ,"Whenever I run Keras sessions on PyCharm, and I ""forget"" to properly close the session (example: I log off the computer, or close session). It seems like the CUDA driver stops working. I can still run `$nvcc -V` and `$nvidia-smi` successfully but running the `./deviceQuery` sanity check from the CUDA samples fails, which means the system has stopped detecting the graphics card.

Has anyone else had this problem? I definitely recall that this is not the first time this has happened to me using tensorflow vs say using torch, pytorch that also use the graphics card and ""forgetting"" to close my session.

Perhaps I am just being spoiled? Note that this has happened both using tensorflow directly and using keras with tensorflow as backend (where I do not have direct control over closing sessions directly as far as I know)."
12393,Anaconda installation example doesn't match the description on web site,"On this page:

https://www.tensorflow.org/install/install_linux#installing_with_anaconda

Step 4 says: 

> where tfBinaryURL is the URL of the TensorFlow Python package. For example, the following command installs the CPU-only version of TensorFlow for Python 2.7:

However, the example command below it actually installs TensorFlow for Python 3.4, not 2.7:

> pip install --ignore-installed --upgrade \
>  https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.3.0-cp34-cp34m-linux_x86_64.whl"
12390,Running ops with restored model gives FailedPreconditionError,"I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me 
```
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213
	 [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable_213""], _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_213)]]
```
It might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:
### Source code / logs
```
import tensorflow as tf
import csv
import os
import glob
from PIL import Image
from datetime import datetime

def read_image_file(filename):
    image = tf.image.decode_png(tf.read_file(filename), channels=3)
    image = tf.cast(image, tf.float32)
    image = tf.reshape(image, [224, 112, 3])
    return image

def labelFiles(filename):
    label_list = []
    for name in filename:
        label_list.append(getLabel(name))
    return label_list

def readCSV():
    with open('label.csv', 'r') as infile:
        reader = csv.reader(infile)
        label = {rows[0]:rows[1] for rows in reader}
        return label

def getWord(filename):
    word = ''
    (name, extension) = os.path.splitext(filename)
    (path, name) = os.path.split(name)
    for i in name:
        if i.isalpha():
            start = name.index(i)
            word = word + name[start:]
            #print(word)
            break
    return word

def getLabel(filename):
    label = [0]*len(labels)
    index =  int(labels.get(getWord(str(filename)), 0))
    label[index] = 1
    label = tf.constant(label, dtype = tf.float32)
    tf.reshape(label, [-1])
    print(label)
    return label

def weight(shape):
    initial = tf.truncated_normal(shape, stddev=0.01)
    return tf.Variable(initial)

def bias_0(shape):
    initial = tf.constant(0.0, shape=shape)
    return tf.Variable(initial)

def conv2d(image, weight, stride):
    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')

def max_pool_3x3(image, stride):
    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')

def batch_norm(image, out_size):
    mean, var = tf.nn.moments(image, [0, 1, 2])
    scale = tf.Variable(tf.ones([out_size]))
    beta = tf.Variable(tf.zeros([out_size]))
    epsilon = 0.001
    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)
    return bn

def inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):
    conv1_weight = weight(conv1_size)
    conv1_bias = bias_0([conv1_size[3]])
    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))

    conv2_1_weight = weight(conv2_1_size)
    conv2_1_bias = bias_0([conv2_1_size[3]])
    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))
    conv2_2_weight = weight(conv2_2_size)
    conv2_2_bias = bias_0([conv2_2_size[3]])
    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))

    conv3_1_weight = weight(conv3_1_size)
    conv3_1_bias = bias_0([conv3_1_size[3]])
    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))
    conv3_2_weight = weight(conv3_2_size)
    conv3_2_bias = bias_0([conv3_2_size[3]])
    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))

    pool4_1 = max_pool_3x3(image, 1)
    conv4_2_weight = weight(conv4_2_size)
    conv4_2_bias = bias_0([conv4_2_size[3]])
    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))
    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)


now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")

root_logdir = ""/home/tensorflow/tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
savedir = ""/home/tensorflow/saves""

labels = readCSV()
num_labels = len(labels)
trainpath = '/home/resized/trainset/'
testpath = '/home/resized/testset/'

trainnames = glob.glob(trainpath + '*.png')
testnames = glob.glob(testpath + '*.png')


train_label = labelFiles(trainnames)
test_label = labelFiles(testnames)

batch_size = 256
epochs = 50
num_batch = int(len(trainnames)/batch_size)
initial_learning_rate = 0.001
decay_steps = num_batch*8
decay_rate = 0.96
global_step = tf.Variable(0, trainable = False)
learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)


# TRAIN QUEUE
train_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])

enqueue_train = train_queue.enqueue_many([trainnames, train_label])

train_image, train_image_label = train_queue.dequeue()

train_image = read_image_file(train_image)

train_batch, train_label_batch = tf.train.batch(
    [train_image, train_image_label],
    batch_size=batch_size,
    num_threads=1,
    capacity=10*batch_size,
    enqueue_many=False,
    shapes=[[224,112,3], [len(labels),]],
    allow_smaller_final_batch=True
)

train_close = train_queue.close()

# TEST QUEUE

test_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])

enqueue_test = test_queue.enqueue_many([testnames, test_label])

test_image, test_image_label = test_queue.dequeue()

test_image = tf.expand_dims(read_image_file(test_image), 0)

test_close = test_queue.close()

# MODEL

keep_prob = tf.placeholder(tf.float32)

weights_conv1 = weight([7, 7, 3, 64])

bias_conv1 = bias_0([64])

weights_conv2 = weight([1, 1, 64, 64])

bias_conv2 = bias_0([64])

weights_conv3 = weight([3, 3, 64, 192])

bias_conv3 = bias_0([192])

weights_fc13 = weight([1024, num_labels])

bias_fc13 = bias_0([num_labels])


def GoogleNet(data):

    # First Conv. Layer
    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))

    pool1 = max_pool_3x3(conv1, 2)

    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)

    # Second Conv. Layer
    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))

    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))

    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)

    pool2 = max_pool_3x3(lrn2, 2)

    # First Inception Layer
    incep4 = inception(
        pool2,
        [1, 1, 192, 64],
        [1, 1, 192, 96],
        [3, 3, 96, 128],
        [1, 1, 192, 16],
        [5, 5, 16, 32],
        [1, 1, 192, 32]
    )

    incep5 = inception(
        incep4,
        [1, 1, 256, 128],
        [1, 1, 256, 128],
        [3, 3, 128, 192],
        [1, 1, 256, 32],
        [5, 5, 32, 96],
        [1, 1, 256, 64]
    )

    pool3 = max_pool_3x3(incep5, 2)

    # Second Inception Layer
    incep6 = inception(
        pool3,
        [1, 1, 480, 192],
        [1, 1, 480, 96],
        [3, 3, 96, 208],
        [1, 1, 480, 16],
        [5, 5, 16, 48],
        [1, 1, 480, 64]
    )

    incep7 = inception(
        incep6,
        [1, 1, 512, 160],
        [1, 1, 512, 112],
        [3, 3, 112, 224],
        [1, 1, 512, 24],
        [5, 5, 24, 64],
        [1, 1, 512, 64]
    )

    incep8 = inception(
        incep7,
        [1, 1, 512, 128],
        [1, 1, 512, 128],
        [3, 3, 128, 256],
        [1, 1, 512, 24],
        [5, 5, 24, 64],
        [1, 1, 512, 64]
    )

    incep9 = inception(
        incep8,
        [1, 1, 512, 112],
        [1, 1, 512, 144],
        [3, 3, 144, 288],
        [1, 1, 512, 32],
        [5, 5, 32, 64],
        [1, 1, 512, 64]
    )

    incep10 = inception(
        incep9,
        [1, 1, 528, 256],
        [1, 1, 528, 160],
        [3, 3, 160, 320],
        [1, 1, 528, 32],
        [5, 5, 32, 128],
        [1, 1, 528, 128]
    )

    pool4 = max_pool_3x3(incep10, 2)

    # Third Inception Layer
    incep11 = inception(
        pool4,
        [1, 1, 832, 256],
        [1, 1, 832, 160],
        [3, 3, 160, 320],
        [1, 1, 832, 32],
        [5, 5, 32, 128],
        [1, 1, 832, 128]
    )

    incep12 = inception(
        incep11,
        [1, 1, 832, 384],
        [1, 1, 832, 192],
        [3, 3, 192, 384],
        [1, 1, 832, 48],
        [5, 5, 48, 128],
        [1, 1, 832, 128]
    )

    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')

    pool5_flat = tf.reshape(pool5, [-1, 1024])

    # FC Layers
    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)

    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13

    return fc13

#Training
with tf.name_scope(""cost_function"") as scope:
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))
    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)


#Accuracy
with tf.name_scope(""accuracy"") as scope:
    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))
    accuracy = tf.cast(correct_prediction, tf.float32)

cost_summary = tf.summary.scalar(""cost_function"", cost)
file_writer = tf.summary.FileWriter(logdir)

#Session
with tf.Session() as sess:
    saver = tf.train.Saver()
    saver.restore(sess, ""/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt"")
    print(""Model restored..."")
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)

    sess.run(enqueue_test)
    accuracy_vector = []
   
    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))
    
    for num in range(len(testnames)):
        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))
    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))

    print(""test accuracy %g""%mean_accuracy)
    sess.run(test_close)

    coord.request_stop()
    coord.join(threads)

    file_writer.close()
```

It was the line `print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))` that messed this up.
A side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?"
12389,Python quit unexpectedly while using the _batch_ops.so plug-in.,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: both source and binary
- **TensorFlow version (use command below)**: ('v1.3.0-0-g9e76bf324', '1.3.0')
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.5.3-homebrew
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:pydoc modules



### Describe the problem


Every time I use _pydoc modules_, it crashes with this message:
_python(9152,0x7fffbdeb33c0) malloc: *** error for object 0x11531fb58: pointer being freed was not allocated_

The same result for both 1.2.1 and 1.3.0, official build or custom builds.


### Source code / logs

The backtrace:
Thread 0 Crashed:: Dispatch queue: com.apple.main-thread
0   libsystem_kernel.dylib        	0x00007fffb50cdd42 __pthread_kill + 10
1   libsystem_pthread.dylib       	0x00007fffb51bb457 pthread_kill + 90
2   libsystem_c.dylib             	0x00007fffb5033420 abort + 129
3   libsystem_malloc.dylib        	0x00007fffb5122fe7 free + 530
4   _batch_ops.so                 	0x0000000119fb853c tensorflow::OpDef::SharedDtor() + 108
5   _batch_ops.so                 	0x0000000119fb838c tensorflow::OpDef::~OpDef() + 28
6   _batch_ops.so                 	0x0000000119f4b274 _GLOBAL__sub_I_batch_ops.cc + 388
7   dyld                          	0x000000010ea9aa1b ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 385
8   dyld                          	0x000000010ea9ac1e ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40
9   dyld                          	0x000000010ea964aa ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 338
10  dyld                          	0x000000010ea95524 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 138
11  dyld                          	0x000000010ea955b9 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75
12  dyld                          	0x000000010ea8a7cd dyld::runInitializers(ImageLoader*) + 87
13  dyld                          	0x000000010ea923ec dlopen + 556
14  libdyld.dylib                 	0x00007fffb4f9c832 dlopen + 59
15  _pywrap_tensorflow_internal.so	0x0000000110c3165f tensorflow::internal::LoadLibrary(char const*, void**) + 47
16  _pywrap_tensorflow_internal.so	0x0000000110c30b74 tensorflow::(anonymous namespace)::PosixEnv::LoadLibrary(char const*, void**) + 20
17  _pywrap_tensorflow_internal.so	0x0000000110b27f17 tensorflow::LoadLibrary(char const*, void**, void const**, unsigned long*) + 1431
18  _pywrap_tensorflow_internal.so	0x000000010edaff44 TF_LoadLibrary + 52
19  _pywrap_tensorflow_internal.so	0x000000010eb2b452 _wrap_TF_LoadLibrary(_object*, _object*) + 162
20  org.python.python             	0x00000001096fd4d4 PyEval_EvalFrameEx + 14624
21  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
22  org.python.python             	0x00000001097003e2 0x109677000 + 562146
23  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
24  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
25  org.python.python             	0x00000001097003e2 0x109677000 + 562146
26  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
27  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
28  org.python.python             	0x00000001096f9367 PyEval_EvalCode + 48
29  org.python.python             	0x000000010970e6bd PyImport_ExecCodeModuleEx + 241
30  org.python.python             	0x00000001097113c7 0x109677000 + 631751
31  org.python.python             	0x0000000109710e2c 0x109677000 + 630316
32  org.python.python             	0x0000000109710a00 0x109677000 + 629248
33  org.python.python             	0x000000010970fc10 PyImport_ImportModuleLevel + 1185
34  org.python.python             	0x00000001096f5006 0x109677000 + 516102
35  org.python.python             	0x00000001096816fb PyObject_Call + 99
36  org.python.python             	0x00000001096ffdbb PyEval_CallObjectWithKeywords + 165
37  org.python.python             	0x00000001096fbc0f PyEval_EvalFrameEx + 8283
38  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
39  org.python.python             	0x00000001096f9367 PyEval_EvalCode + 48
40  org.python.python             	0x000000010970e6bd PyImport_ExecCodeModuleEx + 241
41  org.python.python             	0x00000001097113c7 0x109677000 + 631751
42  org.python.python             	0x000000010971164f 0x109677000 + 632399
43  org.python.python             	0x0000000109710e2c 0x109677000 + 630316
44  org.python.python             	0x0000000109710a00 0x109677000 + 629248
45  org.python.python             	0x000000010970fc10 PyImport_ImportModuleLevel + 1185
46  org.python.python             	0x00000001096f5006 0x109677000 + 516102
47  org.python.python             	0x00000001096fd4d4 PyEval_EvalFrameEx + 14624
48  org.python.python             	0x0000000109696b00 0x109677000 + 129792
49  org.python.python             	0x00000001096fa0ec PyEval_EvalFrameEx + 1336
50  org.python.python             	0x0000000109696b00 0x109677000 + 129792
51  org.python.python             	0x00000001096fa0ec PyEval_EvalFrameEx + 1336
52  org.python.python             	0x0000000109696b00 0x109677000 + 129792
53  org.python.python             	0x00000001096fa0ec PyEval_EvalFrameEx + 1336
54  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
55  org.python.python             	0x00000001097003e2 0x109677000 + 562146
56  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
57  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
58  org.python.python             	0x00000001097003e2 0x109677000 + 562146
59  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
60  org.python.python             	0x0000000109700475 0x109677000 + 562293
61  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
62  org.python.python             	0x0000000109700475 0x109677000 + 562293
63  org.python.python             	0x00000001096fce4e PyEval_EvalFrameEx + 12954
64  org.python.python             	0x00000001096f99be PyEval_EvalCodeEx + 1617
65  org.python.python             	0x00000001096f9367 PyEval_EvalCode + 48
66  org.python.python             	0x00000001097195dd 0x109677000 + 665053
67  org.python.python             	0x0000000109719680 PyRun_FileExFlags + 133
68  org.python.python             	0x00000001097191d1 PyRun_SimpleFileExFlags + 702
69  org.python.python             	0x000000010972ab6a Py_Main + 3094
70  libdyld.dylib                 	0x00007fffb4f9f235 start + 1"
12388,Why my tensorflow-gpu runs only on cpu? Ubuntu,"### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from **: `pip3 install tensorflow-gpu`
- **TensorFlow version**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Python version**:  Python 3.5.2
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61


My tensorflow-gpu runs only on cpu, how to fix it?
I've already tried to set 
`CUDA_DEVICE_VISIBLE=all my gpus`  But it didn't work.

Then I tried to use this code:
`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`
The output is following:
`Device mapping: no known devices.`
`2017-08-18 16:44:44.654177: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:`
Then nothing. It doesn't output the map.
I've also tried to install it with virtualenv and run the program in virtualenv or reinstall, still not working. 
Why is this happening? How can I fix it? 



"
12387,The precision difference between tensorflow and numpy when using tf.reduce_mean and np.mean,"```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **TensorFlow installed from (source or binary)**:binary
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
== cat /etc/issue ===============================================
Linux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.1 LTS (Xenial Xerus)""
VERSION_ID=""16.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)
tensorflow-fold (0.0.1)
tensorflow-gpu (1.2.1)
tensorflow-tensorboard (0.1.4)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Aug 18 16:14:35 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 0000:05:00.0     Off |                  N/A |
| 28%   51C    P0    67W / 250W |      0MiB / 12188MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 0000:06:00.0     Off |                  N/A |
| 31%   55C    P0    66W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN Xp            Off  | 0000:09:00.0     Off |                  N/A |
| 54%   84C    P2   130W / 250W |  11655MiB / 12189MiB |     52%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN Xp            Off  | 0000:0A:00.0     Off |                  N/A |
| 23%   33C    P0    61W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    2      5854    C   python3                                      11651MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
/usr/local/cuda-8.0/lib64/libcudart_static.a



python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.2.0-5-g435cdfc 1.2.1
```
### Describe the problem
When using `tf.reduce_mean`, i found the behaviour between `tensorflow` and `numpy` was different when dtype was `float32`. The experiment shows that this maybe caused by the precision problem in `tensorflow`. When I changed the dtype to `tf.float64`, the problem fixed. 
Or if I calculate the mean axis by axis, the problem also disappear.
However, if I use `numpy` array, the result was right no matter the dtype is  `float32` or `float64`.
Is this a normal behaviour or will be fixed later?

The test code I used as following:
### Source code / logs
```Python
In [61]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)

In [62]: y = tf.reduce_mean(x)

In [63]: sess.run(y)
Out[63]: 0.26278782

In [64]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))

In [65]: sess.run(y)
Out[65]: 0.693142

In [66]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)

In [67]: y = tf.reduce_mean(x)

In [68]: sess.run(y)
Out[68]: 0.26278782

In [69]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float64)

In [70]: y = tf.reduce_mean(x)

In [71]: sess.run(y)
Out[71]: 0.6931000008030902

In [72]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))

In [73]: sess.run(y)
Out[73]: 0.69310000000034644

In [74]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float64)

In [75]: yn = np.mean(xn)

In [76]: yn
Out[76]: 0.69310000000032723

In [77]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float32)

In [78]: yn = np.mean(xn)

In [79]: yn = np.mean(xn)

In [80]: yn
Out[80]: 0.69321907
```"
12386,program hung when trying to use the result of ConditionalAccumulator's take_grad method after The dequeue operator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 & Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I tried to extend `SyncReplicasOptimizer` where I need to use the result of `ConditionalAccumulator`'s `take_grad` method after `sync_token_queue.dequeue()`, the program blocked. Then I looked into the code and thought that it might be caused  by  the step state in `ConditionalAccumulator`, but it still blocked.

### Source code / logs
I could modify the official example a bit to reproduce the problem.
The cmds I used are as belowï¼š
`python mnist_replica.py --job_name ps --task_index 0 --sync_replicas True`
`python mnist_replica.py --job_name worker --task_index 0 --sync_replicas True`
`python mnist_replica.py --job_name worker --task_index 1 --sync_replicas True`

Where `mnist_replica.py` only changed a line of importing module, [link](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-mnist_replica-py). 
And I edited `sync_replicas_optimizer.py` a bit, two versions in [link1](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-my_sync_replicas_optimizer_0-py) and in [link2](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-my_sync_replicas_optimizer_1-py)."
12385,Slim.learning.train support tf-debug,"tfdbg is a great tool for debugging TensorFlow programs. But if I use slim for training(like in TensorFlow object detection API), it seems not support to use tf-dbg, and brings inconvenience."
12384,tf.train.batch does not preserve the order of data and miss some data.,"I use tf.train.batch to produce data in tensorflow but get unexpected data order as following: 
```
# Create a queue that produces the filenames to read.
filename_queue = tf.train.string_input_producer(string_tensor=filenames, num_epochs=1, shuffle=False)

......

num_preprocess_threads = 1 
images_x, images_y, label_batch = tf.train.batch(
            [image[0], image[1], label],
            batch_size=1,
            num_threads=num_preprocess_threads,
            capacity=1)
```

the label produced by tf.train.batch: [ 6.03125     5.2734375   4.03125   5.84375     5.15625   ....]. A total of 33 labels.

the label of original data: [ 6.03125     5.2734375   **4.546875**    4.03125     **6.1875**  5.8438 ...]. A total of 68 labels.


It seems that the generated data differs from the original data in the order and is less. Why?
"
12382,using two different models for prediction,"Im using the following code to import graph def since i want to use two model files individually.

```
def initialSetup():
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    start_time = timeit.default_timer()

    # This takes 2-5 seconds to run
    # Unpersists graph from file
    with tf.gfile.FastGFile('age/output_graph.pb', 'rb') as f:
        age_graph_def = tf.GraphDef()
        age_graph_def.ParseFromString(f.read())
        tf.import_graph_def(age_graph_def, name='')

    with tf.gfile.FastGFile('output_graph.pb', 'rb') as f:
        gender_graph_def = tf.GraphDef()
        gender_graph_def.ParseFromString(f.read())
        tf.import_graph_def(gender_graph_def, name='')

    print ('Took {} seconds to unpersist the graph'.format(timeit.default_timer() - start_time))

```

but how do I use them after importing individually ? I tried adding name ='age' and name = 'gender' to differentiate but it is not working out as well. "
12381,Limit GPU memory usage not working in distributed training of inception,"I'm using the example provided for distributed trainig of [inception](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py). I have three hosts, with one for parameter server and two for workers. All hosts have TensorFlow 1.2 installed with CUDA 8.0, cuDNN 5.1 and Titan X Pascal GPU running Ubuntu 14.04.

I followed the instruction for distributed training as provided in the [readme](https://github.com/tensorflow/models/blob/master/inception/README.md) and it runs successfully. But when I try to limit the GPU usage by making the following changes.

Change

sess_config = tf.ConfigProto(
          allow_soft_placement=True,
          log_device_placement=FLAGS.log_device_placement)

to

sess_config = tf.ConfigProto(
          allow_soft_placement=True,
          log_device_placement=FLAGS.log_device_placement,
          gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.6))

in [inception_distributed_train.py](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L255)

I recompiled the example using 'bazel build //inception:imagenet_distributed_train' and run the example. But the process still occupies all GPU memory available observed using nvidia-smi."
12380, Who can explain the usage of this c++ api,"Who can explain the usage of this apiï¼š
Status Run(
  const FeedType & inputs,
  const std::vector< Output > & fetch_outputs,
  const std::vector< Operation > & run_outputs,
  std::vector< Tensor > *outputs
) const 


I read the doc but i still can not understand it ....
I donâ€˜t know  the role of each parameter.
"
12379,[Feature] InputStream variant for ReadBinaryProto and ReadTextProto,"### System information

N/A

### Describe the problem
Currently in master branch both ReadBinaryProto and ReadTextProto accepts only file path to load model proto, while in some cases we may want to load from a network stream or any other input stream of model proto file that input stream is our best option.

So I think adding two variants accepts stream input instead of file path should be good. As both these ReadBinaryProto and ReadTextProto are in C++ public API, may I know is it feasible to add? Or any other security or implementation difficulty that is blocking it? Or design conflict? 

If it is just lacking of people, sure I can help.

### Source code / logs

N/A
"
12378,ssd_mobilenets model faild using in android demo,"I am glad to see android demo is updated to be available for ssd_mobilenets models. I tried the demo on my phone, and it works well. But when I tried my own model, the app can not work normally. 
1.I used  `export_inference_graph.py` in object_detection to convert .ckpt to .pb files
2.I replaced the original model by my .pb file
3. Also, I changed the label_list.txt. 
After doing this, I generate the app, but the app crushed. Is there anything wrong with my handle? "
12375,Eager tensor execution not inferring dtype attributes,"Hi,

I've noticed that when I use the ""StridedSlice"" op, for example, with the new eager tensor execution API, the type arguments (""T"" and ""Index"" in this case) are not automatically inferred and the library complains that they are not provided. Also, when I do provide them, I get an error saying `NodeDef mentions attr '`, which I cannot figure out. @alextp maybe you have some idea of what's happening.

Thanks,
Anthony"
12374,Tensor Assignment,"Hi,

I've been wondering why there are no simple tensor assignment ops. Currently one can only assign values to tensors by creating variables. Why is there no simple assignment op for tensors directly? An example would be a case where we simply zero-out a column of a tensor, for example, at some point in execution. In order to do that now we would either need to create a variable, or a series of slicing and concatenation ops (along with creating a new tensor containing the zero-ed out column).

Thanks!"
12373,tf.contrib.data.Dataset behaves strangely when re-defined.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 on GCP
- **TensorFlow installed from (source or binary)**: pip (implies binary install I think).
- **TensorFlow version (use command below)**: GIT Version: v1.2.0-5-g435cdfc, Version: 1.2.1
- **Python version**: 2.7 and 3.5 (I checked with both)
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Run the attached script.
[tf_repro.txt](https://github.com/tensorflow/tensorflow/files/1232707/tf_repro.txt)

### Describe the problem
It seems like once we start initializing and using datasets, redefining them even in minor ways leads to errors? I think? This is rather unlike any other error in python. Normally I wouldn't expect redefinitions of python variables to cause problems.

I've noticed the following:
* Using separate variables for the datasets does not help (regardless of whether the old dataset is kept around or not).
* Creating the datasets first and using them after does work (again regardless of whether separate variables.

If this is intended behavior, it might be worth documenting it. It's easy enough to work around I suppose. I don't know enough about TensorFlow internals to know what might be causing it.

### Source code / logs
Here's the output I get when I run the attached script:
```
2017-08-17 21:56:55.536140: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-17 21:56:55.536187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-17 21:56:55.536194: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-17 21:56:55.536199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-17 21:56:55.536203: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
First run works.
2017-08-17 21:57:00.353667: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Function tf_map_func_065aa1a3 is not defined.
Second run failed...
Function tf_map_func_065aa1a3 is not defined.
         [[Node: MapDataset_1 = MapDataset[Targuments=[], f=tf_map_func_065aa1a3[], output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/cpu:0""](RepeatDataset_1)]]

Caused by op 'MapDataset_1', defined at:
  File ""/home/prakhar/scratch/tf_repro.py"", line 22, in <module>
    iterator = ds.make_initializable_iterator()
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 396, in make_initializable_iterator
    return Iterator.from_dataset(self, shared_name)
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 98, in from_dataset
    initializer = gen_dataset_ops.make_iterator(dataset.make_dataset_resource(),
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 1457, in make_dataset_resource
    output_shapes=nest.flatten(self.output_shapes))
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 297, in map_dataset
    output_shapes=output_shapes, name=name)
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Function tf_map_func_065aa1a3 is not defined.
         [[Node: MapDataset_1 = MapDataset[Targuments=[], f=tf_map_func_065aa1a3[], output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/cpu:0""](RepeatDataset_1)]]

Third run works.
```

-- PG"
12372,tf.contrib.data.Dataset does not correctly handle nested dictionaries,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
`tf.contrib.data.Dataset` objects do not correctly deal with nested dictionary structures. When using a dataset with a nested dictionary, the inner dictionaries are replaced with the first tensor in that inner dictionary, and following tensors are restored for incorrect keys.

This is not an issue with `tf.contrib.framework.nest`, only with datasets, which appear to instead use `tensorflow.contrib.data.python.util.nest`. The particular difference causing the bug appears to be https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/util/nest.py#L279 vs. https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/data/python/util/nest.py#L184

### Source code / logs
```python
import tensorflow as tf
from tensorflow.contrib.data.python.util import nest as data_nest

test_value = {
    ""a"": {
        ""aa"": tf.constant(1),
        ""ab"": tf.constant([2,2]),
    },
    ""b"": tf.constant([3, 3, 3]),
}


print tf.contrib.framework.nest.map_structure(lambda t: t.shape, test_value)
# {
#   'a': {
#       'aa': TensorShape([]),
#       'ab': TensorShape([Dimension(2)])
#   },
#   'b': TensorShape([Dimension(3)])
# } <- these are the correct shapes


d = tf.contrib.data.Dataset.from_tensors(test_value)
print d.output_shapes
# {
#   'a': TensorShape([]),
#   'b': TensorShape([Dimension(2)])
# } <- incorrect

print data_nest.map_structure(lambda t: t.shape, test_value)
# {
#   'a': TensorShape([]),
#   'b': TensorShape([Dimension(2)])
# } <- incorrect
```
"
12371,tf.contrib.distributions.NegativeBinomial numerical stability (inf),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

I'm working on a negative binomial regression model and ran into `inf`/`nan` values during training. I was able to trace it back to the NegativeBinomial implementation, and specificially when dealing with relatively large probability logits.

Example to demonstrate the issue:
```python
import tensorflow as tf

logits = tf.constant([1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0])
nb = tf.contrib.distributions.NegativeBinomial(10.0, logits)

sess = tf.Session()
x = sess.run(nb.log_prob(10.0))
print(x)
```
Results in
```
[  -4.83159161  -38.70070267  -88.56268311 -137.00408936 -147.99020386
          -inf          -inf          -inf]
```

Looking at the values before going to `-inf` (somewhere between 16.0 and 17.0) it doesn't seem to approaching any sort of limit, so I figured something else was going on.

Inside the `NegativeBinomial` class, the provides logits are first transformed to real probabilities by passing them through a sigmoid, which is `1.0 / (exp(-x) + 1.0)`. Later, in `_log_unnormalized_prob`, these probabilities go through a `log1p(-x)` function. The problem is that for big values and limited floating point precision, sigmoid will output exactly 1.0, which results in taking the log of -1.0 + 1.0, and the log of 0 yields negative infinity. 

I first tried to fix it by simplifying the combination of functions and getting rid of the sigmoid.

I replaced ` math_ops.log1p(-self.probs)` with `tf.log(1.0 / (tf.exp(self.logits) + 1.0))`, which is equivalent (`log((1 / (1 + exp(-x)) + 1) = log(1 / (exp(x) + 1))` but gives better numerical stability. For the same script but with higher values (1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0, 70.0, 80.0, 90.0), the output now is:
```
[  -4.8315897   -38.70066452  -88.567276   -138.56636047 -148.56636047
 -158.56636047 -188.56636047 -488.56634521 -688.56634521 -788.56634521
          -inf]
```

This is better, but it still goes to -inf. Finally, I inspected the raw values of the function I changed, and noticed that it converges to `-x` somewhere after 15.0 as input, so my final solution looks like this:
```python
tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)
```

So ultimately I changed from
```python
def _log_unnormalized_prob(self, x):
    if self.validate_args:
        x = distribution_util.embed_check_nonnegative_integer_form(x)
    return (self.total_count * math_ops.log1p(-self.probs)
            + x * math_ops.log(self.probs))
```
to
```python
def _log_unnormalized_prob(self, x):
    if self.validate_args:
        x = distribution_util.embed_check_nonnegative_integer_form(x)
    return (self.total_count
            * tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)
            + x * math_ops.log(self.probs))
```

which gives the correct probabilities for arbitrarily large probability logits.

I don't feel like creating a PR since I'm not convinced this is the most elegant solution, and not sure whether this problem might exist at other placed in the code base, for example other distribution types. For now, I made my own copy of BinomialDistribution with this fix included, and I'd like to invite you to verify the issue, check the math and perhaps come up with a better solution.

Thanks."
12368,CrashLoopBackOff,"I am trying to follow the tensorflow deployment on kubernetes following the tutorial here:
https://www.tensorflow.org/serving/serving_inception#part_2_deploy_in_kubernetes

Instead of running on gclound, I am trying to run on local machine. But I ran into the following problem.



pangolins:serving$ kubectl get pods
NAME                                    READY     STATUS             RESTARTS   AGE
inception-deployment-2217120516-jmkbm   0/1       CrashLoopBackOff   9          31m
inception-deployment-2217120516-rr04x   0/1       CrashLoopBackOff   9          31m
inception-deployment-2217120516-xvc58   0/1       CrashLoopBackOff   9          31m
monolith                                1/1       Running            0          1d
nginx-1803751077-5cst4                  1/1       Running            0          2d

pangolins:serving$ bazel-bin/tensorflow_serving/example/inception_client --server=10.0.0.45:32683 --image=./tensorflow/tensorflow/contrib/pi_examples/label_image/data/grace_hopper.jpg
Traceback (most recent call last):
  File ""/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py"", line 56, in <module>
    tf.app.run()
  File ""/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/pangolins/software/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py"", line 51, in main
    result = stub.Predict(request, 10.0)  # 10 secs timeout
  File ""/home/pangolins/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py"", line 324, in __call__
    self._request_serializer, self._response_deserializer)
  File ""/home/pangolins/anaconda2/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py"", line 210, in _blocking_unary_unary
    raise _abortion_error(rpc_error_call)
grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.UNAVAILABLE, details=""Connect Failed"")


"
12367,Estimator classification export feature request: input single string Tensor input limitation.,"Within class `ClassificationOutput` contains the `as_signature_def` method. I was wondering why there is a limitation that this method only allows recievor_tensors of length 1.

https://github.com/tensorflow/tensorflow/blob/53aef8a3a5920e53f7da3ea2140374546d1bf708/tensorflow/python/estimator/export/export_output.py#L109-L118

For example the following code should seemingly just ""work"" for the model created from the [wide and deep model tutorial.
](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/learn/wide_n_deep_tutorial.py)

```
feature_inputs = {
        'age': tf.placeholder(dtype=tf.float32, shape=[1,1], name='age'),
        'capital_gain': tf.placeholder(dtype=tf.float32, shape=[1,1], name='age'),
        'capital_loss': tf.placeholder(dtype=tf.float32, shape=[1,1], name='capital_loss'),
        'education': tf.placeholder(dtype=tf.string, shape=[1,1], name='education'),
        'education_num': tf.placeholder(dtype=tf.float32, shape=[1,1], name='education_num'),
        'gender': tf.placeholder(dtype=tf.string, shape=[1,1], name='gender'),
        'hours_per_week': tf.placeholder(dtype=tf.float32, shape=[1,1], name='hours_per_week'),
        'native_country': tf.placeholder(dtype=tf.string, shape=[1,1], name='native_country'),
        'occupation': tf.placeholder(dtype=tf.string, shape=[1,1], name='occupation'),
        'relationship': tf.placeholder(dtype=tf.string, shape=[1,1], name='relationship'),
        'workclass': tf.placeholder(dtype=tf.string, shape=[1,1], name='workclass') 
    }

serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_inputs)

# model is the loaded wide and deep model produced
model.export_savedmodel(""./my_dir"",  serving_input_receiver_fn)
```

However there is an error here:
```
/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export.pyc in build_all_signature_defs(receiver_tensors, export_outputs)
    158       '{}'.format(output_key or 'None'):
    159       export_output.as_signature_def(receiver_tensors)
--> 160       for output_key, export_output in export_outputs.items()}
    161 
    162   return signature_def_map

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export.pyc in <dictcomp>((output_key, export_output))
    158       '{}'.format(output_key or 'None'):
    159       export_output.as_signature_def(receiver_tensors)
--> 160       for output_key, export_output in export_outputs.items()}
    161 
    162   return signature_def_map

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/export/export_output.pyc in as_signature_def(self, receiver_tensors)
    110     if len(receiver_tensors) != 1:
    111       raise ValueError('Classification input must be a single string Tensor; '
--> 112                        'got {}'.format(receiver_tensors))
    113     (_, examples), = receiver_tensors.items()
    114     if dtypes.as_dtype(examples.dtype) != dtypes.string:

ValueError: Classification input must be a single string Tensor; got {'hours_per_week': <tf.Tensor 'hours_per_week_14_1:0' shape=(?, 1) dtype=float32>, 'native_country': <tf.Tensor 'native_country_14_1:0' shape=(?, 1) dtype=string>, 'relationship': <tf.Tensor 'relationship_14_1:0' shape=(?, 1) dtype=string>, 'gender': <tf.Tensor 'gender_14_1:0' shape=(?, 1) dtype=string>, 'age': <tf.Tensor 'age_14_1:0' shape=(?, 1) dtype=float32>, 'capital_gain': <tf.Tensor 'capital_gain_14_1:0' shape=(?, 1) dtype=float32>, 'workclass': <tf.Tensor 'workclass_14_1:0' shape=(?, 1) dtype=string>, 'capital_loss': <tf.Tensor 'capital_loss_14_1:0' shape=(?, 1) dtype=float32>, 'education': <tf.Tensor 'education_14_1:0' shape=(?, 1) dtype=string>, 'education_num': <tf.Tensor 'education_num_14_1:0' shape=(?, 1) dtype=float32>, 'occupation': <tf.Tensor 'occupation_14_1:0' shape=(?, 1) dtype=string>}
```

"
12366,Documentation for windows install is wrong/misleading,"https://www.tensorflow.org/install/install_windows states

Requirements to run TensorFlow with GPU support
cuDNN v5.

This is now incorrect.
TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')

I also suggest you add a comment and link to this gist

https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c

He just saved me hours.

Regards

Aidan"
12363,Freeze tensor_forest graph for Android problem,"Hi,

I built a aimple Random Forest model in tensorflow, and want to freeze & optimize it for android.
I used the following function for building the tesnor_forest estimator:

      def build_estimator(_model_dir, _num_classes, _num_features, _num_trees, _max_nodes):
      params = tensor_forest.ForestHParams(
      num_classes=_num_classes, num_features=_num_features,
      num_trees=_num_trees, max_nodes=_max_nodes, min_split_samples=3)

    graph_builder_class = tensor_forest.RandomForestGraphs
    return random_forest.TensorForestEstimator(
      params, graph_builder_class=graph_builder_class,
      model_dir=_model_dir)




This function stores the textual model to graph.pbtxt file in the specified model directory.

Then I train it using:

    est = build_estimator(output_model_dir, 3,np.size(features_eval,1), 5,6)
    train_X = features_eval.astype(dtype=np.float32)
    train_Y = labels_y.astype(dtype=np.float32)
    est.fit(x=train_X, y=train_Y, batch_size=np.size(features_eval,0))


(in this simple example: number of trees = 5, max_nodes=6)

Now I want to freeze the model, so I call this function:
`
def save_model_android(model_path):
    checkpoint_state_name = ""model.ckpt-1""
    input_graph_name = ""graph.pbtxt""
    output_graph_name = ""freezed_model.pb""
    checkpoint_path = os.path.join(model_path, checkpoint_state_name)

    input_graph_path = os.path.join(model_path, input_graph_name)
    input_saver_def_path = None
    input_binary = False
    output_node_names = ""output""
    restore_op_name = ""save/restore_all""
    filename_tensor_name = ""save/Const:0""
    output_graph_path = os.path.join(model_path, output_graph_name)
    clear_devices = True

    freeze_graph(input_graph_path, input_saver_def_path,
                              input_binary, checkpoint_path,
                              output_node_names, restore_op_name,
                              filename_tensor_name, output_graph_path,
                              clear_devices, """")
`

and in the freezed_model.pb file generated I get only 1 op which is the output node.
in the console I get the following message when the freeze_graph function is called:

> Converted 0 variables to const ops.
1 ops in the final graph.

Does anyone knows why only one node get exported when calling to freeze_graph?

I'm using Tensorflow version  1.2.1 with cuda support , installed from sources on linux"
12362,tensorflow with cocos2d-x failed to build the project,"**tensorflow with cocos2d-x failed to build the project
when building project the terminal throws error**

`jni/../../../../tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory  #include ""unsupported/Eigen/CXX11/Tensor""`"
12360,BUILD:1227:1,"bazel build //tensorflow:libtensorflow.so
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.
"
12359,error,"bazel build //tensorflow:libtensorflow.so
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /Users/dile/tensorflow/tensorflow/core/BUILD:1227:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/dile/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted.
"
12358,[Feature Request]  Add an extra argument `is_duplicated` to `scatter_sub/*` Ops and make the kernel to multi-thread for speedup. ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ubuntu 16.04
- **TenorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.3.0-rc2
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.5.2
- **CUDA/cuDNN version**:
8.0/5.1.10
- **GPU model and memory**:
nvidia M40
- **CPU**
32-cores


### Describe the problem
I profile the `embedding_lookup_sparse` on single machine with tfprof, in which i put the `params` on cpu. So the `gather` and `scatter_sub` Op are also put on cpu.
Profiling result shows `gather` and `scatter_sub` Op takes so much time, the former takes about 33%, the latter takes about 43%. Making the `gather` op kernel to multi-thread gain about 10x speedup, relate to [PR](https://github.com/tensorflow/tensorflow/pull/12246) and [11709](https://github.com/tensorflow/tensorflow/issues/11709).
Follow the same thought, I want to make the `scatter_sub` to be multi-thread. 
- If there is no duplicate index, we can realize lock-free code which could gain about 10x speedup too. 
- If there are duplicate indices, use lock which can gain little speedup.

So, I think an extra argument `is_duplicated` is a good choice to divide the two situation.

Here is some profiling result:
### tfprof
```
# orignial
ScatterSub                   8589.93MB (65.56%, 32.67%),       45.63ms (62.09%, 43.75%),            0us (45.25%, 0.00%),       45.63ms (62.90%, 45.87%)
# with lock
ScatterSub                   8589.93MB (65.56%, 32.67%),       38.10ms (86.95%, 54.44%),            0us (48.97%, 0.00%),       38.10ms (89.98%, 58.78%)
# without lock
ScatterSub                   8589.93MB (65.56%, 32.67%),        4.22ms (73.05%, 12.58%),            0us (48.86%, 0.00%),        4.22ms (76.99%, 14.63%)
```
### Code
[embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624)"
12357,tf.contrib.slim evaluation: outdated documentation,"https://github.com/tensorflow/tensorflow/blob/d7fa7ae8ac15118393b6a549eb98ec9ca23497c0/tensorflow/contrib/slim/python/slim/evaluation.py
The documentation to this, in the first section (Evaluating metrics) uses this code to do the evaluation of the metrics directly (within an existing session, without having to reference a specific checkpoint):
```
  with tf.Session() as sess:
    metric_values = slim.evaluation(
        sess,
        num_evals=1,
        inital_op=initial_op,
        eval_op=names_to_updates.values(),
        final_op=name_to_values.values())
```
This code, however, does not work anymore, as the function `slim.evaluation` doesn't exist now.
what would now be the preferred way to do this?  "
12356,CTC decoding with dictionary,"Is there a possibility to use the CTC decoding algorithms provided by TF with a (word) dictionary?
I've seen that there is some testing-code which seems to do just what I want:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_beam_search_test.cc

Is there a Python interface for this task?
Or do I have to build a custom operation out of the code shown above to be usable in Python?

--
EDIT: solved it by implementing custom op: https://github.com/githubharald/CTCWordBeamSearch"
12355,404 for linux GPU python2 download,"HTTP ERROR 404

Click the url in readme, following occurs:
Problem accessing /view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0rc2-cp27-none-linux_x86_64.whl. Reason:
 Not Found"
12353,how to do 3-D image classification with timedistributed+conv2d,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
12352,Resource Exhausted Error.,"### This is my first experience with git hub, so, please mention if i didn't provide the right details
Here is the Screen shot of the error.
I am running my program on Tesla K20c GPU which has the following properties:
**name: Tesla K20c
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:41:00.0
Total memory: 4.63GiB
Free memory: 4.57GiB**

![res](https://user-images.githubusercontent.com/31095828/29406999-6f69a618-8360-11e7-97c3-1b36b40503f3.png)

------------------------

### System information
- **OS Platform and Distribution ( Linux Ubuntu 16.04)**:
- **TensorFlow installed from (binary)**:
- **TensorFlow version (0.10.0)**:
- **Python version(3.5.2)**: 


### Describe the problem
I am trying to train the model from scratch and have about 500 train and test images and i am using the batch size as 128. the size of the each image is 512*512.

I tried changing batch size to even 1, but still it is not working.
Why is it showing that error?

Thanks. 

"
12351,Tensorflow android can not check results for optimized model,"I used this tutorial to run model on android : https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/index.html?index=..%2F..%2Findex#4 . But after pruning `DecodeJpeg` by command : 

    python -m tensorflow.python.tools.optimize_for_inference \
    --input=tf_files/retrained_graph.pb \
    --output=tf_files/optimized_graph.pb \
    --input_names=""Cast"" \
    --output_names=""final_result""
I can not check the result with this command : 

    python -m scripts.label_image \
    tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg \
    tf_files/optimized_graph.pb

,it shows error : `TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.`. So how can i check result for optimized android model on computer ?"
12350,"Windows 7 :  python mnist_with_summaries.py, having the error","## When running "" python tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py "", it has occurred the following error: 

- 
![image](https://user-images.githubusercontent.com/20549473/29401037-213d52c6-8362-11e7-818d-58cf652e5efa.png)


## Why?Any idea on how to resolve this problem?Thank youï¼
"
12348,convert .ckpt to .pb,"I trained my own model using ssd_mobilenets in object_detection, and it works well on my computer. I see the update of android demo, I want to use the ssd_mobilenets model on my android phone. But I only have .ckpt files, and do not know how to trans .ckpt to .pb, because the graph is too comlex to see which is the final tensor name. Does only one know how to genetate .pb of ssd_mobilenets"
12346,string_input_producer num_epochs not working as expected,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Reading a (csv) file with `string_input_producer()` / `start_queue_runners()` works up to a certain limit. Using `tf.train.shuffle_batch()`, only the first N lines of the file are read.
Also, num_epochs seems limited to a certain number (30 without batching, lower with batch).

### Source code / logs

Simple script for reading a csv file / training a linear model. The code WORKS. But it does NOT respond to changes in num_epochs / the number of steps printed at the end is not what we expect to see.

csv file: (model: W=-1, b=1 , y=Wx+b)

2,-1
3,0
4,1
5,2
(repeated)


    import tensorflow as tf

    W = tf.Variable([.3], dtype=tf.float32)
    b = tf.Variable([-.3], dtype=tf.float32)
    x = tf.placeholder(tf.float32, name=""x"")
    y = tf.placeholder(tf.float32)
    linear_model = tf.add(W * x, b, name=""model"")
    squared_deltas = tf.square(linear_model - y)
    loss = tf.reduce_sum(squared_deltas)

    train = tf.train.AdamOptimizer(1e-3).minimize(loss)

    # reading input
    filename_queue = tf.train.string_input_producer([""/tmp/input.csv""], num_epochs=100)
    reader = tf.TextLineReader(skip_header_lines=0)
    _, csv_row = reader.read(filename_queue)
    record_defaults = [[0.], [0.]]
    col_x, col_label = tf.decode_csv(csv_row, record_defaults=record_defaults)
    features = tf.stack([col_x])

    with tf.Session() as sess:
      sess.run(tf.local_variables_initializer())
      sess.run(tf.global_variables_initializer())
      coord = tf.train.Coordinator()
      threads = tf.train.start_queue_runners(coord=coord)
      n = 0
      while True:
        try:
          n += 1
          inp, lab = sess.run([features, col_label])
          sess.run(train, feed_dict={x:inp, y:lab})
        except tf.errors.OutOfRangeError:
          break
        finally:
          coord.request_stop()
      coord.join(threads)
      print(n)

with batch: adding those lines in reading input

    batch_size = 20
    min_after_dequeue = 10000
    col_x, col_label = tf.decode_csv(csv_row, record_defaults=record_defaults)
    capacity = 3 * min_after_dequeue + batch_size
    x_batch, label_batch = tf.train.shuffle_batch(
        [col_x, col_label], batch_size=batch_size, capacity=capacity,
        min_after_dequeue=min_after_dequeue)
"
12345,PEP 484 Type Annotations (feature request),"### System information
N/A

### Describe the problem
## Background
PEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.

When adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the ""untyped"" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.

## Benefits of Adding Type Annotations
 * The expected inputs and outputs of functions become much clearer
 * Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs
 * Static analysis can uncover latent bugs (case study here[5])

## Difficulties/Drawbacks
 * People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages ""Power Features"" [4] I would argue that striving towards code that is explicit is a similar philosophy
 * The protobuf compiler would need to be augmented to generate type annotations.
 * The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.
 * Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.

## Final thoughts
I realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.

[1] https://www.python.org/dev/peps/pep-0484/
[2] http://mypy-lang.org/
[3] https://github.com/python/typeshed
[4] https://google.github.io/styleguide/pyguide.html#Power_Features
[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/"
12344,Feature request: RecordInput to support gzipped tfrecord files.,"Currently [RecordInput](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/record-input) doesn't support gzipped tfrecord files, would be great to have this supported."
12343,[Feature Request] provide a option to not call SessionRunHook in sess.run(),"### Describe the problem
It is not necessary to execute all the bundled SessionRunHooks in some ```sess.run()``` cases.
For example, `sess.run(enqueue)` in another thread.
its better to have an option like this:
`session.run(run_hooks=True)`

I would like to contribute this if someone think its a good idea."
12340,tf.multinomial with arbitrarily shaped tensors,"It would be nice if once could use tf.multinomial with arbitrary tensors instead of just rank-2 ones. Currently this is only possible by pretending the extends along the other dimensions make for more examples in the batch, i.e. `tf.reshape(tf.multinomial(tf.reshape(x, [None, num_classes]), num_samples), x.get_shape().as_list[:-1])`."
12339,Feature suggestion: Keep_dim for slicing and list-based slicing with __getitem__ in Tensors and Variables,"Sometimes it would be helpful to maintain the dimension when accessing a particular slice of a tensor or a variable. In NumPy this is possible by slicing with a list index like so:

    ndarr = np.ones((5, 4, 3))
    ndarr[: 1, :]  # Shape [5, 3]
    ndarr[: [1], :]  # Shape [5, 1, 3]

It would be great to have similar list-based indexing which is essentially similar to what `tf.gather_nd`."
12338,Tensorflow crashes when i try to quantize my output,"BUG
(https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
```
$ lspci -vvv|grep -i nvi
00:1e.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: NVIDIA Corporation GM204GL [Tesla M60]
        Kernel driver in use: nvidia
        Kernel modules: nvidia_375_drm, nvidia_375
- **Exact command to reproduce**:
```
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When I tried to quantize my output

-            result = SESS.run(layers[0], {'incept/DecodeJpeg/contents:0': f.read()})
+            result = SESS.run(tf.quantize_v2(layers[0], min_range=0, max_range=10, T=tf.quint8), {'incept/DecodeJpeg/contents:0': f.read()})
 
I run out of memory on my GPU


```
2017-08-16 20:59:40.313241: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ***********************
*****************************************************************************
2017-08-16 20:59:40.313284: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when al
locating tensor with shape[720,1280,3]
Traceback (most recent call last):
  File ""generate_hashes.py"", line 89, in <module>
    for (fn, points) in map_results.result():
  File ""/usr/local/lib/python2.7/dist-packages/pebble/pool/base_pool.py"", line 208, in next
    raise result
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[720,
1280,3]
         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device=""/job:unknown_job/replica:0/task
:0/gpu:0""](incept/DecodeJpeg_G511)]]
         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device=""/job:unknown_job/replica
:0/task:0/cpu:0"", send_device=""/job:unknown_job/replica:0/task:0/gpu:0"", send_device_incarnation=-69665892
98495454362, tensor_name=""edge_1131_incept/pool_3"", tensor_type=DT_FLOAT, _device=""/job:unknown_job/replic
a:0/task:0/cpu:0""]()]]

Caused by op u'incept/Cast', defined at:
  File ""generate_hashes.py"", line 78, in <module>
    load_network(False)
  File ""generate_hashes.py"", line 40, in load_network
    return tf.import_graph_def(graph_def, name='incept')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 311, in impo
rt_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2506, in create_o
p
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[720,1280,3]
         [[Node: incept/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, _device=""/job:unknown_job/replica:0/task
:0/gpu:0""](incept/DecodeJpeg_G511)]]
         [[Node: incept/pool_3_G513 = _Recv[client_terminated=false, recv_device=""/job:unknown_job/replica
:0/task:0/cpu:0"", send_device=""/job:unknown_job/replica:0/task:0/gpu:0"", send_device_incarnation=-69665892
98495454362, tensor_name=""edge_1131_incept/pool_3"", tensor_type=DT_FLOAT, _device=""/job:unknown_job/replic
a:0/task:0/cpu:0""]()]]
```
"
12332,Building Error on Windows of master branch,"After fixing the error mention in #12288 , there are new error as shown below:

```
.....................................
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_event.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_fft.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_gpu_executor.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_platform.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_platform_id.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_rng.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_stream.obj""
""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_stream_executor.dir\Release\cuda_timer.obj""
Creating library C:/v-clhian/tensorflow/tensorflow/contrib/cmake/build/Release/pywrap_tensorflow_internal.lib and object C:/v-clhian/tensorflow/tensorflow/contrib/cmake/build/Release/pywrap_tensorflow_internal.exp
5>pywrap_tensorflow_internal.exp : warning LNK4070: /OUT:_pywrap_tensorflow_internal.pyd directive in .EXP differs from output filename 'C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\Release\pywrap_tensorflow_internal.dll'; ignoring directive [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
5>zero_initializer_op.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::TensorSetZero<struct Eigen::GpuDevice,struct Eigen::half>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$TensorSetZero@UGpuDevice@Eigen@@uhalf@2@@functor@tensorflow@@qeaaxaebugpudevice@Eigen@@v?$TensorMap@V?$Tensor@Uhalf@Eigen@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@z) referenced in function ""public: virtual void __cdecl tensorflow::ZeroInitializerOp<struct Eigen::GpuDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ZeroInitializerOp@UGpuDevice@Eigen@@uhalf@2@@tensorflow@@ueaaxpeavopkernelcontext@2@@z) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
5>zero_initializer_op.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::TensorSetZero<struct Eigen::GpuDevice,float>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$TensorSetZero@UGpuDevice@Eigen@@m@functor@tensorflow@@qeaaxaebugpudevice@Eigen@@v?$TensorMap@V?$Tensor@M$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@z) referenced in function ""public: virtual void __cdecl tensorflow::ZeroInitializerOp<struct Eigen::GpuDevice,float>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ZeroInitializerOp@UGpuDevice@Eigen@@m@tensorflow@@ueaaxpeavopkernelcontext@2@@z) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
5>zero_initializer_op.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::TensorSetZero<struct Eigen::GpuDevice,double>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double,1,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$TensorSetZero@UGpuDevice@Eigen@@n@functor@tensorflow@@qeaaxaebugpudevice@Eigen@@v?$TensorMap@V?$Tensor@N$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@z) referenced in function ""public: virtual void __cdecl tensorflow::ZeroInitializerOp<struct Eigen::GpuDevice,double>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$ZeroInitializerOp@UGpuDevice@Eigen@@n@tensorflow@@ueaaxpeavopkernelcontext@2@@z) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
5>C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\Release\pywrap_tensorflow_internal.dll : fatal error LNK1120: 3 unresolved externals [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
5>Done Building Project ""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj"" (default targets) -- FAILED.
1>Done Building Project ""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default targets) -- FAILED.

Build FAILED.
```

May be some other Cmakelist error, could you help fix this?

"
12331,Jenkins' PR builds have been failing regularly since Aug. 10,"I don't know if the commonality is the state of master when the branches for these changes were taken, or if there is something endemic to the current builders, or ? but the failures appear to have nothing to do with actual code changes being submitted in at least some cases.

https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/
"
12330,Request for computation of hessian wrt tensor of shape 1 x n,"There currently exists tf.hessians function which returns 2nd order *y* derivatives w.r.t. to a specified tensor *x*. Currently *x* has to be one dimensional.

In my application, I need to differentiate with respect to a tensor that has dimensions batch_size x layer_width. (I am interested in second derivative of output from neural net with respect to a particular layer.) I can bring dimensionality down to 1 x layer_width, by setting batch_size to 1. Unfortunately, this still does not let me use tf.hessian.

Would it be possible to add functionality where hessian w.r.t. *effectively* one-dimensional tensors can be computed? So that, for example tensors of shape 1 x n (or 1 x 1 x n) can be used. It feels like it shouldn't be a hard problem, but I'm completely lost when I look at tf.gradients code.

Thanks"
12328,Issues related to integrating syntaxnet/parsey mcparseface in Java + tf.reduce_sum,"Summary/tldr: 
It would be nice with more String support in the Java API and a tf.reduce_sum which supports string concatenation. 

Body:
I have successfully hacked in the custom ops of syntaxnet into the Tensorflow master as I wish to run syntaxnet from my Java NLP pipeline. 

I do this by saving a SavedModelBundle and load it again from Java. 

My project is based on a saved model of parsey_mcparseface from this branch: 
https://github.com/dmansfield/models/tree/documents-from-tensor
By user @dmansfield.

And a tensorflow build based on the custom ops from: 
https://github.com/tensorflow/models/tree/master/syntaxnet
Hacked into master of tensorflow.

Hacking in the ops, as build in ops, was necessary as the Java api through the JNI does not support user loaded ops yet (AFAIK). 

The code with the syntaxnet ops hacked in can be found here:
https://github.com/marhlder/syntaxnet-ops-hack

I have everything running except that the strings in the output tensor from my saved model includes junk (non-printable chars) which I think is due to the offset table described in tensor_jni.cc file. 

I can run the same model from Python without any issues. 

The Java API does currently not seem to support anything but scalar string tensors. It looks like using a scalar string tensor will solve my problem as that part of the codes seems to handle the offset table. 

I would therefore like to reduce the string tensor produced by my parsey mcparseface model. 
I then found that the tf.reduce_sum does not yet support string concatenation. 

There should already be an op for it: 
https://github.com/tensorflow/tensorflow/issues/701

User @mrry commented that he was looking for a use case for this here:
https://stackoverflow.com/questions/34247374/merge-string-tensors-in-tensorflow


 "
12327,No half-precision support for DepthwiseConv2Native,"Hi, I'm trying to convert a network from float32 to float16.
Conversion is done postmortem, for inference only.
I converted both weights and activation tensors to float16.
When trying to run inference, I get the following error:
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; label='neon'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]

Assuming that this op is not implemented for float16, what is the easiest way to bypass this issue?
- implement a python op (run-time is not very crucial)?
- implement a CPU version of this op?
-cast to f32 and cast back to f16?

System details
- **OS Platform: Ubuntu 14.04
- **TensorFlow installed from binary
- **TensorFlow version 1.3.0-rc2
- **CUDA/cuDNN version**: cuda 8.0, cudnn 6.0


"
12326,"import tensorflow error with correct installation,  the problem  is ""Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options""","Hi, everyone.

------------------------

### System information
Operating System: Ubuntu 16.04 LTS
Graphics card: Tesla K40
Installed version of CUDA: 8.0 
Installed version of cuDNN: v5 , for CUDA 8.0 
pip --version 9.0.1
pip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)

Tensorflow-gpu installed from pip
$sudo pip install tensorflow-gpu
Version: 1.2.1

----------------------------------------------------------------
pip version
Name: pip
Version: 9.0.1
Summary: The PyPA recommended tool for installing Python packages.
Home-page: https://pip.pypa.io/
Author: The pip developers
Author-email: python-virtualenv@groups.google.com
License: MIT
Location: /usr/local/lib/python2.7/dist-packages
Requires: 
--------------------------------
tensorboard version 

Name: tensorboard
Version: 1.0.0a6
Summary: Standalone TensorBoard for visualizing in deep learning
Home-page: https://github.com/dmlc/tensorboard
Author: zihaolucky
Author-email: zihaolucky@gmail.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, Pillow, numpy, protobuf, wheel, six, werkzeug
-------------------------------------------------------------
tensorflow-gpu version

Name: tensorflow-gpu
Version: 1.2.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, numpy, bleach, markdown, wheel, six, protobuf, backports.weakref, html5lib, werkzeug
---------------------------------------------------------






### The problem
When I open the terminal and type
$python
$import tensorflow 
I get

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/bids/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/bids/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 75, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/bids/.local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 10, in <module>
    from google.protobuf import descriptor_pb2
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/descriptor_pb2.py"", line 409, in <module>
    options=None),
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/descriptor.py"", line 501, in __new__
    return _message.default_pool.FindFieldByName(full_name)
KeyError: ""Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options""



### Source code / logs
Before I upgrade the tensorboard and pip version(the default pip version is 8.x  in Ubuntu 16.04 LTS)
After that I type following code in the terminal 
$python /home/wcm/.local/lib/python2.7/site-packages/tensorboard/tensorboard.py --logdir='/tmp/log'
The problem is KeyError: ""Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options""
Next, I type 
$ import tensorflow
The same problem is KeyError: ""Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options"".

Finally, I re-installing tensorflow-gpu, tensorflow and tensorboard step by step :no change. The same problem for import tensorflow.

 Anyone have an idea for this problem?
Thanks in advance
jiandanjinxin
"
12325,summary_image_op_test failure on ppc64le,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SLES/RHEL ppc64le
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7.4
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

bazel test **-c opt** --local_resources 4096,4.0,1.0 -j 1 --test_output=errors //tensorflow/python/kernel_tests:summary_image_op_test

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This test fails when run with -c opt, it passes with dbg & fastbuild. Recently there was a commit (link below) adjusting the tolerance value to pass this test, it however fails for 'opt' build even after the change. 

Issue: https://github.com/tensorflow/tensorflow/issues/11967
Commit fixing this:  https://github.com/tensorflow/tensorflow/commit/901a3b5014e2061e16a251cda24de44179d7c21a

Is it okay to ignore the 'opt' build failure or should this issue be root-caused further? thanks!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

...
Not equal to tolerance rtol=2e-05, atol=2e-05
None
(mismatch 2.85714285714%)
 x: array([[[ 221.],
        [ 102.],
        [ 129.],...
 y: array([[[221],
        [102],
        [129],...

----------------------------------------------------------------------
Ran 3 tests in 0.052s

FAILED (failures=1)
not close where =  (array([3]), array([6]), array([0]))
not close lhs =  [0]
not close rhs =  [ 1.]
not close dif =  [ 1.]
not close tol =  [  3.99999990e-05]
dtype = uint8, shape = (5, 7, 1)
================================================================================
Target //tensorflow/python/kernel_tests:summary_image_op_test up-to-date:
  bazel-bin/tensorflow/python/kernel_tests/summary_image_op_test
INFO: Elapsed time: 9.223s, Critical Path: 1.12s
//tensorflow/python/kernel_tests:summary_image_op_test                   FAILED
  /root/.cache/bazel/_bazel_root/55751bac5fd009d8cf4f164f81814728/execroot/tenso

Executed 1 out of 1 test: 1 fails locally.
"
12324,cannot bazel build freeze_graph on windows,"using bazel 0.53, python 3.6.2 on windows 10 64 with tf-gpu 1.3.0rc2

bazel build tensorflow/python/tools:freeze_graph
.
ERROR: C:/users/eyaler/downloads/tensorflow-master/tensorflow-master/tensorflow/python/tools/BUILD:32:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""C:/users/eyaler/downloads/tensorflow-master/tensorflow-master/tensorflow/workspace.bzl"", line 119
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""C:/users/eyaler/downloads/tensorflow-master/tensorflow-master/tensorflow/workspace.bzl"", line 110, in _apply_patch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""C:/users/eyaler/downloads/tensorflow-master/tensorflow-master/tensorflow/workspace.bzl"", line 94, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'c:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d C:/users/eyaler/appdata/local/temp/_bazel_eyaler/trcps2te/external/protobuf_archive -i C:/users/eyaler/downloads/tensorflow-master/tensorflow-master/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr:       1 [main] patch (14524) C:\Program Files\Git\usr\bin\patch.exe: *** fatal error - cygheap base mismatch detected - 0x1802F3408/0x1802FD408.
This problem is probably due to using incompatible versions of the cygwin DLL.
Search for cygwin1.dll using the Windows Start->Find/Search facility
and delete all but the most recent version.  The most recent version *should*
reside in x:\cygwin\bin, where 'x' is the drive on which you have
installed the cygwin distribution.  Rebooting is also suggested if you
are unable to find another cygwin DLL.
 and referenced by '//tensorflow/python/tools:freeze_graph'.
ERROR: Analysis of target '//tensorflow/python/tools:freeze_graph' failed; build aborted.
INFO: Elapsed time: 85.042s"
12323,tf.device() is allocating all available GPU memory,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.1 LTS
- **TensorFlow installed from (source or binary)**: source (git repo)
- **TensorFlow version (use command below)**: ('unknown', '1.3.0-rc2')
using commit 3686ef0d51047d2806df3e2ff6c1aac727456c1d

- **Python version**: Python 2.7.12 (default, Nov 19 2016, 06:48:10)
- **Bazel version (if compiling from source)**:
Build label: 0.5.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jun 6 10:34:11 2017 (1496745251)
Build timestamp: 1496745251
Build timestamp as int: 1496745251

- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN 5.0.5
- **GPU model and memory**: GTX 1080 Ti / 11172MiB
- **Exact command to reproduce**:

```python
$ python
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> a = tensorflow.device('/cpu:0')
2017-08-16 18:14:10.490730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Found device 0 with properties:
name: Graphics Device major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 10.91GiB freeMemory: 3.02GiB
2017-08-16 18:14:10.490782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1052] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:02:00.0, compute capability: 6.1)
>>> a
<contextlib.GeneratorContextManager object at 0x7fdf6f0e8790>
>>>
```
At this point, tensorflow is allocating all of the available GPU memory. This can be checked using `nvidia-smi`.
Expected behavior is nothing is done with GPU.

I have used tensorflow 1.2.1, which will allocate nothing on GPU until a `tf.Session` is created. `tf.Session` also allows setting `config.gpu_options.per_process_gpu_memory_fraction` to limit the usage of GPU memory."
12322,Single Image Inference in Tensorflow [Python],"I have already converted a pre-trained .ckpt file to .pb file freezing the model and saving the weighs as well. What I am trying to do now is to make a simple inference using that .pb file and extract and save output image. The model is a (Fully Convolutional Network for Semantic Segmentation) downloaded from here : https://github.com/MarvinTeichmann/KittiSeg . So far I have managed to, load the image, set the default tf graph and import the graph defined by the model on that, read the input and the output tensors and run the session (error here).

```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.platform import gfile
from PIL import Image

# Read the image & get statstics
img=Image.open('/path-to-image/demoImage.png')
img.show()
width, height = img.size
print(width)
print(height)

#Plot the image
#image.show()

with tf.Graph().as_default() as graph:

        with tf.Session() as sess:

                # Load the graph in graph_def
                print(""load graph"")

                # We load the protobuf file from the disk and parse it to retrive the unserialized graph_drf
                with gfile.FastGFile(""/path-to-FCN-model/FCN8.pb"",'rb') as f:

                                #Set default graph as current graph
                                graph_def = tf.GraphDef()
                                graph_def.ParseFromString(f.read())
                                #sess.graph.as_default() #new line

                                # Import a graph_def into the current default Graph
                                tf.import_graph_def(graph_def, name='')

                                # Print the name of operations in the session
                                #for op in sess.graph.get_operations():

                                    #print ""Operation Name :"",op.name            # Operation name
                                    #print ""Tensor Stats :"",str(op.values())     # Tensor name

                                # INFERENCE Here
                                l_input = graph.get_tensor_by_name('Placeholder:0')
                                l_output = graph.get_tensor_by_name('save/Assign_38:0')

                                print ""l_input"", l_input
                                print ""l_output"", l_output
                                print
                                print

                                # Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.                              
                                result = sess.run(l_output, feed_dict={l_input : img}) #   <= error here
                                print(results)

                                print(""Inference done"")


                                # Info
                                # First Tensor name : Placeholder:0
                                # Last tensor name  : save/Assign_38:0""
```

Why inference of a single image in tensorflow is so complicated?
"
12321,[Feature]Any plan to add Dynamic Graph support in TF?,"We know that TF follows the define-and-run paradigm, while some other DL frameworks support define-by-run paradigm, such as pyTorch and Chainer. 

""define-and-run"" actually reduce the mind-set overhead for DL modeling guys due to that it can help reduce the try-and-feedback loop.

MxNet(another originally define-and-run DL framework) already adds support for dynamic graph through Gluon(https://github.com/zackchase/mxnet-the-straight-dope).

As far as I know, there are TensorFlow Fold project(https://github.com/tensorflow/fold) which provides dynamic batching support, but it doesn't provides the define-by-run support.

Is there any plan for TFer to add the dynamic graph support in the near future?
If not, my team is going to work on this stuff, before jumping in, I just want to make sure our energies  are invested on unique features rather than duplicated ones. 

Thanks"
12320,Indexing problem in tensorflow with tf.where(),"I want to clip the variable 'a' values which are less than or equal to max_val.

     ValueError: Shape must be rank 1 but is rank 3 for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [13,20], [1,?,2], [1,?,2], [1].

     tf.clip_by_value(a[tf.where(tf.less_equal(a,max_val))],0,0)

"
12319,session_t->Create() Failed using tensorflow exported model on IOS,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Generating Tensorflow model:  Ubuntu 16.04 LTS (GNU/Linux 4.4.0-91-generic x86_64)
Load Tensorflow model: macOS 10.12 Sierra
- **TensorFlow installed from (source or binary)**: TensorFlow is installed from source
- **TensorFlow version (use command below)**: TensorFlow version 1.2.0
- **Python version**: Python version 3.5
- **Bazel version (if compiling from source)**: Bazel version  0.5.3
- **CUDA/cuDNN version**: None
- **GPU model and memory**: NA
- **Exact command to reproduce**:


### Describe the problem
Error happen when call ""status = session_t->Create(graph)""

error message:
2017-08-16 13:22:27.692781+0800 CameraExample[6322:2753781] Error adding graph to session: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>


### Source code / logs
Export models as *.pb file: (In python)
self.saver.save(self.sess, self.my_path_to_model + ""model.ckpt"", global_step=i)
		output_graph_def = tf.graph_util.convert_variables_to_constants(self.sess,self.sess.graph_def,output_node_names=['models_simple/y_conv'])
			with tf.gfile.FastGFile(self.my_path_to_model+ 'graph.pb', mode = 'wb') as f:
				f.write(output_graph_def.SerializeToString())
------------------------------------------------------------------------------------
IOS:

use the following operations to process exported model
 ------------------------------------------------------------------------------------
#Call freeze_graph, synthesize graph.pb and CKPT, generate frozen.pb
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=XXXX/graph.pb \
  --input_checkpoint=XXXX/model.ckpt-0 \
  --output_node_names=models_simple/Placeholder,models_simple/y_conv \
  --input_binary \
  --output_graph=XXXX/frozen.pb

#Call optimize_for_inference, reduce the operation, replace the calculation can not run in the iOS version, generate inference.pb
bazel-bin/tensorflow/python/tools/optimize_for_inference \
  --input=XXXX/frozen.pb \
  --output=XXXX/inference.pb \
  --input_names=models_simple/Placeholder \
  --output_names=models_simple/y_conv \
  --frozen_graph=True

#Call quantize_graph to further optimize the graph, generate rounded_graph.pb
bazel-bin/tensorflow/tools/quantization/quantize_graph \
  --input=XXXX/inference.pb \
  --output=XXXX/rounded_graph.pb \
  --output_node_names=models_simple/y_conv \
  --mode=weights_rounded
-------------------------------------------------------------------------------------------

Then, execute the following code, (BOOL)loadGraphFromPath executed successfully, but error raised at ""status = session_t->Create(graph)""

- (BOOL)loadGraphFromPath:(NSString *)path
{
  auto status = ReadBinaryProto(tensorflow::Env::Default(), path.fileSystemRepresentation, &graph);
  if (!status.ok()) {
    NSLog(@""Error reading graph: %s"", status.error_message().c_str());
    return NO;
  }
  
  // This prints out the names of the nodes in the graph.
  auto nodeCount = graph.node_size();
  NSLog(@""Node count: %d"", nodeCount);
  for (auto i = 0; i < nodeCount; ++i) {
    auto node = graph.node(i);
    NSLog(@""Node %d: %s '%s'"", i, node.op().c_str(), node.name().c_str());
  }
  
  return YES;
}

create tf session:
- (BOOL)createSession
{
  tensorflow::SessionOptions options;
  auto status = tensorflow::NewSession(options, &session_t);
  if (!status.ok()) {
    NSLog(@""Error creating session: %s"", status.error_message().c_str());
    return NO;
  }
  
  status = session_t->Create(graph);
  if (!status.ok()) {
  // error
    NSLog(@""Error adding graph to session: %s"", status.error_message().c_str());
    return NO;
  }
  
  return YES;
}

error message:
2017-08-16 13:22:27.692781+0800 CameraExample[6322:2753781] Error adding graph to session: No OpKernel was registered to support Op 'RandomUniform' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=24](dropout/Shape)]]

# It seems that RandomUniform was not supported on iOS , but when I called freeze_graph, those which are not supported on iOS should be deleted. What can I do for this problem?
"
12318,freeze_graph giving decode error,"i am using tensorflow_gpu-1.3.0rc2-cp36-cp36m-win_amd64.whl
running freeze_graph in python 3.6.2 on windows 10 64

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 260, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 192, in main
    FLAGS.variable_names_blacklist)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 170, in freeze_graph
    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\tools\freeze_graph.py"", line 137, in _parse_input_graph_proto
    input_graph_def.ParseFromString(f.read())
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 1063, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 1099, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 633, in DecodeField
    if value._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 1099, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 612, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 1099, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 743, in DecodeMap
    if submsg._InternalParse(buffer, pos, new_pos) != new_pos:
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\python_message.py"", line 1089, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 799, in _SkipGroup
    new_pos = SkipField(buffer, pos, end, tag_bytes)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""C:\Anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message."
12314,Feature request: Add FFT to XLA bridge,As there are now CPU FFT implementations it seems like a small extra step to add the FFT ops to the XLA bridge. Surely `tf.spectral` is an immensely important suite of functions for a huge number of use cases.
12311,`fit()` instance method of `skflow.TensorFlowDNNClassifier` raises unexpected error,"### System information

- **Custom code?** no, elementary example for classifying the Iris dataset
- **Operating System**: MacOS 10.12.6
- **TensorFlow installed from (source or binary)**: binary using `pip3 install tensorflow` and later `pip3 install skflow`
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: Python 3.6.2
- **GPU model and memory**: Intel Iris integrated graphics
- **Exact command to reproduce**: the `fit` instance method of `skflow.TensorFlowDNNClassifier`

### Describe the problem

Calling `fit` on instances of `skflow.TensorFlowDNNClassifier` produces an `AttributeError` about `tensorflow` not having an attribute called `histogram_summary`.

### Source code / logs

```python
import skflow
from sklearn import datasets, metrics
 
iris = datasets.load_iris()
clf = skflow.TensorFlowDNNClassifier(hidden_units=[10, 20, 10], n_classes=3)
clf.fit(iris.data, iris.target)
```

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/skflow/estimators/base.py"", line 200, in fit
    self._setup_training()
  File ""/usr/local/lib/python3.6/site-packages/skflow/estimators/base.py"", line 133, in _setup_training
    tf.histogram_summary(""X"", self._inp)
AttributeError: module 'tensorflow' has no attribute 'histogram_summary'
```"
12309,Tensorflow installation fails on python 3.3.6,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: See below
- **TensorFlow installed from (source or binary)**: Tried with pip3
- **TensorFlow version (use command below)**: 
- **Python version**: 3.3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:
== cat /etc/issue ===============================================
Linux 11bb6e68305e 4.9.36-moby #1 SMP Wed Jul 12 15:29:07 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 11bb6e68305e 4.9.36-moby #1 SMP Wed Jul 12 15:29:07 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================

== check for virtualenv =========================================
True

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named 'tensorflow'

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem
When i try to install tensorflow on Ubunutu 14.04 (python 3.3.6, pip 9.0.1) it fails because of numpy version >=1.11.0 which requires Python version 2.7 or >= 3.4. I followed the installations steps mentioned in https://www.tensorflow.org/install/install_linux. Installation works fine on python 3.4/3.6. I think it would be good to specify if 3.3 is not supported here (make Python: 3.4+) https://www.tensorflow.org/install/install_linux#prerequisite_python_and_pip

### Source code / logs
(env33)root@11bb6e68305e:/home# python --version
Python 3.3.6

(env33)root@11bb6e68305e:/home# pip3 install --upgrade
You must give at least one requirement to install (see ""pip help install"")
(env33)root@11bb6e68305e:/home# pip3 install --upgrade pip
Downloading/unpacking pip from https://pypi.python.org/packages/b6/ac/7015eb97dc749283ffdec1c3a88ddb8ae03b8fad0f0e611408f196358da3/pip-9.0.1-py2.py3-none-any.whl#md5=297dbd16ef53bcef0447d245815f5144
  Downloading pip-9.0.1-py2.py3-none-any.whl (1.3MB): 1.3MB downloaded
Installing collected packages: pip
  Found existing installation: pip 1.5.4
    Uninstalling pip:
      Successfully uninstalled pip
Successfully installed pip
Cleaning up...

(env33)root@11bb6e68305e:/home# pip3 install --upgrade tensorflow
Collecting tensorflow
  Downloading tensorflow-1.2.1-cp33-cp33m-manylinux1_x86_64.whl (35.0MB)
    100% |################################| 35.0MB 58kB/s
Collecting numpy>=1.11.0 (from tensorflow)
  Downloading numpy-1.12.1.zip (4.8MB)
    100% |################################| 4.8MB 429kB/s
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-build-bbyh55/numpy/setup.py"", line 34, in <module>
        raise RuntimeError(""Python version 2.7 or >= 3.4 required."")
    RuntimeError: Python version 2.7 or >= 3.4 required.

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-bbyh55/numpy/

(env33)root@11bb6e68305e:/home# pip --version
pip 9.0.1 from /home/env33/lib/python3.3/site-packages (python 3.3)"
12308,LNK 2019 errors while building tf_label_image_example on latest github version for tensorflow on windows,"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 Bit
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): Output copied below
Python version: 3.5.3 (Anaconda)
Bazel version (if compiling from source): -
CUDA/cuDNN version: 8.0
GPU model and memory:-
Exact command to reproduce: Building tf_label_image_example project in Release mode
== cat /etc/issue ===============================================
MINGW64_NT-10.0 DESKTOP-SL66NSK 2.8.0(0.309/5/3) 2017-05-19 13:17 x86_64 Msys

== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== uname -a =====================================================
MINGW64_NT-10.0 DESKTOP-SL66NSK 2.8.0(0.309/5/3) 2017-05-19 13:17 x86_64 Msys

== check pips ===================================================
numpy (1.12.1)
numpydoc (0.6.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
File """", line 1, in 
ModuleNotFoundError: No module named 'tensorflow'

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
bash: nvidia-smi: command not found

== cuda libs ===================================================

Describe the problem

I downloaded the latest source code from github and used the windows instructions:
https://www.tensorflow.org/install/install_windows
I used Anaconda installation instructions and followed this with installation from source.

I am running into LNK 2019 errors while building tf_label_image_example.

Source code / logs:
105> Creating library E:/AIMLDL/TensorFlow/tensorflow_8_14/tensorflow/tensorflow/contrib/cmake/build/Release/tf_label_image_example.lib and object E:/AIMLDL/TensorFlow/tensorflow_8_14/tensorflow/tensorflow/contrib/cmake/build/Release/tf_label_image_example.exp
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreeconfig@trees@boosted_trees@tensorflow@@peav012@@z)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::Swap(class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?Swap@DecisionTreeConfig@trees@boosted_trees@tensorflow@@qeaaxpeav1234@@z) referenced in function ""public: virtual void __cdecl tensorflow::AddTreesToEnsembleOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@AddTreesToEnsembleOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreeconfig@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)"" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreemetadata@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreemetadata@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)"" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::DecisionTreeEnsembleConfig(void)"" (??0DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig * cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreeensembleconfig@trees@boosted_trees@tensorflow@@peav012@@z)
105>model_ops.cc.obj : error LNK2001: unresolved external symbol ""public: cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::DecisionTreeEnsembleConfig(void)"" (??0DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::DecisionTreeEnsembleConfig(class google::protobuf::Arena *)"" (??0DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig * cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdecisiontreeensembleconfig@trees@boosted_trees@tensorflow@@peav012@@z)
105>model_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::DecisionTreeEnsembleConfig(class google::protobuf::Arena *)"" (??0DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::AveragingConfig_default_instance"" (?AveragingConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VAveragingConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""public: cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@qeaa@QEAVOpKernelConstruction@2@@z)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::LearnerConfig_default_instance"" (?LearnerConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VLearnerConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::LearnerConfig_default_instance"" (?LearnerConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VLearnerConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::LearningRateConfig_default_instance"" (?LearningRateConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VLearningRateConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::LearningRateConfig_default_instance"" (?LearningRateConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VLearningRateConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig_default_instance"" (?DecisionTreeEnsembleConfig_default_instance@trees@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VDecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::DoCompute(class tensorflow::OpKernelContext *,class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *)"" (?DoCompute@GradientTreesPredictionOp@boosted_trees@tensorflow@@aeaaxpeavopkernelcontext@3@PEAVDecisionTreeEnsembleResource@models@23@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::~DecisionTreeConfig(void)"" (??1DecisionTreeConfig@trees@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GradientTreesPartitionExamplesOp::DoCompute(class tensorflow::OpKernelContext *,class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *)"" (?DoCompute@GradientTreesPartitionExamplesOp@boosted_trees@tensorflow@@aeaaxpeavopkernelcontext@3@PEAVDecisionTreeEnsembleResource@models@23@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::~DecisionTreeEnsembleConfig(void)"" (??1DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::DoCompute(class tensorflow::OpKernelContext *,class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *)"" (?DoCompute@GradientTreesPredictionOp@boosted_trees@tensorflow@@aeaaxpeavopkernelcontext@3@PEAVDecisionTreeEnsembleResource@models@23@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::DecisionTreeEnsembleConfig(class tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig const &)"" (??0DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::DoCompute(class tensorflow::OpKernelContext *,class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *)"" (?DoCompute@GradientTreesPredictionOp@boosted_trees@tensorflow@@aeaaxpeavopkernelcontext@3@PEAVDecisionTreeEnsembleResource@models@23@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::AveragingConfig::AveragingConfig(void)"" (??0AveragingConfig@learner@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@qeaa@QEAVOpKernelConstruction@2@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::AveragingConfig::~AveragingConfig(void)"" (??1AveragingConfig@learner@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""public: virtual void * __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::scalar deleting destructor'(unsigned int)"" (??_GGradientTreesPredictionOp@boosted_trees@tensorflow@@UEAAPEAXI@Z) 105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::learner::AveragingConfig::CopyFrom(class tensorflow::boosted_trees::learner::AveragingConfig const &)"" (?CopyFrom@AveragingConfig@learner@boosted_trees@tensorflow@@QEAAXAEBV1234@@Z) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@QEAA@QEAVOpKernelConstruction@2@@Z) 105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::LearningRateDropoutDrivenConfig(void)"" (??0LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@QEAA@XZ) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@QEAA@QEAVOpKernelConstruction@2@@Z) 105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::LearningRateDropoutDrivenConfig(void)"" (??0LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@QEAA@XZ) 105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::~LearningRateDropoutDrivenConfig(void)"" (??1LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""public: virtual void * __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::scalar deleting destructor'(unsigned int)"" (??_GGradientTreesPredictionOp@boosted_trees@tensorflow@@ueaapeaxi@Z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::~LearningRateDropoutDrivenConfig(void)"" (??1LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@UEAA@XZ)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig const & __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::default_instance(void)"" (?default_instance@LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@saaebv1234@XZ)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig const & __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::default_instance(void)"" (?default_instance@LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@saaebv1234@XZ)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::CopyFrom(class tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig const &)"" (?CopyFrom@LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@qeaaxaebv1234@@z) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@qeaa@QEAVOpKernelConstruction@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig::CopyFrom(class tensorflow::boosted_trees::learner::LearningRateDropoutDrivenConfig const &)"" (?CopyFrom@LearningRateDropoutDrivenConfig@learner@boosted_trees@tensorflow@@qeaaxaebv1234@@z)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::LearnerConfig::LearnerConfig(void)"" (??0LearnerConfig@learner@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@qeaa@QEAVOpKernelConstruction@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::LearnerConfig::LearnerConfig(void)"" (??0LearnerConfig@learner@boosted_trees@tensorflow@@qeaa@XZ)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::LearnerConfig::LearnerConfig(void)"" (??0LearnerConfig@learner@boosted_trees@tensorflow@@qeaa@XZ)
105>prediction_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::LearnerConfig::~LearnerConfig(void)"" (??1LearnerConfig@learner@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""public: __cdecl tensorflow::boosted_trees::GradientTreesPredictionOp::GradientTreesPredictionOp(class tensorflow::OpKernelConstruction * const)"" (??0GradientTreesPredictionOp@boosted_trees@tensorflow@@qeaa@QEAVOpKernelConstruction@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::LearnerConfig::~LearnerConfig(void)"" (??1LearnerConfig@learner@boosted_trees@tensorflow@@UEAA@XZ)
105>prediction_ops.cc.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::LearnerConfig::~LearnerConfig(void)"" (??1LearnerConfig@learner@boosted_trees@tensorflow@@UEAA@XZ)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl boosted_trees::QuantileConfig::QuantileConfig(void)"" (??0QuantileConfig@boosted_trees@@qeaa@XZ) referenced in function ""void __cdecl tensorflow::anonymous namespace'::ParseConfig(class tensorflow::OpKernelConstruction * const,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class boosted_trees::QuantileConfig,class std::allocator<class boosted_trees::QuantileConfig> > *)"" (?ParseConfig@?A0xff956ef5@tensorflow@@YAXQEAVOpKernelConstruction@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$vector@VQuantileConfig@boosted_trees@@V?$allocator@VQuantileConfig@boosted_trees@@@std@@@5@@Z) 105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl boosted_trees::QuantileConfig::~QuantileConfig(void)"" (??1QuantileConfig@boosted_trees@@UEAA@XZ) referenced in function ""void __cdecl tensorflow::anonymous namespace'::ParseConfig(class tensorflow::OpKernelConstruction * const,class std::basic_string<char,struct std::char_traits,class std::allocator > const &,class std::vector<class boosted_trees::QuantileConfig,class std::allocator > *)"" (?ParseConfig@?A0xff956ef5@tensorflow@@yaxqeavopkernelconstruction@2@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@peav?$vector@VQuantileConfig@boosted_trees@@v?$allocator@VQuantileConfig@boosted_trees@@@std@@@5@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl boosted_trees::QuantileConfig::QuantileConfig(class boosted_trees::QuantileConfig const &)"" (??0QuantileConfig@boosted_trees@@qeaa@AEBV01@@z) referenced in function ""class boosted_trees::QuantileConfig * __cdecl std::_Uninitialized_move_al_unchecked1<class boosted_trees::QuantileConfig *,class boosted_trees::QuantileConfig *,class std::allocator >(class boosted_trees::QuantileConfig *,class boosted_trees::QuantileConfig *,class boosted_trees::QuantileConfig *,struct std::_Wrap_alloc<class std::allocator > &,struct std::_General_ptr_iterator_tag,struct std::_Any_tag)"" (??$_Uninitialized_move_al_unchecked1@PEAVQuantileConfig@boosted_trees@@peav12@V?$allocator@VQuantileConfig@boosted_trees@@@std@@@std@@yapeavquantileconfig@boosted_trees@@peav12@00AEAU?$_Wrap_alloc@V?$allocator@VQuantileConfig@boosted_trees@@@std@@@0@U_General_ptr_iterator_tag@0@U_Any_tag@0@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl boosted_trees::QuantileEntry::QuantileEntry(void)"" (??0QuantileEntry@boosted_trees@@qeaa@XZ) referenced in function ""public: static class boosted_trees::QuantileEntry * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VQuantileEntry@boosted_trees@@@arena@protobuf@google@@sapeavquantileentry@boosted_trees@@peav012@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl boosted_trees::QuantileEntry::QuantileEntry(class google::protobuf::Arena *)"" (??0QuantileEntry@boosted_trees@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class boosted_trees::QuantileEntry * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VQuantileEntry@boosted_trees@@@arena@protobuf@google@@sapeavquantileentry@boosted_trees@@peav012@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl boosted_trees::QuantileSummaryState::QuantileSummaryState(void)"" (??0QuantileSummaryState@boosted_trees@@qeaa@XZ) referenced in function ""public: static class boosted_trees::QuantileSummaryState * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VQuantileSummaryState@boosted_trees@@@arena@protobuf@google@@sapeavquantilesummarystate@boosted_trees@@peav012@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl boosted_trees::QuantileSummaryState::QuantileSummaryState(class google::protobuf::Arena *)"" (??0QuantileSummaryState@boosted_trees@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: virtual void __cdecl tensorflow::QuantileAccumulatorFlushSummaryOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@QuantileAccumulatorFlushSummaryOp@tensorflow@@ueaaxpeavopkernelcontext@2@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl boosted_trees::QuantileStreamState::QuantileStreamState(void)"" (??0QuantileStreamState@boosted_trees@@qeaa@XZ) referenced in function ""public: virtual void __cdecl tensorflow::QuantileAccumulatorDeserializeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@QuantileAccumulatorDeserializeOp@tensorflow@@ueaaxpeavopkernelcontext@2@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl boosted_trees::QuantileStreamState::~QuantileStreamState(void)"" (??1QuantileStreamState@boosted_trees@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::QuantileAccumulatorDeserializeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@QuantileAccumulatorDeserializeOp@tensorflow@@ueaaxpeavopkernelcontext@2@@z)
105>quantile_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl boosted_trees::QuantileStreamState::QuantileStreamState(class google::protobuf::Arena *)"" (??0QuantileStreamState@boosted_trees@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: virtual void __cdecl tensorflow::QuantileAccumulatorSerializeOp::Compute(class tensorflow::OpKernelContext *)"" (?Compute@QuantileAccumulatorSerializeOp@tensorflow@@ueaaxpeavopkernelcontext@2@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""bool __cdecl tensorflow::boosted_trees::learner::LearnerConfig_MultiClassStrategy_IsValid(int)"" (?LearnerConfig_MultiClassStrategy_IsValid@learner@boosted_trees@tensorflow@@YA_NH@Z) referenced in function ""public: __cdecl tensorflow::BaseBuildSplitOp::BaseBuildSplitOp(class tensorflow::OpKernelConstruction * const)"" (??0BaseBuildSplitOp@tensorflow@@qeaa@QEAVOpKernelConstruction@1@@z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::clear_node(void)"" (?clear_node@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::Leaf::clear_leaf(void)"" (?clear_leaf@Leaf@trees@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: void __cdecl tensorflow::BaseBuildSplitOp::FillLeaf(int,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &,class tensorflow::boosted_trees::trees::Leaf *)const "" (?FillLeaf@BaseBuildSplitOp@tensorflow@@qebaxhaebunodestats@stochastic@learner@boosted_trees@2@PEAVLeaf@trees@62@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::Leaf::clear_leaf(void)"" (?clear_leaf@Leaf@trees@boosted_trees@tensorflow@@aeaaxxz)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::Leaf::clear_leaf(void)"" (?clear_leaf@Leaf@trees@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(void)"" (??0Vector@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: void __cdecl tensorflow::BaseBuildSplitOp::FillLeaf(int,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &,class tensorflow::boosted_trees::trees::Leaf *)const "" (?FillLeaf@BaseBuildSplitOp@tensorflow@@qebaxhaebunodestats@stochastic@learner@boosted_trees@2@PEAVLeaf@trees@62@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(void)"" (??0Vector@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(void)"" (??0Vector@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(class google::protobuf::Arena *)"" (??0Vector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: void __cdecl tensorflow::BaseBuildSplitOp::FillLeaf(int,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &,class tensorflow::boosted_trees::trees::Leaf *)const "" (?FillLeaf@BaseBuildSplitOp@tensorflow@@qebaxhaebunodestats@stochastic@learner@boosted_trees@2@PEAVLeaf@trees@62@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(class google::protobuf::Arena *)"" (??0Vector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::Vector::Vector(class google::protobuf::Arena *)"" (??0Vector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(void)"" (??0SparseVector@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseVector * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseVector@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsevector@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(void)"" (??0SparseVector@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(void)"" (??0SparseVector@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(class google::protobuf::Arena *)"" (??0SparseVector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseVector * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseVector@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsevector@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(class google::protobuf::Arena *)"" (??0SparseVector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseVector::SparseVector(class google::protobuf::Arena *)"" (??0SparseVector@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(void)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::DenseFloatBinarySplit * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDenseFloatBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdensefloatbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(void)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(void)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(class google::protobuf::Arena *)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::DenseFloatBinarySplit * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VDenseFloatBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavdensefloatbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(class google::protobuf::Arena *)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::DenseFloatBinarySplit(class google::protobuf::Arena *)"" (??0DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(void)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsefloatbinarysplitdefaultleft@trees@boosted_trees@tensorflow@@peav012@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(void)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(void)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsefloatbinarysplitdefaultleft@trees@boosted_trees@tensorflow@@peav012@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::SparseFloatBinarySplitDefaultLeft(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@aeaaxxz)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(void)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsefloatbinarysplitdefaultright@trees@boosted_trees@tensorflow@@peav012@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(void)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(void)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VSparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavsparsefloatbinarysplitdefaultright@trees@boosted_trees@tensorflow@@peav012@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::SparseFloatBinarySplitDefaultRight(class google::protobuf::Arena *)"" (??0SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@aeaaxxz)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::_slow_mutable_split(void)"" (?_slow_mutable_split@SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(void)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::CategoricalIdBinarySplit * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VCategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavcategoricalidbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(void)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(void)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(class google::protobuf::Arena *)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::CategoricalIdBinarySplit * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VCategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavcategoricalidbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(class google::protobuf::Arena *)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::CategoricalIdBinarySplit(class google::protobuf::Arena *)"" (??0CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::SplitInfo::SplitInfo(void)"" (??0SplitInfo@learner@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::learner::SplitInfo::SplitInfo(void)"" (??0SplitInfo@learner@boosted_trees@tensorflow@@qeaa@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::SplitInfo::~SplitInfo(void)"" (??1SplitInfo@learner@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::learner::SplitInfo::~SplitInfo(void)"" (??1SplitInfo@learner@boosted_trees@tensorflow@@UEAA@XZ)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::_slow_mutable_split_node(void)"" (?_slow_mutable_split_node@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::_slow_mutable_split_node(void)"" (?_slow_mutable_split_node@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::_slow_mutable_left_child(void)"" (?_slow_mutable_left_child@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::_slow_mutable_left_child(void)"" (?_slow_mutable_left_child@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz)
105>split_handler_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::_slow_mutable_right_child(void)"" (?_slow_mutable_right_child@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::BuildSparseInequalitySplitsOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@BuildSparseInequalitySplitsOp@tensorflow@@ueaaxqeavopkernelcontext@2@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""private: void cdecl tensorflow::boosted_trees::learner::SplitInfo::slow_mutable_right_child(void)"" (?slow_mutable_right_child@SplitInfo@learner@boosted_trees@tensorflow@@aeaaxxz)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::trees::GrowingMetadata_default_instance"" (?GrowingMetadata_default_instance@trees@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VGrowingMetadata@trees@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""public: virtual void cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@GrowTreeEnsembleOp@boosted_trees@tensorflow@@ueaaxqeavopkernelcontext@3@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::trees::TreeNodeMetadata_default_instance"" (?TreeNodeMetadata_default_instance@trees@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeNodeMetadata@trees@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""private: void cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::PruneTree(class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?PruneTree@GrowTreeEnsembleOp@boosted_trees@tensorflow@@aeaaxpeavdecisiontreeconfig@trees@23@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeConstraintsConfig_default_instance"" (?TreeConstraintsConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeConstraintsConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""private: struct std::pair<class tensorflow::boosted_trees::trees::DecisionTreeConfig *,class tensorflow::boosted_trees::trees::DecisionTreeMetadata *> cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::UpdateAndRetrieveGrowableTree(class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *,float,unsigned int64)"" (?UpdateAndRetrieveGrowableTree@GrowTreeEnsembleOp@boosted_trees@tensorflow@@Aeaa?AU?$pair@PEAVDecisionTreeConfig@trees@boosted_trees@tensorflow@@peavdecisiontreemetadata@234@@std@@peavdecisiontreeensembleresource@models@23@M_K@Z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeConstraintsConfig_default_instance"" (?TreeConstraintsConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeConstraintsConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeConstraintsConfig_default_instance"" (?TreeConstraintsConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeConstraintsConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeConstraintsConfig_default_instance"" (?TreeConstraintsConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeConstraintsConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(void)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(void)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(void)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(void)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(void)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class tensorflow::boosted_trees::trees::TreeNode const &)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class tensorflow::boosted_trees::trees::TreeNode const &)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class tensorflow::boosted_trees::trees::TreeNode const &)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class tensorflow::boosted_trees::trees::TreeNode const &)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class tensorflow::boosted_trees::trees::TreeNode const &)"" (??0TreeNode@trees@boosted_trees@tensorflow@@qeaa@AEBV0123@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::TreeNode::CopyFrom(class tensorflow::boosted_trees::trees::TreeNode const &)"" (?CopyFrom@TreeNode@trees@boosted_trees@tensorflow@@qeaaxaebv1234@@z) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::PruneTree(class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?PruneTree@GrowTreeEnsembleOp@boosted_trees@tensorflow@@aeaaxpeavdecisiontreeconfig@trees@23@@z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::TreeNode::CopyFrom(class tensorflow::boosted_trees::trees::TreeNode const &)"" (?CopyFrom@TreeNode@trees@boosted_trees@tensorflow@@qeaaxaebv1234@@z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::TreeNode::CopyFrom(class tensorflow::boosted_trees::trees::TreeNode const &)"" (?CopyFrom@TreeNode@trees@boosted_trees@tensorflow@@qeaaxaebv1234@@z)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::TreeNode::CopyFrom(class tensorflow::boosted_trees::trees::TreeNode const &)"" (?CopyFrom@TreeNode@trees@boosted_trees@tensorflow@@qeaaxaebv1234@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::TreeNode::TreeNode(class google::protobuf::Arena *)"" (??0TreeNode@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::TreeNode * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VTreeNode@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavtreenode@trees@boosted_trees@tensorflow@@peav012@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNode::_slow_mutable_node_metadata(void)"" (?_slow_mutable_node_metadata@TreeNode@trees@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""void __cdecl tensorflow::boosted_trees::anonymous namespace'::RecursivePruneTree(unsigned __int64,class std::vector<class tensorflow::boosted_trees::trees::TreeNode,class std::allocator<class tensorflow::boosted_trees::trees::TreeNode> > *)"" (?RecursivePruneTree@?A0xaf924787@boosted_trees@tensorflow@@YAX_KPEAV?$vector@VTreeNode@trees@boosted_trees@tensorflow@@V?$allocator@VTreeNode@trees@boosted_trees@tensorflow@@@std@@@std@@@Z) 105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::TreeNodeMetadata::_slow_mutable_original_leaf(void)"" (?_slow_mutable_original_leaf@TreeNodeMetadata@trees@boosted_trees@tensorflow@@AEAAXXZ) referenced in function ""void __cdecl tensorflow::boosted_trees::anonymous namespace'::RecursivePruneTree(unsigned __int64,class std::vector<class tensorflow::boosted_trees::trees::TreeNode,class std::allocator > *)"" (?RecursivePruneTree@?A0xaf924787@boosted_trees@tensorflow@@YAX_KPEAV?$vector@VTreeNode@trees@boosted_trees@tensorflow@@v?$allocator@VTreeNode@trees@boosted_trees@tensorflow@@@std@@@std@@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::Leaf::Leaf(void)"" (??0Leaf@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::Leaf * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VLeaf@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavleaf@trees@boosted_trees@tensorflow@@peav012@@z)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::Leaf::Leaf(void)"" (??0Leaf@trees@boosted_trees@tensorflow@@qeaa@XZ)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::Leaf const & __cdecl tensorflow::boosted_trees::trees::Leaf::default_instance(void)"" (?default_instance@Leaf@trees@boosted_trees@tensorflow@@saaebv1234@XZ) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::SplitTreeNode(int,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate *,class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?SplitTreeNode@GrowTreeEnsembleOp@boosted_trees@tensorflow@@AEAAXHPEAUSplitCandidate@?A0xaf924787@23@PEAVDecisionTreeConfig@trees@23@@Z) 105>multiple_additive_trees.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::Leaf const & __cdecl tensorflow::boosted_trees::trees::Leaf::default_instance(void)"" (?default_instance@Leaf@trees@boosted_trees@tensorflow@@SAAEBV1234@XZ) 105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::trees::Leaf::CopyFrom(class tensorflow::boosted_trees::trees::Leaf const &)"" (?CopyFrom@Leaf@trees@boosted_trees@tensorflow@@QEAAXAEBV1234@@Z) referenced in function ""void __cdecl tensorflow::boosted_trees::anonymous namespace'::RecursivePruneTree(unsigned __int64,class std::vector<class tensorflow::boosted_trees::trees::TreeNode,class std::allocator > *)"" (?RecursivePruneTree@?A0xaf924787@boosted_trees@tensorflow@@YAX_KPEAV?$vector@VTreeNode@trees@boosted_trees@tensorflow@@v?$allocator@VTreeNode@trees@boosted_trees@tensorflow@@@std@@@std@@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::Leaf::Leaf(class google::protobuf::Arena *)"" (??0Leaf@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::Leaf * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VLeaf@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavleaf@trees@boosted_trees@tensorflow@@peav012@@z)
105>bias-feature-column-handler.obj : error LNK2001: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::Leaf::Leaf(class google::protobuf::Arena *)"" (??0Leaf@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::Vector const & __cdecl tensorflow::boosted_trees::trees::Vector::default_instance(void)"" (?default_instance@Vector@trees@boosted_trees@tensorflow@@saaebv1234@XZ) referenced in function ""public: virtual void __cdecl tensorflow::boosted_trees::CenterTreeEnsembleBiasOp::Compute(class tensorflow::OpKernelContext * const)"" (?Compute@CenterTreeEnsembleBiasOp@boosted_trees@tensorflow@@ueaaxqeavopkernelcontext@3@@z)
105>multiple_additive_trees.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::Vector const & __cdecl tensorflow::boosted_trees::trees::Vector::default_instance(void)"" (?default_instance@Vector@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>training_ops.cc.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::SparseVector const & __cdecl tensorflow::boosted_trees::trees::SparseVector::default_instance(void)"" (?default_instance@SparseVector@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>multiple_additive_trees.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::SparseVector const & __cdecl tensorflow::boosted_trees::trees::SparseVector::default_instance(void)"" (?default_instance@SparseVector@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""private: void __cdecl tensorflow::boosted_trees::trees::DecisionTreeEnsembleConfig::_slow_mutable_growing_metadata(void)"" (?_slow_mutable_growing_metadata@DecisionTreeEnsembleConfig@trees@boosted_trees@tensorflow@@aeaaxxz) referenced in function ""private: class tensorflow::boosted_trees::trees::Leaf * __cdecl tensorflow::boosted_trees::CenterTreeEnsembleBiasOp::RetrieveBias(class tensorflow::boosted_trees::models::DecisionTreeEnsembleResource *)"" (?RetrieveBias@CenterTreeEnsembleBiasOp@boosted_trees@tensorflow@@aeaapeavleaf@trees@23@PEAVDecisionTreeEnsembleResource@models@23@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: _cdecl tensorflow::boosted_trees::learner::SplitInfo::SplitInfo(class tensorflow::boosted_trees::learner::SplitInfo const &)"" (??0SplitInfo@learner@boosted_trees@tensorflow@@qeaa@AEBV0123@@z) referenced in function ""public: struct std::Tree_node<struct std::pair<int const ,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate>,void *> * __cdecl std::_Tree_comp_alloc<class std::_Tmap_traits<int,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate,struct std::less,class std::allocator<struct std::pair<int const ,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate> >,0> >::_Buynode<struct std::pair<int,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate> >(struct std::pair<int,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate> &&)"" (??$_Buynode@U?$pair@HUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@@std@@@?$_Tree_comp_alloc@V?$_Tmap_traits@HUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@U?$less@H@std@@V?$allocator@U?$pair@$$CBHUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@@std@@@6@$0A@@std@@@std@@QEAAPEAU?$_Tree_node@U?$pair@$$CBHUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@@std@@PEAX@1@$$QEAU?$pair@HUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@@1@@Z) 105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::boosted_trees::learner::SplitInfo::CopyFrom(class tensorflow::boosted_trees::learner::SplitInfo const &)"" (?CopyFrom@SplitInfo@learner@boosted_trees@tensorflow@@QEAAXAEBV1234@@Z) referenced in function ""void __cdecl tensorflow::boosted_trees::anonymous namespace'::UpdateBestSplit(class tensorflow::boosted_trees::learner::LearnerConfig const &,int,struct tensorflow::boosted_trees::A0xaf924787::SplitCandidate *,class std::map<int,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate,struct std::less<int>,class std::allocator<struct std::pair<int const ,struct tensorflow::boosted_trees::anonymous namespace'::SplitCandidate> > > *)"" (?UpdateBestSplit@?A0xaf924787@boosted_trees@tensorflow@@yaxaebvlearnerconfig@learner@23@HPEAUSplitCandidate@123@PEAV?$map@HUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@U?$less@H@std@@v?$allocator@U?$pair@$$CBHUSplitCandidate@?A0xaf924787@boosted_trees@tensorflow@@@std@@@6@@std@@@z)
105>training_ops.cc.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::trees::TreeNode_default_instance"" (?TreeNode_default_instance@trees@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeNode@trees@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""private: void __cdecl tensorflow::boosted_trees::GrowTreeEnsembleOp::SplitTreeNode(int,struct tensorflow::boosted_trees::`anonymous namespace'::SplitCandidate *,class tensorflow::boosted_trees::trees::DecisionTreeConfig *)"" (?SplitTreeNode@GrowTreeEnsembleOp@boosted_trees@tensorflow@@aeaaxhpeausplitcandidate@?A0xaf924787@23@PEAVDecisionTreeConfig@trees@23@@z)
105>bias-feature-column-handler.obj : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::TreeNode::~TreeNode(void)"" (??1TreeNode@trees@boosted_trees@tensorflow@@UEAA@XZ) referenced in function ""void __cdecl std::_Destroy_range<class std::allocator,struct tensorflow::boosted_trees::learner::stochastic::FeatureSplitCandidate *>(struct tensorflow::boosted_trees::learner::stochastic::FeatureSplitCandidate *,struct tensorflow::boosted_trees::learner::stochastic::FeatureSplitCandidate *,struct std::_Wrap_alloc<class std::allocator > &)"" (??$_Destroy_range@V?$allocator@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@@std@@peaufeaturesplitcandidate@stochastic@learner@boosted_trees@tensorflow@@@std@@yaxpeaufeaturesplitcandidate@stochastic@learner@boosted_trees@tensorflow@@0aeau?$_Wrap_alloc@V?$allocator@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@@std@@@0@@z)
105>categorical-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::TreeNode::~TreeNode(void)"" (??1TreeNode@trees@boosted_trees@tensorflow@@UEAA@XZ)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::TreeNode::~TreeNode(void)"" (??1TreeNode@trees@boosted_trees@tensorflow@@UEAA@XZ)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::boosted_trees::trees::TreeNode::~TreeNode(void)"" (??1TreeNode@trees@boosted_trees@tensorflow@@UEAA@XZ)
105>bias-feature-column-handler.obj : error LNK2019: unresolved external symbol ""public: virtual void __cdecl tensorflow::boosted_trees::trees::TreeNode::Clear(void)"" (?Clear@TreeNode@trees@boosted_trees@tensorflow@@ueaaxxz) referenced in function ""public: virtual void __cdecl tensorflow::boosted_trees::learner::stochastic::BiasFeatureColumnHandler::GenerateFeatureSplitCandidates(class tensorflow::boosted_trees::learner::LearnerConfig const &,class std::vector<int,class std::allocator > const &,class std::vector<struct tensorflow::boosted_trees::learner::stochastic::NodeStats,class std::allocator > const &,class tensorflow::boosted_trees::learner::FeatureStatsAccumulator<struct tensorflow::boosted_trees::learner::stochastic::GradientStats,struct tensorflow::boosted_trees::learner::stochastic::GradientStatsAccumulator> const &,class std::vector<struct tensorflow::boosted_trees::learner::stochastic::FeatureSplitCandidate,class std::allocator > *)const "" (?GenerateFeatureSplitCandidates@BiasFeatureColumnHandler@stochastic@learner@boosted_trees@tensorflow@@uebaxaebvlearnerconfig@345@AEBV?$vector@HV?$allocator@H@std@@@std@@aebv?$vector@UNodeStats@stochastic@learner@boosted_trees@tensorflow@@v?$allocator@UNodeStats@stochastic@learner@boosted_trees@tensorflow@@@std@@@8@AEBV?$FeatureStatsAccumulator@UGradientStats@stochastic@learner@boosted_trees@tensorflow@@ugradientstatsaccumulator@2345@@345@PEAV?$vector@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@v?$allocator@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@@std@@@8@@z)
105>categorical-feature-column-handler.obj : error LNK2019: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::CategoricalIdBinarySplit const & __cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::default_instance(void)"" (?default_instance@CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@saaebv1234@XZ) referenced in function ""public: virtual void cdecl tensorflow::boosted_trees::learner::stochastic::CategoricalFeatureColumnHandler::GenerateFeatureSplitCandidates(class tensorflow::boosted_trees::learner::LearnerConfig const &,class std::vector<int,class std::allocator > const &,class std::vector<struct tensorflow::boosted_trees::learner::stochastic::NodeStats,class std::allocator > const &,class tensorflow::boosted_trees::learner::FeatureStatsAccumulator<struct tensorflow::boosted_trees::learner::stochastic::GradientStats,struct tensorflow::boosted_trees::learner::stochastic::GradientStatsAccumulator> const &,class std::vector<struct tensorflow::boosted_trees::learner::stochastic::FeatureSplitCandidate,class std::allocator > *)const "" (?GenerateFeatureSplitCandidates@CategoricalFeatureColumnHandler@stochastic@learner@boosted_trees@tensorflow@@uebaxaebvlearnerconfig@345@AEBV?$vector@HV?$allocator@H@std@@@std@@aebv?$vector@UNodeStats@stochastic@learner@boosted_trees@tensorflow@@v?$allocator@UNodeStats@stochastic@learner@boosted_trees@tensorflow@@@std@@@8@AEBV?$FeatureStatsAccumulator@UGradientStats@stochastic@learner@boosted_trees@tensorflow@@ugradientstatsaccumulator@2345@@345@PEAV?$vector@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@v?$allocator@UFeatureSplitCandidate@stochastic@learner@boosted_trees@tensorflow@@@std@@@8@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::CategoricalIdBinarySplit const & cdecl tensorflow::boosted_trees::trees::CategoricalIdBinarySplit::default_instance(void)"" (?default_instance@CategoricalIdBinarySplit@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>categorical-feature-column-handler.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeRegularizationConfig_default_instance"" (?TreeRegularizationConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeRegularizationConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""public: cdecl tensorflow::boosted_trees::learner::stochastic::SplitStats::SplitStats(class tensorflow::boosted_trees::learner::LearnerConfig const &,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &,struct tensorflow::boosted_trees::learner::stochastic::NodeStats const &)"" (??0SplitStats@stochastic@learner@boosted_trees@tensorflow@@qeaa@AEBVLearnerConfig@234@AEBUNodeStats@1234@11@Z)
105>dense-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeRegularizationConfig_default_instance"" (?TreeRegularizationConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeRegularizationConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>sparse-quantized-feature-column-handler.obj : error LNK2001: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::learner::TreeRegularizationConfig_default_instance"" (?TreeRegularizationConfig_default_instance@learner@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VTreeRegularizationConfig@learner@boosted_trees@tensorflow@@@Internal@protobuf@google@@A)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::DenseFloatBinarySplit const & __cdecl tensorflow::boosted_trees::trees::DenseFloatBinarySplit::default_instance(void)"" (?default_instance@DenseFloatBinarySplit@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft const & __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultLeft::default_instance(void)"" (?default_instance@SparseFloatBinarySplitDefaultLeft@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight const & __cdecl tensorflow::boosted_trees::trees::SparseFloatBinarySplitDefaultRight::default_instance(void)"" (?default_instance@SparseFloatBinarySplitDefaultRight@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>decision_tree.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit::CategoricalIdSetMembershipBinarySplit(void)"" (??0CategoricalIdSetMembershipBinarySplit@trees@boosted_trees@tensorflow@@qeaa@XZ) referenced in function ""public: static class tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit * __cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VCategoricalIdSetMembershipBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavcategoricalidsetmembershipbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>decision_tree.obj : error LNK2001: unresolved external symbol ""public: static class tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit const & __cdecl tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit::default_instance(void)"" (?default_instance@CategoricalIdSetMembershipBinarySplit@trees@boosted_trees@tensorflow@@saaebv1234@XZ)
105>decision_tree.obj : error LNK2019: unresolved external symbol ""protected: __cdecl tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit::CategoricalIdSetMembershipBinarySplit(class google::protobuf::Arena *)"" (??0CategoricalIdSetMembershipBinarySplit@trees@boosted_trees@tensorflow@@iEAA@PEAVArena@protobuf@google@@@z) referenced in function ""public: static class tensorflow::boosted_trees::trees::CategoricalIdSetMembershipBinarySplit * cdecl google::protobuf::Arena::CreateMessage(class google::protobuf::Arena *)"" (??$CreateMessage@VCategoricalIdSetMembershipBinarySplit@trees@boosted_trees@tensorflow@@@arena@protobuf@google@@sapeavcategoricalidsetmembershipbinarysplit@trees@boosted_trees@tensorflow@@peav012@@z)
105>decision_tree.obj : error LNK2019: unresolved external symbol ""class google::protobuf::internal::ExplicitlyConstructed tensorflow::boosted_trees::trees::DenseFloatBinarySplit_default_instance"" (?DenseFloatBinarySplit_default_instance@trees@boosted_trees@tensorflow@@3v?$ExplicitlyConstructed@VDenseFloatBinarySplit@trees@boosted_trees@tensorflow@@@Internal@protobuf@google@@A) referenced in function ""public: static class std::vector<int,class std::allocator > __cdecl tensorflow::boosted_trees::trees::DecisionTree::GetChildren(class tensorflow::boosted_trees::trees::TreeNode const &)"" (?GetChildren@DecisionTree@trees@boosted_trees@tensorflow@@sa?AV?$vector@HV?$allocator@H@std@@@std@@aebvtreenode@234@@z)
105>E:\AIMLDL\TensorFlow\tensorflow_8_14\tensorflow\tensorflow\contrib\cmake\build\Release\tf_label_image_example.exe : fatal error LNK1120: 85 unresolved externals
========== Rebuild All: 104 succeeded, 1 failed, 0 skipped =========="
12307,Exception when shutting down TensorFlow after running some ops,"## Error

The following error appears when exiting the Python session after a TensorFlow op has been run in a session:
```
Exception TypeError: ""'NoneType' object is not callable"" in <bound method Context.__del__ of <tensorflow.python.eager.context.Context object at 0x7fcd7f98b450>> ignored
```

The Python session still exits cleanly (status 0).

## Repro

```
$ cat test.py
import sys
import tensorflow as tf
tf.Session().run(tf.square(0))
sys.stderr.write(""Done.\n"")
$ . tensorflow-nightly-20170814/bin/activate
$ python test.py 2>&1 | grep -Fv 'could speed up CPU computations'
Done.
$ . tensorflow-nightly-20170815/bin/activate
$ python test.py 2>&1 | grep -Fv 'could speed up CPU computations'
Done.
Exception TypeError: ""'NoneType' object is not callable"" in <bound method Context.__del__ of <tensorflow.python.eager.context.Context object at 0x7f9d8795c450>> ignored
```

Most ops seem to trigger this: I tried `tf.square`, `tf.add`, and `tf.summary.scalar` and `tf.summary.text`, and they all do. However, `tf.constant` does not.

## Introduction

Bug introduced between 20170814 and 20170815 nightlies. (In particular, [build 588] does not have the bug, while [build 589] does.) The changelog for these two builds is b93fd37e143bcdd6339f8e6081c948384a262e0b...1d33a59a9d554be863cfb06e9aa0ce1fa33ce9e6, which does include changes to eager.

[build 588]: http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/588/
[build 589]: http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/589/
"
12305,"Failed to load the native TensorFlow runtime: error when installing tensorflow. #10026 did not work, #10794 was abandoned","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Linux Ubuntu 16.04**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.2.1**:
- **Python version - have tried on both 2.7 and 3.6**: 
- **CUDA Version 8.0.61, Cudnn Verison 5.1**:
- **GeForce GTX 950M**:
- **import tensorflow as tf**:


### Describe the problem

I am trying to install GPU-supported version of tensorflow. Regardless of installation method or python version I use, I always get the message down below. Note that CPU version installs and runs without problem. 

I have tried uninstalling packages/anaconda/pip and then installing them through ATP, but nothing seems to help.

Thanks beforehand! 

### Source code / logs
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/kpk227133/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
12302,memory leak bug,"Hi, I have seen many issues reporting Tensorflow memory leak, so I wanted to report another sample of leak. 
I was testing the GA3C code and after a while (about a day) the process taking all of 32g of memory.
system spec is 
OS : Ubuntu 17.04
tensorflow version : 1.2.1 binary instalation
cuda version : 7.5
gpu : GeForce GTX 1080 SLI, 8gig of gpu memory

you can get the code from : 
https://github.com/babak-badnava/GA3C

with many thanks 
"
12301,graph transforms tool missing in windows,"i installed tensorflow_gpu-1.3.0rc2-cp36-cp36m-win_amd64.whl
i cannot seem to find the graph transforms tools: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms"
12298,"iOS: thread pools appear to spin forever, even after session close","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Built for iOS using provided scripts. Defined ops to be included in build as documented (using `ops_to_register.h`).
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Built on macOS 10.12.6 for distribution on iOS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: Unsure, sorry, my python environment is all messed up right now. My HEAD is at 0d2f6918322c7bf29d1de3075b0d4ed3b1b72919
- **Python version**: 2.7.12, but I believe that's irrelevant for this issue
- **Bazel version (if compiling from source)**: 0.5.1-homebrew, but I'm using `build_all_ios.sh` instead of Bazel to build
- **CUDA/cuDNN version**: Unknown
- **GPU model and memory**: iPhone 7 Plus, but AFAIK GPU is unavailable on iOS
- **Exact command to reproduce**: No particular command. Please see description of issue

Also [asked about this](https://stackoverflow.com/questions/45682837/tensorflow-thread-pools-appear-to-spin-forever-even-after-session-is-closed) on Stack Overflow, where it was suggested I file an issue

### Describe the problem
I believe this is a bug in Tensorflow. I am running Tensorflow on iOS, using the C++ API. I'm doing some image classification. I have a long-lived session, and I call `Run` many times on it, to evaluate different images from a backlog. Once I'm done, the `RunQueue`s (via `NonBlockingThreadPool`s) continue to pin the CPU at near max usage. They appear to be stuck in the `Steal` loop, presumably with no work to do.

### Source code / logs
I tried `Close()`ing and then `delete`ing the session, and having read some of the C++ source, this *should* have shut down the thread pools that belong to the session, but this didn't change the situation:
```objc++
auto status = session->Close();
delete session;
session = nil;
```
I tried setting some specific configuration options so I could be sure that the session did in fact own its thread pools instead of using a global thread pool, but this didn't help either:
```objc++
tensorflow::SessionOptions options;
options.config.clear_session_inter_op_thread_pool();
options.config.set_use_per_session_threads(true);
auto status = tensorflow::NewSession(options, &session);
```

One thing to note: while my understanding is that this isn't necessary, I did also try using a mutex to ensure that `Close` and `delete` would not be called concurrently with any call to `Run`, but again, no luck.

The only thing that *has* reduced CPU load to a reasonable level is to set `inter_op_parallelism_threads` to `1`. This doesn't resolve the underlying problem (that the threads are never cleaned up), but it *does* mean that the `Steal` loop is avoided, so the thread just blocks forever."
12295,Tutorial code on linear regression is missing a line,"Hello,
The tutorial code on the main website is missing a line that makes it crash when testing.
Here is the page of the tutorial: https://www.tensorflow.org/get_started/get_started
Here is the said code:
```
import numpy as np
import tensorflow as tf
# Declare list of features, we only have one real-valued feature
def model(features, labels, mode):
  # Build a linear model and predict values
  W = tf.get_variable(""W"", [1], dtype=tf.float64)
  b = tf.get_variable(""b"", [1], dtype=tf.float64)
  y = W*features['x'] + b
  # Loss sub-graph
  loss = tf.reduce_sum(tf.square(y - labels))
  # Training sub-graph
  global_step = tf.train.get_global_step()
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = tf.group(optimizer.minimize(loss),
                   tf.assign_add(global_step, 1))
  # ModelFnOps connects subgraphs we built to the
  # appropriate functionality.
  return tf.contrib.learn.ModelFnOps(
      mode=mode, predictions=y,
      loss=loss,
      train_op=train)

estimator = tf.contrib.learn.Estimator(model_fn=model)
# define our data sets
x_train = np.array([1., 2., 3., 4.])
y_train = np.array([0., -1., -2., -3.])
x_eval = np.array([2., 5., 8., 1.])
y_eval = np.array([-1.01, -4.1, -7, 0.])
input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_train}, y_train, 4, num_epochs=1000)

# train
estimator.fit(input_fn=input_fn, steps=1000)
# Here we evaluate how well our model did. 
train_loss = estimator.evaluate(input_fn=input_fn)
eval_loss = estimator.evaluate(input_fn=eval_input_fn)
print(""train loss: %r""% train_loss)
print(""eval loss: %r""% eval_loss)
```

The `eval_input_fn` is missing, just add this line:
```
eval_input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_eval}, y_eval, 4, num_epochs=1000)
```
after the `input_fn = tf.contrib.learn.io.numpy_input_fn({""x"": x_train}, y_train, 4, num_epochs=1000)` line.
Hope it'll help someone."
12294,Computation of Mean IoU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
Source.
- **TensorFlow version (use command below)**:
r1.2
- **Python version**: 
3.5

### Describe the problem

I believe that the calculation of the IoU metric in tf.metrics.mean_iou is incorrect. Consider an image which can have 3 classes (sky, road, building). In one image there is no building and everything has been perfectly classified as either sky or road so the tp + fn + fp for buildings is zero. To avoid divisions by zero in the IoU calculation (tp / (tp + fn + fp)) the current implementation sets the denominator to 1. This causes a result of 0 because tp=0 and we are computing 0=0/1. 

Both the sky and road class get a score of 1 so the overall image gets a mean_iou score of 2/3 even though every single pixel was correctly classified. I propose to change the metric so that when the denominator is 0 the result is 1. I can send a pull request if this is needed. 

### Source code / logs

Complete example as a unit test:

    class TestMetrics(tf.test.TestCase):

        def get_data(self):
            label_int64 = tf.constant([
                [[0, 1, 2, 2, 1, 0],
                 [2, 1, 0, 0, 1, 2]],
                [[2, 2, 2, 2, 2, 2],
                 [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)

            predicted_perfect_int64 = tf.constant([
                [[1, 1, 2, 2, 1, 2],
                 [2, 1, 9, 3, 1, 2]],
                [[2, 2, 2, 2, 2, 2],
                 [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)
            return label_int64, predicted_perfect_int64


        def test_iou_metric(self):
            with self.test_session():
                l, p = self.get_data()
                num_classes = 9
                weights = tf.cast(tf.not_equal(l, 0), tf.int32)

                iou, update_op = tf.metrics.mean_iou(
                    l - 1, p - 1, num_classes, weights=weights)
                tf.global_variables_initializer().run()
                tf.local_variables_initializer().run()
                update_op.eval()
                self.assertAlmostEqual(iou.eval(), 1)

Currently this test fails with: 0.222 is not equal to 1.
"
12293,tf.nn.avg_pool NaN bug with pool size 7 and stride 1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**: Python 3.6.1 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0 / libcudnn.so.5.1.10
- **GPU model and memory**: titan x (pascal) 12gb
- **Exact command to reproduce**:



### Describe the problem
When performing tf.nn.avg_pool over a large binary image, containing a few small blobs (diameter ~ 100px), I get NaN errors and all-zero outputs depending on stride and poolsize option. I've added a test script below, which results in the following output on my setup. The results for strides 1 and pool >= 7 are incorrect:

```
strides (1, 1, 1, 1)
Pool: 5 len where: 7955
min: 0.0 max: 1.0
Pool: 6 len where: 7997
min: 0.0 max: 1.0
Pool: 7 len where: 0
min: 0.0 max: 0.0
Pool: 8 len where: 0
min: 0.0 max: 0.0
Pool: 9 len where: 49939008
min: nan max: nan
Pool: 10 len where: 33141389
min: nan max: nan

strides (1, 2, 2, 1)
Pool: 5 len where: 12045
min: 0.0 max: 1.0
Pool: 6 len where: 12584
min: 0.0 max: 1.0
Pool: 7 len where: 13093
min: 0.0 max: 1.0
Pool: 8 len where: 13608
min: 0.0 max: 1.0
Pool: 9 len where: 10588
min: 0.0 max: 1.0
Pool: 10 len where: 2707
min: 0.0 max: 1.0
```

### Source code / logs
```python
import numpy as np
import tensorflow as tf

maps = np.load('/tmp/test.npy')
in_shape = (9, 4096, 4096, 2)
padding = 'VALID'
sess = tf.Session()
input = tf.placeholder('float32', in_shape)

for strides in [(1, 1, 1, 1), (1, 2, 2, 1)]:
    print('strides', strides)
    for pool in range(5, 11):
        pool_size = (1, pool, pool, 1)
        x = tf.nn.avg_pool(input, pool_size, strides, padding=padding)
        pooled_maps = sess.run(x, {input: maps.astype('float32')})
        print('Pool:', pool, 'len where:', len(np.where(pooled_maps[..., 1] != 0)[0]))
        print('min:', pooled_maps[..., 1].min(), 'max:', pooled_maps[..., 1].max())
```

maps is too large to upload for me, but can be replaced with `maps = np.random.binomial(1, 0.0000001, in_shape)` to reproduce the NaNs for pool >= 8 


```

== cat /etc/issue ===============================================
Linux DTA-160200 4.4.0-91-generic #114-Ubuntu SMP Tue Aug 8 11:56:56 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux DTA-160200 4.4.0-91-generic #114-Ubuntu SMP Tue Aug 8 11:56:56 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
numpydoc (0.6.0)
protobuf (3.3.0)
tensorflow-gpu (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /home/basv/.local/lib/cuda:/usr/local/cuda-8.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Aug 15 11:50:59 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |
|  0%   44C    P0    58W / 250W |      0MiB / 12189MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
```"
12291,"Tensorflow Bug (load avg rises very high, cannot kill process, system freeze)","The following script run.sh will increase load avg to several 100 (it takes usually 100 iterations)

run.sh
```
#!/bin/bash
var=0
while :
do
        echo ""round --->"" $((var++))
        python testTF.py
        sleep 3
done
```

testTF.py
```
#!/usr/bin/python
# -*- coding: utf-8 -*-

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'


import tensorflow as tf
print(""START SESSION..."")
session = tf.Session()
print(""...SESSION DONE"")

```

The error started to appear in Ubuntu 14.04 (where it freezes the workstation).
Updating to Ubuntu 16.04 makes the workstation more responsible, but a reboot is still necessary.
observations:
* I can not kill the PID of the python process when the load avg rises
* changing to python3 did not resolve the issue
* compiling tf (1.3.0) from source did not resolve the issue

== cat /etc/issue ===============================================
Linux bipower6 4.4.0-89-generic #112-Ubuntu SMP Mon Jul 31 19:38:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux bipower6 4.4.0-89-generic #112-Ubuntu SMP Mon Jul 31 19:38:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow-gpu (1.2.1)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)


== env ==========================================================
LD_LIBRARY_PATH /home/a/torch/install/lib:/home/a/torch/install/lib::/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/:/usr/local/lib/:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/:/usr/local/lib/
DYLD_LIBRARY_PATH /home/a/torch/install/lib:/home/a/torch/install/lib:

== nvidia-smi ===================================================
Tue Aug 15 11:25:32 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 0000:02:00.0     Off |                  N/A |
| 29%   44C    P0    41W / 180W |      0MiB /  8114MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 0000:04:00.0     Off |                  N/A |
|  0%   52C    P0    56W / 250W |      0MiB / 12189MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P40           Off  | 0000:83:00.0     Off |                    0 |
| N/A   69C    P0   128W / 250W |  21897MiB / 22912MiB |     93%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    2     89766    C   /home/a/dl/bin/python                        21895MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
(END)


"
12290,Environment Variable KERB_TICKET_CACHE_PATH is unnecessary,"https://github.com/tensorflow/tensorflow/commit/83559ee1d8cce32dd80cb017cd8bb76880790b23 have provided a commit to add kerberos ticket cache file.

In `com.sun.security.auth.module.Krb5LoginModule#initialize(),` it default set ticketCacheName from ticketCache of options, ticketCache was set in `org.apache.hadoop.security.HadoopConfiguration` by `System.getenv(""KRB5CCNAME"");`.

So, according to https://www.tensorflow.org/deploy/hadoop, set ticket cache by **KRB5CCNAME** instead of **KERB_TICKET_CACHE_PATH**, the program still working properly.

Therefore, https://github.com/tensorflow/tensorflow/commit/83559ee1d8cce32dd80cb017cd8bb76880790b23 is unnecessary, just remind users set **KRB5CCNAME** in https://www.tensorflow.org/deploy/hadoop"
12289,"saver.restore(sess,modelpath) is able to normal run in tensorflow1.0.1,but have a problem in tensorflow 1.2.1","### System information
- **OS Platform and Distribution**:Ubuntu 14.04
- **TensorFlow installed from**binary
- **TensorFlow version**:Tensorflow1.0.1 and Tensorflow1.2.1
- **Python version**: Python 2.6
- **GPU model and memory**:1080 Ti

### Describe the problem
I meet this problem at following. My predict.py is able to normal run in Tensorflow 1.0.1, but have shown this proble in Tensorflow 1.2.1. I don't resovle this problem. I need help. Please!

### Source code / logs
Source codeï¼š
restore_dict = {}
    for i in variables[:]:  # the first is global step
        restore_dict[i.name.replace(':0', '')] = i
        print('restore variable: ', i.name.replace(':0', ''))
saver = tf.train.Saver(restore_dict) 
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
saver.restore(sess, 'news_tf_model/model.ckpt') #the path is real
predict = sess.run(pred, feed_dict={x: imgs})
sess.close()



logsï¼š
2017-08-15 16:10:46.657837: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
2017-08-15 16:10:46.658229: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.658870: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.659232: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/bias not found in checkpoint
2017-08-15 16:10:46.660554: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.684510: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.742777: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.742852: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.743000: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.743241: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.825494: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.826179: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.826199: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.826309: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.826419: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.826988: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.833507: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
2017-08-15 16:10:46.843310: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
Traceback (most recent call last):
  File ""demo.py"", line 25, in <module>
    result_dict = news_demo.newsAggreg({image_path})
  File ""/home/rszj/liutao/news_aggreg/news_demo.py"", line 32, in newsAggreg
    predict = news_predict.run(images_path)
  File ""/home/rszj/liutao/news_aggreg/news_predict.py"", line 196, in run
    saver.restore(sess, module_file)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
	 [[Node: save/RestoreV2/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_112_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'save/RestoreV2_1', defined at:
  File ""demo.py"", line 25, in <module>
    result_dict = news_demo.newsAggreg({image_path})
  File ""/home/rszj/liutao/news_aggreg/news_demo.py"", line 32, in newsAggreg
    predict = news_predict.run(images_path)
  File ""/home/rszj/liutao/news_aggreg/news_predict.py"", line 176, in run
    saver = tf.train.Saver(restore_dict)  # when you want to save model
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1139, in __init__
    self.build()
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/rszj/liutao/virtualenv/liutao_py2/mpy2tf1.2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key LSTM/basic_lstm_cell/kernel not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
	 [[Node: save/RestoreV2/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_112_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
"
12288,Build error for finding some generated header files,"I just build the master branch on windows based-on the CMake method, as shown in link: https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake

I first build the PIP package, as described in the above link. But there is build error says that:

C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/utils/dropout_utils.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\dropout_utils.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  sparse_column_iterable.cc
  tensor_utils.cc
  example_partitioner.cc
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\common\partitioners\example_partitioner.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  bias-feature-column-handler.cc
  categorical-feature-column-handler.cc
  dense-quantized-feature-column-handler.cc
  sparse-quantized-feature-column-handler.cc
  multiple_additive_trees.cc
  decision_tree.cc
  masked_matmul_ops.cc
  wals_solver_ops.cc
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\trees\decision_tree.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\sparse-quantized-feature-column-handler.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  factorization_ops.cc
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\categorical-feature-column-handler.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  generate_vocab_remapping_op.cc
  load_and_remap_matrix_op.cc
  zero_initializer_op.cc
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/models/multiple_additive_trees.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\models\multiple_additive_trees.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\bias-feature-column-handler.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
C:\v-clhian\tensorflow\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\v-clhian\tensorflow\tensorflow\contrib\boosted_trees\lib\learner\stochastic\handlers\dense-quantized-feature-column-handler.cc) [C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  checkpoint_ops.cc
  sparse_feature_cross_kernel.cc
  sparse_feature_cross_op.cc
  resampler_ops.cc
  tensor_forest_ops.cc
  reinterpret_string_to_float_op.cc
  scatter_add_ndim_op.cc
  tree_utils.cc
  hard_routing_function_op.cc
  k_feature_gradient_op.cc
  k_feature_routing_function_op.cc
  routing_function_op.cc
  routing_gradient_op.cc
  stochastic_hard_routing_function_op.cc
  stochastic_hard_routing_gradient_op.cc
  unpack_path_op.cc
  utils.cc
  skip_gram_kernels.cc
  skip_gram_ops.cc
  cross_replica_ops.cc
  infeed_ops.cc
  outfeed_ops.cc
  replication_ops.cc
  tpu_configuration_ops.cc
Done Building Project ""C:\v-clhian\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj"" (default targets) -- FAILED.

It cannot find the generated header files: 
learner.pb.h and tree_config.pb.h
But I find them on my disk, under folder C:\xxx\tensorflow\tensorflow\contrib\cmake\build\tensorflow\contrib\boosted_trees\proto, illustrating that it already generate the files.

So is there any configuration for this?


"
12287,Loss stays a constant 0.6931 while training ,"i just add the out maps to a  big value



= >   step: 0 loss: 0.6773 accuracy:0.61
= >   step: 1 loss: 2.4587 accuracy:0.66
= >   step: 2 loss: 5.3353 accuracy:0.32
= >   step: 3 loss: 1.0234 accuracy:0.52
= >   step: 4 loss: 10.6901 accuracy:0.67
= >   step: 5 loss: 1.9899 accuracy:0.62
= >   step: 6 loss: 3.5713 accuracy:0.63
= >   step: 7 loss: 0.6869 accuracy:0.64
= >   step: 8 loss: 0.6931 accuracy:0.65
= >   step: 9 loss: 0.6931 accuracy:0.60
= >  = = = = = =  > > > > > >    step: 9 loss: 0.6931 accuracy:0.66
= >   step: 10 loss: 0.6931 accuracy:0.70
= >   step: 11 loss: 0.6931 accuracy:0.58
= >   step: 12 loss: 0.6931 accuracy:0.72
= >   step: 13 loss: 0.6931 accuracy:0.81
= >   step: 14 loss: 0.6931 accuracy:0.67
= >   step: 15 loss: 0.6931 accuracy:0.68
= >   step: 16 loss: 0.6931 accuracy:0.72
= >   step: 17 loss: 0.6931 accuracy:0.72
= >   step: 18 loss: 0.6931 accuracy:0.71
= >   step: 19 loss: 0.6931 accuracy:0.72
= >  = = = = = =  > > > > > >    step: 19 loss: 0.6931 accuracy:0.66
= >   step: 20 loss: 0.6931 accuracy:0.60
= >   step: 21 loss: 0.6931 accuracy:0.66
= >   step: 22 loss: 0.6931 accuracy:0.70
= >   step: 23 loss: 0.6931 accuracy:0.73
= >   step: 24 loss: 0.6931 accuracy:0.69
= >   step: 25 loss: 0.6931 accuracy:0.59
= >   step: 26 loss: 0.6931 accuracy:0.69
= >   step: 27 loss: 0.6931 accuracy:0.76
= >   step: 28 loss: 0.6931 accuracy:0.72
= >   step: 29 loss: 0.6931 accuracy:0.67
= >  = = = = = =  > > > > > >    step: 29 loss: 0.6931 accuracy:0.67
= >   step: 30 loss: 0.6931 accuracy:0.66
= >   step: 31 loss: 0.6931 accuracy:0.74
= >   step: 32 loss: 0.6931 accuracy:0.75
= >   step: 33 loss: 0.6931 accuracy:0.80
= >   step: 34 loss: 0.6931 accuracy:0.57
.........................












"
12286,android detection,"I use the android demo, but in my mobile, I can't get the bounding box.I don't know what result it.
"
12285,build_all_ios.sh fail with Errorï¼š redefinition of 'NoBarrier_CompareAndSwap',"```
./google/protobuf/stubs/atomicops_internals_atomicword_compat.h:53:19: error: redefinition of
      'NoBarrier_CompareAndSwap'
inline AtomicWord NoBarrier_CompareAndSwap(volatile AtomicWord* ptr,
                  ^
./google/protobuf/stubs/atomicops_internals_x86_gcc.h:55:17: note: previous definition is here
inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr
```
"
12284,Cudnn `params_to_canonical` failed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0-rc2
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.52
- **CUDA/cuDNN version**: 8.0/6.0.21
- **GPU model and memory**:  Tesla K80/11.17GiB
- **Exact command to reproduce**:

```
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))

# Works 
cell = cudnn_rnn_ops.CudnnGRU(1, 5, 5, input_mode=""linear_input"")
sess.run(cell.params_to_canonical(tf.zeros([cell.params_size()])))

# aborts with ""Check failed""
cell = cudnn_rnn_ops.CudnnGRU(1, 5, 5, input_mode=""skip_input"")
sess.run(cell.params_to_canonical(tf.zeros([cell.params_size()])))
```
### Describe the problem
If `input_mode` is ""skip_input"", `params_to_canonical` fails with a ""Check failed"" error for at least CudnnGRU

### Logs
2017-08-15 03:05:45.807704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-15 03:05:45.808201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-08-15 03:05:45.808229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-08-15 03:05:45.808241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-08-15 03:05:45.808261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
2017-08-15 03:05:46.543863: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:627] Check failed: size == width * height Params size mismatch. Expected 25, got 0
Aborted (core dumped)
"
